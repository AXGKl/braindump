+++
title = "Synaptic Current Model"
author = ["Jethro Kuan"]
lastmod = 2020-03-07T23:43:10+08:00
draft = false
+++

Synaptic currents are generated by synaptic currents triggered by
arrival of presynaptic spikes \\(S\_{j}^{(l)}(t)\\). Spike trains
\\(S\_{j}^{(l)}(t)\\) are denoted as a sum of Dirac delta functions
\\(S\_{j}^{(l)}(t)=\sum\_{s \in C\_{j}^{(l)}} \delta(t-s)\\), where \\(s\\) runs
over the firing times \\(C\_j^{(l)}\\) of neuron \\(j\\) in layer \\(l\\).

A good first-order approximation of the synaptic current is one of
exponential decay. Synaptic currents are also assumed to sum linearly.

\begin{equation} \label{eq:scm}
  \frac{\mathrm{d} I\_{i}^{(l)}}{\mathrm{d} t}=-\underbrace{\frac{I\_{i}^{(l)}(t)}{\tau\_{\mathrm{syn}}}}\_{\mathrm{exp} . \text { decay }}+\underbrace{\sum\_{j} W\_{i j}^{(l)} S\_{j}^{(l-1)}(t)}\_{\text {feed-forward }}+\underbrace{\sum\_{j} V\_{i j}^{(l)} S\_{j}^{(l)}(t)}\_{\text {recurrent }}
\end{equation}

A single LIF neuron can be simulated with 2 linear differential
equations whose initial conditions change instantaneously when a spike
occurs. Combining the reset term with the equation for the
[§leaky\_integrate\_and\_fire]({{< relref "leaky_integrate_and_fire" >}}) model, we get:

\begin{equation} \label{eq:lif\_with\_reset}
  \frac{\mathrm{d} U\_{i}^{(l)}}{\mathrm{d} t}=-\frac{1}{\tau\_{\mathrm{mem}}}\left(\left(U\_{i}^{(l)}-U\_{\mathrm{rest}}\right)+R I\_{i}^{(l)}\right)+S\_{i}^{(l)}(t)\left(U\_{\mathrm{rest}}-\vartheta\right)
\end{equation}

The solutions to Equations [eq:scm](#eq:scm) and [eq:lif_with_reset](#eq:lif_with_reset) are
approximated numerically by discretizing time, and expressing the
output spike-train \\(S\_{i}^{(l)}(n)\\) of neuron \\(i\\) in layer \\(l\\) at
time-step \\(n\\) as a non-linear function of the membrane voltage
\\(S\_i^{(l)}(n) \equiv \Theta(U\_i^{(l)(n)}  - \theta)\\) where \\(\theta\\) is
the Heaviside step function, and \\(\theta\\) is the firing threshold.

Setting \\(U\_{\text{rest}} = 0\\), \\(R=1\\), \\(\theta=1\\), and using some small
simulation time step \\(\delta t > 0\\), we get:

\begin{equation}
I\_{i}^{(l)}[n+1]=\alpha I\_{i}^{(l)}[n]+\sum\_{j} W\_{i j}^{(l)} S\_{j}^{(l)}[n]+\sum\_{j} V\_{i j}^{(l)} S\_{j}^{(l)}[n]
\end{equation}

with decay strength \\(\alpha \equiv \exp
\left(-\frac{\Delta\_{t}}{\tau\_{\mathrm{syn}}}\right)\\). Equation
[eq:lif_with_reset](#eq:lif_with_reset) can then be expressed as:

\begin{equation}
  U\_{i}^{(l)}[n+1]=\beta U\_{i}^{(l)}[n]+I\_{i}^{(l)}[n]-S\_{i}^{(l)}[n]
\end{equation}

with \\(\beta \equiv \exp
\left(-\frac{\Delta\_{t}}{\tau\_{\operatorname{mem}}}\right)\\). These two
equations characterise the dynamics of a RNN. Specifically, the state
of neuron \\(i\\) is given by the instantaneous synaptic currents \\(I\_i\\)
and the membrane voltage \\(U\_i\\).


## References {#references}

-   <a id="6c46e273de1ecbecce7f8f1ac7329a57" href="#neftci19_surrog_gradien_learn_spikin_neural_networ">(Neftci et al., 2019)</a>

# Bibliography
<a id="neftci19_surrog_gradien_learn_spikin_neural_networ" target="_blank">Neftci, E. O., Mostafa, H., & Zenke, F., *Surrogate gradient learning in spiking neural networks*, CoRR, *()*,  (2019). </a> [↩](#6c46e273de1ecbecce7f8f1ac7329a57)
