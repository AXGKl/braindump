+++
title = "Self-supervised Learning"
author = ["Jethro Kuan"]
lastmod = 2020-07-08T14:54:24+08:00
slug = "self_supervised_learning"
draft = false
+++

### Backlinks {#backlinks}

- [zhu\_ev-flownet\_2018: EV-FlowNet: self-supervised optical flow estimation for event-based cameras]({{< relref "zhu_ev_flownet_2018" >}})
- [Contrastive Methods]({{< relref "contrastive_methods" >}})

[Prediction is the Essence of Intelligence]({{< relref "prediction_is_the_essence_of_intelligence" >}})

## Self-supervised Learning vs Other Learning Paradigms {#self-supervised-learning-vs-other-learning-paradigms}

In self-supervised learning, the machine predicts any part of its input for any observed part. For example, it may predict future frames in videos. This results in a lot of feedback.

In [Reinforcement Learning ‚≠ê]({{< relref "reinforcement_learning" >}}), the machine predicts a scalar reward given weak feedback once in a while. Since there is very little feedback, it seems impossible to learn any complex representations in a short amount of time.

See [LeCun's Cake Analogy]({{< relref "lecun_cake_analogy" >}}).
