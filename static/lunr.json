[
    {
        "uri": "/books/are_we_smart_enough_to_know_how_smart_animals_are",
        "title": "Are We Smart Enough to Know How Smart Animals Are?",
        "content": "\ntitle\n: Are We Smart Enough to Know How Smart Animals Are?\n\nauthor\n: Frans de Waal\n\npersonal rating\n: 4/5\n\ntags\n: §books, §neuroscience\n\nWhy is it that facial expression is universally consistent?\n\nLanguage As Root Of Thought {#language-as-root-of-thought}\n\nIf language were at the root of thought, why is it that we sometimes\nfind ourselves at a loss for words?\n\nLanguage is one of the distinguishing features of the human species.\nOther species are able to exhibit similar or higher levels of\ncognitive function without an equally powerful language.\n\nAs humans, we are able to use language to communicate ideas that\ntranscend time, and this has been essential for our survival.\n\nThe Social Brain Hypothesis {#the-social-brain-hypothesis}\n\nThe intelligence required to effectively deal with social networks may\nexplain why the primate order underwent its remarkable brain\nexpansion.\n\nConsciousness {#consciousness}\n\nThere is claim that humans are the only species that consciously\naccess the past and the future, but recent evidence shows otherwise.\nEither animals also posses consciousness, or consciousness is not a\nnecessary ingredient for such introspection.\n\nNames {#names}\n\nSome species like dolphins use signature whistles, which is\nanalogous to human names.\n\nStudying cognition {#studying-cognition}\n\nNeuroscience may tell us where things happen in the brain, but does\nnot help us formulate new theories or design insightful tests of\ncognition.\n\nInstead of making humanity the measure of most things, we need to\nevaluate other species for what they are.\n",
        "tags": []
    },
    {
        "uri": "/books/the_art_of_doing_science_and_engineering",
        "title": "The Art Of Doing Science And Engineering",
        "content": "\ntitle\n: The Art Of Doing Science And Engineering\n\nauthor\n: Richard Hamming\n\nlinks\n: Goodreads\n\npersonal rating\n: 4/5\n\ntags\n: §books\n\nSynopsis {#synopsis}\n\nThe goal of this book is to teach the art of thinking about science\nand engineering. Hamming argues that this art is not taught in the\nschool curriculum, but is crucial to being a successful practitioner.\n\nNotes {#notes}\n\nHamming recommends doing quick modelling to verify the truth of\n    various claims, for example using plausible models of the human\n    population to relate the plausibility of 2 independent but\n    correlated claims together.\nOne should take their own opinions and try to first express them\n    clearly, and then examine them with counter arguments, back and\n    forth, until they are clear as to what they believe and why they\n    believe it.\nThe book was slightly painful to read on the Kindle. And a lot of\n    the content is covered in MacKay's fantastic information theory\n    book, which makes the mathematics less tedious.\nHamming distinguishes creativity and originality: there is a\n    implicit desire for value in creativity. How does one boost\n    creativity? Hamming doesn't know, but he identifies several traits:\n    false starts and solutions sharpen the next try\n    temporary abandonment is a common phase\n    \"if I had a solution, what would it look like?\" is a good question\n        to ask\nHamming's advice for \"experts\":\n    Don't automatically reject every crazy idea, but also don't pursue\n        all of them\n    Be conscious of your choices, and be wary of experts\nHamming on unreliable data:\n    Hamming's rule: 90% of the time the next independent measurement\n        will fall outside the previous 90% confidence limits\n    You get what you measure: the way you choose to measure things\n        controls to a large extent what happens. This may result in bias.\nHamming on research:\n    He oft repeats this quote from Pasteur: \"Luck favors the prepared mind\".\n    It is important to believe you can go on to do great things.\n    Have drive, and desire excellence. Intellectual interest is a\n        compound interest.\n    Take time on a regular basis to ask the larger questions.\n    Great people can tolerate ambiguity, both believe and disbelieve\n        at the same time.\n    3 essential skills (selling the idea through clear presentation):\n        Giving formal presentations\n        Producing written reports\n        Giving informal presentations as they occur\n\nNon-essentials {#non-essentials}\n\nErrors in continuous signals are amplified, but in discrete\n    signalling using repeaters rather than amplifiers, noise introduced\n    is removed if the repeater is able to correct for it. Do we have an\n    internal error-correction mechanism when sending discrete binary\n    signals across neurons?\n",
        "tags": []
    },
    {
        "uri": "/books/the_art_of_unix_programming",
        "title": "The Art Of Unix Programming",
        "content": "\ntitle\n: The Art Of Unix Programming\n\nauthor\n: Ken Thompson\n\npersonal rating\n: 5/5\n\ntags\n: §books\n\nIn The Art of Unix Programming, Ken Thompson was quoted to have\nprovided the following design rules:\n\nBuild modular programs\nWrite readable programs\nUse composition\nSeparate mechanisms from policy\nWrite simple programs\nWrite small programs\nWrite transparent programs\nWrite robust programs\nMake data complicated when required, not the program\nBuild on potential users’ expected knowledge\nAvoid unnecessary output\nWrite programs which fail in a way easy to diagnose\nValue developer time over machine time\nWrite abstract programs that generate code instead of writing code by hand\nPrototype software before polishing it\nWrite flexible and open programs\nMake the program and protocols extensible\n\nParticularly fascinating was the point on separating mechanisms from\npolicy. The argument is that mechanisms don’t evolve as quickly as\npolicies, hence coupling the two would make it difficult to improve\nupon the software without breaking the mechanism. Eric gave the\nexample of X, and the survival of X was attributed to its mechanisms\n(the raster operations) being separated from the GUI implementations,\nwhich had been phased out multiple times.\n",
        "tags": []
    },
    {
        "uri": "/books/work_clean",
        "title": "Work Clean",
        "content": "\ntitle\n: Work Clean\n\nauthor\n: Dan Charnas\n\ntags\n: Productivity\n\nPreparation {#preparation}\n\nAlways turn up early, greet the day\nCalculate Meeze point\n    Optimal number of Actions done per day\n\nProcess {#process}\n\nOptimize the workplace:\n    Put pens etc near dominant hand, not so important things within\n        arms reach. This is to prevent hands from crossing over, reducing friction.\n    Think about all the pain points, and think about how to automate\n        them.\n    Make checklists for recurring tasks. Checklists are immutable,\n        unlike task lists.\nDifference between immersive time and process time\n    The first minutes matter more than the last\n    process time sometimes has more value\n    Assign a block of 30min in the morning for process time, then\n        alternate between immersive and process time\nClean as you go\n    Remove distractions from workplace\n    Close emails etc as you go\n    Email mark and sweep process:\n        Look through once, flagging and marking all as read\n        Revisit flagged ones, take action if any\n        Unflag when done\nSlow down to move faster\n    Deliberately slow down to access quality velocity\n    Maintain some forward momentum\nAim to finish/keep the finish in mind\n    Do effort estimates, and try to keep tasks small, so that they're\n        completable in shorter bursts\n    Clock in time, take deliberate breaks in between, weigh the value\n        of a pause in between work\n",
        "tags": []
    },
    {
        "uri": "/papers/andrychowicz2017_hindsight_experience_replay",
        "title": "Hindsight Experience Replay",
        "content": "\npaper\n:\n\nHindsight Experience Replay {#hindsight-experience-replay}\n\nKey Challenges {#key-challenges}\n\nIt is challenging for agents to learn in environments where the\nrewards are sparse. It is desirable to design algorithms where manual\nreward engineering is not required.\n\nKey Insight {#key-insight}\n\nConsider a state sequence \\\\(s\\1, \\dots, s\\T\\\\), and a goal \\\\(g \\ne s\\_1,\n\\dots s\\_T\\\\). We may re-examine this trajectory with a different goal --\nwhile this trajectory may not help us learn how to achieve state \\\\(g\\\\),\nit tells us how to achieve the state \\\\(s\\_T\\\\). This information can be\nharvested by using an off-policy algorithm and experience replay where\n\\\\(g\\\\) in the replay buffer is replaced with \\\\(s\\_T\\\\). This trajectory can\nalso be replayed with goal \\\\(g\\\\) intact.\n\nThis modification results in at least half of the replayed\ntrajectories containing meaningful rewards, and makes learning\npossible. Using Universal Value Function Approximators, which are\npolicies and value functions that take as input state \\\\(s \\in S\\\\) and\ngoal \\\\(g \\in G\\\\).\n\nHER as Implicit Curriculum {#her-as-implicit-curriculum}\n\nGoals used for replay naturally shift from simple to achieve goals\nachievable by a random agent, to more difficult ones. HER has no\nexplicit control over the distribution of initial environment states.\n",
        "tags": []
    },
    {
        "uri": "/papers/comsa2019_temp_coding",
        "title": "Temp Coding with Alpha Synaptic Function",
        "content": "\nTemporal Coding with Alpha Synaptic Function (Comsa et al., 2019) {#temporal-coding-with-alpha-synaptic-function}\n\nMotivation {#motivation}\n\nAtemporal networks (think LSTMs) don't have the benefits of\n    encoding information directly in the temporal domain\n    They remain sequential (require all previous layers of\n        computation to produce answer)\n    Information in the real world are typically temporal\n\nKey Ideas {#key-ideas}\n\nTemporal Coding: Information is encoded in the relative timing of\n    neuron spikes. Using temporal coding allows shift of differentiable\n    relationship into the temporal domain.\n    Find differentiable relationship of the time of postsynaptic\n        spike with respect to the weights and times of the presynaptic\n        spikes.\n\nAlpha synaptic transfer function: Use the SRM, but with the\n    exponential decay of form \\\\(t e^{-t}\\\\).\n\nSynchronization pulses: input-independent spikes, used to\n    facilitate transformations of the class boundaries.\n\nThe Coding Scheme {#the-coding-scheme}\n\nMore salient information about a feature is encode as an earlier\nspike in the corresponding input neuron (think time-to-first-spike).\n\nIn a classification problem with \\\\(m\\\\) inputs and \\\\(n\\\\) possible classes:\n\ninput\n: spike times of \\\\(m\\\\) input neurons\n\noutput\n: index of output neuron that fires first (among the \\\\(n\\\\)\n    output neurons)\n\nAlpha Synaptic Function {#alpha-synaptic-function}\n\nIncoming exponential synaptic kernels are of the form \\\\(\\epsilon(t) =\n\\tau^{-1}e^{-\\tau t}\\\\) for some decay constant \\\\(\\tau\\\\). Potential of\nmembrane in response to the spike is then \\\\(u(t) = t e^{-\\tau t}\\\\). It\nhas a gradual rise, and slow decay.\n\n{{}}\n\nModelling Membrane Potential {#modelling-membrane-potential}\n\nThe membrane potential is a weighted sum of the presynaptic inputs:\n\n\\begin{equation}\n  V\\{mem}(t) = \\sum\\{i} w\\i (t-t\\i)e^{\\tau(t\\_i - t)}\n\\end{equation}\n\nWe can compute the spike time \\\\(t\\_{out}\\\\) of a neuron by considering the\nminimal subset of presynaptic inputs \\\\(I\\{t\\{out}}\\\\) with \\\\(t\\_i \\le\nt\\_{out}\\\\) such that:\n\n\\begin{equation} \\label{eqn:threshold}\n  \\sum\\{i \\in {I\\{t\\{out}}}} w\\i \\left( t\\{out} - t\\{i} \\right)\n  e^{\\tau (t\\i - t\\{out})} = \\theta\n\\end{equation}\n\n has 2 solutions: 1 on rising part of function and\nanother on decaying part. The spike time is the earlier solution.\n\nSolving for the Equation  {#solving-for-the-equation}\n\nLet \\\\(A\\{I} = \\sum\\{i \\in I} w\\i e^{\\tau t\\i}\\\\), and \\\\(B\\{I} = \\sum\\{i\n\\in I} w\\i e^{\\tau t\\i} t\\_i\\\\), we can compute:\n\n\\begin{equation}\n  t\\{out} = \\frac{B\\I}{A\\_I} - \\frac{1}{\\tau}W\\left( -\\tau\n  \\frac{\\theta}{A\\I}e^{\\tau \\frac{B\\I}{A\\_I}} \\right)\n\\end{equation}\n\nwhere \\\\(W\\\\) is the Lambert W function.\n\nThe Loss Function {#the-loss-function}\n\nThe loss minimizes the spike time of the target neuron, and maximizes\nthe spike time of non-target neurons (cross-entropy!)\n\nSoftmax on the negative values of the spike times \\\\(o\\_{i}\\\\) (which\nare always positive):\n\n\\begin{equation}\n  p\\j = \\frac{e^{- o\\j}}{\\sum\\{i=1}^{n} e^{- o\\i}}\n\\end{equation}\n\nThe cross entropy loss \\\\(L(y\\i, p\\i) = - \\sum\\{i=1}^{n} y\\i \\ln p\\_i\\\\) is\nused.\n\nChanging the weights of the network alters the spike times. We can\ncompute the exact derivative of the post synaptic spike time wrt any\npresynaptic spike time \\\\(t\\j\\\\) and its weight \\\\(w\\j\\\\) as:\n\n\\begin{equation}\n  \\frac{\\partial t\\{out}}{\\partial t\\j} = \\frac{w\\j e^{t\\j} \\left( t\\_j\n      \\frac{B\\I}{A\\I} + W\\I + 1\\right)}{A\\I (1 + W\\_I)}\n\\end{equation}\n\n\\begin{equation}\n  \\frac{\\partial t\\{out}}{\\partial w\\j} = \\frac{e^{t\\j} \\left( t\\j\n      \\frac{B\\I}{A\\I} + W\\I + 1\\right)}{A\\I (1 + W\\_I)}\n\\end{equation}\n\nwhere\n\n\\begin{equation}\n  W\\I = W\\left( -\\frac{\\theta}{A\\I}e^{\\frac{B\\I}{A\\I}} \\right)\n\\end{equation}\n\nSynchronization Pulses {#synchronization-pulses}\n\nThese act as a temporal form of bias, adjusting class boundaries in\nthe temporal domain. Per network, or per layer biases are added. Spike\ntimes for each pulse are learned with the rest of the parameters of\nthe network.\n\nHyperparameters {#hyperparameters}\n\n{{}}\n\nExperiments {#experiments}\n\nBoolean Logic Problems {#boolean-logic-problems}\n\nInputs encoded as individual spike times of two input neurons. All\nspikes occur between 0 and 1. True and False values are drawn from\ndistributions \\\\([0.0, 0.45]\\\\) and \\\\([0.55, 1.0]\\\\) respectively.\n\nTrained for maximum of 100 epochs, 1000 training examples. Tested on\n150 randomly generated test examples. 100% accuracy on all problems.\n\nNon-convolutional MNIST {#non-convolutional-mnist}\n\n784 neurons of the input layer corresponding to pixels of the image.\nDarker pixels encoded as earlier spike times. Output of network is the\nindex of the earliest neuron to spike.\n\nTrained with evolutionary-neural hybrid agents. Best networks achieved\n99.96% and 97.96% accuracy on train and test sets.\n\nThe network learns two operating modes: slow-regime and fast-regime.\nOperating in the slow regime has higher accuracy, but takes more time.\nFast regime makes quick decisions, with the first spike in the output\nlayer occurring before the mean spike in the hidden layer.\n\n{{}}\n\nBibliography\nComsa, I. M., Potempa, K., Versari, L., Fischbacher, T., Gesmundo, A., & Alakuijala, J., Temporal coding in spiking neural networks with alpha synaptic function, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/papers/dabney2020_distributional_rl",
        "title": "A Distributional Code for Value in Dopamine-based Reinforcement Learning",
        "content": "\npaper\n: https://www.nature.com/articles/s41586-019-1924-6\n\nrelated\n: Reinforcement Learning ⭐\n\nAbstract {#abstract}\n\nReward prediction errors are typically represented by a single scalar\nquantity, as in temporal-difference learning. In distributional RL,\nthe reward prediction error consists of a diverse set of channels,\npredicting multiple future outcomes. Different channels have different\nrelative scalings for positive and negative reward prediction errors.\nImbalances between these scalings cause each channel to learn a\ndifferent value prediction, and collectively represent a distribution\nover possible rewards.\n\nResources {#resources}\n\nDistributional RL – Simple Machine Learning\n",
        "tags": []
    },
    {
        "uri": "/papers/dacrema19_progress_neural_rec",
        "title": "Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches",
        "content": "\ntitle\n: Are We Really Making Much Progress? A Worrying Analysis of\n    Recent Neural Recommendation Approaches\n\npaper\n: (Dacrema et al., 2019)\n\ntags\n: Recommender Systems, Machine Learning Papers\n\nThe authors analyzed various recent publications on recommendation\nsystems techniques, and found that these have:\n\nWeak baselines\nEstablish weak methods as baselines\nAre outperformed by simple, sometimes non-neural approaches\n\nThe simple approaches that work well include ItemKNN, a\ncollaborative-filtering approach that uses k-nearest neighbours and\nitem-item similarities:\n\n\\begin{equation}\n  s\\{ij} = \\frac{r\\i \\dot r\\j}{\\lvert r\\i \\rvert \\lvert r\\_j \\rvert + h}\n\\end{equation}\n\nTo alleviate these issues:\n\nUse appropriate evaluation methods\nEvaluate on appropriate datasets (size is important)\nRelease reproducible code\n\nBibliography\nDacrema, M. F., Cremonesi, P., & Jannach, D., Are we really making much progress? a worrying analysis of recent neural recommendation approaches, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/papers/henderson_deep_rl_that_matters",
        "title": "Deep Reinforcement Learning That Matters",
        "content": "\ntags\n: Deep Reinforcement Learning, Machine Learning Papers\n\nThis paper is a comprehensive study of several model-free policy\ngradient methods:\n\nTrust Region Policy Optimization (TRPO)\nDeep Deterministic Policy Gradients (DDPG)\nProximal Policy Optimization (PPO)\nActor Critic using Kronecker-Factored Trust Region (ACKTR)\n\nNetwork Architecture {#network-architecture}\n\nIt was shown that network architecture is highly interconnected with\nalgorithm methodology. For example, using a large network in PPO\nrequired tweaking hyperparameters like trust region clipping. This\nsuggests the need to design hyperparameter agnostic algorithms.\n\nReward Scale {#reward-scale}\n\nLarge, and sparse rewards can lead to saturation and inefficiency.\nReward scaling was shown to have large effects, but results were\ninconsistent across environments and scaling values. _Is there a more\nprincipled approach?_\n\nEnvironments {#environments}\n\nHow do the environment properties affect variability in reported RL\nalgorithm performance? Algorithm performance can vary across\nenvironments, and the best performing algorithm across all\nenvironments is not always clear. It is important to present results\nacross multiple environments. It is also important to shown the learnt\npolicy in action. IT is possible that algorithms only optimize local\nminima, rather than reaching the desired behaviour.\n\nCodebases {#codebases}\n\nSmall implementation details are not reflected in publications, but\nthese can have dramatic effect on performance.\n\nEvaluation Metrics {#evaluation-metrics}\n\nWhat metrics should an RL algorithm report?\n\nAverage cumulative reward (average returns): misleading, range of\n    performance across random seeds and trials unknown\nMaximum reward achieved over a fixed number of timesteps: inadequate\n    under high variance\n\nPerhaps one can use bootstrap and significance testing to add\nconfidence intervals. Some techniques:\n\nConfidence Bounds {#confidence-bounds}\n\nObtain a bootstrap estimator by resampling with replacement many times\nto generate a statistically relevant mean and confidence bound. TRPO\nand PPO are the most stable with small confidence bounds from the bootstrap.\n\nPower Analysis {#power-analysis}\n\nIf we use our sample and give it some uniform lift (e.g. scaling\neverything by 1.25), we can run many bootstrap simulations and\ndetermine what percentage of simulations result in statistically\nsignificant values with the lift. If there is a small percentage of\nsignificant values, more trials need to be run.\n\nSignificance {#significance}\n\nIn supervised learning, k-folt t-test, and corrected resampling t-test\nare some significance metrics used, but these make assumptions about\nthe underlying data that do not necessarily hold in RL.\n\nRecomendations {#recomendations}\n\nFind the working hyperparameter set that matches the original\n    reported performance\nNew baseline algorithm implementations should match original\n    codebase results if possible\nAveraging multiple runs over different random seeds to get better\n    insight into algorithm performance\nReport all hyperparameters, implementation details, experimental\n    setup, and evaluation methods\n",
        "tags": []
    },
    {
        "uri": "/papers/markov_logic_networks",
        "title": "Markov Logic Networks",
        "content": "\nMarkov Logic Networks (Matthew Richardson \\& Pedro Domingos, 2006). {#markov-logic-networks-dot}\n\nProblem {#problem}\n\nTraditionally, first-order logic imposes hard constraints on the\nworld. This poses problems in the real world: formulae that may be\ntypically true in the real world are not always true. In most domains,\nit is difficult to devise non-trivial formulae that are always true.\nProbabilistic graphical models is a decent solution.\n\nWhat are Markov Logic Networks ? {#what-are-markov-logic-networks}\n\nMarkov logic networks relax the hard constraints that first-order\nlogic enforces. When a world violates one formula in a KB, it is less\nprobable, but not impossible. The fewer formulae a world violates, the\nmore probable it is. Each formula is associated with a weight that\nreflects how strong a constraint it is: the higher the weight, the\ngreater the difference in log probability between a world that\nsatisfies the formula, and one that does not, other things equal.\n\nFormally,\n\nA Markov Logic Network \\\\(L\\\\) is a set of pairs \\\\((F\\i, w\\i)\\\\), where \\\\(F\\_i\\\\)\nis a formula in first-order logic, and \\\\(w\\_i\\\\) is a real number.\nTogether with a finite set of constants \\\\(C = \\left\\\\{ c\\_1, c-2, \\dots,\nc\\_{|C|} \\right\\\\}\\\\), it defines a Markov Logic Network as follows:\n\n\\\\(M\\_{L,C}\\\\) contains one binary node for e ach possible grounding of\n    each predicate appearing in \\\\(L\\\\). The binary node takes on value \\\\(1\\\\)\n    if the ground atom is true, and 0 otherwise.\n\\\\(M\\_{L,C}\\\\) contains one feature for each possible grounding of each\n    formula \\\\(F\\_i\\\\) in \\\\(L\\\\). The value of this feature is \\\\(1\\\\) if the\n    ground formula is true, and 0 otherwise. The weight of the feature\n    is the \\\\(w\\i\\\\) associated with \\\\(F\\i\\\\) in \\\\(L\\\\).\n\nBibliography\nRichardson, M., & Domingos, P., Markov Logic Networks, Machine Learning, 62(1-2), 107–136 (2006).  http://dx.doi.org/10.1007/s10994-006-5833-1 ↩\n",
        "tags": []
    },
    {
        "uri": "/papers/miconi_differentiable_plasticity",
        "title": "Differentiable plasticity: training plastic neural networks with backpropagation",
        "content": "\ntags\n: Machine Learning Papers\n\npaper\n:\n\nGoal {#goal}\n\nTo build networks that are plastic: quick and efficient learning from\nexperience, inspired by synaptic plasticity. This is to bridge the gap\nwith biological agents, which are able to learn quickly from prior\nexperience, mastering environments with changing features.\n\nAn alternative to Meta Learning, synaptic plasticity strengthens and\nweakens connections between neurons based on neural activity: whether\nthey fire together.\n\nPlasticity has traditionally been explored with evolutionary\nalgorithms, differential plasticity allows for learning such\nplasticity updates via backpropagation.\n\nKey Idea {#key-idea}\n\nWe include an additional plastic component for each neuron. The fixed\npart contains regular neuronal weights \\\\(w\\_{i,j}\\\\), while the plastic\npart is stored in a Hebbian trace \\\\(\\mathrm{Hebb}\\_{i,j}\\\\).\n\n\\begin{equation}\n  x\\{i,j} = \\sigma \\left\\\\{ \\sum\\{i \\in inputs} w\\{i,j} x\\i (t-1) +\n  \\alpha\\{i,j} \\mathrm{Hebb}\\{i,j}(t)x\\_{i}(t-1) \\right\\\\}\n\\end{equation}\n\nwhere \\\\(\\alpha\\\\) is the plasticity coefficient, governing how much of\nthe weight is from the plastic component, and \\\\(\\sigma\\\\) is some\nnon-linearity.\n\nThe Hebbian trace is updated based on Hebbian dynamics:\n\n\\begin{equation}\n  \\mathrm{Hebb}\\{i,j}(t+1) = \\eta x\\i(t-1)x\\j(t) + (1 - \\eta) \\mathrm{Hebb}\\{i,j}(t)\n\\end{equation}\n\nThe Hebbian trace is initialized to zero, at the beginning of each\nepisode, and is purely a lifetime quantity.\n\n\\\\(\\eta\\\\) is a weight decay term, to prevent runaway positive feedback on\nHebbian traces. In the absence of input, the Hebbian trace decays to\nzero. One can use Oja's rule to prevent such runaway divergences,\nwhile maintaining stable long-term memories in absence of stimulation.\n",
        "tags": []
    },
    {
        "uri": "/papers/mnih2013_atari_deeprl",
        "title": "Playing Atari with Deep RL",
        "content": "\nPlaying Atari With Deep RL (Mnih et al., 2013) {#playing-atari-with-deep-rl}\n\nPreprocessing Steps {#preprocessing-steps}\n\nObtain raw pixels of size \\\\(210 \\times 160\\\\)\nGrayscale and downsample to \\\\(110 \\times 84\\\\)\nCrop representative \\\\(84 \\times 84\\\\) region\nStack the last 4 frames in history to form the \\\\(84 \\times 84 \\times\n       4\\\\) input\n\nDQN {#dqn}\n\nUse of §experience\\_replay buffer\nSeparate target network stabilizes optimization targets:\n\n\\begin{equation}\n  \\delta = r\\t + \\gamma \\mathrm{max}\\a Q(s\\_{t+1}, a ; \\theta') -\n  Q(s\\t, a\\t; \\theta)\n\\end{equation}\n\nThe network parameterized with \\\\(\\theta '\\\\) is a snapshot of the network\nat some point in time, so the optimization target doesn't change so\nrapidly.\n\nClip \\\\(\\delta\\\\) to \\\\(\\left[1, -1\\right]\\\\)\n\nImproving DQN {#improving-dqn}\n\nDouble Q-learning reduces bias (Van Hasselt et al., 2016)\nAverage Q-learning reduces variance (Anschel et al., 2017)\n§andrychowicz2017\\hindsight\\experience\\_replay (Andrychowicz et al., 2017)\nDistributional RL (Dabney et al., 2018)\n\nReferences {#references}\n\nDefeating the Deadly Triad: | random walks and lots of ♥s\n\nBibliography\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M., Playing atari with deep reinforcement learning, arXiv preprint arXiv:1312.5602, (),  (2013).  ↩\n\nVan Hasselt, H., Guez, A., & Silver, D., Deep reinforcement learning with double q-learning, In , Thirtieth AAAI conference on artificial intelligence (pp. ) (2016). : . ↩\n\nAnschel, O., Baram, N., & Shimkin, N., Averaged-dqn: variance reduction and stabilization for deep reinforcement learning, In , Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 176–185) (2017). : . ↩\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., …, Hindsight experience replay, In , Advances in Neural Information Processing Systems (pp. 5048–5058) (2017). : . ↩\n\nDabney, W., Rowland, M., Bellemare, M. G., & Munos, R\\'emi, Distributional reinforcement learning with quantile regression, In , Thirty-Second AAAI Conference on Artificial Intelligence (pp. ) (2018). : . ↩\n",
        "tags": []
    },
    {
        "uri": "/papers/neftci2019_surrogate_gradient_learning_snn",
        "title": "Surrogate Gradient Learning In Spiking Neural Networks",
        "content": "\npaper\n:\n\nSpiking Neural Networks enable power-efficient network models, which\nhave become of increasing importance in embedded and auto-motive\napplications. The power efficiency stems from dispensing of expensive\nfloating-point computations.\n\nSurrogate gradient methods overcome the difficulties associated with\nthe discontinuous non-linearity. Rather than changing the neuronal\nmodel (Smoothed Spiking Neural Networks), surrogate gradients are\nintroduced to allow for numerical optimisation.\n\nSurrogate gradients can also improve the memory access overhead of the\nlearning process. For example, a global loss can be replaced by a\nnumber of local loss functions. Surrogate gradient methods also allow\nfor end-to-end training without specifying a coding scheme in the\nhidden layers.\n\nThere are many different available surrogate functions used, and all\nhave reportedly some success\n(Neftci et al., 2019). All of the\nfunctions used are non-linear and monotonically increasing towards the\nfiring threshold. This suggests that the details of the surrogate are\nnot crucial in ensuring success of the method.\n\nBibliography\nNeftci, E. O., Mostafa, H., & Zenke, F., Surrogate gradient learning in spiking neural networks, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/papers/neural_ode",
        "title": "Neural Ordinary Differential Equations (Review)",
        "content": "\nThis is a paper review of the NIPS 2018 best paper award-winning paper\nNeural Ordinary Differential Equations.\n\nMotivation {#motivation}\n\nIn this section, I motivate the benefits of Neural Ordinary\nDifferential Equations (ODEs). Many physical phenomena can be modeled\nnaturally with the language of differential equations. These include\npopulations of predator and prey, or in physics with regards to motion\nbetween bodies. Differential equations shine where it is easier to\nmodel changes in the systems over time rather than the value\nthemselves.\n\nConsider a simple pendulum, with the dampening effect of air\nresistance. One can model the dynamic system using a second-order ODE,\nas such:\n\n\\begin{equation}\n  \\ddot{\\theta}(t) = - \\mu \\dot{\\theta}(t) - \\frac{g}{L}\\sin\\left( \\theta(t) \\right)\n\\end{equation}\n\nThere are infinitely many solutions to this ODE, but generally only\none that satisfies the initial conditions at \\\\(t = 0\\\\). We'd like to\nfind \\\\(\\theta(t)\\\\) for some any \\\\(t\\\\). It turns out finding the solutions\nto these problems are hard, and numerical methods are required to find\nthe solutions. These numerical methods range from simplest Euler's\nmethod, to Runge-Kutta methods.\n\nSuppose now that we have some dynamic system (for example, the\npendulum), and we have measured some data from the system\n\\\\(\\hat{\\theta}(t)\\\\) (the pendulum's position, at time \\\\(t\\\\)). **Can a neural\nnetwork learn the dynamics of the system from data?**\n\nRegular neural networks states are transformed by a series of discrete\ntransformations:\n\n\\begin{equation}\n\\mathbf{h}\\{t+1} = f(\\mathbf{h}\\t)\n\\end{equation}\n\nwhere \\\\(f\\\\) could be of different kinds of layers, including convolutional\nand dense layers. \\\\(t\\\\) can be interpreted as a time index, transforming\nsome input data at \\\\(t=0\\\\) to an output in a different space at \\\\(t=N\\\\),\nwhere there are \\\\(N\\\\) layers.\n\nBecause neural networks apply discrete transformations, to learn\ndynamical systems with (recurrent) neural networks, one must\ndiscretize the time steps, for example through binning the\nobservations into fixed time intervals. However, expressing time as a discrete\nvariable can be unnatural: this includes processes where events\noccur at irregular intervals. This means that the current\nstate-of-the-art neural networks are still unable to model continuous\nsequential data.\n\nThe Analogy to Residual Neural Networks {#the-analogy-to-residual-neural-networks}\n\nIn neural networks, every layer introduces error that compounds through the neural\nnetwork, hindering overall performance. The only way to bypass this is\nto add more layers, and limit the complexity of each step. This means\nthat the highest performing neural networks would have infinite\nlayers, and infinitesimal step-changes, an infeasible task.\n\nTo address this problem, deep residual neural networks were presented\n(He et al., 2015).\n\nInstead of learning \\\\(h\\{t+1} = f(h\\t, \\theta\\_t)\\\\), deep residual neural\nnetworks now learn the difference between the layers: \\\\(h\\{t+1} = h\\t +\nf(h\\t, \\theta\\t)\\\\). For example, feed-forward residual neural networks\nhave a composition that looks like these:\n\n\\begin{align\\*}\n  h\\1 &= h\\0 + f(h\\0, \\theta\\0) \\\\\\\\\\\\\n  h\\2 &= h\\1 + f(h\\1, \\theta\\1) \\\\\\\\\\\\\n  h\\3 &= h\\2 + f(h\\2, \\theta\\2) \\\\\\\\\\\\\n  \\dots \\\\\\\\\\\\\n  h\\{t+1} &= h\\t + f(h\\t, \\theta\\t) \\\\\\\\\\\\\n\\end{align\\*}\n\nThese iterative updates correspond to the infinitesimal step-changes\ndescribed earlier, and can be seen to be analogous to an Euler\ndiscretization of a continuous transformation\n(Lu et al., 2017). In the limit, one can\ninstead represent the continuous dynamics between the hidden units\nusing an ordinary differential equation (ODE) specified by some neural\nnetwork:\n\n\\begin{equation}\n  \\frac{d\\mathbf{h}(t)}{dt} = f(\\mathbf{h}(t), t, \\theta)\n\\end{equation}\n\nwhere the neural network has parameters \\\\(\\theta\\\\). The equivalent of\nhaving \\\\(T\\\\) layers in the network, is finding the solution to this ODE at\ntime \\\\(T\\\\).\n\nThe analogy between ODEs and neural networks is not new, and has been\ndiscussed in previous papers\n(Lu et al., 2017), (Haber \\& Ruthotto, 2017).\nThis paper popularized this idea, by proposing a new method for\nscalable backpropagation through ODE solvers, allowing end-to-end\ntraining within larger models.\n\nThe NeuralODE Model {#the-neuralode-model}\n\nThe Neural ODE model introduces a new type of block the authors term the\nODE block. This block replaces the ResNet-like skip connections, with\nan ODE that models the neural network's dynamics:\n\n\\begin{equation}\n  \\frac{d\\mathbf{h}(t)}{dt} = f(\\mathbf{h}(t), t, \\theta)\n\\end{equation}\n\nThis ODE can then be solved using a black box solver, with the output\nstate being used to compute the loss:\n\n\\begin{equation}\n  L(\\mathbf{z}(t\\1)) = L\\left( \\mathbf{z}(t\\0) + \\int\\{t\\0}^{t\\_1}\n    f(\\mathbf{z}(t), t, \\theta)dt \\right) =\n  L(\\textrm{ODESolve}(\\mathbf{z}(t\\0), f, t\\0, t\\_1, \\theta))\n\\end{equation}\n\nThe loss is used to compute gradients, but, as mentioned in the paper,\nperforming backpropagation through the ODE solver incurs too high a\nmemory cost. Here, I illustrate why, with a simple example.\n\nSuppose we use the Euler method to solve the ODE. The Euler solver update\nstep is similar to a ResNet block:\n\n\\begin{equation}\n  h\\{t+1} =  h\\t + NN(h\\_{t})\n\\end{equation}\n\nContinuous-depth networks will have large \\\\(t\\\\). Despite the ODE solvers\nbeing easily differentiable, backpropagating through the neural\nnetwork in this case is equivalent to computing and storing the\ngradients in a $t$-depth ResNet. With higher-order ODE\nsolvers, the memory requirements are also higher.\n\nThe paper proposes a method of computing gradients by solving a\nsecond, augmented ODE backwards in time, that is applicable to all ODE\nsolvers.\n\nGradient Computation via Adjoint Sensitivity Analysis {#gradient-computation-via-adjoint-sensitivity-analysis}\n\nIf one wishes to train a Neural ODE via gradient descent, one would\nneed to compute gradients for the loss function\n\\\\(L(\\textrm{ODESolve}(\\mathbf{z}(t\\0), f, t\\0, t\\_1, \\theta))\\\\). This\nrequires propagation of gradients through the ODE-solver, that is,\ngradients with respect to \\\\(\\theta\\\\). The paper proposes a technique\nthat scales linearly with problem size, has low memory cost, and\nexplicitly controls numerical error.\n\nSensitivity analysis defines a new ODE whose solution gives the\ngradients to the cost function w.r.t. the parameters, and solves this\nsecondary ODE. Because the gradients of the loss is dependent on the\nhidden state \\\\(z(t)\\\\) at each instant, the dynamics of \\\\(z(t)\\\\) can be\nrepresented with yet another ODE. Obtaining the gradients would\nrequire a single solve by recomputing \\\\(z(t)\\\\) backwards together with\nthe adjoint. The derivations are provided in the appendix of the\npaper, and will not be repeated here.\n(Chen et al., 2018)\n\nSince a large part of the paper's contribution is the ability to\nbridge many years of mathematical advancements on solving differential\nequations, it is wise to analyse the pros and cons of other solvers in\nthe context of training machine learning models.\n\nTraditional adjoint sensitivity analysis require multiple forward\nsolutions of the ODE, which can become prohibitively costly in large\nmodels. The paper's proposal reduces the computational complexity to a\nsingle solve, while retaining low memory cost by solving the backwards\nsolution together with the adjoint. One issue that the paper has\nfailed to address is that their proposed method requires that the ODE\nintegrator is time-reversible.  There are no ODE solvers for\nfirst-order ODEs that are time-reversible, implying that the method\nproposed will diverge on some systems. (Rackauckas et al., 2019)\n\nIn general, while the model is agnostic of the choice of ODE solver,\nthe ideal choice of differential equation solver depends on the\nproblem to be solved. For different classes of differential equations\n(under certain assumptions), some solvers will prove to be more\nefficient or more accurate. A good rule of thumb is that forward-mode\nautomatic differentiation is efficient for differential equations with\na small number of parameters, while reverse-mode automatic\ndifferentiation is more efficient when the model size grows bigger.\n\nReplacing ResNets {#replacing-resnets}\n\nBecause an ODE block is simply the continuous version of the Residual\nblock, it seems plausible to use ODE blocks as replacements for ResNet\nblocks. The authors of the paper experimented with MNIST, and found\nthat using ODE blocks they were able to achieve roughly equivalent\ntest-error, with a third of the parameters (0.22M compared to 0.60M)\nand constant memory cost during training.\n\n{{}}\n\nWhile this looks promising, it would be more instructive to train the\nODEnet on different datasets.\n\nIt turns out that because of the continuous limit, there is a class of\nfunctions that Neural ODEs. In particular, Neural ODEs can only learn\nfeatures that are homeomorphic to the input space.\n(Dupont et al., 2019) The errors arising from\ndiscretization allow ResNet trajectories to cross, allowing them to\nrepresent certain flows that Neural ODEs cannot.\n\nExperiments {#experiments}\n\nTo understand Neural ODEs, I referenced several implementations.\nFirst, I ran the implementation provided with the paper. With the\nprovided code, it was easy to reproduce the results of the Neural ODE\nmodel for MNIST.\n\nTo further understand the how to write solvers for ODEs and the\nAdjoint method, I referenced the implementations from a seminar. The\nexample notebooks provided were small and self-contained, I wrote the\nnaive ODE solver using Euler's method, and swapped out the adjoint ODE\nsolver. Even for a relatively dataset of relatively small\ndimensionality, using it to train on the MNIST dataset took an hour.\nEach epoch took slightly longer than the previous, which the authors\nattribute to the increasing number of function evaluations, as a\nresult of the model adapting to increasing complexity. Perhaps one\ncould place a penalty on model complexity (something like MDL), to\nprevent overfitting, and maintain interpretability of the learnt model.\n\nIn general, I found that the gains from having to train a model with\nfewer parameters is offset by the difficulty in training. In my\nexperiments conducted, the full data-set is passed for evaluation and\ngradient descent is used to update the parameters. The authors have\nmentioned that mini-batching, and using stochastic gradient descent is\ntricky. Doing a full gradient descent may be infeasible where the\ndataset is too large, and the sub-gradients cannot fit into memory.\n\nODE as Prior Knowledge {#ode-as-prior-knowledge}\n\nA use-case that I have not seen discussed with the introduction of\nNeural ODEs is the ability to introduce structure to the machine\nlearning models.\n\nSuppose we have collected data from some known dynamic system, and\nwish to learn the system. Traditional machine learning models have no\nway of specifying the structure of the dynamic system, and the model\nwould have to learn its model parameters solely from the data.\nHowever, supposing we know the equations governing the dynamic system:\nfor example, it is a system for which the laws of physics govern the\nsystem (gravitational attraction between objects). We can then\nrestrict the hypothesis class to the family of functions that satisfy\nthe equations, while ensuring that learning is still realizable (there\nexists the correct hypothesis \\\\(h^\\*\\\\) in the subclass \\\\(\\mathcal{H'}\\\\)).\nRestricting the hypothesis space would lead to lower sample\ncomplexity.\n\nClosing Thoughts {#closing-thoughts}\n\nThis paper presented a new family of models called Neural ODEs. They\nnaturally arise by taking the continuous limit in residual neural\nnetworks. The paper proposes a numerically unstable, but empirically\nworking method for performing back-propagation through black-box ODE\nsolvers, making training neural ODEs feasible.\n\nWhile not novel, this paper brought into the limelight the idea of\nmarrying differential equations with machine-learning, an area that\nhas seemingly a lot of potential.\n\nFrom experimentation, I find that Neural ODEs are still difficult to\ntrain beyond simple problems, and mathematical theory shows that the\nchoice of the ODE solver is still important.\n\nIn this review, I did not cover the applications of Neural ODEs in\nother areas. First, they have a particularly convenient formulation\nfor Continuous Normalizing Flows. Normalizing flows is a technique for\nsampling from complex distributions via sampling from a simple\ndistribution, and has applications in techniques like variational\ninference.\n\nEDIT: During NeurIPS 2019, [David Duvenaud reflects on the claims and\nthe hype of the Neural ODE paper](https://slideslive.com/38921897/retrospectives-a-venue-for-selfreflection-in-ml-research-4).\n\nBibliography\nHe, K., Zhang, X., Ren, S., & Sun, J., Deep residual learning for image recognition, CoRR, (),  (2015).  ↩\n\nLu, Y., Zhong, A., Li, Q., & Dong, B., Beyond finite layer neural networks: bridging deep architectures and numerical differential equations, CoRR, (),  (2017).  ↩\n\nHaber, E., & Ruthotto, L., Stable architectures for deep neural networks, CoRR, (),  (2017).  ↩\n\nChen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D., Neural Ordinary Differential Equations, CoRR, (),  (2018).  ↩\n\nRackauckas, C., Innes, M., Ma, Y., Bettencourt, J., White, L., & Dixit, V., Diffeqflux.jl - a julia library for neural differential equations, CoRR, (),  (2019).  ↩\n\nDupont, E., Doucet, A., & Teh, Y. W., Augmented Neural Odes, CoRR, (),  (2019).  ↩\n",
        "tags": [
            "machine-learning",
            "deep-learning"
        ]
    },
    {
        "uri": "/papers/single_layer_xor",
        "title": "Single Layer XOR",
        "content": "\npaper\n:\n\ntags\n: §neuroscience, §machine\\_learning\n\nFinding {#finding}\n\nThe way the biological human neuron implements XOR is by a formerly\nunknown type of local response to inputs, which is low below the\nthreshold, maximal at the threshold and decreases as the input\nintensifies above the threshold.\n\n{{}}\n\nWhy is this interesting? {#why-is-this-interesting}\n\nXOR used to be impossible to compute without inhibitory mechanisms,\n    but this is makes that possible.\nHence the human brain can use a single-layered network to compute\n    the XOR function.\n",
        "tags": []
    },
    {
        "uri": "/papers/zador19_critique_pure_learning",
        "title": "A critique of pure learning and what artificial neural networks can learn from animal brains",
        "content": "\nThe Genomic Bottleneck {#the-genomic-bottleneck}\n\nThe compression into the genome whatever innate processes are captured\nby evolution. This acts as a regularizing constraint on the rules for\nwiring up the brain.\n\nIn large and sparsely connected brains, most of the information in the\ngenome has to be allocated to specify the non-zero elements of the\nconnection matrix in the brain, rather than their precise values. Even\nif every nucleotide of the human genome is devoted to specifying brain\nconnections, the information capacity would still be at least six\norders of magnitude too small.\n\nThe implication of this is that the genome does not encode the\nconnections directly, but rules in forming these connections.\nEvolution acts on the brain only indirectly through the genome.\n\nWhat this means for ANNs {#what-this-means-for-anns}\n\nThere may be an outer-loop (evolution) that optimizes learning\n    mechanisms, and an inner-loop that allows us to learn inductive\n    biases quickly (i.e. §meta\\_learning)\nANNs should attempt as much as possible to build on solutions to\n    related problems (transfer learning)\nWiring rules and topology should be studies as a target for\n    optimization in artificial neural systems\n",
        "tags": []
    },
    {
        "uri": "/talks/emacs_should_be_emacs_lisp",
        "title": "Emacs Should Be Emacs Lisp - Tom Tromey",
        "content": "\ntalk\n:\n\nspeaker\n: Tom Tromey\n\ntags\n: Emacs, Emacs Lisp\n\nSeveral competing ideas and their problems {#several-competing-ideas-and-their-problems}\n\nGuile Emacs\n    Scheme is not Emacs Lisp! Impedance mismatch -- largely solved by\n        Guile Scheme, at the expense of Guile not being a proper Scheme\n    Scripting fragmentation (rather than have)\nRebase on Common Lisp\n    Experiment in 2012, an attempt to unify the type systems\n    Impedance mismatch, hard to maintain\n\nEmacs Should Be Emacs Lisp! {#emacs-should-be-emacs-lisp}\n\nEasier to hack\nThreads\nGC\nLibrary-only\nFFI - tromey/emacs-ffi\n",
        "tags": []
    },
    {
        "uri": "/talks/emti_dl_with_bayesian_principles",
        "title": "Deep Learning With Bayesian Principles - Emtiyaz Khan",
        "content": "\nLink\n: Mohammad Emtiyaz Khan | Deep Learning with Bayesian Principles · SlidesLive\n\nSpeaker\n: Emtiyaz Khan\n\nGoals {#goals}\n\nTo understand the fundamental principles of learning from data\nuse them to develop learning algorithms\n\nDeep learning assumes that the environment is stationary: e.g. all the\ninformation required to classify images is already available in large\namounts, and these don't change quickly over time.\n\nWish to design life-long learning systems: Bayesian learning has a\nnatural update rule to update posterior beliefs given new data.\n\n|                                              | Bayes | DL |\n|----------------------------------------------|-------|----|\n| Can handle large data and complex models?    | ❌    | ✔️  |\n| Scalable training?                           | ❌    | ✔️  |\n| Can estimate uncertainty?                    | ✔️     | ❌ |\n| Can perform sequential/incremental learning? | ✔️     | ❌ |\n\nWe start with the Bayes rule, and derive many existing algorithms\nthrough various approximations.\n\nDeep Learning {#deep-learning}\n\nFrequentist: Empirical Risk Minimization (ERM) or Maximum Likelihood\n\n\\begin{equation}\n  \\mathrm{min}\\{\\theta} l(D, \\theta) = \\sum\\{i=1}^{N} [y\\i - f\\{\\theta}(x\\_i)^2]\n  \\gamma \\theta^T \\theta\n\\end{equation}\n\nScales well to large data and complex models, good performance in\npractice. Does not tell you what the model does not know (uncertainty).\n\nBayesian Principles {#bayesian-principles}\n\nSample \\\\(\\theta \\sim p(\\theta)\\\\) : prior\nScore \\\\(p (D| \\theta) = \\prod\\{i=1}^{N} p(y\\i | f\\{\\theta}(x\\i))\\\\) : likelihood\nNormalize:\n\n\\begin{equation}\n  p(\\theta | D) = \\frac{p(D|\\theta)p(\\theta)}{\\int p(D|\\theta)p(\\theta) d \\theta}\n\\end{equation}\n\nThis is a global method, integrating over all models, but does not\nscale to large problems. Global properties enable sequential update.\n\n{{}}\n\nTo scale Bayesian learning, we need to **approximate the posterior\ndistribution**. Different posterior approximation methods lead to\ndifferent learning rules.\n\n{{}}\n\nBayesian Learning Rule {#bayesian-learning-rule}\n\nDeep learning rule: \\\\(\\min\\_{\\theta} l(\\theta)\\\\)\nBayesian learning rule: \\\\(\\min\\{q \\in Q} E\\{q(\\theta)} [l (\\theta)] -\n      H(q)\\\\) where \\\\(H(q)\\\\) is entropy (regularization)\n\nBayesian learning rule optimizes over all distributions.\n\nDeep learning algo: \\\\(\\theta \\leftarrow \\theta-\\rho H\\{\\theta}^{-1} \\nabla\\{\\theta} \\ell(\\theta)\\\\)\nBayes learning rule: \\\\(\\lambda \\leftarrow \\lambda-\\rho \\nabla\\{\\mu}\\left(\\mathbb{E}\\{q}[\\ell(\\theta)]-\\mathcal{H}(q)\\right)\\\\)\n    \\\\(\\lambda\\\\) is the natural parameter, and \\\\(\\mu\\\\) the expectation\n        parameter of an exponential family distribution \\\\(q\\\\).\n\nDeep learning algorithms are obtained by choosing an appropriate\napproximation \\\\(q\\\\), and giving away the global property of the rule.\n\nDeriving Gradient Descent {#deriving-gradient-descent}\n\nGradient descent is derived from using a Gaussian with fixed\ncovariance, and approximating \\\\(E\\_q[l(\\theta)] \\approx l(m)\\\\).\n\nResources {#resources}\n",
        "tags": []
    },
    {
        "uri": "/talks/how_to_know_kidd",
        "title": "How To Know: Celeste Kidd",
        "content": "\nLink: Celeste Kidd | How to Know · SlidesLive\n\nWhy are we curious about some things but not others? What are the core\ncognitive systems that people use to guide their learning about the\nworld?\n\n5 Key things in ML that we should know about humans {#5-key-things-in-ml-that-we-should-know-about-humans}\n\nHumans continuously form beliefs {#humans-continuously-form-beliefs}\n\nUpon seeing a bowl, we update our belief about what a bowl is. This\nalso influences what we wish to sample next. Infants look away when\nevents are predictable, but also when they are surprising!\n\n{{}}\n\n**When present data to learning algorithms, we should show data that is\nat ideal region on the \"surprising\" spectrum.**\n\nCertainty diminishes interest {#certainty-diminishes-interest}\n\nWe are not curious about things that we already know. If we think we\nknow an answer, when presented with the objective truth, we are still\nunlikely to change our beliefs.\n\nCertainty is driven by feedback {#certainty-is-driven-by-feedback}\n\nWhen feedback isn't available, our estimates about our certainty may\nnot be accurate. When feedback is readily available, we can sometimes\nbe certain, when we should not be.\n\nLess feedback may encourage overconfidence {#less-feedback-may-encourage-overconfidence}\n\nHumans form beliefs quickly {#humans-form-beliefs-quickly}\n",
        "tags": []
    },
    {
        "uri": "/talks/leslie_lamport_2020",
        "title": "If You're Not Writing a Programming Language, Don't Use A Programming Language",
        "content": "\ntitle\n: If You're Not Writing a Programming Language, Don't Use A Programming Language\n\nspeaker\n: Leslie Lamport\n\ndate\n: &lt;2020-01-16 Thu&gt;\n\n> Inside every program, there is an algorithm trying to get out.\n\nWe should find and understand the algorithm before writing the\nprogram. The best way to describe these algorithms is with mathematics.\n\nDon't be \"brainwashed\" by programming languages. Free your mind with mathematics.\n\nAlgorithms vs Programs {#algorithms-vs-programs}\n\nPrograms tend to contain more low-level details:\n\nWhat types are the arguments?\nWhat are the boundary conditions?\nShould I throw an exception?\n\nKey insights:\n\nPrograms are hard to debug, because we're debugging an algorithm at\n    the code level.\nAlgorithms are hard to optimize at the code level\n\nSolution: Describe algorithms in math!\n\nDescribing an execution of an algorithm {#describing-an-execution-of-an-algorithm}\n\nAlgorithms are described by a sequence of states, characterized by a\nset of behaviours. Set of behaviours are described by an initial\npredicate on state \\\\(s\\1\\\\), and predicates on pairs of states \\\\(s\\m,\ns\\_n\\\\).\n\nE.g. Euclid's algorithm:\n\nInitial Predicate: \\\\((x = M) \\wedge (y = N)\\\\)\nNext state predicate:\n\n\\begin{equation}\n  \\text{Next}\\_E : ((x > y) \\wedge (x' = x - y) \\wedge (y' = y)) \\vee\n  ((y > x) \\wedge (y' = y - x) \\wedge (x' = x))\n\\end{equation}\n\nPredicate on Behaviours {#predicate-on-behaviours}\n\nThis can be written as:\n\n\\begin{equation}\n  \\mathrm{Init}\\E \\wedge \\Box \\mathrm{Next}\\E\n\\end{equation}\n\nSafety and Liveness {#safety-and-liveness}\n\nsafety\n: what is allowed to happen\n\nliveness\n: what must eventually happen\n\nAny property can be expressed as \\\\(\\text{safety} \\wedge\n\\text{liveness}\\\\).\n\nInvariance {#invariance}\n\nIf Euclid's algorithm has terminated, then \\\\(x = GCD(M, N)\\\\). This can\nbe expressed as a property:\n\n\\begin{equation}\n  \\Box ((x = y) \\rightarrow (x = GCD(M,N)))\n\\end{equation}\n\nInvariance can be proved by showing:\n\n\\begin{equation}\n  \\text{Init}\\E \\wedge \\Box \\text{Next}\\E \\rightarrow \\Box I\\_E\n\\end{equation}\n\nwhere \\\\(I\\_E\\\\) is the invariance property.\n\nImpact of Using Math to Describe Systems {#impact-of-using-math-to-describe-systems}\n\nExample 1: Virtuoso {#example-1-virtuoso}\n\nThe next iteration of Virtuoso used the [TLA+] abstraction.\n\n> We witnessed first-hand the brainwashing done by years of C programming\n\nBetter algorithm led to 10x size decrease in Virtuoso.\n\nExample 2: Amazon Web Services (Chris Newcombe et al., 2015) {#example-2-amazon-web-services}\n\nAWS uses formal methods (TLA+). Key insights:\n\nFormal methods allow for finding bugs that other methods cannot discover\nFormal methods are routinely applied to the design of complex,\n    real-world software\nThey are surprisingly applicable to daily work\n\nTLA+ is also used at Microsoft.\n\nTL;DR {#tl-dr}\n\nUse TLA+.\n\nRelated {#related}\n\nHoare logic\n\nBibliography\nNewcombe, C., Rath, T., Zhang, F., Munteanu, B., Brooker, M., & Deardeuff, M., How Amazon Web Services Uses Formal Methods, Communications of the ACM, 58(4), 66–73 (2015).  http://dx.doi.org/10.1145/2699417 ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/asian_cinema",
        "title": "Asian Cinema",
        "content": "\nFollowing the success of Parasite, there is heightened interest in\nAsian cinema.\n\n\"since parasite won 4 awards today at the oscars, here's a thread of bong joo...\n",
        "tags": []
    },
    {
        "uri": "/zettels/sonke_ahrens",
        "title": "Sonke Ahrens",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/mental_models",
        "title": "Mental Models",
        "content": "\nTo Refile {#to-refile}\n\nTODO An Astronaut's Guide to Mental Models {#an-astronaut-s-guide-to-mental-models}\n",
        "tags": []
    },
    {
        "uri": "/zettels/elisp_bufferpassing_style",
        "title": "Elisp: Buffer-passing Style",
        "content": "\ntags\n: Emacs Lisp\n\nRather than have the callee instantiate the buffer, the caller\ninstantiates the buffer, and passes it implicitly as the current\nbuffer. The callee fills it with something. The caller should use\nsomething like with-temp-buffer so that the buffer has a clean\nlife-cycle, fully managed by the caller (Source).\n",
        "tags": []
    },
    {
        "uri": "/zettels/richard_feynman",
        "title": "Richard Feynman",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/writing_with_zettekasten",
        "title": "Writing with Zettekasten",
        "content": "\nThe status quo for writing is:\n\nFind topic/research question\nResearch/find literature\nRead and take notes\nDraw conclusions / outline text\nWrite\n\nThis process is extremely linear. The student actually has to do many\nother things that aren't part of writing the paper, and that has to\nhappen before the process of writing begins.\n\nIf we flip this process in Zettelkasten, and make the notes the\nthinking process rather than a record of of the thinking process.\n\nOnly written down ideas count\nBottom-up\nQuestions arise from material\nCategories emerge\nExternalized system to think in\nHighly standardized\nProcess oriented\n\nAs a result:\n\nThinking, connecting and understanding become concrete actions\nFocus is on the process, not the outcome\nWriting is broken down into reasonable steps (one note at a time)\nThe value of each idea compounds\nThere is a clear distinction between permanent and temporary notes\n",
        "tags": []
    },
    {
        "uri": "/zettels/web_performance",
        "title": "Web Performance",
        "content": "\ntags\n: Web Development\n\nResources {#resources}\n\nthedaviddias/Front-End-Performance-Checklist\n",
        "tags": []
    },
    {
        "uri": "/zettels/robotics_algorithms",
        "title": "Robotics Algorithms",
        "content": "\nResources {#resources}\n\nAtsushiSakai/PythonRobotics\n",
        "tags": []
    },
    {
        "uri": "/zettels/tom_tromey",
        "title": "Tom Tromey",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/learning_complex_information",
        "title": "Learning Complex Information",
        "content": "\ntags\n: Learning, Reading, Note-taking\n",
        "tags": []
    },
    {
        "uri": "/zettels/reading",
        "title": "Reading",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/learning_how_to_learn",
        "title": "Learning How To Learn",
        "content": "\ntags\n: Learning\n\nWhat is learning? {#what-is-learning}\n\nThe brain operates in 2 modes: diffuse mode and focused mode. It is\nimportant to exploit both modes for efficient learning.\n\nTips:\n\nWhen procrastinating, just start.\nWhen learning something new, take time to rest and come back to it.\n    One effective method is to sleep right after learning something.\nSleep and exercise are important.\n\nChunking {#chunking}\n\nChunking is the act of grouping concepts into compact packages of\ninformation for easy access.\n\nTips:\n\nTurn off distractions.\nSolve the problem yourself; avoid the illusion of competence.\nA chunk you have mastered in one area can often help you more\n    easily learn other chunks of information.\nLearn the large-picture concept first, before diving into the details.\nTry to recall new material learnt, in different places.\nTest yourself.\nDon't always trust your initial intuition.\nInterleave problems from different chapters. This helps connect\n    different chunks of information.\n\nProcrastination {#procrastination}\n\nHabit is an energy saver, it reduces the need for focus when\nperforming tasks.\n\nGetting rid of procrastination can be done by:\n\nRecognizing the trigger that launches you into a bad habitual routine.\nActively focus on rewiring bad habits.\nReward yourself for achieving step goals towards breaking the habits.\nChange the underlying belief that causes the bad habit.\n\nMemory {#memory}\n\nVisual memory is powerful. Images help encapsulate a hard to\n    remember concept.\nConsider spaced repetition.\nCreate meaningful structure to help remember.\nNumbers can be memorised by linking to events.\nMemory palace technique: use a familiar place, and associate visual\n    images of things with physical places.\nUse metaphors and analogies to help memorise and understand\n    different concepts\n\nStudying Practices {#studying-practices}\n\nStep back and check, to consolidate knowledge.\nFind focused people to analyse your work with.\nDon't fool yourself.\n\nChecklist:\n\nDid you make a serious effort to understand the text?\nDid you attempt to outline every homework problem solution?\nDid you understand all your homework problem's solutions?\n\nTest-taking Technique {#test-taking-technique}\n\nTake a quick look at the test.\nStart with the hardest problem, and look at it for a few minutes.\n    Let the mind work on it while your work on other problems.\nSolve what you can, and move back to the hard problem.\n",
        "tags": []
    },
    {
        "uri": "/zettels/learning_how_to_do_hard_things",
        "title": "Learning How To Do Hard Things",
        "content": "\ntags\n: Learning\n\npaper\n: (David MacIver, 2019)\n\nThe key is to isolate one aspect of the problem that is difficult and\nwork on it. This provides a direct feedback loop.\n\n> 1.  Find something that is like the hard thing but is easy.\n> 2.  Modify the easy thing so that it is like the hard thing in exactly one way that you find hard.\n> 3.  Do the modified thing until it is no longer hard.\n> 4.  If you get stuck, do one of the following:\n>     1.  Go back to step 3 and pick a different way in which the problem is hard.\n>     2.  Recursively apply the general system for learning to do hard things to the thing you’re stuck on.\n>     3.  Go ask an expert or a rubber duck for advice.\n>     4.  If you’re still stuck after trying the first three, it’s possible that you may have hit some sort of natural difficulty limit and may not be able to make progress.\n> 5.  If the original hard thing is now easy, you’re done. If not, go back to step 2.\n\nBibliography\nMacIver, D. R. (2019). How to do hard things. Retrieved from https://www.drmaciver.com/2019/05/how-to-do-hard-things/. Online; accessed 20 May 2019. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/feynman_technique",
        "title": "Feynman Technique",
        "content": "\ntags\n: Richard Feynman\n",
        "tags": []
    },
    {
        "uri": "/zettels/how_to_read_a_book",
        "title": "How To Read A Book",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/git_scalar",
        "title": "Git Scalar",
        "content": "\ntags\n: Git\n\nGit has trouble handling large repositories. This problem is resolved\nusing VFS for Git. This is essentially a wrapper around GVFS,\nreplacing the git command.\n\n1. Focusing on Files that Matter {#1-dot-focusing-on-files-that-matter}\n\nscalar clone uses Git's sparse-checkout feature in cone mode,\ncloning a subset of the Git repositories' files. Scalar uses Git's\nupdated index file format, which reduces the size of the Git index.\nThe Git index is a list of every tracked path at current HEAD.\n\nWith Scalar, the populated size is at most as large as the number of\nfiles in tracked files.\n\nScalar also configures the Git repository to work better with modified\nfiles using Git's fsmonitor feature. Without this, Git needs to scan\nthe entire working directory to find which paths were modified.\n\n2. Reducing Object Transfer {#2-dot-reducing-object-transfer}\n\nThe VFS for Git protocol reduces object transfer.\n\n3. Reducing Waiting for Expensive Operations {#3-dot-reducing-waiting-for-expensive-operations}\n\nDisable auto Git garbage collection by setting gc.auto=0, and run\n    do incremental GC instead.\nPeriodically run git fetch\nWrite the commit graph using the incremental format\nCleanup loose objects\nGit multi-pack-files to efficiently pack and store Git objects. It\n    does some sort of reference counting: unreferenced pack-files can\n    be deleted.\n",
        "tags": []
    },
    {
        "uri": "/zettels/vfs_for_git",
        "title": "VFS for Git",
        "content": "\nVFS for Git is a virtualized filesystem used to bypass assumptions\nabout repository size, allowing Git repositories to scale up to large\nrepositories.\n\nWith GVFS, an initial clone downloads a set of pack-files containing\nonly commits and trees. These objects are sufficient for generating a\nview of the working directory, and examining the commit history with\ngit log.\n\nGVFS allows dynamically downloading objects as needed.\n",
        "tags": []
    },
    {
        "uri": "/zettels/version_control",
        "title": "Version Control",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/definition_of_deep_learning",
        "title": "Definition of Deep Learning",
        "content": "\nThere are many conflicting definitions of Deep Learning, here's some\ndefinitions by various people:\n\nYann LeCun\n: DL is constructing networks of parameterized\n    functional modules & training them from examples using\n    gradient-based optimization (Source)\n    Definition is orthogonal to the learning paradigm: paradigm,\n        reinforcement, supervised, or self-supervised\n",
        "tags": []
    },
    {
        "uri": "/zettels/yann_lecun",
        "title": "Yann LeCun",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/noninformative_priors",
        "title": "Non-informative Priors",
        "content": "\nNon-informative priors are used in Bayesian Inference, in the\nsituation where no prior information exists, or inference based\ndominantly on the data is desired. We wish to find a distribution\n\\\\(p(\\theta)\\\\) that contained \"no information\" about \\\\(\\theta\\\\) (favours no\nvalue of \\\\(\\theta\\\\) over another).\n\nAn example of such a non-informative prior in the discrete space would\nbe a uniform distribution over \\\\(\\theta\\\\).\n",
        "tags": []
    },
    {
        "uri": "/zettels/reference_prior",
        "title": "Reference Prior",
        "content": "\nThe notion of reference prior is similar to that of a [non-informative\nprior]({{}}), but there are subtle differences. The uniform prior is arguably\nnon-informative, but is it not a good reference, because it is not\nalways invariant under reparamaterization.\n\nConsider a reparameterization \\\\(\\gamma = \\log \\theta\\\\), converting the\nsupport of the parameter to the real line. The prior on \\\\(\\gamma\\\\) is\ngiven by:\n\n\\begin{equation}\n  p\\_\\gamma (\\gamma) = p(\\theta)|J| = |J|\n\\end{equation}\n\nwhere \\\\(|J| = \\frac{d \\theta}{d \\gamma}\\\\) from the [Change of Variables\nTheorem]({{}}). Then\n\n\\begin{equation}\n  p\\_\\gamma (\\gamma) = e^\\gamma, - \\infty }}).\n",
        "tags": []
    },
    {
        "uri": "/zettels/change_of_variables_theorem",
        "title": "Change of Variables Theorem",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/jeffreys_prior",
        "title": "Jeffreys Prior",
        "content": "\nThe Jeffrey's prior is an easy-to-compute reference prior that is\ninvariant to transformation, used in Bayesian Inference. If the model\nonly has a univariate parameter \\\\(\\theta\\\\), the prior is given by:\n\n\\begin{equation}\n  p(\\theta) \\propto \\sqrt{I(\\theta)}\n\\end{equation}\n\nwhere \\\\(I(\\theta)\\\\) is the expected Fisher information in the model.\n\nIf \\\\(\\mathbf{\\theta}\\\\) is multi-dimensional, then the Jeffrey's prior is\ngiven by:\n\n\\begin{equation}\n  p(\\theta) \\propto \\sqrt{\\operatorname{det}\\\\{l(\\theta)\\\\}}\n\\end{equation}\n\nwhere I is the Fisher information matrix. When the number of\ndimensions is large, this method becomes cumbersome. A common approach\nis to obtain non-informative priors for the parameters individually,\nand form the joint prior as a product of these individual priors.\n",
        "tags": []
    },
    {
        "uri": "/zettels/fisher_information",
        "title": "Fisher information",
        "content": "\nThe Fisher information in a univariate model is given by:\n\n\\begin{equation}\n  I(\\theta)=-\\mathrm{E}\\_{\\mathbf{Y} | \\theta}\\left[\\frac{\\partial^{2}}{\\partial \\theta^{2}} \\log p(\\boldsymbol{y} | \\theta)\\right]\n\\end{equation}\n\nfor data \\\\(\\mathbf{Y}\\\\). In a multivariate model, the Fisher information\nmatrix, has \\\\(ij\\\\) entry:\n\n\\begin{equation}\n  I\\{i j}(\\boldsymbol{\\theta})=-\\mathrm{E}\\{\\mathbf{Y} | \\theta}\\left[\\frac{\\partial^{2}}{\\partial \\theta\\{i} \\partial \\theta\\{j}} \\log p(\\boldsymbol{y} | \\boldsymbol{\\theta})\\right]\n\\end{equation}\n",
        "tags": []
    },
    {
        "uri": "/zettels/an_opinionated_guide_to_ml_research",
        "title": "Article: An Opinionated Guide to ML Research",
        "content": "\ntags\n: Machine Learning, Research\n\nauthor\n: John Schulman\n\nsource\n: An Opinionated Guide to ML Research\n\nThe keys to success are \\*working on the right problems, making\n    continual progress on them, and achieving continual personal\n    growth.\\* This essay is comprised of three sections, each covering\n    one of these topics.\n\nChoosing Problems {#choosing-problems}\n\nDevelop good taste for what problems to work on.\n\nRead a lot of papers, and assess them critically.\nWork in a research group with other people working on similar topics.\nSeek advice from experienced researchers on what to work on.\nReflect on what research is useful;\n    When is theory useful?\n    What causes some ideas to get wide uptake?\n\nIdea-Driven vs Goal-Driven Research {#idea-driven-vs-goal-driven-research}\n\nIdea driven\n: follow some sector of literature, and improve on\n    existing ideas.\n    Cons: High risk of getting snooped, or duplicating work,\n        requires deep understanding of subject\n\nGoal-driven\n: Have vision of new AI capabilities, and experiment\n    with diferent methods across the literature. Develop your own\n    methods to improve on them.\n    Pros: much more motivating. goals also give differentiating\n        perspective from rest of the community.\n\nJohn Schulman recommends goal-driven research.\n\nTips for Goal-Driven research {#tips-for-goal-driven-research}\n\nDon't take the goal too literally. **Restrict yourself to general\n    solutions.**\nAim high, and climb incrementally towards high goals\n    Keep a notebook, and record daily ideas and experiments\n        (Note-taking). John Schulman creates an entry for each day, and\n        conducts a review to condense entries every 1 or 2 weeks.\nKnow when to switch problems. Also, don't switch problems too\n    often. Notice the dead-ends in half-finished projects, and if there\n    aren't any, make a commitment towards following through in the\n    future.\nHave a fixed time budget for trying out new ideas that diverge from\n    the main line of work.\n\nPersonal Development {#personal-development}\n\nRead textbooks, theses and papers, reimplement algorithms from these\n    sources. Especially textbooks, because they condense information in\n    an ordered fashion, and using proper notation!\nTheses are a good place to find a literature review of an active field.\n",
        "tags": []
    },
    {
        "uri": "/zettels/research",
        "title": "Research",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/john_schulman",
        "title": "John Schulman",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/actor_critic",
        "title": "Actor-Critic",
        "content": "\nActor-Critic improves on Policy Gradients methods by introducing a\ncritic.\n\nRecall the objective:\n\n\\begin{equation}\n  \\nabla\\{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum\\{i=1}^{N} \\sum\\{t=1}^{T} \\nabla\\{\\theta} \\log \\pi\\{\\theta}\\left(\\mathbf{a}\\{i, t} | \\mathbf{s}\\{i, t}\\right) \\hat{Q}\\{i, t}\n\\end{equation}\n\nThe question we want to address is: **can we get a better estimate of\nthe reward-to-go?**\n\nOriginally, we were using the single-trajectory estimate of the\nreward-to-go. If we knew the true expected reward-to-go, then we would\nhave a lower variance version of the policy gradient.\n\nWe define the advantage function as \\\\(A^\\pi(s\\t,a\\t) = Q^\\pi(s\\_t,\na\\t) - V^\\pi(s\\t)\\\\). \\\\(V^\\pi(s\\_t)\\\\) can be used as baseline \\\\(b\\\\), and we\nobtain the objective:\n\n\\begin{equation}\n\\nabla\\{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum\\{i=1}^{N} \\sum\\{t=1}^{T} \\nabla\\{\\theta} \\log \\pi\\{\\theta}\\left(\\mathbf{a}\\{i, t} | \\mathbf{s}\\{i, t}\\right) A^\\pi(s\\{i,t}, a\\_{i,t})\n\\end{equation}\n\nValue Function Fitting {#value-function-fitting}\n\nRecall:\n\n\\begin{array}{l}\n  {Q^{\\pi}\\left(\\mathbf{s}\\_{t},\n  \\mathbf{a}\\{t}\\right)=\\sum\\{t^{\\prime}=t}^{T}\n  E\\{\\pi\\{\\theta}}\\left[r\\left(\\mathbf{s}\\_{t^{\\prime}},\n  \\mathbf{a}\\{t^{\\prime}}\\right) | \\mathbf{s}\\{t},\n  \\mathbf{a}\\_{t}\\right]} \\\\\\\\\\\\\n  {V^{\\pi}\\left(\\mathbf{s}\\{t}\\right)=E\\{\\mathbf{a}\\_{t} \\sim\n  \\pi\\{\\theta}\\left(\\mathbf{a}\\{t} |\n  \\mathbf{s}\\{t}\\right)}\\left[Q^{\\pi}\\left(\\mathbf{s}\\{t},\n  \\mathbf{a}\\_{t}\\right)\\right]} \\\\\\\\\\\\\n  {A^{\\pi}\\left(\\mathbf{s}\\_{t},\n  \\mathbf{a}\\{t}\\right)=Q^{\\pi}\\left(\\mathbf{s}\\{t},\n  \\mathbf{a}\\{t}\\right)-V^{\\pi}\\left(\\mathbf{s}\\{t}\\right)} \\\\\\\\\\\\\n  {\\nabla\\{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum\\{i=1}^{N}\n  \\sum\\{t=1}^{T} \\nabla\\{\\theta} \\log \\pi\\{\\theta}\\left(\\mathbf{a}\\{i,\n  t} | \\mathbf{s}\\{i, t}\\right) A^{\\pi}\\left(\\mathbf{s}\\{i, t},\n  \\mathbf{a}\\_{i, t}\\right)}\n\\end{array}\n\nWe can choose to fit \\\\(Q^{\\pi}\\\\), \\\\(V^{\\pi}\\\\) or \\\\(A^{\\pi}\\\\), each have\ntheir pros and cons.\n\nWe can write:\n\n\\begin{equation}\n  Q^\\pi (s\\t, a\\t) \\approx r(s\\t, a\\t) + V^{\\pi}(s\\_{t+1})\n\\end{equation}\n\n\\begin{equation}\n  A^{\\pi}(s\\t, a\\t) \\approx r(s\\t, a\\t) + V^{\\pi}(s\\{t+1}) - V^{\\pi}(s\\t)\n\\end{equation}\n\nClassic actor-critic algorithms fit \\\\(V^\\pi\\\\), and pay the cost of 1\ntime-step to get \\\\(Q^\\pi\\\\).\n\nWe do Monte Carlo evaluation with function approximation, estimating\n\\\\(V^\\pi (s\\_t)\\\\) as:\n\n\\begin{equation}\n  V^\\pi (s\\t) \\approx \\sum\\{t'=t}^{T}r(s\\{t'}, a\\{t'})\n\\end{equation}\n\nOur training data consists of \\\\(\\left\\\\{\\left(s\\{i,t}, \\sum\\{t'=t}^T r\n(s\\{i,t'}, a\\{i,t'})\\right)\\right\\\\}\\\\), and we can just fit a neural\nnetwork with regression.\n\nAlternatively, we can decompose the ideal target, and use the old\n\\\\(V^\\pi\\\\):\n\n\\begin{equation}\n  y\\{i,t} = \\sum\\{t'=t}^{T} E\\{\\pi\\{\\theta}} [r(s\\{t'}, a\\t') |\n  s\\{i,t}] \\approx r(s\\{i,t}, a\\{i,t}) + \\hat{V}\\{\\phi}^\\pi(s\\_{i,t+1})\n\\end{equation}\n\nThis is a biased estimate, but might have much lower variance. This\nworks when the policy does not change much and the previous value\nfunction is a decent estimate. Since it is using a the previous value\nfunction, it is also called a bootstrapped estimate.\n\nDiscount Factors {#discount-factors}\n\nThe problem with the bootstrapped estimate is that with long horizon\nproblems, \\\\(\\hat{V}\\_\\phi^\\pi\\\\) can get infinitely large. A simple trick\nis to use a discount factor:\n\n\\begin{equation}\n  y\\{i,t} \\approx r(s\\{i,t}, a\\{i,t}) + \\gamma \\hat{V}\\\\phi^\\pi(s\\_{i,t+1})\n\\end{equation}\n\nwhere \\\\(\\gamma \\in [0,1]\\\\).\n\nWe can think of \\\\(\\gamma\\\\) as changing the MDP, introducing a death\nstate with reward 0, and the probability of transitioning to this\ndeath state is \\\\(1 - \\gamma\\\\). This causes the agent to prefer better\nrewards now than later.\n\n{{}}\n\nWe can then modify \\\\(\\hat{A}^\\pi\\\\):\n\n\\begin{equation}\n  \\hat{A}^{\\pi}(s\\t, a\\t) \\approx r(s\\t, a\\t) + \\gamma \\hat{V}^{\\pi}(s\\{t+1}) - \\hat{V}^{\\pi}(s\\t)\n\\end{equation}\n\n{{}}\n\n\\\\(\\gamma\\\\) can be interpreted as a way to limit variance, and prevent\nthe infinite sum (think about what happens when \\\\(\\gamma\\\\) gets bigger).\n\nAlgorithm {#algorithm}\n\nsample \\\\(\\left\\\\{s\\i, a\\i\\right\\\\}\\\\) from \\\\(\\pi\\_{\\theta}(a\\s)\\\\)\nFit \\\\(\\hat{V}\\_\\phi^{\\pi}(s)\\\\) to the sampled reward sums\nEvaluate \\\\(\\hat{A}^\\pi(s\\i, a\\i) = r(s\\i a\\i) + \\hat{V}\\\\phi^\\pi(s\\i')-\\hat{V}\\\\phi^\\pi(s\\i)\\\\)\n\\\\(\\nabla\\{\\theta} J(\\theta) \\approx \\sum\\i \\nabla\\{\\theta} \\log \\pi\\{\\theta}\\left(\\mathbf{a}\\{i} | \\mathbf{s}\\{i}\\right) \\hat{A}^{\\pi}\\left(\\mathbf{s}\\_{i},\n       \\mathbf{a}\\_{i}\\right)\\\\)\n\\\\(\\theta \\leftarrow \\theta + \\alpha \\nabla\\_{\\theta}J(\\theta)\\\\)\n\nOnline Actor-critic {#online-actor-critic}\n\nonline actor-critic uses a single sample batch, which is a bad idea in\nlarge neural networks. We need to use multiple samples to perform\nupdates.\n\nThe purpose of multiple workers here is not to make the algorithm\nfaster, but to make it work by increasing the batch size.\n\n{{}}\n\n{{}}\n\nGeneralized Advantage Estimation {#generalized-advantage-estimation}\n\n\\begin{equation}\n  \\hat{A}\\{n}^\\pi (s\\t, a\\t) = \\sum\\{t'=t}^{t+n}\\gamma^{t'-t}\n  r(s\\{t'}, a\\{t'}) - \\hat{V}\\{\\phi}^\\pi (s\\t) + \\gamma^n \\hat{V}\\\\phi^\\pi(s\\{t+n})\n\\end{equation}\n\n\\begin{equation}\n  \\hat{A}\\{GAE}^\\pi (s\\t, a\\t) = \\sum\\{n=1}^{\\infty} w\\n \\hat{A}\\n^\\pi\n  (s\\t, a\\t)\n\\end{equation}\n\nis some weighted combination of n-step returns. If we choose \\\\(w\\_n\n\\propto \\lambda^{n-1}\\\\), we can show that:\n\n\\begin{equation}\n  \\hat{A}\\{GAE}^\\pi (s\\t, a\\t) = \\sum\\{n=1}^{\\infty} (\\gamma\n  \\lambda)^{t'-t} \\delta\\_{t'}\n\\end{equation}\n\nwhere\n\n\\begin{equation}\n\\delta\\{t'} = r(s\\{t'}, a\\{t'}) + \\gamma \\hat{V}\\\\phi^\\pi (s\\{t'+1}) - \\hat{V}\\\\phi^\\pi(s\\_{t'})\n\\end{equation}\n\n**the role of \\\\(\\gamma\\\\) and the role of \\\\(\\lambda\\\\) turns out to b\nsimilar, trading off bias and variance!**\n\nNeed to balance between learning speed, stability.\n\nConservative Policy Iteration (CPI)\n    propose surrogate objective, guarantee monotonic improvement under\n        specific state distribution\nTrust Region Policy Optimization (TRPO)\n    approximates CPI with trust region constraint\nProximal Policy Optimization (PPO)\n    replaces TRPO constraint with RL penalty + clipping\n        (computationally efficient)\nSoft Actor-Critic (SAC)\n    stabilize learning by jointly maximizing expected reward and\n        policy entropy (based on maximum entropy RL)\nOptimistic Actor Critic (OAC)\n    Focus on exploration in deep Actor critic approaches.\n    Key insight: existing approaches tend to explore conservatively\n    Key result: Optimistic exploration leads to efficient, stable\n        learning in modern Actor Critic methods\n\nResources {#resources}\n\nCS285 Fa19 9/18/19 - YouTube\n",
        "tags": []
    },
    {
        "uri": "/zettels/antifragile_ideas",
        "title": "Anti-fragile Ideas",
        "content": "\nThis system was originally discussed by John Carmack here. We want to\nwork on ideas that are not brittle, but how do we know they aren't?\n\nWhen we get the idea, we should instantly try to defeat it\nIf the idea survives the brutal scrutiny, then it has legs for\n    further investigation\nIf the idea is implemented, and it works, great\nElse, we can quickly move on to the next idea\n\nThis allows us to generate more ideas by quickly going through ideas\nthat may not work. It also has a quick filter to poor ideas.\n",
        "tags": []
    },
    {
        "uri": "/zettels/api_design",
        "title": "API Design",
        "content": "\n§game\\api\\design\n",
        "tags": []
    },
    {
        "uri": "/zettels/arguments_against_bayesian_inference",
        "title": "Arguments Against Bayesian Inference",
        "content": "\ntags\n: §bayesian\\_inference\n\nWhereas frequentist methods derive solutions via inventing estimators\n(a multitude of them may exist) and computing a likelihood function,\nBayesian methods only offer one answer to a well-posed problem.\n\nThe Bayesian inference method is conditioned on assumptions (for\nexample, the prior). Some say the prior introduces subjectivity. **But\nhow can one make inferences without assumptions?** Bayesian methods\nforce us to make these assumptions explicit.\n\nOnce the assumptions are made, the inferences are objective, unique,\nand can be agreed upon by everyone. These assumptions are easy to\nmodify, and we can quantify the sensitivity of our inferences to our\nassumptions. It also quantifies the uncertainty in our inferences.\n\nThere is a common misconception that the aim of inference is to find\nthe most probable explanation for some data. While the most probable\nexplanation may be of some interest, this is only the peak of a\nprobability distribution, and it is the whole distribution of\nexplanations itself that is of interest. _The most probable outcome\nfrom a source is often not a typical outcome from that source._\n",
        "tags": []
    },
    {
        "uri": "/zettels/arguments_against_zettelkasten",
        "title": "Arguments Against Zettelkasten",
        "content": "\ntags\n: §zettelkasten\n\nDensely interconnecting ideas across domains results in complex\n    mental models, may obscure reality (Source: [Venkatesh Rao, 2016\n    podcast with Farnam Street](https://fs.blog/venkatesh-rao/))\n",
        "tags": []
    },
    {
        "uri": "/zettels/arm",
        "title": "ARM Assembly Programming",
        "content": "\ntags\n: §operating\\systems, §prog\\lang\n\nBackground {#background}\n\nIn a machine language, instructions are encoded as 0's and 1's. This\nis unwieldy for programmers  to write. Hence, to tell processors what\nto execute, programmers use an assembly language, writing these\ninstructions in textual form, for example:\n\nMOV R9, R3\n\nAn assembler translates the assembly language into machine language.\n\nThere are many machine languages, each designed with a processor in\nmind, enabling fast and simple circuits to be built to execute the\ninstructions. Each machine language requires a corresponding assembly\nlanguage, since the assembly language must support a different set of\nmachine instructions. The design of the machine language encoding is\ncalled the instruction set architecture (ISA).\n\nMemory Organization {#memory-organization}\n\nByte-addressable, 32-bit address space\nlittle- or big-endian addressable\n32-bit word length\nword, half-word and byte data transfers to and from processor registers\nword and half-world transfers must be aligned\n\nArm {#arm}\n",
        "tags": []
    },
    {
        "uri": "/zettels/art",
        "title": "Art",
        "content": "\nResources {#resources}\n\nArt Resources and Tutorials\n",
        "tags": []
    },
    {
        "uri": "/zettels/artificial_intelligence",
        "title": "Artificial Intelligence",
        "content": "\ntags\n: §machine\\_learning\n\nWhat is Artificial Intelligence? {#what-is-artificial-intelligence}\n\nDesigning agents that act rationally (e.g. through maximising a reward\nfunction).\n\nHumans often act in ways that do not maximise their own benefit\n(irrational).\n\nActing Humanly: Turing Test {#acting-humanly-turing-test}\n\nA computer would require:\n\nnatural language processing\nknowledge representation\nautomated reasoning\nmachine learning\n\nThinking Humanly {#thinking-humanly}\n\nCognitive science brings together computer models and experimental\ntechniques in psychology to construct testable and provable theories\nof the human mind.\n\nThinking rationally {#thinking-rationally}\n\nTaking informal knowledge and expressing it in logical terms.\n\nActing Rationally {#acting-rationally}\n\nA rational agent is one that acts so as to achieve the best outcome\nor,when there is uncertainty, the best expected outcome.\n\nAn agent is a function from percept histories to actions, i.e. \\\\(f: P^\\*\n\\rightarrow A\\\\). We seek the best-performing agent for a certain task;\nmust consider computation limits.\n\nIntelligent Agents {#intelligent-agents}\n\nAgents perceive the environment through sensors\nAgents act upon the environment through actuators\n\nRational Agents {#rational-agents}\n\nFor each possible percept sequence, select an action that is\n    expected to maximise its performance measure. The performance\n    measure is a function of a given sequence of environment states.\nGiven the evidence provided by the percept sequence and whatever\n    built-in knowledge the agent has.\nAgents can perform actions that help them gather useful information\n    (exploration)\nAn agent is autonomous if its behaviour is determined by its own\n    experience (with ability to learn and adapt)\n\nExploration vs Exploitation {#exploration-vs-exploitation}\n\nDoing actions that modify future percepts (information gathering) is\nan important part of rationality. In most scenarios, agents don't know\nthe entire environment a priori.\n\nSpecifying Task Environment (PEAS) {#specifying-task-environment--peas}\n\nPerformance measure\nEnvironment\nActuators\nSensors\n\nProperties of Task Environments {#properties-of-task-environments}\n\nFully observable\n: whether an agent's sensors gives it access to\n    the complete state of the environment at any given point in time\n\nDeterministic\n: if the next state is completely determined by the\n    current environment. Otherwise it is stochastic.\n\nEpisodic\n: whether an agents experience is divided into atomic\n    episodes. In each episode, an agent receives a percept\n    and performs a single action. In sequential\n    environments short-term actions can have long-term\n    consequences. For this reason, episodic environments are\n    generally simpler.\n\nStatic\n: whether the environment can change while the agent is\n    deliberating.\n\nDiscrete\n: whether the state of the environment, how time is\n    handled, and the percepts and actions of the agent\n    discretely quantized.\n\nSingle agent\n: in some environments, for example chess, there are\n    multiple agents acting in the same environment.\n    cooperative: if the two agents need to work together.\n    competitive: if the two agents are working against each other.\n\nKnown\n: whether the agent knows the outcome of its actions.\n\nTable-Driven Agent {#table-driven-agent}\n\nSimple to implement, and works. However, the number of table entries\nis exponential in time: \\\\(\\text{#percepts}^\\text{time}\\\\). Hence it is\ndoomed to failure. The key challenge to AI is to produce rational\nbehaviour from a small program rather than a vast table.\n\nReflex agents {#reflex-agents}\n\nA simple reflex agent is one that selects actions on the basis of the\ncurrent percept, ignoring the rest of the percept history. A\ncondition-action rule is triggered upon processing the current\npercept. E.g. if the car in front is braking, then brake too.\n\nBasing actions on only the current percept can be highly limiting, and\ncan also lead to infinite loops. Randomized actions of the right kind\ncan help escape these infinite loops.\n\nModel-based Reflex Agents {#model-based-reflex-agents}\n\nThe agent maintains some internal state that depends on percept\nhistory and reflects at least some of the unobserved aspects of the\ncurrent state. Information about how the world evolves independently\nfrom the agent is encoded into the agent. This knowledge is called a\nmodel of the world, and this agent is hence a model-based agent.\n\nGoal-based agents {#goal-based-agents}\n\nKnowing about the current state of the environment may not be enough\nto decide on what to do. Agents may need goal information that\ndescribes situations that are desirable. Sometimes goal-based action\nselection is straightforward, but in others searching and planning\nare required to achieve the goal. Goal-based agents are flexible\nbecause the knowledge that supports its decisions is represented\nexplicitly and can be modified, although it is less efficient.\n\nUtility-based agents {#utility-based-agents}\n\nGoals provide a binary distinction between good and bad states. A more\ngeneral performance measure should allow a comparison between world\nstates according to exactly how good it is to the agent. An agent's\nutility function is an internalisation of the performance measure.\nAn agent chooses actions to maximise its expected utility. A\nutility-based agents has to model and keep track of its environment.\n\nLearning agents {#learning-agents}\n\nA learning agent can be divided into four conceptual components.\n\nlearning element\n: responsible for making improvements\n\nperformance element\n: responsible for selecting extrenal actions\n\nproblem generator\n: suggests actions that will lead to new and\n    informative experiences\n\nthe learning element takes in feedback from the critic on how the\nagent is doing and determines show the performance element should be\nmodified to do better in the future.\n\nState representations {#state-representations}\n\nAtomic Representation\n\n    In an atomic representation each state of the world is indivisible,\n    and has no internal structure. Search, game-playing, hidden Markov\n    models and Markov decision processes all work with atomic\n    representations.\n\nFactored Representation\n\n    A factored representation splits up each state into a fixed set of\n    variables or attributes, each of which can have a value.\n\n    Constraint satisfaction algorithms, propositional logic, planning,\n    Bayesian networks and machine learning algorithms work with factored\n    representations.\n\nStructured Representations\n\n    Structured representations underlie relational databases and\n    first-order logic, first-order probability models, knowledge-based\n    learning and much of natural language understanding.\n\nImplications\n\n    A more expressive representation can capture, at least as concisely, a\n    everything a more expressive one can capture, plus more. On the other\n    hand, reasoning and learning become more complex as the expressive\n    power of the representation increases.\n\nProblem-Solving {#problem-solving}\n\nProblem-solving agents use atomic representations, as compared to\ngoal-based agents, which use more advanced factored or structured\nrepresentations.\n\nThe process of looking for a sequence of actions that reaches the goal\nis called search. A search algorithm takes a problem as input and\nreturns a solution in the form of an action sequence.\n\nClassical Search {#classical-search}\n\nThis addresses observable, deterministic, and known environments where\nthe solution is a sequence of actions.\n\nHow Search Algorithms Work {#how-search-algorithms-work}\n\nSearch algorithms consider various possible action sequences. The\npossible action sequences start at the initial state form a _search\ntree_.\n\nSearch algorithms require a data structure to keep track of the search\ntree that is being constructed.\n\nstate\n: state in the state space to which the node corresponds\n\nparent\n: the node in the search tree that generated this node\n\naction\n: the action that was applied to the parent to generate this node\n\npath-cost\n: the cost, traditionally denoted by \\\\(g(n)\\\\), of the path\n    from the initial state to the node, as indicated by the\n    parent pointers\n\nMeasuring Performance {#measuring-performance}\n\ncompleteness\n: is the algorithm guaranteed to find a solution if\n    it exists?\n\noptimality\n: does the strategy find the optimal solution?\n\ntime complexity\n: how long does it take to find a solution?\n\nspace complexity\n: how much memory is required to do the search?\n\nUninformed Search Strategies {#uninformed-search-strategies}\n\nBreadth-first Search\n\n    The root node is expanded first, then all the successors of the root\n    node are expanded next, then their successors, and so on.\n\n    | performance      | rating       |\n    |------------------|--------------|\n    | completeness     | YES          |\n    | optimal          | NO           |\n    | time complexity  | \\\\(O(b^d)\\\\) |\n    | space complexity | \\\\(O(b^d)\\\\) |\n\n    The shallowest node may not be the most optimal node.\n\n    The space used in the explored set is \\\\(O(b^{d-1})\\\\) and the space\n    used in the frontier is \\\\(O(b^d)\\\\).\n\n    In general, exponential-complexity search problems cannot be solved by\n    uninformed methods for any but the smallest instances.\n\nUniform-cost Search\n\n    Uniform-cost search expands the node \\\\(n\\\\) with the lowest path\n    cost \\\\(g(n)\\\\). The goal test is applied to a node when it is selected\n    for expansion rather than when it is first generated.\n\n    This is equivalent to BFS if all step costs are qual.\n\n    | performance  | rating                                                                                      |\n    |--------------|---------------------------------------------------------------------------------------------|\n    | completeness | MAYBE                                                                                       |\n    | optimal      | YES                                                                                         |\n    | time         | \\\\(O(b^{1+\\lfloor{\\frac{C^\\}{\\epsilon}}\\rfloor})\\\\), where \\\\(C^\\\\\\) is the optimal cost. |\n    | space        | \\\\(O(b^{1+\\lfloor{\\frac{C^\\*}{\\epsilon}}\\rfloor})\\\\)                                        |\n\n    Completeness is guaranteed only if the cost of every step exceeds some\n    small positive constant \\\\(\\epsilon\\\\). an infinite loop may occur if\n    there is a path with an infinite sequence of zero-cost actions.\n\nDepth-first Search\n\n    Always expands the deepest node in the current frontier of the\n    search tree.\n\n    | performance      | rating                                |\n    |------------------|---------------------------------------|\n    | completeness     | YES                                   |\n    | optimal          | NO                                    |\n    | time complexity  | \\\\(O(b^m)\\\\)                          |\n    | space complexity | \\\\(O(b^m)\\\\), \\\\(O(m)\\\\) if backtrack |\n\n    The time complexity of DFS may be worse than BFS: \\\\(O(b^m)\\\\) might be\n    larger than \\\\(O(b^d)\\\\).\n\n    DFS only requires storage of \\\\(O(bm)\\\\) nodes, where \\\\(m\\\\) is the maximum\n    depth of any node. backtracking search only generates one successor\n    at a time, modifying the current state description rather than copying\n    it. Memory requirements reduce to one state description and \\\\(O(m)\\\\)\n    actions.\n\nDepth-limited Search\n\n    In depth-limited search, nodes at depth of pre-determined limit \\\\(l\\\\)\n    are treated as if they had no successors. This limit solves the\n    infinite-path problem.\n\n    | performance      | rating                                |\n    |------------------|---------------------------------------|\n    | completeness     | YES                                   |\n    | optimal          | NO                                    |\n    | time complexity  | \\\\(O(b^l)\\\\)                          |\n    | space complexity | \\\\(O(b^l)\\\\), \\\\(O(l)\\\\) if backtrack |\n\nIterative Deepening Depth-first Search\n\n    Key idea is to gradually increase the depth limit: first 0, then\n    1, then 2... until a goal is found.\n\n    {{}}\n\n    \\\\(N(IDS) = (d)b + (d-1)b^2 + \\dots + (1)b^d\\\\), which gives a time\n    complexity of \\\\(O(b^d)\\\\)\n\n    | performance      | rating                                |\n    |------------------|---------------------------------------|\n    | completeness     | YES                                   |\n    | optimal          | NO (unless step cost is 1)            |\n    | time complexity  | \\\\(O(b^d)\\\\)                          |\n    | space complexity | \\\\(O(b^d)\\\\), \\\\(O(m)\\\\) if backtrack |\n\n    BFS and IDS are complete if \\\\(b\\\\) is finite.\n    UCS is complete if \\\\(b\\\\) is finite and step cost is \\\\(\\ge \\epsilon\\\\).\n    BFS and IDS are optimal if all step costs are identical.\n\nBidirectional Search\n\n    Conduct two simultaneous searches -- one forward from the initial\n    state, and the other backward from the goal. This is implemented by\n    replacing the goal test with a check to see whether the frontiers of\n    two searches intersect. This reduces the time ad space complexity to \\\\(O(b^{d/2})\\\\).\n\nInformed Search Strategies {#informed-search-strategies}\n\nGreedy best-first search\n\n    Greedy best-first search tries to expand the node that is closest to\n    the goal, on the grounds that this is likely to lead to a solution\n    quickly. It evaluates nodes by using just the heuristic function:\n    \\\\(f(n) = h(n)\\\\).\n\n    Greedy best-first tree search is incomplete even in a finite state\n    space. The graph search version is complete in finite spaces, but not\n    in infinite ones. The worst case time and space complexity is\n    \\\\(O(b^m)\\\\). However, with a good heuristic function, the complexity can\n    be reduced substantially.\n\nA\\* search\n\n    It evaluates nodes by combining \\\\(g(n)\\\\) the cost to reach the node, and\n    \\\\(h(n)\\\\) the cost to get to the goal: \\\\(f(n) = g(n) + h(n)\\\\). Since \\\\(g(n)\\\\)\n    gives the path cost from the start node to node \\\\(n\\\\), and \\\\(h(n)\\\\) is the\n    estimated cost of the cheapest path from \\\\(n\\\\) to the goal,$f(n) = $\n    estimated cost of the cheapest solution through \\\\(n\\\\).\n\n    \\\\(h(n)\\\\) is an admissible heuristic iff it never overestimates the\n    cost to reach the goal. For A\\*, this means that \\\\(f(n)\\\\) would never\n    overestimate the cost of a solution along the current path.\n\n    Admissible heuristics are by nature optimistic because they think the\n    cost of solving the problem is less than it actually is.\n\n    A second, slightly stronger condition is called consistency, and is\n    required only for applications of A\\* to graph search. A heuristic\n    \\\\(h(n)\\\\) is consistent iff for every node \\\\(n\\\\) and every successor \\\\(n'\\\\)\n    of \\\\(n\\\\) generated by any action \\\\(a\\\\), the estimated cost of reaching the\n    goal from \\\\(n\\\\) is no greater than the step cost of getting to \\\\(n'\\\\) plus\n    the estimated cost of reaching the goal from \\\\(n'\\\\): \\\\(h(n) \\le\n    c(n,a,n') + h(n')\\\\). This is a form of the general triangle inequality.\n\n    A\\* search is complete, optimal and optimally efficient with a\n    consistent heuristic. The latter means that no other optimal algorithm\n    is guaranteed to expand fewer nodes than A\\*.\n\n    However, for most problems, the number of states within the goal\n    contour search space is still exponential in the length of the\n    solution.\n\n    The absolute error of a heuristic is defined as \\\\(\\Delta = h^\\*-h\\\\),\n    and the relative error is defined as \\\\(\\epsilon = \\frac{h^\\-h}{h\\}\\\\).\n    The complexity results depend strongly on the assumptions made about\n    the state space. For constant step costs, it is \\\\(O(b^{\\epsilon d})\\\\),\n    and the effective branching factor is \\\\(b^\\epsilon\\\\).\n\n    A\\* keeps all generated nodes in memory, and hence it usually runs out\n    of space  long before it runs of time. Hence, it is not practical for\n    large-scale problems.\n\n    Other memory-bounded heuristic searches include:\n\n    iterative-deepening A\\* (IDA\\*)\n    Recursive best-first search (RBFS)\n    Memory-bounded A\\* (MA\\*)\n    simplified MA\\* (SMA\\*)\n\nLearning to Search Better {#learning-to-search-better}\n\nEach state in a metalevel state space captures the internal\ncomputational state of a program that is searching in an _object-level\nstate space. A metalevel learning_ algorithm can learn from\nexperiences to avoid exploring unpromising subtrees. The goal of the\nlearning is to minimise the total cost of problem solving, trading off\ncomputational expense and path cost.\n\nHeuristic Functions {#heuristic-functions}\n\nIf for any node n \\\\(h\\2(n) \\ge h\\1(n)\\\\), we say that \\\\(h\\2\\\\) dominates_\n\\\\(h\\1\\\\). Domination translates directly into efficiency: A\\* using \\\\(h\\2\\\\)\nwill never expand more nodes than \\\\(h\\_1\\\\). Hence it is generally better\nto use a heuristic function with higher value, while making sure it is\nconsistent, and computing the heuristic function is computationally\nfeasible.\n\nGenerating Admissible Heuristics {#generating-admissible-heuristics}\n\nFrom Relaxed Problems\n\n    Because the relaxed problem adds edges to the state space, any optimal\n    solution in the original problem is, by definition, also a solution in\n    the relaxed problem. Hence, the cost of an optimal solution to a\n    relaxed problem is an admissible heuristic for the original problem.\n    Because the derived heuristic is an exact cost for the relaxed\n    problem, it must obey the triangle inequality and is therefore\n    consistent.\n\nFrom Subproblems: Pattern Databases\n\n    Pattern Databases store exact solution costs for every possible\n    subproblem instance. In the case of the 8-puzzle, every possible\n    configuration of the four tiles and the blank. Each pattern database\n    yields an admissible heuristic, and these heuristics can be combined\n    by taking the maximum value. Solutions to subproblems can overlap:\n    disjoint pattern databases account for this. These work by dividing\n    the problem in a way that each move affects only one subproblem.\n\nFrom Experience\n\n    Inductive learning methods work best when supplied with features of\n    a state that are relevant to predicting the state's value. A common\n    approach to combining features would be through a linear combination:\n    \\\\(h(n) = c\\1x\\1(n) + c\\2x\\2(n)\\\\).\n\n    These heuristics satisfy the requirement that \\\\(h(n) = 0\\\\) for goal\n    states, but are not necessarily admissible or consistent.\n\nBeyond Classical Search {#beyond-classical-search}\n\nHere, we cover algorithms that perform purely local search in the\nstate space, evaluating and modifying one or more current states\nrather than systematically exploring paths from an initial state.\nThese include methods inspired by statistical physics (simulated\nannealing) and evolutionary biology (genetic algorithms).\n\nIf an agent cannot predict exactly what percept it will receive, then\nit will need to consider what to do under each contingency that its\npercepts may reveal.\n\nIf the path to the goal doesn't matter, we giht consider a different\nclass of algorithms, ones that do not worry about the paths at all.\nLocal search algorithms operate using a single current node and\ngenerally move only to neighbours of that node. Its advantages\ninclude:\n\nThey generally use a constant amount of memory\nThey can often find reasonable solutions in large or infinite\n    state spaces where systematic algorithms are not suitable.\n\nHill-climbing Search {#hill-climbing-search}\n\nThe hill-climbing search is a loop that continually moves in the\ndirection of increasing value.\n\nConsider the 8-queens problem.\n\nLocal search algorithms typically use a complete-state formation. The\nsuccessors of a state are all possible states generated by moving a\nsingle queen to another square in the same column.\n\nWe could use a heuristic cost function \\\\(h\\\\) equal to the number of\nqueens that are attacking each other, either directly or indirectly.\n\nThe global minimum of this function is zero, which only occurs for\nperfect solutions. Hill-climbing algorithms typically choose randomly\namong the set of best successors having the lowest \\\\(h\\\\).\n\nHill-climbing algorithms can get stuck for the following reasons:\n\nlocal maxima\nsequence of local maxima\nflat local maximum, or shoulder, from which progress\n    is possible.\n\nVariants\n\n    stochastic hill-climbing\n    : chooses at random from among the uphill\n        moves; the probability of selection can vary with the steepness\n        of the uphill move. Usually converges more slowly, but finds\n        better solutions.\n\n    first-choice hill-climbing\n    : stochastic hill-climbing with randomly\n        generated successors until one is generated that is better than\n        the current state. Good when state has many successors.\n\n    random-restart hill-climbing\n    : conducts hill-climbing searches from\n        randomly generated initial states, until a goal is found.\n        Trivially complete with probability approaching 1.\n\nSimulated Annealing {#simulated-annealing}\n\nA hill-climbing algorithm that never makes 'downhill' moves towards\nstates with lower-value is guaranteed to be incomplete, because it can\nbe stuck on a local maximum.\n\nfunction SIMULATED-ANNEALING(problem, schedule)\n  inputs: problem, a problem\n          schedule, a mapping from time to 'temperature'\n\n  current ← MAKE-NODE(problem, INITIAL-STATE)\n  for t = 1 to ∞ do\n    T ← schedule(t)\n    if T = 0 then return current\n    next ← a randomly selected successor of current\n    𝞓E ← next.VALUE - current.VALUE\n    if 𝞓E > 0 then current ← next\n    else current ← next only with probability e^(𝞓E/T)\n\nLocal Beam Search {#local-beam-search}\n\nLocal beam search keeps track of \\\\(n\\\\) states rather than just one. It\nbegins with \\\\(n\\\\) randomly generated states, at each step all the\nsuccessors of all states are generated. If any one is a goal, the\nalgorithm halts.\n\nLocal-beam search passes useful information between the parallel\nsearch threads (compared to running random-restart \\\\(n\\\\) times), quickly\nabandoning unfruitful searches and moves its resources to where the\nmost progress is being made.\n\nStochastic local beam search chooses \\\\(n\\\\) successors at random, with\nthe probability of choosing a given successor being an increasing\nfunction of its value.\n\nGenetic Algorithms {#genetic-algorithms}\n\nA genetic algorithm is a variant of stochastic beam search in which\nsuccessor states are generated by combining two parent states rather\nthan by modifying a single state.\n\nGA begins with a set of \\\\(n\\\\) randomly generated states, called the\npopulation. Each state is also called an individual.\n\nThe production of the next generation of states is rated by the\nobjective function, or fitness function. A fitness function returns\nhigher values for better states.\n\nLike stochastic beam search, genetic algorithms combine an uphill\ntendency with random exploration and exchange of information among\nparallel search threads. crossover in genetic algorithms raises the\nlevel of granularity at which the search operates.\n\nfunction GENETIC-ALGORITHM(population, FITNESS-FN) returns an individual\n  inputs: population, a set of individuals\n          FITNESS-FN, a function that measures the fitness of an\n  individual\n\n  repeat\n    new_population ← empty set\n    for i = 1 to SIZE(population) do\n      x ← RANDOM-SELECTION(population, FITNESS-FN)\n      y ← RANDOM-SELECTION(population, FITNESS-FN)\n      child ← REPRODUCE(x,y)\n      if (small random probability) then child ← MUTATE(child)\n      add child to new_population\n    population ← new_population\n  until some individual is fit enough, or enough has elapsed\n  return the best individual in population, according to FITNESS-FN\n\nfunction REPRODUCE(x,y) returns an individual\n  inputs: x,y, parent individuals\n\n  n ← LENGTH(x); c ← random(1,n)\n  return APPEND(SUBSTRING(x,1,c), SUBSTRING(y, c+1, n))\n\nLocal Search in Continuous Spaces {#local-search-in-continuous-spaces}\n\nOne way to avoid continuous problems is simply to discretize the\nneighbourhood of each state. Many methods attempt to use the\ngradient of the landscape to find a maximum: \\\\(x \\leftarrow x +\n\\delta \\nabla (x)\\\\), where \\\\(\\delta\\\\) is a small constant called the _step\nsize. For many problems, the Newton-Raphson_ method is effective. It\nsolves the roots for equations \\\\(g(x) = 0\\\\), by computing a new\nestimate: \\\\(x \\leftarrow x - g'(x)/g(x)\\\\). To find a maximum or minimum\nof \\\\(f\\\\), we need to find \\\\(x\\\\) such that the gradient is zero. In this\ncase \\\\(g(\\mathbf{x})\\\\) in Newton's formula becomes \\\\(\\nabla\nf(\\mathbf{x})\\\\) and the update equation can be written in matrix-vector\nform as:\n\n\\begin{align\\*}\n\\mathbf{x} \\leftarrow \\mathbf{x} - H\\_f^{-1}(\\mathbf{x})\\nabla f(\\mathbf{x})\n\\end{align\\*}\n\nwhere \\\\(H\\f\\\\) is the Hessian_ matrix of second derivatives. For\nhigh-dimensional problems, computing the \\\\(n^2\\\\) entries of the Hessian\nand inverting it may be expensive, and approximate versions have been\ndeveloped.\n\nLocal search methods suffer from local maxima, ridges and plateaux\nin continuous spaces just as much as in discrete spaces.\n\nSearching with Non-deterministic Actions {#searching-with-non-deterministic-actions}\n\nWhen the environment is either partially observable or\nnon-deterministic, percepts become useful. In a partially observable\nenvironment, every percept helps narrow down the set of possible\nstates the agent might be in. In a non-deterministic environment,\npercepts tell the agent which of the possible outcomes of its actions\nhas actually occurred. Future percepts cannot be determined in\nadvance, and the agent's future actions will depend on those future\npercepts. The solution to a problem is not a sequence but a\ncontingency plan\n\nThe solutions for no-deterministic problems can contain nested\nif-then-else statements, meaning they are trees and not sequences.\n\nAND-OR search trees\n\n    A solution for an AND-OR search problem is a subtree that:\n\n    includes every outcome branch leaf\n    specifies one action at each of its OR nodes\n    includes every outcome branch at each of its AND nodes\n\n        function AND-OR-GRAPH-SEARCH(problem) returns a conditional plan, or failure\n      OR-SEARCH(problem, INITIAL-STATE, problem, [])\n\n    function OR-SEARCH(state,problem,path) returns a conditional plan, or failure\n      if problem, GOAL-TEST(state) then return the empty plan\n      if state is on path then return failure\n      for each action in problem, ACTIONS(state) do\n        plan ← AND-SEARCH(RESULTS(state,action), problem, [state | path])\n        if plan ≠ failure then return [action | plan]\n      return failure\n\n    function AND-SEARCH(states,problem,path) returns a conditional plan, or failure\n      for each s_i in states do\n        plani ← OR-SEARCH(si,problem,path)\n        if plan_i = failure then return failure\n      return [if s1 then plan1 else if s2 then plan2 ...]\n\n    (stop at AIMA 4.4)\n\nAdversarial Search {#adversarial-search}\n\nCompetitive environments, in which the agent's goals are in conflict,\ngive rise to adversarial search problems.\n\nGame theory views any multi-agent environment as a game, provied that\nthe impact of each agent on the others is significant.\n\nGames often have large branching factors, and require making some\ndecision even before computing the optimal decision.\n\nPruning allows us to ignore portions of the search tree that make no\ndifference to the final choice, and heuristic evaluation functions\nallow us to approximate the true utility of a state without doing a\ncomplete search.\n\nA game can be formally defined as a search problem with the following\nelements:\n\n\\\\(S\\_0\\\\)\n: the initial state, which specifies how the game is set up\n    at the start\n\n\\\\(Player(s)\\\\)\n: Defines which player has the move in a state\n\n\\\\(Actions(s)\\\\)\n: Returns the set of legal moves in a state\n\n\\\\(Result(s,a)\\\\)\n: The transition model, which defines the result of a move\n\n\\\\(TerminalTest(s)\\\\)\n: Terminal test, which is true when the game is\n    over, and false otherwise.\n\n\\\\(Utility(s,p)\\\\)\n: A utility function defines the numeric value for a\n    game that ends in terminal state \\\\(s\\\\) for a player\n    \\\\(p\\\\).\n\nThe initial state, \\\\(Actions\\\\) function and \\\\(Result\\\\) function define the\ngame tree for the game.\n\nOptimal Strategy {#optimal-strategy}\n\nThe optimal strategy can be determined from the minimax value of\neach node (\\\\(Minimax(n)\\\\)). The minimax value of a node is the utility\nof being in the corresponding state, assuming that players play\noptimally from there to the nd of the game. The minimax value of a\nterminal state is its utility.\n\n\\begin{align}\n  Minimax(s) =\n  \\begin{cases}\n    Utility(s), \\text{ if } TerminalTest(s) \\\\\\\\\\\\\n    max\\_{a \\in Actions(s)}MINIMAX(Result(s,a)), \\text{if Player(s) =\n      Max} \\\\\\\\\\\\\n    min\\_{a \\in Actions(s)}MINIMAX(Result(s,a)), \\text{if Player(s) = Min}\n  \\end{cases}\n\\end{align}\n\nMinimax uses utility function on leaf nodes, backing up through the\ntree, setting the node value to be the minimum of the children.\n\nAlpha-Beta Pruning {#alpha-beta-pruning}\n\nEliminate parts of the search tree that do not affect decision.\n\nClassical Planning {#classical-planning}\n\nProblem-solving agents that deal with atomic representations of states\nrequire good domain-specific heuristics to perform well. The hybrid\npropositional logical agent can find plans without domain-specific\nheuristics because it uses domain-independent heuristics based on the\nlogical structure of the problem. However, it relies on ground\npropositional inference, and suffers when there are large numbers of\nactions and states.\n\nPlanning Domain Definition Language (PDDL) was created in response to\nthese deficiencies.\n\nEach state is represented as a conjunction of fluents that are ground,\nfunctionless atoms. Database semantics is used, which involves:\n\nclosed-world assumption\n: all fluents not mentioned are false\n\nunique names assumption\n: fluents with different names are distinct\n\nState representations are carefully designed so that they can be\nmanipulated by set operations or logical inference.\n\nActions are defined in terms of the preconditions and effects.\nPreconditions and effects are described in terms of a conjunction of\nliterals.\n\nComplexities of classical planning {#complexities-of-classical-planning}\n\nPlanSAT is the question of whether there exists any plan that solves a\nplanning problem. Bounded PlanSAT asks whether there is a solution of\nlength k or less.\n\nWhile the number of states is finite, adding function symbols make\nthem infinite, making these problems semi-decidable at best. Certain\nrestrictions can reduce the questions into a P class problem.\n\nHowever, most agents would not be asked to derive plans for worst-case\nproblem instances. For many problem domains, bounded PlanSAT is\nNP-complete, while PlanSAT is in P.\n\nState-space search for planning {#state-space-search-for-planning}\n\nThe first approach is forward (propogation) state-space search, which\nsearches forward from the initial state. However, it is inefficient,\nfor the following reasons:\n\nIt tends to explore irrelevant actions\nPlanning problems tend to have large state spaces, and relatively\n    small problems will be an issue without a good heuristic\n\nIt turn out that good domain-independent heuristics can be derived for\nforward search, which makes it feasible.\n\nAlternatively, we can do a backward (regression) state-space search,\nwhich looks for actions that can lead to the goal. Unlike forward\nsearch, backward search only explores relevant actions, hence has a\nlow branching factor. However, backward search deals with sets, which\nare make it harder to derive good domain-independent heuristics.\n\nHeuristics for planning {#heuristics-for-planning}\n\nFraming the search problem as a graph where the nodes are states and\nthe edges are actions. We can think of a number of ways to relax the\nproblem, generating admissible heuristics:\n\nAdd more edges to the graph, making it easier to find a path\n\nThe ignore preconditions heuristic drops all preconditions from\nactions, and every action becomes applicable in every state. We count\nthe number of actions required such that the union of the action's\neffects satisfy the goal. This is called the set-cover problem, which\nis unfortunately NP-hard. We can also ignore selected preconditions of\nactions.\n\nThe ignore delete list heuristic, drops all negative literals in goals\nand preconditions. This way, an action cannot undo progress towards\nthe goal, and each action taken would monotonically progress towards\nit.\n\nGrouping multiple nodes together, shrinking the size of the graph\n\nWe can reduce the number of states by forming a state abstraction -- a\nmany-to-one mapping from states in the ground representation of the\nproblem to the abstract representation. For example, one can ignore\nsome fluents.\n\nThe key idea in defining heuristics is decomposition: diving a problem\ninto parts. Subgoal independence is the assumption that the cost\nof solving a conjunction of subgoals is approximated by the sum of the\ncosts of solving a subgoal independently.\n\nOther classical planning approaches {#other-classical-planning-approaches}\n\nWe can translate a problem description in PDDL to  a form that can be\nprocessed by SATPlan. The steps are below:\n\nPropositionalize the actions: replace each action schema with a set\n    of ground actions formed by substituting constants for each of the\n    variables. These ground actions are not part of the translation,\n    but will be used in subsequent steps.\nDefine the initial state: assert \\\\(F^0\\\\) for every fluent \\\\(F\\\\) in the\n    problem's initial state, and \\\\(\\neg F^0\\\\) for every fluent not mentioned\n    in the final state.\nPropositionalize the goal: the goal becomes a disjunction over all\n    of its ground instances, where variables are replaced by constants.\nAdd successor-state axioms: For each fluent \\\\(F\\\\), add an axiom of\n    the form:\n\n\\begin{equation}\n  F^{t+1} \\iff ActionCausesF^t \\vee \\left( F^t \\wedge \\neg ActionCausesNotF^t \\right)\n\\end{equation}\n\nWhere \\\\(ActionCausesF\\\\) is a disjunction of all the ground actions that\nhave \\\\(F\\\\) in their add list, and \\\\(ActionCausesNotF\\\\) is a disjunction of\nall ground actions that have \\\\(F\\\\) in their delete list.\n\nAdd precondition axioms: for each ground action \\\\(A\\\\), add the axiom\n    \\\\(A^t \\iff PRE(A)^t\\\\), i.e. if an action is taken at time \\\\(t\\\\), then the\n    preconditions must be true.\nAdd action exclusion axioms: say that exactly one ground action can\n    occur at each step.\n\nfunction SATPLAN(init, transition, goal, Tmax) returns solution or failure\n  \"\"inputs: init, transition, goal constitute the problem\n    description\n    Tmax: upper limit for plan length\"\"\n  for t = 0 to Tmax do\n    cnf  q) \\iff [p, A; 1-p B] \\succ [q, A; 1-q,\n                      B]\\\\)\n\nDecomposability\n: \\\\([p, A; 1-p, [q, B; 1-q, C]] \\sim[p, A;(1-p)q, B;\n         (1-p)q, C]\\\\)\n\nvon Neumann and Morgenstern showed that following these axioms, it can\nbe shown that.\n\nUtility functions exist. \\\\(U(A) > U(B) \\iff A \\succ B\\\\) and \\\\((U(A) = U(B)\n       \\iff A \\sim B)\\\\).\nThe utility of a lottery, can be represented as the sum of\n    probability of each outcome, multiplied by the utility of that\n    outcome: \\\\(U([p\\1, S\\1; \\dots; p\\n, S\\n]) = \\sum\\i p\\iU(S\\_i)\\\\).\n\nUtility functions are not unique: in fact, that are not changed with\naffine transformations, of the form \\\\(U'(s) = aU(s) + b\\\\). This is\nbecause the agent only needs a preference on the ranking of states,\nand the actual value of the utility does not matter.\n\nUtility assessment and utility scales {#utility-assessment-and-utility-scales}\n\nTo build a decision-theoretic system, we must first work out what the\nagent's utility function is. This process is called **preference\nelicitation**, which involves presenting choices to the agent and using\nthe observed preferences to pin down the underlying utility function.\n\nWe have established above that utility functions are not unique, but\nit helps to have a normalized utility. Normalized utilities fix the\nutility of a \"best possible prize\" at \\\\(u\\_\\top = 1\\\\) and the \"worst possible\nprize\" at \\\\(u\\_\\bot = 0\\\\).\n\nGiven this utility scale, we can assess the utility of a prize \\\\(S\\\\), by\nasking the agent to choose between \\\\(S\\\\) and the standard lottery \\\\([p,\nu\\\\top;1-p, u\\\\bot]\\\\). \\\\(p\\\\) is adjusted until the agent is indifferent\nbetween \\\\(S\\\\) and the lottery. This is done for each prize \\\\(S\\\\).\n\nMulti-attribute utility functions {#multi-attribute-utility-functions}\n\nWhen outcomes are characterized by two or more attributes, we need a\nmulti-attribute utility function.\n\nDominance\n\n    We say that there is strict dominance of \\\\(S\\1\\\\) over \\\\(S\\2\\\\) if \\\\(S\\_2\\\\) is\n    lower on all attributes as compared to \\\\(S\\_1\\\\). Under uncertainty, we use\n    a more useful generalization: stochastic dominance.\n\n    If \\\\(A\\1\\\\) stochastically dominates \\\\(A\\2\\\\), then for any monotonically\n    increasing utility function \\\\(U(x)\\\\), the expected utility of \\\\(A\\_1\\\\) is at\n    least as high as the expected utility of \\\\(A\\_2\\\\). We can understand this\n    via the cumulative distribution. For any action \\\\(A\\1\\\\) and \\\\(A\\2\\\\), that\n    lead to probability distributions \\\\(p\\1(x)\\\\) and \\\\(p\\2(x)\\\\) on the random\n    variable \\\\(X\\\\):\n\n    \\begin{equation}\n      \\forall x \\int\\{-\\infty}^{x} p\\1(x')dx' \\le \\int\\{-\\infty}^{x} p\\2(x')dx'\n    \\end{equation}\n\nPreference structure and multi-attribute utility\n\n    Multi-attribute utility theory is based on the supposition that\n    preferences of typical agents have structure. This alleviates the\n    difficulty in expressing a utility function with many attributes and\n    possible distinct values.\n\n    One simple regularity in preference structures is called preference\n    independence. Two attributes \\\\(X\\1\\\\) and \\\\(X\\2\\\\) are preferentially\n    independent if the preference between outcomes \\\\((x\\1, x\\2, x\\_3)\\\\) and\n    \\\\((x\\1', x\\2', x\\3)\\\\) does not depend on the value of the value \\\\(x\\3\\\\) of\n    attribute \\\\(X\\_3\\\\).\n\n    If all attributes are mutually preferentially independent, then an\n    agent's preference can be described by maximising the function:\n\n    \\begin{equation}\n      V(x\\1, x\\2, \\dots, x\\n) = \\sum\\i V\\i(x\\i)\n    \\end{equation}\n\n    An example is:\n\n    \\begin{equation}\n    V(noise, cost, deaths) = - noise \\times 10^4 - cost - deaths \\times 10^{12}\n    \\end{equation}\n\n    Under uncertainty, the mathematics gets a bit more complicated.\n    Utility independence extends preference independence to cover\n    lotteries: a set of attributes \\\\(\\mathbb{X}\\\\) is utility independent of\n    a set of attributes \\\\(\\mathbb{Y}\\\\) if preference between lotteries on\n    the attributes in \\\\(\\mathbb{X}\\\\) are independent of the particular\n    values of the attributes in \\\\(\\mathbb{Y}\\\\).\n\n    Mutual utility independence implies that the agent's behaviour can be\n    described using a multiplicative utility function. If we denote \\\\(U\\_i\\\\)\n    to be \\\\(U\\i(x\\i)\\\\), then a 3 attribute utility function can be expressed\n    as:\n\n    \\begin{equation}\n     U = k\\1U\\1 + k\\2U\\2 + k\\3U\\3 + k\\1k\\2U\\1U\\2 + k\\1k\\3U\\1U\\3 +\n     k\\2k\\3U\\2U\\3 + k\\1k\\2k\\3U\\1U\\2U\\3\n    \\end{equation}\n\nDecision Networks\n\n    Decision networks extend Bayesian networks with nodes for actions and\n    utilities. An example of a decision network is below:\n\n    {{}}\n\n    The notation is as such:\n\n    chance nodes (oval)\n    : these represent random variables\n\n    decision nodes (rectangles)\n    : these represent points where the\n        decision maker has a choice of actions.\n\n    utility nodes (diamonds)\n    : represent the agent's utility function\n\n    A simplified form is used in many cases. The notation remains\n    identical, but the chance nodes describing the outcome state are\n    omitted. Rather than representing a utility function on outcome states,\n    the utility node represents the expected utility associated with each\n    action. This node is associated with an action-utility function (also\n    known as Q-function).\n\nInformation Value Theory\n\n    The value of a given piece of information is defined to be the\n    difference in expected value between the best action before and after\n    information is obtained.  This is one of the most important parts of\n    decision-making: knowing what information to obtain.  Information has\n    value to the extent that it is likely to cause a change of plan and to\n    the extent that the new plan is significantly better than the old\n    plan.\n\n    Mathematically, the value of perfect information (VPI) is defined as:\n\n    \\begin{equation}\n    VPI\\e(E\\j) = \\left( \\sum\\k P(E\\j = e\\_{jk} | \\mathbb{e})\n      MEU(\\alpha\\{e\\{jk}} | \\mathbb{e}, E\\j = e\\{jk}) \\right) - MEU(\\alpha |\n    \\mathbb{e})\n    \\end{equation}\n\n    This is obtained by considering the best action (maximum expected\n    utility) before and after obtaining the information, and averaging it\n    across all possible values for the new information, using our current\n    beliefs of its value.\n\n    Properties of the value of information\n\n        First, the expected value of information is non-negative.\n\n        \\begin{equation}\n          \\forall \\mathbb{e}, E\\j VPI\\{\\mathbb{e}} (E\\_j) \\ge 0\n        \\end{equation}\n\n        The theorem is about the expected value, and not the real value. This\n        means that information can sometimes lead to a plan that is harmful.\n\n        It is important to note that VPI is dependent on the current state of\n        information. VPI is not additive in general:\n\n        \\begin{equation}\n          VPI\\{\\mathbb{e}}(E\\j, E\\k) \\ne VPI\\{\\mathbb{e}}(E\\j) + VPI\\{\\mathbb{e}}(E\\_k)\n        \\end{equation}\n\n        VPI is order independent. That is:\n\n        \\begin{equation}\n          VPI\\{\\mathbb{e}}(E\\j, E\\k) = VPI\\{\\mathbb{e}}(E\\_j) +\n          VPI\\{\\mathbb{e}, e\\j}(E\\k) = VPI\\{\\mathbb{e}}(E\\_k) +\n          VPI\\{\\mathbb{e}, e\\k}(E\\_j)\n        \\end{equation}\n\n    Information Gathering Agents\n\n        we can implement a myopic information-gathering agent, by using the\n        VPI formula shortsightedly.\n\n                function INFORMATION-GATHERING-AGENT(percept) returns an action\n          persistent: D, a decision network\n\n        integrate percept into D\n        j  Cost(E_j)\n          return REQUEST(E_j)\n        else return the best action from D\n\n        If we know the associated cost of observing evidence, it simply\n        retrieves the evidence if the cost of observing it is less than the\n        value it provides.\n\nRANDOM {#random}\n\nSimon's Ant {#simon-s-ant}\n\nSimon, noble prize in economics\n\n> The complexity of the behaviour is the manifestation of the complexity\n> of the environment and not the complexity of the program.\n\nREFILE {#refile}\n\n{{}}\n\nPapers {#papers}\n\nImproving Policy Gradient by Exploring Under-Appreciated Rewards {#improving-policy-gradient-by-exploring-under-appreciated-rewards}\n\nThe REINFORCE algorithm minimizes the KL divergence between \\\\(\\pi\\_\\theta\\\\)\nand \\\\(\\pi^\\*\\\\), the optimal policy. However, learning an optimal policy by\noptimizing this direction of the KL is known to be mode-seeking.\nHence, it the agent is prone to falling into a local optimum, and miss\nout some modes of \\\\(\\pi^\\*\\\\).\n\nHence, a entropy regularization loss is added to encourage\nexploration.\n\nHowever, this exploration is undirected, and requires a small\nregularization coefficient to prevent too much random exploration.\n\nOptimizing in the mean-seeking direction of the KL divergence is to\nlearn a policy by following:\n\n\\begin{equation}\n  O\\{RAML}(\\theta; \\tau) = \\mathcal{E}\\{h \\sim p(h)}\\left\\\\{ \\tau\n    \\sum\\{a \\in A} \\pi\\\\tau^\\* (a | h) \\log \\pi\\_\\theta (a | h) \\right\\\\}\n\\end{equation}\n\nIn RL, the reward landscape is unknown, hence sampling from\n\\\\(\\pi\\_\\tau^\\*\\\\) is not straightforward. We can approximate the\nexpectation with respect to \\\\(\\pi\\_\\tau^\\*\\\\) by using self-normalized\nimportance sampling. For importance sampling, one draws \\\\(K\\\\) i.i.d\nsamples \\\\(\\\\{ a^{(k)}\\\\}^{K}\\{k=1}\\\\) from \\\\(\\pi\\\\theta\\\\) and computes a set\nof normalized weights to approximate \\\\(O\\_{RAML}(\\theta; \\tau | h)\\\\).\n\n\\begin{equation}\nO\\_{RAML} (\\theta; \\tau | h) \\approx \\tau\n\\sum\\{k=1}^{K}\\frac{w\\\\tau(a^{(k)} | h)}{\\sum\\{m=1}^{K}w\\\\tau(a^{(m)}\n  | h)} \\log \\pi\\_\\theta(a^{(k)} | h)\n\\end{equation}\n\nwhere \\\\(w\\\\tau(a^{(k)} | h) \\propto \\pi\\\\tau^\\* / \\pi\\_\\theta\\\\) denotes an\nimportance weight defined by:\n\n\\begin{equation}\n  w\\_\\tau (a^{(k)} | h) = exp \\left\\\\{ \\frac{1}{\\tau}r (a^{(k)} | h) -\n    \\log \\pi\\_\\theta (a^{(k)} | h) \\right\\\\}\n\\end{equation}\n\nOne can view the weights as the discrepancy between scaled rewards \\\\(r /\n\\tau\\\\) and the policy's log-probabilities \\\\(\\log \\pi\\_\\theta\\\\).\n\nIn UREX, both the RAML objective and the expected reward objective is\ncombined and jointly maximized.\n",
        "tags": []
    },
    {
        "uri": "/zettels/bayes_filter",
        "title": "Bayes Filter",
        "content": "\ntags\n: Gaussian Filter\n\nNotation {#notation}\n\n\\\\(\\eta\\\\)\n: normalizing constant, to make probability distribution sum\n    to 1.\n\n\\\\(\\text{bel}(t) = p(x\\t | z\\{1:t}, u\\_{1:t})\\\\)\n: posterior\n    probabilities over state variables conditioned on available data\n\n\\\\(\\overline{\\text{bel}}(t) = p(x\\t | z\\{1:t-1}, u\\_{1:t})\\\\)\n: belief\n    taken before incorporating the measurement \\\\(z\\_t\\\\)\n\n\\\\(\\overline{\\text{bel}}(t)\\\\) often called the prediction in Bayes\nfiltering. Computing \\\\(\\text{bel}(t)\\\\) from\n\\\\(\\overline{\\text{bel}}(t)\\\\) is called correction or the _measurement\nupdate_.\n\nAlgorithm {#algorithm}\n\n\\begin{algorithm}\n  \\caption{Bayes Filtering}\n  \\label{bayes\\_filter}\n  \\begin{algorithmic}[1]\n    \\Procedure{BayesFilter}{$\\text{bel}(x\\{t-1}), u\\t, z\\_t$}\n    \\ForAll{$x\\_t$}\n    \\State $\\overline{\\text{bel}}(t) = \\int p(x\\t | u\\t, x\\_{t-1})\n    \\text{bel}(x\\_{t-1}) dx$\n    \\State $\\text{bel}(t) = \\eta p(z\\t | x\\t)\\overline{\\text{bel}}(t) (x\\_t)$\n    \\EndFor\n    \\State \\Return $bel(x\\_t)$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\nDetails {#details}\n\nMathematical derivation makes the Markovian Assumption.\n\nExact techniques for belief calculation are reserved for specialized\ncases. In most scenarios, these beliefs have to be approximated, and\nthese approximations have important ramifications on the complexity of\nthe algorithm.\n\nApproximations requires trading off:\n\nComputational Efficiency\n: Linear Gaussian approximations can\n    calculate beliefs in time polynomial to the state space. Other\n    approximations may be exponential. Particle filter techniques have\n    the any-time characteristic, allowing them to trade off accuracy\n    with computational efficiency.\n\nAccuracy\n: Linear Gaussian approximations are limited to unimodal\n    distributions, whereas histogram approximations are multi-modal, but\n    have limited accuracy. Particle representations can approximate\n    a wide array of distributions, but large number of particles may be\n    required for high accuracy.\n\nEase of implementation\n: Difficulty in implementation typically\n    arises from the form of the transition and measurement probabilities\n    (see Robotics Probabilistic Generative Laws). Particle\n    representations lend themselves to simple implementations for\n    complex non-linear systems.\n",
        "tags": []
    },
    {
        "uri": "/zettels/bayesian_deep_learning",
        "title": "Bayesian Deep Learning",
        "content": "\ntags\n: §deep\\_learning\n\nThe Case For Bayesian Learning (Wilson, 2019) {#the-case-for-bayesian-learning}\n\nVague parameter prior + structured model (e.g. CNN) = structured\n    function prior!\nThe success of ensembles encourages Bayesians, since ensembles\n    approximate the Bayesian Model Average\n\nBayesian Perspective on Generalization (Sam Smith \\& Quoc Le, 2018) {#bayesian-perspective-on-generalization}\n\nBayesian model comparisons were first made on Neural Networks by\nMackay. Consider a classification model \\\\(M\\\\) with a single parameter\n\\\\(w\\\\), training inputs \\\\(x\\\\) and training labels \\\\(y\\\\). We can infer a\nposterior probability distribution over the parameter by applying\nBayes theorem:\n\n\\begin{equation}\n  P(w|y,x;M) = \\frac{P(y|w,x;M)P(w;M)}{P(y|x;M)}\n\\end{equation}\n\nThe assumption of a Gaussian prior for \\\\(P(w;M)\\\\) leads to a posterior\ndensity of the parameter given the new training data \\\\(P(w|y;x;M)\n\\propto \\sqrt{\\lambda/2\\pi}e^{-C(w;M)}\\\\), where \\\\(C(w;M) = H(w;M) +\n\\lambda w^2 / 2\\\\), which is the L2 regularized cross-entropy.\n\nWe can evaluate the normalizing constant, \\\\(P(y|x;M) =\n\\sqrt{\\frac{\\lambda}{2\\pi}} \\int dw e^{-C(w;M)}\\\\). Assuming that the\nintegral is dominated by the region near the minimum \\\\(w\\_0\\\\), we can\nestimate the evidence by Taylor expanding \\\\(C(w;M) \\approx C(w\\_0) +\nC''(w\\0)(w-w\\0)^2\\\\).\n\n\\begin{equation}\n  P(y|x;M) = \\mathrm{exp} \\left\\\\{ -\\left( C(w\\_0) +\n      \\frac{1}{2}ln(C''(w\\_0)/\\lambda) \\right) \\right\\\\}\n\\end{equation}\n\nIn models with many parameters, \\\\(P(y|x;M) \\approx\n\\frac{\\lambda^{p/2}f^{-C(w\\0)}} {| \\nabla \\nabla C(w) |\\{w\\_0}^{1 / 2}}\\\\),\nwhere the denominator can be thought of as an \"Occam factor\", causing\nthe network to prefer broad minima.\n\nBibliography\nWilson, A. G., The case for Bayesian deep learning, NYU Courant Technical Report, (),  (2019).  ↩\n\nSmith, S., & Le, Q. V., A bayesian perspective on generalization and stochastic gradient descent, In ,  (pp. ) (2018). : . ↩\n",
        "tags": [
            "bayes",
            "deep-learning"
        ]
    },
    {
        "uri": "/zettels/bayesian_inference",
        "title": "Bayesian Inference",
        "content": "\nSetup {#setup}\n\nWe have some unknown quantity \\\\(\\theta\\\\) (possibly a vector) that we\nwish to learn about, and we observe some data \\\\(y\\\\). In Bayesian\nstatistics, we specify:\n\nA sampling model, often expressed as a probability density\n    function \\\\(p(y|\\theta)\\\\), which we call the likelihood function\nA prior distribution \\\\(p(\\theta)\\\\), which expresses any prior\n    knowledge or beliefs that we have about their values before\n    observing the data\n\nScalable Bayesian Inference {#scalable-bayesian-inference}\n\nThere is an increasingly immense literature focused on big data. Most\nof the focus has been on optimization methods. Rapidly obtaining a\npoint estimate even when sample size \\\\(n\\\\) & overall \"size\" of data is\nimmense. There has been a huge focus on specific settings - e.g.\nlinear regression, labelling images, etc. This leads to many people\nworking on similar problems, while critical open problems remain\nuntouched.\n\nThe end goal is having general probabilistic inference algorithms for\ncomplex data, and being able to handle arbitrarily complex probability models.\nWe want algorithms to be scalable to huge data, and also be able to\naccurately quantify uncertainty.\n\nClassical Posterior Approximations {#classical-posterior-approximations}\n\nIn conjugate models, one can express the posterior in simple form -\n    e.g. as a multivariate Gaussian (see §exponential\\_family)\nIn more complex settings, one can approximate the posterior using\n    some tractable class of distributions\nLarge sample Gaussian approximations:\n\n\\begin{equation}\n  \\pi\\n(\\theta|Y^{(n)}) \\approx N(\\hat{\\mu}\\s, \\Sigma\\_n)\n\\end{equation}\n\nThis is also known as the Bayesian central limit theorem (Bernstein\nvon Mises). This relies on:\n\nsample size \\\\(n\\\\) being large relative to the number of parameters\n    \\\\(p\\\\)\nLikelihood being smooth and differentiable\nTrue value of \\\\(\\theta\\_0\\\\) in interior of parameter space\nRelated class of approximations include the Laplace approximation\n    Do well to approximating first and second moments, whereas\n        variational Bayes may have trouble with 2nd moment\n\nAlternative Analytic approximations {#alternative-analytic-approximations}\n\nWe can define some approximating class \\\\(q(\\theta)\\\\), which may be\n    something like a product of exponential family distributions.\nWe could think to define some discrepancy between \\\\(q(\\theta)\\\\) and\n    \\\\(\\pi\\_n(\\theta)\\\\), e.g. KL divergence.\nThis forms the basis of variational Bayes, expectation-propagation\n    and related methods.\nOptimize parameters to minimize discrepancy\n\nSee ICML 2018 Tutorial by Tamara Broderick for an overview. In\ngeneral, we have no clue how accurate the approximation is. See also\n(Pati et al., 2017).\n\nMarkov Chain Monte Carlo {#markov-chain-monte-carlo}\n\nAccurate analytic approximations to the posterior have proven elusive\noutside of narrow settings. MCMC and other posterior sampling\nalgorithms provide an alternative.\n\nMCMC: sequential algorithm to obtain correlated draws from the\nposterior, and bypasses the need to approximate the marginal\nlikelihood.\n\nOften, samples are more useful than an analytic form anyway.\n\nWe can get MCMC-based summaries of the posterior for any functional\n\\\\(f(\\theta)\\\\). As the number of samples \\\\(T\\\\) increases, these summaries\nbecome more accurate. MCMC constructs a Markov chain with stationary\ndistribution \\\\(\\pi\\n(\\theta|Y^{(n)})\\\\). A transition kernel_ is carefully\nchosen and iterative sampling proceeds. Most MCMC algorithms types of\nMetropolis-Hastings (MH):\n\n\\\\(\\theta^\\* \\sim g(\\theta^{(t-1)})\\\\) sample a proposal\n    (\\\\(\\theta^{(t)}\\\\) is a sample at step t)\nAccept a proposal by letting \\\\(\\theta^{(t)} = \\theta^\\*\\\\) with\n    probability\n\n\\begin{equation}\n  \\mathrm{min} \\left(1, \\frac{\\pi(\\theta^\\)L(Y^{(n)}|\\theta)}{\\pi(\\theta^{(t-1)})L(Y^{(n)}|\\theta^{(t-1)})} \\frac{g(\\theta^{(t-1)})}{g(\\theta^\\)} \\right)\n\\end{equation}\n\nWe want to design efficient MH algorithms by choosing good proposals\n\\\\(g\\\\). \\\\(g(\\cdot)\\\\) can depend on the previous value of \\\\(\\theta\\\\) and on\nthe data but not on further back samples - except in adaptive MH.\n\nFor example, in Gibbs sampling, let \\\\(\\theta = (\\theta\\_1, \\dots,\n\\theta\\_p)'\\\\) we draw subsets of \\\\(\\theta\\\\) from their exact conditional\nposterior distributions fixing the other.\n\nIn random walk, \\\\(g(\\theta^{(t-1)})\\\\) is a distribution centered on\n\\\\(\\theta^{(t-1)}\\\\) with a tunable covariance.\n\nIn HMC/Langevin, we exploit gradient information to generate samples\nfar from \\\\(\\theta^{(t-1)}\\\\) having high posterior density.\n\nMCMC & Computational Bottlenecks\n\nTime per iteration of MCMC increases with the number of parameters and\nunknowns, and also the increase with sample size \\\\(n\\\\). This is due to\nthe cost of sampling proposal & calculating acceptance probability.\nThis is similar to costs that occur in most optimization algorithms.\n\nMCMC does not produce independent samples from the posterior\ndistribution \\\\(\\pi\\_n(\\theta)\\\\). These draws are auto-correlated, and as the\nlevel of correlation increases, the information provided by each\nsample decreases. \"Slow mixing\" Markov chains have highly\nautocorrelated draws.\n\nA well designed MCMC algorithm with a good proposal should ideally\nexhibit rapid convergence and mixing.\n\nEmbarrassingly Parallel MCMC e.g. (Sanvesh Srivastava et al., 2015), (Li et al., 2016)\n\nWe can replace expensive transition kernels with approximations (Johndrow et al., 2015). for\nexample, we approximate a conditional distribution in Gibbs sampler\nwith a Gaussian or using a subsample of data, vastly speeding up MCMC\nsampling in high-dimensional settings.\n\nRobustness in Big Data {#robustness-in-big-data}\n\nIn standard Bayesian inference, it is assumed that the model is\ncorrect. Small violations of this assumption sometimes have a large\nimpacts, particularly in large datasets. The ability to carefully\nmodelling assumptions decreases for big/complex data. This appeals to\ntweaking the Bayesian paradigm to be inherently more robust.\n\nHigh-p problems {#high-p-problems}\n\nThere is a huge literature proposing different penalties: adaptive\n    lasso, fused lasso, elastic net, etc.\nIn general, these methods only produce a sparse point estimate are\n    dangerous scientifically, and there are many errors in interpreting\n    the zero vs non-zero elements\nParallel Bayesian literature on shrinkage priors - horseshoe,\n    generalized double Pareto, Dirichlet-Laplace, etc.\n\nWhat's an appropriate \\\\(\\pi(\\beta)\\\\) for the high dimensional vector of\ncoefficients? Most commonly used is a local-global scale mixture of\nGaussians. (Johndrow et al., 2017)\n\n{#}\n\nBibliography\nPati, D., Bhattacharya, A., & Yang, Y., On statistical optimality of variational bayes, CoRR, (),  (2017).  ↩\n\nSrivastava, S., Cevher, V., Dinh, Q., & Dunson, D., WASP: Scalable Bayes via barycenters of subset posteriors, In G. Lebanon, & S. V. N. Vishwanathan, Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (pp. 912–920) (2015). San Diego, California, USA: PMLR. ↩\n\nLi, C., Srivastava, S., & Dunson, D. B., Simple, scalable and accurate posterior interval estimation, CoRR, (),  (2016).  ↩\n\nJohndrow, J. E., Mattingly, J. C., Mukherjee, S., & Dunson, D., Optimal approximating markov chains for bayesian inference, CoRR, (),  (2015).  ↩\n\nJohndrow, J. E., Orenstein, P., & Bhattacharya, A., Bayes shrinkage at gwas scale: convergence and approximation theory of a scalable mcmc algorithm for the horseshoe prior, CoRR, (),  (2017).  ↩\n",
        "tags": [
            "machine-learning"
        ]
    },
    {
        "uri": "/zettels/bayesian_statistics",
        "title": "Bayesian Statistics",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/bias_complexity_tradeoff",
        "title": "The Bias-Complexity Tradeoff",
        "content": "\nTraining data can mislead the learner, and result in overfitting. To\novercome this problem, we can restrict the search space to some\nhypothesis space \\\\(\\mathcal{H}\\\\). This can be seen as introducing prior\nknowledge to the learning task. Is such prior knowledge necessary?\n\nThe No-Free-Lunch Theorem {#the-no-free-lunch-theorem}\n\nThe No-Free-Lunch theorem states that for binary classification tasks,\nfor every learner there exists a distribution on which it fails. We\nsay that the learner fails if, upon receiving i.i.d. examples from that\ndistribution, its output hypothesis is likely to have a large risk,\nwhereas for the same distribution, there exists another learner that\nwill output a hypothesis with a small risk. In other words, every\nlearner has tasks on which it fails while others succeed.\n\nTherefore, when approaching a particular learning problem, defined by\nsome distribution \\\\(D\\\\), we should have some prior knowledge on \\\\(D\\\\). One\ntype of such prior knowledge is that \\\\(D\\\\) comes from some specific\nparametric family of distributions. Another type of such prior\nknowledge is that there exists \\\\(h\\\\) in some predefined hypothesis class\n\\\\(H\\\\), such that \\\\(L\\_D(h) = 0\\\\).\n\nError Decomposition {#error-decomposition}\n\nwe can decompose the error of an \\\\(ERM\\_H\\\\) predictor into two components\nas follows. Let \\\\(h\\S\\\\) be an \\\\(ERM\\H\\\\) hypothesis. Then we can write:\n\n\\begin{equation}\n  L\\D(h\\S) = \\epsilon\\{\\textrm{app}} + \\epsilon\\{\\textrm{est}}\n\\end{equation}\n\nwhere \\\\(\\epsilon\\{\\textrm{app}} = \\textrm{min}\\{h\\in H} L\\_D(h)=\\\\), and\n\\\\(\\epsilon\\{\\textrm{est}} = L\\D(h\\S) - \\epsilon\\{\\textrm{app}}\\\\).\n\nThe approximation error is the minimum risk achievable by a predictor\nin the hypothesis class. This term measures how much risk we have\nbecause we restrict ourselves to a specific class (how much\ninductive bias we have). This approximation error does not depend on\nthe sample size, and is solely determined by the hypothesis class\nchosen.\n\nUnder the realizability assumption, the approximate error is zero. In\nthe agnostic case, the approximation error can be large (it always\nincludes the error of the Bayes optimal predictor).\n\nThe estimation error is the difference between the approximation error\nand the error achieved by the ERM predictor. The estimation error\nresults because the empirical risk (training error) is only an\nestimate of the true risk.\n\nSince our goal is to minimize the total error, we face a tradeoff,\ncalled the bias-complexity tradeoff. Choosing \\\\(H\\\\) to be a very rich\nclass decreases the approximation error, but may increase the\nestimation error, as a rich \\\\(H\\\\) might lead to overfitting. Learning\ntheory studies how rich we can make \\\\(H\\\\) while still maintaining\nreasonable estimation error.\n",
        "tags": [
            "machine-learning"
        ]
    },
    {
        "uri": "/zettels/bittorrent",
        "title": "BitTorrent",
        "content": "\nWhat is BitTorrent? {#what-is-bittorrent}\n\nWhen a file is made available using HTTP, all upload cost is placed on\nthe hosting machine. BitTorrent redistributes the cost of upload\nto downloaders.\n\nThe BitTorrent protocol faces some issues:\n\nSimply figuring out which peers have what parts of the file and\n    where they should be sent is difficult to do without incurring a\n    huge overhead.\nReal deployments face large churn rates, with peers staying\n    connected for only a few minutes.\nThe strategy for allocating upload to users needs to be fair\n    (proportional to download rate).\n\nHow does BitTorrent work? {#how-does-bittorrent-work}\n\nDeployment {#deployment}\n\nA static .torrent file is placed on a web server, containing\ninformation about the file, its length, name, hashing information, and\nthe tracker.\n\nA simple protocol layered on top of HTTP allows downloaders to send\ninformation about what file is downloading, and the tracker will\nrespond with contact information for peers which are downloading the\nsame file. To make the file available, the downloader which happens to\nhave the complete file (the seed) must be started.\n\nThe tracker's primary responsibility is helping peers find each other.\nThe standard tracker algorithm is to return a random list of peers,\nsince random graphs have good robustness properties. Many peer\nselection algorithms result in a power law graph, which can get\nsegmented after a small amount of churn.\n\nIn order to keep track of which peers have what, BitTorrent cuts files\ninto pieces of fixed size, typically a quarter megabyte. Each\ndownloader reports to all its peers what pieces it has. To verify data\nintegrity, the SHA1 hash of each piece is included in the .torrent\nfile.\n\nPipelining {#pipelining}\n\nWhen transferring data over TCP, like BitTorrent does, it is important\nto always have several requests pending at once, to avoid a delay in\npieces being sent. BitTorrent facilitates this by breaking piecse\nfurther into sub-pieces over the wire, typically several kilobytes in\nsize, and always keeping some number, typically five, requests\npipelined at once. Every time a sub-piece arrives a new request is\nsent. This can reliably saturate most connections.\n\nPiece selection {#piece-selection}\n\nSelecting pieces to download in good order is important for good\nperformance.\n\nThe strict priority policy is that once a single sub-piece has been\nrequested, the remaining sub-pieces from that sub-piece should be\nrequested before any other piece.\n\nThe rarest first policy suggests downloading pieces which fewest of\ntheir own peers have first. When the downloading starts, the peer has\nnothing to upload, so it's important to get a complete piece as\nquickly as possible. The choice of the first piece is random, an this\nis an exception to the rarest first policy, applying only to the first\npiece a peer has downloaded.\n\nSometimes a piece will be requested from a peer with slow transfer\nrates. To reduce delay in finishing a download, BitTorrent sends\nrequests for sub-pieces from all peers. Cancels are sent to peers for\nsub-pieces have already arrived to reduce bandwidth wastage, but in\npractice, this wastage is small.\n\nChoking Algorithms {#choking-algorithms}\n\nBitTorrent does no central resource allocation. Peers are responsible\nfor maximizing their own download rate. The choking algorithm in\nBitTorrent is not part of the wire protocol, but is necessary for good\nperformance. It should aim to:\n\nutilize all available resources\nprovide reasonable download rates for everyone\nbe somewhat resistant to peers only downloading and not uploading\n\nPareto efficiency is the concept where no two counterparties can make\nan exchange and both be happier. BitTorrent achieves pareto efficiency\nby using a more fleshed out version of tit-for-tat. Peers reciprocate\nuploading to peers which upload to them, with the goal of at any time\nhaving several connections which are actively transferring in both\ndirections.\n\nEach BitTorrent peer always chokes on a fixed number of peers (default\nis four). This approach allows TCP's built-in congestion control to\nreliably saturate upload capacity.\n\nDecisions as to which peer to unchoke are based strictly on download\nrate. The current implementation uses a rolling 20-second average. To\navoid rapidly choking and unchoking peers, BitTorrent peers\nrecalculate who they want to choke once every 10 seconds, and leave\nthe situation as s until the 10 second period is up. This period is\nsufficient for TCP to ramp up transfers to full capacity.\n\nThe current protocol of simply uploading to peers which provide the\nbest download rate would suffer from having no method of discovering\nif currently unused connections are better than the ones being used.\nOptimistic unchoking unchokes a peer regardless of the current\ndownload rate. Which peer to optimistically unchoke is rotated every\nthird rechoke period (30 seconds). This period is sufficient for the\nupload to reach full capacity, for thte download to reciprocate, and\nthe download to get to full capacity.\n\nOccasionally a BitTorrent peer will be choked by all peers which it\nwas formerly downloading from. In such cases it will usually continue\nto get poor download rates, until an optimistic unchoke finds better\npeers. To mitigate this problem, when over a minute goes sby without\ngetting a single piece from a particular peer, BitTorrent assumes it\nis snubbed by that peer, and doesn't upload to it except as an\noptimistic unchoke, which causes download rates to recover much more\nquickly.\n\nOnce a peer is done downloading it no longer has useful\ndownload rates to decide which peers to upload to The current\nimplementation then switches to preferring peres which it has better\nupload rates to.\n\nThe current scaling bottleneck in the real world seems to be the\nbandwidth overhead of the tracker.\n\nReferences {#references}\n\nIncentives Build Robustness in BitTorrent\n",
        "tags": []
    },
    {
        "uri": "/zettels/blockchain",
        "title": "Blockchain",
        "content": "\nStellar {#stellar}\n\nPayment-focused crypto-currency.\nopen-source, non-profit organization.\n\nConsensus Protocols {#consensus-protocols}\n\nProof of Work (POW) {#proof-of-work--pow}\n\n1 CPU 1 vote.\nsusceptible to 51% attack\n\nProof of S(?) {#proof-of-s}\n\n1 token 1 vote\nRich dominate the network\n\nDelegated Proof of S {#delegated-proof-of-s}\n\nLike POS, but can borrow, in exchange for incentives\n\nRCP {#rcp}\n\nRipple protocol\nPermissions-based\nUses Byzantine Fault Tolerance (BFT)\n\nTODO See 2 General Problem {#see-2-general-problem}\n\nStellar {#stellar}\n\nFork of Ripple\nPBFT\n    Tiered permissions\n",
        "tags": []
    },
    {
        "uri": "/zettels/books",
        "title": "Books",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/branch_prediction",
        "title": "Branch Prediction",
        "content": "\ntags\n: §operating\\systems, §computer\\organization\n\nTODO  {#https-comparch-dot-net-2013-06-30-why-tage-is-the-best}\n",
        "tags": []
    },
    {
        "uri": "/zettels/c_lang",
        "title": "The C Language",
        "content": "\ntags\n: §prog\\_lang\n\nTODO Read Modern C {#read-modern-c}\n",
        "tags": []
    },
    {
        "uri": "/zettels/cartographer",
        "title": "Google Cartographer",
        "content": "\nSystem Overview {#system-overview}\n\nGoogle Cartographer provides a real-time solution for indoor mapping\nin the form of a sensor-equipped backpack that generates 2D grid maps\nwith a \\\\(r = 5cm\\\\) resolution (Wolfgang Hess {\\it et al.}, 2016). It is built to\nscale to large maps, where previous approaches face issues. For\nexample, particle filter SLAM (§slam) approaches become resource intensive as\nmaps become large, since each particle filter must maintain a\nrepresentation of the full system state in each particle.\n\nLaser scans are inserted into a submap at the best estimated position,\nwhich is assumed to be sufficiently accurate for short periods of\ntime. Scan matching happens against a recent submap, so it only\ndepends on recent scans, and the error of pose estimates in the world\naccumulates.\n\nTo cope with this accumulation of error, pose optimization is run\nregularly. When a submap is finished (i.e. no new scans are added in),\nit takes part in scan matching for loop closure. All finish submaps\nand scans are automatically considered for loop closure. If they are\nclose enough based on current pose estimates, a scan matcher tries to\nfind the scan in the submap, and if a good match is found within the\ncurrently estimated pose, it is added as a loop closing constraint to\nthe optimization problem.\n\n{{}}\n\nBibliography\nHess, W., Kohler, D., Rapp, H., & Andor, D., Real-time loop closure in 2d lidar slam, In , 2016 IEEE International Conference on Robotics and Automation (ICRA) (pp. 1271–1278) (2016). : . ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/cmake",
        "title": "CMake",
        "content": "\ntags\n: §c\\_lang, §cplusplus\n\nIntroduction {#introduction}\n\nThe start to end process according to CMake looks like this:\n\n{{}}\n\nCMake starts with a human-readable file called CMakeLists.txt that\ndefines what should be built and how, what tests to run and what\npackages to create. This file is platform-independent.\n\nTypically the project separates the source directory and the binary\ndirectory. The source directory is kept tightly under version control.\n\nIn-source builds are arrangements where the source and build\ndirectories are the same. Having build outputs intermixed with source\nfiles leads to significant clutter. This makes working with version\ncontrol systems more difficult.\n\nHence, the preference is to have an out-of-source build arrangement.\nThe build directory will be called build.\n\nGenerating Project Files {#generating-project-files}\n\nRunning CMake reads the CMakeLists.txt file, and creates project files\nin the build directory. The developer selects the type of project file\nto be created by choosing a particular project file generator.\n\n{{}}\n\nBest Practices {#best-practices}\n\nAvoid globbing for listing source files\n",
        "tags": []
    },
    {
        "uri": "/zettels/code_litmus_tests",
        "title": "Code Litmus Tests",
        "content": "\ntags\n: §software\\_engineering\n\nHere are some questions you can ask yourself when evaluating the code\nyou write:\n\nAPI Modularity: Can you describe well your API, if written in a purely human\n    language with no code extracts?\nCompactness: Does an experienced user need a manual?\nOrthogonality: Does changing a part of code affect other system properties?\nSingle Point of Truth: Do data structures have states with 1-1\n    correspondence with the states of the real-world system?\nCode Modularity: Are there any global variables? Is the size of\n    modules too large? Are there any large functions? What is the call\n    maximum stack depth (excluding recursion)? Are there many internal\n    APIs? What is the number of entry points to the module?\nTransparency: Are there any special cases? Are there any magic\n    numbers? Are each function calls orthogonal? Are there many mode\n    flags? Is the high-level state of the system easily inspectable?\n    Can you see what the system is doing through any debug output?\n\nOthers:\n\nAre you using a binary format? Have you considered the pros and\n    cons of that, against a simple textual format?\n",
        "tags": []
    },
    {
        "uri": "/zettels/coding_interview",
        "title": "Coding Interview Preparation",
        "content": "\nData Structures Review {#data-structures-review}\n\nIntegers in Python\n\nTODO How Primitive Types are represented in {#how-primitive-types-are-represented-in}\n\nPython\nC++\n\nPrimitive Types {#primitive-types}\n\nIntegers in Python 3 are unbounded -- the maximum integer\nrepresentable is a function of available memory. The constant\nsys.maxsize can be used to find the word-size. Bounds on floats are\nspecified in sys.float_info.\n\nBit Manipulation\n\n    Computing the parity of a word.\n\n        def parity(x):\n        result = 0\n        while x:\n            result ^= x & 1\n            x >>= 1\n        return result\n\n    x & x - 1= Drops the lowest set bit of x.\n\n    The parity of 11011111 is the same as the parity of 1101 XORed with 1111.\n\n        def parity(x):\n        x ^= x >> 32\n        x ^= x >> 16\n        x ^= x >> 8\n        x ^= x >> 4\n        x ^= x >> 2\n        x ^= x >> 1\n        return x & 0x1\n\n    With time complexity of \\\\(O(\\log n)\\\\).\n\nSwapping Bits\n\n        def swap_bits(x, i, j):\n        if (x >> i) & 1 != (x >> j) & 1: # if ith and jth bits differ\n            bit_mask = (1  k - 1:\n                right = newpivotidx - 1\n            else:  # newpivotidx  arr[j]:\n                lis[i] = max(lis[i], lis[j] + 1)\n\n    return max(lis)\n\nGenerating a Random Sample {#generating-a-random-sample}\n\nimport random\n\ndef random_sample(k, A):\n    \"\"\"Generates a rondom subset of size k from array A.\"\"\"\n    for i in range(k):\n        r = random.randint(i, len(A)-1)\n        A[i], A[r] = A[r], A[i]\n    return A[:k]\n\nGenerate a random sample from a stream {#generate-a-random-sample-from-a-stream}\n\nThe basic idea is that given the n+1 element, and a random subset of\nsize k, where `k t.\n            j -= 1\n    return False\n\nhas three sum is the same, sort the array and run has two sum.\n\nBig Integer Multiply {#big-integer-multiply}\n\ndef multiply(num1, num2):\n    sign = -1 if (num1[0] = 0\n           and perm[inversionpoint] >= perm[inversionpoint + 1]):\n        inversion_point -= 1\n    if inversion_point == -1:\n        return []  # perm is the last permutation.\n\nSwap the smallest entry after index inversion_point that is greater than\nperm[inversion_point]. Since entries in perm are decreasing after\ninversion_point, if we search in reverse order, the first entry that is\ngreater than perm[inversion_point] is the entry to swap with.\n    for i in reversed(range(inversion_point + 1, len(perm))):\n        if perm[i] > perm[inversion_point]:\n            perm[inversionpoint], perm[i] = perm[i], perm[inversionpoint]\n            break\n\nEntries in perm must appear in decreasing order after inversion_point,\nso we simply reverse these entries to get the smallest dictionary order.\n    perm[inversionpoint + 1:] = reversed(perm[inversionpoint + 1:])\n    return perm\n",
        "tags": []
    },
    {
        "uri": "/zettels/cognitive_hierarchy_model",
        "title": "Cognitive Hierarchy Model",
        "content": "\ntags\n: §artificial\\_intelligence\n\nCognitive Hierarchy Model {#cognitive-hierarchy-model}\n\nSee (Camerer et al., 2004) for a nice introduction using Keynes'\n\"beauty contest\" game.\n\nIn the cognitive hierarchy model, each player believes that they\nunderstand the game better than other players. Decision rules reflect\nan iterative process of strategic thinking.\n\nThe CH model consists of iterative decision rules for players doing\n\\\\(k\\\\) steps of thinking, and the frequency distribution \\\\(f(k)\\\\) (assumed\nto be Poisson) of step \\\\(k\\\\) players.\n\nStep 0 thinkers assume that the other players behave randomly\naccording to some probability distribution. Step \\\\(k\\\\) thinkers assume\nthat their opponents are distributed according to a normalized Poisson\ndistribution, from step \\\\(0\\\\) to step \\\\(k-1\\\\), but ignore the possibility\nthat some players may be doing as much or more.\n\n{#}\n\nBibliography\nCamerer, C. F., Ho, T., & Chong, J., A cognitive hierarchy model of games, The Quarterly Journal of Economics, 119(3), 861–898 (2004).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/collaborative_editing",
        "title": "Collaborative Editing",
        "content": "\nCollaborative Editing software are powered either (or both) of these\ntwo techniques: Conflict-free replicated data types (CRDTs) or\nOperational Transforms (OT).\n\nI once gave a talk on OT: Real-time Collaboration - Google Slides\n\nReadings {#readings}\n\nBuilding real-time collaboration applications: OT vs CRDT\n",
        "tags": []
    },
    {
        "uri": "/zettels/compilers",
        "title": "Compilers",
        "content": "\nWhat are compilers? {#what-are-compilers}\n\nCompilers are programs that read in one (source) language, and translate them into another (target) language.\n\nCompilers need to perform two main tasks:\n\nAnalysis: breaking up a source program into constituent parts, and\n    create an intermediate representation\nSynthesis: Construct the desired target program from an intermediate representation\n\nA compiler is often designed in a number of phases:\n\nlexical analyses\n: reads a stream of characters, and groups them\n    into a stream of tokens (logically cohesive character sequences).\n    This often requires maintaining a symbol table\n\nsyntax analyses\n: imposes a hierarchical structure on the token\n    stream (parse tree)\n\nsemantic analyses\n: checks parse tree for semantic errors (e.g.\n    type errors)\n\nCode optimization\n: Optimizes the intermediate code representation\n    in terms of efficiency\n\nintermediate code generation\n: a program for an abstract machine\n\nTo contrast this with interpreters, interpreters take as input the\nprogram and the data, and produce an output. The interpreters do their\ncomputation \"online\".\n\nCousins of the Compiler {#cousins-of-the-compiler}\n\nPrepreprocessors\n    Macro Processing\n    File inclusion\n    Language Extension\nAssemblers\nLoaders and Link-editors\n\nThe Economy of Programming Languages {#the-economy-of-programming-languages}\n\nWhy are there so many programming languages? Application domains have\ndistinctive/conflicting needs. For example, in scientific computing,\nthere needs to be good support for FP, arrays and parallelism. Julia\nis a good example of a language designed for scientific computing. In\nsystems programming, we require fine control over resources, and\nsatisfy certain real-time constraints. Languages like C are suited for\nthese applications.\n\nProgrammer training is the dominant cost for a programming language.\nIt is difficult to modify a language, but easy to start a new one.\n\nLexical Analyses {#lexical-analyses}\n\nThe lexical analyzer reads input characters of the source program,\ngroup characters into lexemes, and outputs a sequence of tokens to the\nparser.\n\nIt may also:\n\nFilter out comments and whitespace\nCorrelating error messages with source program\nConstructing symbol tables\n\nSeparation of concerns lead to simplicity of design\nCompiler efficiency is improved, when using special techniques\n\nTerm Definitions {#term-definitions}\n\ntoken\n:\n\npattern\n: A description of the formthat can be recognized as a token\n\nlexeme\n: A sequence of characters matching the pattern for a token\n\nE.g. printf(\"Total = %d\", score);\n\n| Tokens      | Lexemes |\n|-------------|---------|\n| id          |         |\n| literal     |         |\n| punctuation |         |\n\nID(printf)  LBRACE LITERAL(\"Total = %d\") COMMA ID(score) RBRACE SEMI\n\nTokens have several categories:\n\n| Keywords    | if else return |\n|-------------|----------------|\n| Operators   | >  != !    |\n| Identifiers | pi score temp1 |\n| Constants   | 3.14 \"hello\"   |\n| Punctuation | ( ) ; :        |\n\nAttributes for tokens distinguish two lexemes belonging to the same symbol table:\n\n, ,\n\nRegular Languages {#regular-languages}\n\nSince the lexical analyzer needs to split splits into token\nclasses, it must be specify the set of strings that belongs into a\ntoken class. To do so, we use regular languages.\n\n{{}}\n\nThompson's construction\n\n    Thompson's construction converts a regular expression into a NFA. The\n    NFAs derived have several specific properties that simplify an\n    implementation:\n\n    Each NFA has one start state and one end state\n    No transition other than the initial transition, enters the start state\n    An \\\\(\\epsilon\\\\)-transtition always connects two states that were,\n        earlier in the process, the start state and accepting state of NFAs\n        for some component REs\n    Each state has at most 2 entering and 2 exiting \\\\(\\epsilon\\\\)-moves,\n        and at most one exiting move on a symbol in the alphabet.\n\n    {{}}\n\nSyntax Definition {#syntax-definition}\n\nWe use context-free grammars to specify the syntax of a language.\n\nA grammar for arithmetic expressions can be constructed from a table\nshowing the associativity and precedence of operators.\n\nleft-associative: + -\nleft-associative: * /\n\nTwo different non-terminals can be constructed for the two levels of\nprecedence:\n\nfactor -> digit | (expr)\nterm -> term * factor\n  | term / factor\n  | factor\nexpr -> expr + term\n  | expr - term\n  | term\n\nParsing {#parsing}\n\nParsers use pushdown automata to do parsing. See [LR online parsing\nmachines](http://jsmachines.sourceforge.net/machines/) for an online parsing tool.\n\nRecursive-Descent Parsing {#recursive-descent-parsing}\n\nConsists of a set of procedures, one for each non-terminal. The\nconstruction of both top-down and bottom-up parsers is aided by two\nfunctions: first and follow.\n\n\\\\(First(\\alpha)\\\\), where \\\\(\\alpha\\\\) is any string of grammar symbols, is\nthe set of terminals that begin strings derived from \\\\(\\alpha\\\\). If\n\\\\(\\alpha\\\\) derives \\\\(\\epsilon\\\\), then \\\\(\\epsilon\\\\) is also in \\\\(First(\\alpha)\\\\).\n\nFollow(A) for noterminal A, is the set of terminals \\\\(a\\\\) that can\nappear immediately to the right of A in some sentential form.: the set\nof terminals \\\\(a\\\\) such that there exists a derivation of the form \\\\(S\n\\overset{\\\\}{\\Rightarrow} \\alpha A a B\\\\).\n\nComputing First(X)\n\n    If X is a terminal, then \\\\(First(X) = \\\\{ X \\\\}\\\\)\n\n    If X is a non-terminal and $X &rarr; Y\\1 Y\\2 &hellip; Y\\_k is a\n        production for some \\\\(k \\ge 1\\\\), then place \\\\(a\\\\) in \\\\(First(X)\\\\) for\n        some i, a in \\\\(First(Y\\i)\\\\), and &epsilon; is in all of \\\\(First(Y\\1),\n           \\dots, First(Y\\{i-1})\\\\). If &epsilon; is in \\\\(First(Y\\j)\\\\), for all \\\\(j\n           = 1, \\dots, k\\\\) then add $&epsilon; to \\\\(First(X)\\\\).\n\n    If \\\\(X \\rightarrow \\epsilon\\\\) is a production, add \\\\(\\epsilon\\\\) to \\\\(First(X)\\\\).\n\nComputing Follow(A)\n\n    Place $ in \\\\(Follow(S)\\\\), where \\\\(S\\\\) is the start symbol, and $ is the\n        input right endmarker\n\n    If there is a production \\\\(A \\rightarrow \\alpha B \\beta\\\\) then\n        everything in \\\\(First(\\beta)\\\\) except \\\\(\\epsilon\\\\) is in \\\\(Follow(B)\\\\).\n\n    IF there is a production \\\\(A \\rightarrow \\alpha \\beta\\\\), or a\n        production \\\\(A \\rightarrow \\alpha B \\beta\\\\), where \\\\(First(B)\\\\)\n        contains \\\\(\\epsilon\\\\), then everything in \\\\(Follow(A)\\\\) is in\n        \\\\(Follow(B)\\\\).\n\n    A good video showcasing the computations\n\nLL(1)\n\n    A grammar \\\\(G\\\\) is LL(1) if and only if whenever \\\\(A \\rightarrow \\alpha |\n    \\beta\\\\) are two distinct productions of \\\\(G\\\\), the following conditions\n    hold:\n\n    For no terminal $a4 do both \\\\(\\alpha\\\\) and \\\\(\\beta\\\\) derive strings\n        beginning with \\\\(a\\\\).\n    At most one of \\\\(\\alpha\\\\) and \\\\(\\beta\\\\) can derive the empty string.\n    If \\\\(\\beta \\overset{\\*}{\\Rightarrow} \\epsilon\\\\) then $&alpha; does n ot\n        derive any string beginning with a terminal in \\\\(Follow(A)\\\\), and\n        vice versa.\n\n    Conditions 1 and 2 are equivalent to \\\\(First(\\alpha)\\\\) and\n    \\\\(First(\\beta)\\\\) being disjoint sets. The third condition is equivalent\n    to stating that if \\\\(\\epsilon\\\\) is in \\\\(First(B)\\\\), then \\\\(First(\\alpha)\\\\)\n    and \\\\(First(A)\\\\) are disjoint sets, and likewise interchanging \\\\(\\alpha\\\\)\n    and \\\\(\\beta\\\\).\n\nBottom-up Parsing {#bottom-up-parsing}\n\nThe parse tree for an input string is constructed beginning from the\nleaves (bottom) and working up towards the root (the top).\n\n{{}}\n\nLR grammars can be parsed with shift-reduce parsers.\n\nOne can think of bottom-up parsing as the process of \"reducing\" a\nstring \\\\(w\\\\) to the start symbol of the grammar. At each reduction, a\nspecific substring matching the body of a production is replaced by\nthe nonterminal at the head of the production.\n\nBottom-up parsing during a left-to-right scan of the input constructs\na right-most derivation in reverse.\n\nA stack holds grammar symbols and an inptu buffer holds the rest of\nthe string to be parsed. During a left-to-right scan of the input\nstring, the parser shifts zero or more input symbols onto the stack,\nuntil it is ready to reduce a string \\\\(\\beta\\\\) of grammar symbols onto\nthe stack. It then reduces \\\\(\\beta\\\\) to the head of the appropriate\nproduction. The parser repeats this cycle until it has detected an\nerror, or until the stack contains the start symbol and the input is\nempty.\n\n{{}}\n\nThe use of a stack in shift-reduce parsing is because the handle will\nalways eventually appear on top of the stack, and never inside.\n\nShift-reduce conflicts {#shift-reduce-conflicts}\n\nCannot decide whether to shift or reduce (shift/reduce conflict)\nCannot decide which of several reductions (reduce/reduce conflict)\n\nThese grammars are not in the \\\\(LR(k)\\\\) class of grammars.\n\nIn \\\\(LR(k)\\\\), L stands for-to-right scanning, R stands for rightmost\nderivation. LR parsers are table-driven. The LR-parsing method is the\nmost general nonbacktracking shit-reduce parsing method known. An LR\nparser can detect a syntactic error as soon as it is possible to do on\na left-to-right scan on the input. The class of grammars that can be\nparsed using LR methods is a proper subset of the class of grammars\nthat can be parsed with predictive or LL methods.\n\nThe main downside to this is that construction of a LR parser is\ntedious by hand.\n\nSyntax Directed Translation {#syntax-directed-translation}\n\nSyntax-directed translation is done by attaching rules or program\nfragments to productions in a grammar. e.g. consider\n\nexpr -> expr_1 + term\n\nWe can translate expr by attaching a semantic action within the\nproduction body:\n\nexpr -> expr_1 + term { print \"+\" }\n\nThe position of the semantic action determines the order in which the\naction is executed.\n\nThe most general approach to SDT is to construct a parse tree or\nsyntax tree, and compute the values of attributes by visiting the\nnodes of the tree. In most cases, SDT can be performed without\nexplicit construction of the tree.\n\nL-attributed translations (Left to right) encompass all translations\nthat can be performed during parsing.\n\nS-attributed translations (synthesized) is a smaller class that can be\nperformed easily in connection with a bottom-up parse.\n\nSyntax-Directed Definitions {#syntax-directed-definitions}\n\nA SDD is a CFG together with attributes and rules. Attributes are\nassociated with grammar symbols, and rules are associated with\nproductions. We write \\\\(X.a\\\\) where \\\\(X\\\\) is a symbol and \\\\(a\\\\) is an\nattribute.\n\nThere are 2 kinds of attributes for non-terminals:\n\nsynthesized attribute\n: defined by semantic rule associated with\n    the production at parse tree.\n\ninherited attribute\n: defined by semantic rule associated with the\n    production of parent at parse-tree.\n\nEvaluating an SDD at the nodes of a parse tree {#evaluating-an-sdd-at-the-nodes-of-a-parse-tree}\n\nWe can construct an annotated parse tree. With synthesized attributes,\n An SDD with both inherited and\nsynthesized attributes has no guarantee that there is one order in\nwhich to evaluate the attributes at nodes. There are useful subclasses\nof SDDs that are sufficient to guarantee an order of evaluation\nexists.\n\nThe dependency graph characterizes the possible orders in which we can\nevaluate the attributes at various nodes in the parse tree.\n\nAn SDD is S-attributed if every attribute is synthesized. In this\nscenario, we can evaluate attributes in any bottom-up order, for\nexample using post-order traversal of the parse tree.\n\nIn L-attributed SDDs, dependency-graph edges can only go from\nleft-to-right, and not right-to-left. This means that each attribute\nmust be either:\n\nSynthesized, or\nInherited, but with rules limited as follows: If there is a\n    production \\\\( A \\rightarrow X\\1 X\\2 \\dots X\\_n \\\\) and there is an\n    inherited attribute \\\\(X\\_i.a\\\\) computed by a rule associated with this\n    production, then the rule may use only:\n    Inherited attributes associated with the head \\\\(A\\\\)\n    Either inherited or synthesized attributes associated with the\n        occurrences of symbols \\\\(X\\1, X\\2 \\dots X\\_{i-1}\\\\) located to the\n        left of \\\\(X\\_i\\\\).\n    Inherited or synthesized attributes associated with \\\\(X\\_i\\\\)\n        itself, in a way that no cycles are formed in the dependency\n        graph by attributes of this \\\\(X\\_i\\\\).\n\nSide effecting {#side-effecting}\n\nWe can control side effects in SDDs by:\n\nPermitting incidental side effects that do not constrain attribute evaluation\nConstrain allowable evaluation orders, so that the translation is\n    produced for any allowable order\n\nSemantic rules executed for their side effects, such as printing, are\ntreade as the definitions of dummy synthesized attributes associated\nwith the head of the production. The modified SDD produces the same\ntranslation under any topological sort, since the statement is\nexecuted at the end.\n\nSyntax-Directed Translation Schemes {#syntax-directed-translation-schemes}\n\nA SDT is a CFG with program fragments embedded within production\nbodies. The program fragments are called semantic actions, and can\nappear at any position within the production body.\n\nRun Time Environment {#run-time-environment}\n\nThe environment deals out layout and allocation of storage locations\nin the source program, linkages between procedure and mechanisms for\npassing parameters, as well as interfaces to the operating system, I/O\ndevices and other programs.\n\nStorage Organization {#storage-organization}\n\nAn example of the run-time representation of an object program in the\nlogical address space is shown below:\n\nThe operating system maps logical addresses into physical addresses.\nRun-time storage typically comes in blocks of contiguous bytes, where\na byte is the smallest unit of addressable memory.\n\naligned\n: addresses must be divisible by 4\n\npadding\n: space left unused due to alignment issues\n\nThe size of a generated target code is fixed at compile time, so the\ncompiler can place the executable target code in a statically\ndetermined area called code. The size of program objects like global\nconstants and data is also known at compile time, and is placed at\nstatic.\n\nTo maximize utilization of space at run-time, the stack and heap are\nat opposite ends of the remainder of the address space. In practice,\nthe stack grows towards lower addresses, and the heap towards higher,\nbut here we assume the opposite so we can use positive offsets for\nnotational convenience.\n\nDynamic storage allocation is handled with 2 strategies:\n\nstack storage\n: names local to a procedure are allocated space on\n    the stack\n\nheap storage\n: data that may outlive the call to the procedure are\n    allocated here\n\nGarbage collection enables the run-time system to detect useless data\nelements and reuse their storage.\n\nActivation Trees {#activation-trees}\n\nThe sequence of procedure calls corresponds to a preorder traversal\n    of the activation tree\nThe sequence of returns corresponds to a postorder traversal of the\n    activation tree\nSuppose that control lies within a particular activation of some\n    procedure, then the activations that are currently open (live) are\n    those that correspond to the node and its ancestors. The order in\n    which these activations are called is the order in which they\n    appear along the path to the node, starting at the root.\n\nActivation Records {#activation-records}\n\nProcedure calls and returns are managed by a run-time stack called the\ncontrol stack. Each live activation has an activation record (frame)\non the control stack, with the root of the activation tree at the\nbottom, and the entire sequence of activation records on the stack\ncorresponding to the path in the activation tree to the activation\nwhere control currently resides.\n\nAn activation record may contain:\n\ntemporary values\n: arising from evaluation of expressions etc.\n\nlocal data\n: belonging to the procedure whose actiavtion record\n    this is\n\nsaved machine status\n: return address (program counter), contents\n    of registers that must be restored when return occurs\n\naccess link\n: locate data needed by the called procedure found\n    elsewhere (e.g. in another activation record)\n\ncontrol link\n: pointing to the activation record of the caller\n\nreturn value\n: space must be allocated for the return value of the\n    called function, if any\n\nparameters\n: Parameters used by the calling procedures, these are\n    however often placed in registers instead of the stack.\n\nCalling Sequences {#calling-sequences}\n\nCalling sequences consists of code that allocates an activation record\non the stack and enters information into its fields. A return sequence\nsimilarly contains code that restores the state of the machine so the\ncalling procedure can continue its execution after the call.\n\nIt is desirable to put as much of the calling sequence into the callee\nas possible, but the callee cannot know everything. This reduces the\namount of code generated. These are some guiding principles:\n\nValues communicated between the caller and callee are placed at the\n    beginning of the callee's activation record, so they are as close\n    as possible to the caller's activation record. The caller can\n    compute the values of the actual parameters of the call and place\n    them on top of its own activation record without having to create\n    the entire activation record for the callee. It also allows for\n    procedures that have multiple arity.\nFixed length items are placed in the middle. These include control\n    links, access links and the machine status fields.\nItems whose size may not be known early enough are placed at the\n    end of the activation record. Most local variables have fixed\n    length, which can be determined by the compiler by examining the\n    type of the variable. However, some local variables like\n    dynamically sized arrays cannot be determined until execution.\nWe must locate the top-of-stack pointer. A common approach is to\n    have it point to the end of the fixed-length fields in the\n    activation record. Fixed-length data can be accessed by fixed\n    offsets relative to the top-of-stack pointer. The offsets to\n    variable-length fields are then calculated at run-time, using a\n    positive offset from the top-of-stack pointer.\n\n{{}}\n\nVariable-Length data of the Stack {#variable-length-data-of-the-stack}\n\nA common scheme that works for dynamically sized arrays is to have a\npointer to the array stored on the stack. This pointer are known\noffsets from the top-of-stack pointer, so the target code can access\narray elements through these pointers.\n\n{{}}\n\nData Access {#data-access}\n\nWithout Nested Procedures\n\n    When procedures cannot be nested, allocation of storage for variables\n    is simple:\n\n    Global variables are allocated static storage, the locations are\n        fixed and known at compile time. Access to any variable that is not\n        local to the currently existing procedure can be accessed using the\n        statically determined address.\n    Any other name must be local to the activation at the top of the\n        stack. These variables are accessed through the top_sp pointer of\n        the stack.\n\n    This allows declared procedures to be passed as parameters or returned\n    as results (a pointer to the function), without changing the\n    data-access strategy.\n\nWith Nested Procedures\n\n    Knowing at compile time that the declaration of p is immediately\n    nested in q, says nothing about the relative positions of their\n    activation records at run time.\n\n    Finding the declaration that applies to a nonlocal name x in a nested\n    procedure p is a static decision, and can be done by extending the\n    static-scope rule for blocks. The fix for this is to use access links.\n\n    If a procedure p is nested immediately within procedure q in the\n    source code, then the access link in any activation of p points to the\n    most recent activation of q. Access links form a chain from the\n    activation record at the top of the stack to a sequence of activations\n    at progressively lower nesting depths.\n\n    When procedure q calls procedure p, we consider 2 cases:\n\n    Procedure p is at higher nesting depth than q. If so, then p must\n        be defined immediately within q, or the call by q would not be at a\n        position that is within the scope of prodecure p. Hence, the\n        nesting depth of p is exactly one greater than that of q. In this\n        case the access link for p is a pointer to the activation record of q.\n    The nesting depth \\\\(n\\p \\le n\\q\\\\). In order for the call within q to\n        be in the scope of p, q must be nested within some procedure r,\n        while p is a procedure defined within r. The top activation record\n        for r can be found by following the chain of access links, starting\n        in the activation record for q, for \\\\(n\\p - n\\q + 1\\\\) hops. Then the\n        access link of p must go to activation of r, including recursive\n        calls. The chain of access links is followed for one hop, and the\n        access links for p and q are the same.\n\n    When a procedure p is passed to another procedure q as a parameter,\n    then q calls its parameter, it is possible that q does not know the\n    context in which p appears in the program. Hence, the caller needs to\n    pass the proper access link for that parameter.\n\n    The caller always knows the link, since if p is passed by procedure r\n    as an actual parameter, then p must be a name accessible to r, and\n    hence r can determine the access link for p as if p were being called\n    by r directly.\n\nDisplays\n\n    In practice, we use an auxilliary array \\\\(d[i]\\\\) called the display,\n    holding a pointer to the activation records at varying nesting depths\n    \\\\(i\\\\). Since the compiler knows \\\\(i\\\\), it can generate code to access\n    nonlocals with a single access to the display.\n\n    To maintain the display properly, we need to save previous values of\n    display entries in the new activation records.\n\nHeap Management {#heap-management}\n\nMemory Manager\n\n    The memory manager is a subsystem that allocates and deallocates space\n    within the heap.\n\n    The desirable properties of a memory manager are:\n\n    space efficiency: minimizing the total heap space required by a\n        program. This allows larger programs to run in a fixed virtual\n        address space.\n    program efficiency: make good use of the memory subsystem to allow\n        programs to run faster. This includes exploiting locality.\n    low overhead: it should take as little time as possible to allocate\n        and deallocate space. This is however amortized over a large amount\n        of computation.\n\nMemory Hierarchy\n\n    {{}}\n\nLocality in programs\n\n    temporal locality\n    : memory locations it accesses are likely to be\n        accessed again within a short period of time\n\n    spatial locality\n    : memory access to locations nearby are likely to\n        be accessed within a short period of time\n\n    Locality allows us to take advantage of the memory hierarchy, by\n    placing the most common instructions into fast-but-small storage,\n    leaving the rest in slow-but-large storage.\n\nReducing fragmentation\n\n    Free chunks of memory are called holes. With each allocation of\n    memory, the memory manager must place the requested chunk of memory in\n    a large enough hole. Over time, holes will be split into smaller holes\n    for allocation.\n\n    Contiguous holes are coalesced into larger holes. However, free memory\n    may end up being fragmented, where a large number of small,\n    non-contiguous holes are available. In this case there is not enough\n    aggregate free space to satisfy a future allocation request.\n\n    Best-fit spares the larger holes for future larger requests. First-fit\n    allocates the first hole in which it fits. This takes a smaller amount\n    of time to place objects, but has been found to be inferior to\n    best-fit in overall performance.\n\n    To implement best-fit, free space is chunked into bins, according to\n    their sizes. One practical idea is to have many more bins for smaller\n    sizes, because there are usually many more small objects. Larger-\n\n    If there is a bin for chunks of that size only, we may take any\n        chunk from that bin\n    For size that do not have a private bin, we find one bin that is\n        allowed to fit chunks of the desired size. Within that bin, we use\n        either a first-fit or best-fit strategy\n    If teh target bin is empty, or all chunks in that bin are too small\n        to satisfy the space request, then repeat the search on bins for\n        larger sizes.\n\n    Best-fit placement tends to improve space utilization, but often at\n    the expense of spatial locality. One modification is to modify the\n    placement in the case when a chunk of the exact requested size cannot\n    be found. The next-fit strategy tries to allocate the object in the\n    chunk that has last been split, whenever enough space for the new\n    object remains in that chunk.\n\nCoalescing Free Space\n\n    When an object is deallocated, we may want to coalesce chunks with\n    adjacent chunks in the heap to form a larger chunk. When we use\n    binning, we may prefer not to do so. Instead, we can simply use a\n    bitmap to indicate whether a chunk is occupied. When a chunk is\n    deallocated, we change the bit from a 1 to 0.\n\n    There are 2 data structures that can be used for coalescing chunks\n    when not using binning, or when moving the resultant coalesced chunk\n    into a larger bin.\n\n    Boundary tags: at both the low and high ends of the chunk, we keep\n        a free/used bit that tells us whether or not the block is currently\n        allocated or available. Adjacent to each free/used bit is a count\n        of the total number of bytes in the chunk.\n    Doubly-linked, embedded free list: The free chunks are linked in a\n        doubly-linked list. The pointers for this list are within the\n        blocks themselves, e.g. adjacent to the boundary tags at either\n        end. No additional space is required for the free list, although\n        its existence place a lower bound on how small chunks can get: they\n        must accommodate 2 boundary tags and 2 pointers, even if the object\n        is a single byte. The order of chunks on the free list is\n        unspecified: it could be sorted by size to facilitate best-fit placement.\n\nGarbage Collection\n\n    There are popular conventions and tools developed to cope with the\n    complexity of managing memory:\n\n    object ownership\n    : An owner is associated with each object at all\n        times. The owner is a pointer to the object, belonging to some\n        function invocation. The owner is responsible for deleting or\n        passing the object to another owner. Nonowning pointers exist,\n        but cannot delete the object. This eliminates memory leaks, as\n        well as attempts to delete the same object twice. This does not\n        solve the dangling-pointer-reference problem, since it is\n        possible to follow a nonowning pointer to an object that has been\n        deleted. This is useful when an object's lifetime can be reasoned\n        about statically.\n\n    Reference counting\n    : A count is associated with each dynamically\n        allocated object. Whenever a reference is removed, the count is\n        decremented. When the count goes to zero, the object can be\n        safely deleted. This does not catch circular data structures.\n        However, it eradicates all dangling-pointer references. This is\n        expensive because it imposes an overhead on every operation that\n        stores a pointer. This is useful when an object's lifetime needs\n        to be determined dynamically.\n\n    Region-based allocation\n    : When objects are created to be used only\n        within some step of a computation, we can allocate all such\n        objects in the same region. We then delete the region once the\n        computational step completes. This has limited applicability but\n        is efficient, since the deletion occurs in a wholesale fashion.\n        This is useful when a collection of objects have lifetimes\n        associated with phases of computation.\n\nGarbage collection Design Goals\n\n    We assume that objects have types that can be determined at run-time.\n    This allows us to determine the size of the object, and which\n    components of the object contain references to other objects. A user\n    program called the mutator, modifies the collection of objects in the\n    heap. Objects become garbage when the mutator program cannot reach\n    them. The garbage collector finds these unreachable objects and\n    reclaims their space by handing them to the memory manager for\n    deallocation.\n\n    Garbage collection can be expensive, so we often want to track\n    performance metrics:\n\n    overall execution time\n    space usage\n    pause time\n    program locality\n\nReachability\n\n    We refer to all data that can be accessed by a program without\n    de-referencing any pointer, as the root set. In Java, this\n    corresponds to all static field members and all variables on its\n    stack.\n\n    To find the correct root set, a compiler may have to:\n\n    restrict the invocation of garbage collection to only certain code\n        points in the program\n    write out information that the garbage collector can use to recover\n        all the references, such as specifying which registers contain\n        references, or how to compute the base address of an object given\n        its internal address\n    the compiler can assure that there is a reference to the base\n        address of all reachable objects whenever the garbage collector is\n        invoked\n\n    The set of reachable objects changes as a program executes.  There are\n    four basic operations that a mutator performs to change the set:\n\n    Object allocations\n    Parameter passing, and return values\n    Reference assignments\n    Procedure returns\n\n    There are 2 approaches to determining reachability:\n\n    Reference counting as an approximation: maintain a count of\n        references to an boject, as the mutator performs actions that may\n        change the set. When the reference count becomes 0, the object is\n        no longer reachable\n    A trace-based GC labels all objects in the root as reachable, and\n        examines iteratively all references in them to find more reachable\n        objects, and labels them as such. Once the reachable set is\n        computed, it can find many unreachable objects at once. An option\n        is to relocate the reachable objects and reduce fragmentation.\n\nTrace-based GCs\n\n    Each chunk is in 1 of 4 states:\n\n    free\n    : ready to be allocated\n\n    unreached\n    : chunks presumed to be unreachable, unless proven\n        reachable by tracing. A chunk is in this state at any\n        point during GC if reachability has not yet been\n        established. Whenever a chunk is allocated by the\n        memory manager, its state is set to unreached. After a\n        round of GC, it is reset to unreached for the next round.\n\n    unscanned\n    : chunks that are known to be reachable are either in\n        state of unscanned or scanned. A chunk is in the\n        unscanned state if it is known to be reachable, but its\n        pointers have not yet been scanned.\n\n    scanned\n    : every unscanned object will eventually be scanned and\n        transition to the scanned state. To scan an object, we\n        examine all pointers within it, and follow these\n        pointers.\n\n    {{}}\n\n    {{}}\n\nMark-and-compact GCs\n\n    Relocating collectors move reachable objects around in the heap to\n    eliminate memory fragmentation. There are 2 types:\n\n    mark-and-compact, which compacts objects in place. This reduces\n        memory usage.\n    copying collector is more efficient, but extra space needs to be\n        reserved for relocation.\n\n    mark-and-compact has 3 phases:\n\n    marking phase\n    scan the allocated section of the heap and compute new addresses\n        for each of the reachable objects.\n    copies objects to the new locations, updating all references in\n        objects that point to the new locations.\n\nTools {#tools}\n\nLex {#lex}\n\nA Lex compiler takes a Lex source program, and outputs a program. This\nprogram is compiled in its source language (originally C, jFlex for\nJava). The result program takes an input stream as input, and produces\na sequence of tokens as output.\n\nA lex program has the following form:\n\ndeclarations\n%%\ntranslation rules\n%%\nauxiliary functions\n\nThe declaration section includes declarations of variables, manifest\nconstants, and regular definitions.\n\nThe translation rules have form: Pattern { Action }.\n\nEach pattern is a regular expression, which may use the definitions of\nthe declaration section. The actions are fragments of code.\n\nThe auxiliary functions section contains used in these actions.\n",
        "tags": [
            "proglang",
            "compilers"
        ]
    },
    {
        "uri": "/zettels/computer_organization",
        "title": "Computer Organization",
        "content": "\ntags\n: §operating\\_systems\n\nPipelining {#pipelining}\n\nIncreases throughput, but not latency\n\nStructural Hazard {#structural-hazard}\n\nData Hazard {#data-hazard}\n\nResolved with extra hardware\n\nControl Hazard {#control-hazard}\n\nBranch instructions need to tell which branch, but have only just\n    read from memory\nBranch prediction (static/dynamic)\n\nPipelined Datapath and Control {#pipelined-datapath-and-control}\n\nIF: Instruction Fetch\nID: Instruction Decode and register file read\nEX: Execution or address calculation\nMEM: Data memory access\nWB: Write Back\n\nIssues {#issues}\n\nWrite-back stage places the result back into the register file in\n    the middle of the data path (Data Hazard)\nSelection of the next value of the PC, between incremented PC and\n    branch address from the MEM stage (Control Hazard)\n\nData flow does not affect current instruction, but only influence\nlater instructions.\n",
        "tags": []
    },
    {
        "uri": "/zettels/computer_vision",
        "title": "Computer Vision",
        "content": "\nPrerequisites {#prerequisites}\n\nLinear Algebra\n\nCamera Basics {#camera-basics}\n\nSingle Lens Reflex (SLR) {#single-lens-reflex--slr}\n\nA camera that typically uses a mirror and prism system that permits\nthe photographer to view through the lens and see exactly what will be\ncaptured.\n\n{{}}\n\nDSLR (Digital SLR) {#dslr--digital-slr}\n\n{{}}\n\nMatte focusing screen: screen on which the light passing through\n    the lens will project\nCondensing lens: A lens that is used to concentrate the incoming light\nPentaprism: To produce a correctly oriented, right side up image\n    and project it into the viewfinder eyepiece\nAF sensor: accomplishes auto-focus\nViewfinder eyepiece: Allows us to see what will be recorded by the\n    image sensor\nLCD Screen: display stored photos or what will be recorded by image sensor\nImage sensor: contains a large number of pixels for converting an\n    optical image into electrical signals. Charge-coupled device (CCD)\n    and Complementary Metal-oxide-semiconductor (CMOS) are the more\n    common ones.\nAE sensor: accomplishes auto exposure\nSub mirror: To reflect light passing through semi transparent main\n    mirror onto AF sensor\nMain mirror: reflect light into viewfinder compartment. Small\n    semi-transparent area to facilitate AF.\n\nIntroduction {#introduction}\n\nDespite the advances in research in computer vision, the dream of\nhaving a computer interpret an image at the same level of a human is\nstill far away. Computer vision is inherently a difficult problem, for\nmany reasons. First, it is an inverse problem, in which we seek to\nrecover some unknowns given insufficient information to specify the\nsolution. Hence, we resort to physics-based and probabilistic models\nto disambiguate between potential solutions.\n\nForward models that we use in computer vision are usually grounded in\nphysics and computer graphics. Both these fields model how objects\nmove and animate, how light reflects off their surfaces, is scattered\nby the atmosphere, refracted through camera lenses and finally\nprojected onto a flat image plane.\n\nIn computer vision, we want to describe the world that we see in one\nor more images and to reconstruct its properties, such as shape,\nillumination and colour distributions. Some examples of computer vision\nbeing used in real-world applications include Optical Character\nRecognition (OCR) , machine inspection, retail, medical imaging, and\nautomotive safety.\n\nIn many applications, it is better to think back from the problem at\nhand to suitable techniques, typical of an engineering approach. A\nheavy emphasis will be placed on algorithms that are robust to noise,\nand are reasonably efficient.\n\n{{}}\n\nThe above figure shows a rough layout of the content of computer\nvision, and we see that from top to bottom, there are increasing\nlevels of modeling and abstraction.\n\nImage Formation {#image-formation}\n\nGeometric Primitives {#geometric-primitives}\n\nGeometric primitives form the basic building blocks used to describe\nthree-dimensional shapes.\n\n2D points can be denoted using a pair of values, \\\\(x = (x, y) \\in\n\\mathbb{R}^2\\\\):\n\n\\begin{equation}\n  x = \\begin{bmatrix}\n    x \\\\\\\\\\\\\n    y\n  \\end{bmatrix}\n\\end{equation}\n\n2D points can also be represented using homogeneous coordinates,\n \\\\(\\tilde{\\mathbf{x}} = (\\tilde{x}, \\tilde{y}, \\tilde{w}) \\in \\mathbb{P}^2\\\\), where vectors\n that differ only by scale are considered to be equivalent.\n \\\\(\\mathbb{P}^2 = \\mathbb{R}^3 - (0, 0, 0)\\\\) is called the _2D projective\n space_.\n\nA homogeneous vector \\\\(\\tilde{\\mathbf{x}}\\\\) can be converted back into an\nin-homogeneous vector \\\\(\\mathbf{x}\\\\) by diving through the last element\n\\\\(\\tilde{w}\\\\).\n\n2D lines can be represented using homogeneous coordinates\n\\\\(\\tilde{\\mathbf{l}} = (a, b, c)\\\\). The corresponding line equation is:\n\n\\begin{equation}\n  \\bar{\\mathbf{x}} \\cdot \\tilde{\\mathbf{l}} = ax + by + c = 0\n\\end{equation}\n\nThe line equation vector can be normalized so that \\\\(\\mathbf{l} =\n(\\hat{n}\\x, \\hat{n}\\y, d) = (\\hat{\\mathbf{n}}, d)\\\\)\n with \\\\(\\lvert\n\\hat{\\mathbf{n}} \\rvert = 1\\\\). When using homogeneous coordinates, we\ncan compute the intersection of two lines as\n\n\\begin{equation}\n  \\tilde{\\mathbf{x}} = \\tilde{\\mathbf{l}}\\1 \\times \\tilde{\\mathbf{l}}\\2\n\\end{equation}\n\nSimilarly, the line joining two points can be written as:\n\n\\begin{equation}\n  \\tilde{\\mathbf{l}} = \\tilde{\\mathbf{x}}\\1 \\times \\tilde{\\mathbf{x}}\\2\n\\end{equation}\n\nConic sections (which arise from the intersection of a plane and a 3d\ncone) can be written using a quadric equation\n\n\\begin{equation}\n  \\tilde{\\mathbf{x}}^T\\mathbf{Q}\\tilde{\\mathbf{x}} = 0\n\\end{equation}\n\n3D points can be written using in-homogeneous coordinates \\\\(\\mathbf{x} =\n(x,y,z) \\in \\mathbb{R}^3\\\\), or homogeneous coordinates \\\\(\\tilde{\\mathbf{x}} =\n(\\tilde{x}, \\tilde{y}, \\tilde{z}, \\tilde{w}) \\in \\mathbb{P}^3\\\\).\n\n3D planes can be represented as homogeneous coordinates \\\\(\\tilde{\\mathbf{m}}\n= (a, b, c, d)\\\\) with the equation:\n\n\\begin{equation}\n\\bar{\\mathbf{x}} \\cdot \\tilde{\\mathbf{m}} = ax + by + cz + d = 0\n\\end{equation}\n\n3D lines can be represented using 2 points on the line \\\\((\\mathbf{p},\n\\mathbf{q})\\\\). Any other point on the line can be expressed as a linear\ncombination of these 2 points.\n\n\\begin{equation}\n  \\mathbf{r} = (1 - \\lambda)\\mathbf{p} + \\lambda \\mathbf{q}\n\\end{equation}\n\n2D Transformations {#2d-transformations}\n\nThe basic primitives introduced above can be transformed, the simplest\nof which occur in the 2D plane.\n\n{{}}\n\n2D translations can be written as \\\\(\\mathbf{x}' = \\mathbf{x} +\n\\mathbf{t}\\\\), or:\n\n\\begin{align}\n  \\mathbf{x}' &= \\begin{bmatrix}\n              \\mathbf{I} & \\mathbf{t}\n              \\end{bmatrix}\\bar{\\mathbf{x}} \\\\\\\\\\\\\n              &= \\begin{bmatrix}\n                 \\mathbf{I} & \\mathbf{t} \\\\\\\\\\\\\n                 \\mathbf{0}^T & 1\n              \\end{bmatrix}\\bar{\\mathbf{x}}\n\\end{align}\n\nwhere \\\\(\\mathbf{0}\\\\) is the zero vector.\n\nThe combination of rotation and translation is known as 2D _rigid body\nmotion_, or the 2D Euclidean transformation, since Euclidean distances\nare preserved. It can be written as \\\\(\\mathbf{x}' =\n\\mathbf{R}\\mathbf{x} + \\mathbf{t}\\\\) or:\n\n\\begin{equation}\n  \\mathbf{x}' = \\begin{bmatrix}\n              \\mathbf{R} & \\mathbf{t}\n              \\end{bmatrix}\\bar{\\mathbf{x}}\n\\end{equation}\n\nwhere\n\n\\begin{equation}\n  \\mathbf{R} = \\begin{bmatrix}\n    \\cos \\theta & - \\sin \\theta \\\\\\\\\\\\\n    \\sin \\theta & \\cos \\theta\n  \\end{bmatrix}\n\\end{equation}\n\nis an orthonormal rotation matrix with\n\\\\(\\mathbf{R}\\mathbf{R}^T=\\mathbf{I}\\\\) and \\\\(\\lVert R \\rVert = 1\\\\).\n\nThe similarity transform, or scaled rotation, can be expressed as\n\\\\(\\mathbf{x}' = s\\mathbf{R}\\mathbf{x} + \\mathbf{t}\\\\). This preserves\nangles between lines.\n\nThe affine transformation is written as \\\\(\\mathbf{x}' =\n\\mathbf{A}\\hat{\\mathbf{x}}\\\\), where \\\\(\\mathbf{A}\\\\) is an arbitrary \\\\(2 \\times\n3\\\\) matrix.\n\nParallel lines remain parallel under affine transformations.\n\nAffine transformations with 6 unknowns can be solved via SVD by\nforming a matrix equation of the form \\\\(Mx = b\\\\). Local transformations\napply different transformations to different regions, and give finer control.\n\nThe projective transformation, also known as the perspective transform\nor homography, operates on homogeneous coordinates:\n\n\\begin{equation}\n  \\hat{\\mathbf{x}}' = \\tilde{\\mathbf{H}}\\tilde{\\mathbf{x}}\n\\end{equation}\n\nwhere \\\\(\\tilde{\\mathbf{H}}\\\\) is an arbitrary \\\\(3 \\times 3\\\\) matrix. Note that\n\\\\(\\tilde{\\mathbf{H}}\\\\) is homogeneous.\n\nEach of these transformation preserves some properties, and can be\npresented in a hierarchy.\n\n{{}}\n\nSome transformations that cannot be classified so easily include:\n\nStretching and Squashing\nPlanar surface flow\nBilinear interpolant\n\nThe set of 3D transformations are similar to the 2D\ntransformations.\n\n{{}}\n\n3D Rotations {#3d-rotations}\n\nThe biggest difference between 2D and 3D coordinate transformations is\nthat the parameterization of the 3D rotation matrix \\\\(\\mathbf{R}\\\\) is\nnot as straightforward.\n\nEuler Angles {#euler-angles}\n\nA rotation matrix can be formed as the product of three rotations\naround three cardinal axes, e.g. \\\\(x\\\\), \\\\(y\\\\), and \\\\(z\\\\). This is generally\na bad idea, because the result depends on the order of\ntransformations, and it is not always possible to move smoothly in a\nparameter space.\n\nAxis/angle (exponential twist) {#axis-angle--exponential-twist}\n\nA rotation can be represented by a rotation axis \\\\(\\hat{\\mathbf{n}}\\\\)\nand an angle \\\\(\\theta\\\\), or equivalently by a 3D vector \\\\(\\mathbf{\\omega} =\n\\theta\\hat{\\mathbf{n}}\\\\). We can write the rotation matrix corresponding to\na rotation by \\\\(\\theta\\\\) around an axis \\\\(\\hat{\\mathbf{n}}\\\\) as:\n\n\\begin{equation}\n  \\mathbf{R}(\\hat{\\mathbf{n}}, \\theta) = \\mathbf{I} + \\sin \\theta\n  [\\hat{\\mathbf{n}}]\\\\times + \\left(1-\\cos\\theta\\right)[\\hat{\\mathbf{n}}]^2\\\\times\n\\end{equation}\n\nAlso known as Rodriguez's formula.\n\nFor small rotations, this is an excellent choice, as it simplifies to:\n\n\\begin{equation}\n  \\mathbf{R}(\\mathbf{\\omega}) \\approx \\mathbf{I} + \\sin\\theta[\\hat{\\mathbf{n}}]\\_\\times = \\begin{bmatrix}\n    1 & -\\omega\\x & -\\omega\\y \\\\\\\\\\\\\n    \\omega\\z & 1 & -\\omega\\x \\\\\\\\\\\\\n    -\\omega\\y & \\omega\\x & 1\n  \\end{bmatrix}\n\\end{equation}\n\nThis gives a nice linearized relationship between the rotation\nparameters \\\\(\\omega\\\\) and \\\\(\\mathbf{R}\\\\).  We can also compute the derivative\nof \\\\(\\mathbf{R}v\\\\) with respect to \\\\(\\omega\\\\),\n\n\\begin{equation}\n\\frac{\\partial \\mathbf{R}v}{\\partial \\omega^T} = -[\\mathbf{v}]\\_\\times = \\begin{bmatrix}\n  0 & z & -y \\\\\\\\\\\\\n  -z & 0 & x \\\\\\\\\\\\\n  y & -x & 0\n\\end{bmatrix}\n\\end{equation}\n\nUnit Quarternions {#unit-quarternions}\n\nA unit quarternion is a unit length 4-vector whose components can be\nwritten as \\\\(\\mathbf{q} = (x, y, z, w)\\\\). Unit quarternions live on the\nunit sphere \\\\(\\lVert q \\rVert = 1\\\\) and antipodal quartenions, \\\\(q\\\\) and\n\\\\(-q\\\\) represent the same rotation. This representation is continuous\nand are very popular representations for pose and for pose\ninterpolation.\n\nQuarternions can be derived from the axis/angle representation through\nthe formula:\n\n\\begin{equation}\n  \\mathbf{q} = (\\mathbf{v}, w) = \\left(\\sin\\frac{\\theta}{2}\\hat{\\mathbf{n}}, \\cos\\frac{\\theta}{2}\\right)\n\\end{equation}\n\nwhere \\\\(\\hat{\\mathbf{n}}\\\\) and \\\\(\\theta\\\\) are the rotation axis and angle.\nRodriguez's formula can be converted to:\n\n\\begin{equation}\n  \\mathbf{R}(\\hat{\\mathbf{n}}, \\theta) = \\mathbf{I} + 2w[\\mathbf{v}]\\\\times + 2[\\mathbf{v}]^2\\\\times\n\\end{equation}\n\nThe nicest aspect of unit quarternions is that there is a simple\nalgebra for composing rotations expressed as unit quartenions:\n\n\\begin{equation}\n  \\mathbf{q}\\2 = \\mathbf{q}\\0 \\mathbf{q}\\1 = (\\mathbf{v}\\0 \\times \\mathbf{v}\\1 + w\\0 \\mathbf{v}\\1 + w\\1 \\mathbf{v}\\0, w\\0 w\\1 - \\mathbf{v}\\0 \\cdot \\mathbf{v}\\_1)\n\\end{equation}\n\nThe inverse of a quarternion is just flipping the sign of \\\\(\\mathbf{v}\\\\)\nor \\\\(w\\\\), but not both. Then quarternion division can be defined as:\n\n\\begin{equation}\n  \\mathbf{q}\\2 = \\mathbf{q}\\0 / \\mathbf{q}\\1 = (\\mathbf{v}\\0 \\times \\mathbf{v}\\1 + w\\0 \\mathbf{v}\\1 - w\\1 \\mathbf{v}\\0, - w\\0 w\\1 - \\mathbf{v}\\0 \\cdot \\mathbf{v}\\_1)\n\\end{equation}\n\n3D to 2D projections {#3d-to-2d-projections}\n\n{{}}\n\nWe need to specify how 3D primitives are projected onto the image\nplane. The simplest model is orthography, which requires no division\nto get the final (in-homogeneous) result. The more commonly used model\nis perspective, since this more accurately models the behavior of\nreal cameras.\n\nOrthography {#orthography}\n\nAn orthographic projection simply drops the \\\\(z\\\\) component of the\nthree-dimensional coordinate \\\\(\\mathbf{p}\\\\) to obtain the 2D point\n\\\\(\\mathbf{x}\\\\).\n\n\\begin{equation}\n  \\mathbf{x} = \\left[\\mathbf{I}\\_{2\\times 2} | \\mathbf{0} \\right] \\mathbf{p}\n\\end{equation}\n\nIn practice, world coordinates need to be scaled to fit onto an image\nsensor, for this reason, scaled orthography is actually more commonly\nused:\n\n\\begin{equation}\n\\mathbf{x} = \\left[s\\mathbf{I}\\_{2 \\times 2}\\right | \\mathbf{0}]\\mathbf{p}\n\\end{equation}\n\nThis model is equivalent to first projecting the world points onto a\nlocal fronto-parallel image plane, and then scaling this image using\nregular perspective projection.\n\nA closely related model is called para-perspective, which projects the\nobject points onto a local reference plane parallel to the image\nplane. However, rather than being projected orthogonally to this\nplane, they are projected parallel to the line of sight to the object\ncenter. This is followed by the usual projection onto the final image\nplane, and the combination of these two projections is affine.\n\n\\begin{equation}\n\\tilde{\\mathbf{x}} = \\begin{bmatrix}\n  a\\{00} & a\\{01} & a\\{02} & a\\{03} \\\\\\\\\\\\\n  a\\{10} & a\\{11} & a\\{12} & a\\{13} \\\\\\\\\\\\\n  0 & 0 & 0 & 1\n\\end{bmatrix}\n\\tilde{\\mathbf{p}}\n\\end{equation}\n\nPerspective {#perspective}\n\nPoints are projected onto the image plane by dividing them by their\n\\\\(z\\\\) component. Using homogeneous coordinates, this can be written as:\n\n\\begin{equation}\n\\tilde{\\mathbf{x}} = \\mathcal{P}\\_z(\\mathbf{p}) = \\begin{bmatrix}\nx / z \\\\\\\\\\\\\ny / z \\\\\\\\\\\\\n1\n\\end{bmatrix}\n\\end{equation}\n\nIn homogeneous coordinates, the projection has a simple linear form,\n\n\\begin{equation}\n\\tilde{\\mathbf{x}} = \\begin{bmatrix}\n  1 & 0 & 0 & 0 \\\\\\\\\\\\\n  0 & 1 & 0 & 0 \\\\\\\\\\\\\n  0 & 0 & 1 & 0 \\\\\\\\\\\\\n\\end{bmatrix}\\tilde{\\mathbf{p}}\n\\end{equation}\n\nwe drop the \\\\(w\\\\) component of \\\\(\\mathbf{p}\\\\). Thus after projection, we\nare unable to recover the distance of the 3D point from the image.\n\nCamera Instrinsics {#camera-instrinsics}\n\nOnce we have projected a 3D point through an ideal pinhole using a\nprojection matrix, we must still transform the resulting coordinates\naccording to the pixel sensor spacing and the relative position of the\nsensor plane to the origin.\n\nImage sensors return pixel values indexed by integer pixel coordinates\n\\\\((x\\s, y\\s)\\\\). To map pixel centers to 3D coordinates, we first scale he\n\\\\((x\\s, y\\s)\\\\) values by the pixel spacings \\\\((s\\x, s\\y)\\\\), and then describe\nthe orientation of the sensor array relative to the camera projection\ncenter \\\\(\\mathbf{O}\\c\\\\) with an origin \\\\(\\mathbf{c}\\s\\\\) and a 3D rotation\n\\\\(\\mathbf{R}\\_s\\\\).\n\n\\begin{equation}\n\\mathbf{p} = \\left[\\mathbf{R}\\s | \\mathbf{c}\\s \\right] \\begin{bmatrix}\ns\\_x & 0 & 0 \\\\\\\\\\\\\n0 & s\\_y & 0 \\\\\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\\\\n0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx\\_s \\\\\\\\\\\\\ny\\_s \\\\\\\\\\\\\n1\n\\end{bmatrix} = \\mathbf{M}\\s \\hat{\\mathbf{x}}\\s\n\\end{equation}\n\nThe first 2 columns of the \\\\(3 \\times 3\\\\) matrix \\\\(\\mathbf{M}\\_s\\\\) are the 3D vectors\ncorresponding to the unit steps in the image pixel array along the\n\\\\(x\\s\\\\) and \\\\(y\\s\\\\) directions, while the third column is the 3D image array\norigin \\\\(\\mathbf{c}\\_s\\\\).\n\nThe matrix \\\\(\\mathbf{M}\\_s\\\\) is parameterized by 8 unknowns, and that\nmakes estimating the camera model impractical, even though there are\nreally only 7 degrees of freedom. Most practitioners assume a general\n\\\\(3 \\times 3\\\\) homogeneous matrix form.\n\n\\begin{align}\n    P &= \\overbrace{K}^\\text{Intrinsic Matrix} \\times \\overbrace{[R \\mid  \\mathbf{t}]}^\\text{Extrinsic Matrix} \\\\\\[0.5em]\n     &=\n        \\overbrace{\n\n            \\underbrace{\n                \\left (\n                \\begin{array}{ c c c}\n                 1  &  0  & x\\_0 \\\\\\\\\\\\\n                 0  &  1  & y\\_0 \\\\\\\\\\\\\n                 0  &  0  & 1\n                \\end{array}\n                \\right )\n            }\\_\\text{2D Translation}\n\n            \\times\n\n            \\underbrace{\n                \\left (\n                \\begin{array}{ c c c}\n                f\\_x &  0  & 0 \\\\\\\\\\\\\n                 0  & f\\_y & 0 \\\\\\\\\\\\\n                 0  &  0  & 1\n                \\end{array}\n                \\right )\n            }\\_\\text{2D Scaling}\n\n            \\times\n\n            \\underbrace{\n                \\left (\n                \\begin{array}{ c c c}\n                 1  &  s/f\\_x  & 0 \\\\\\\\\\\\\n                 0  &    1    & 0 \\\\\\\\\\\\\n                 0  &    0    & 1\n                \\end{array}\n                \\right )\n            }\\_\\text{2D Shear}\n\n        }^\\text{Intrinsic Matrix}\n\n        \\times\n\n        \\overbrace{\n        \\underbrace{\n             \\left( \\begin{array}{c | c}\n            I & \\mathbf{t}\n             \\end{array}\\right)\n        }\\_\\text{3D Translation}\n        \\times\n        \\underbrace{\n             \\left( \\begin{array}{c | c}\n            R & 0 \\\\ \\hline\n            0 & 1\n             \\end{array}\\right)\n        }\\_\\text{3D Rotation}\n        }^\\text{Extrinsic Matrix}\n    \\end{align}\n\nLens distortion {#lens-distortion}\n\nThus far, it has been assumed that the cameras obey a linear\nprojection model. In reality, many wide-angled lens suffer heavily\nfrom radial distortion, which manifests itself as a visible curvature\nin the projection of straight lines. Fortunately, compensating for\nradial distortion  is not that difficult in practice. The radial\ndistortion model says that the coordinates in the observed images are\ndisplaced away (barrel distortion) or towards (pincushion distortion)\nthe image center by an amount proportional to their radial distance.\n\n{{}}\n\nCamera Calibration {#camera-calibration}\n\nWe want to use the camera to tell us things about the world, so we\nneed the relationship between coordinates in the world, and\ncoordinates in the image.\n\nGeometric camera calibration is composed of:\n\nextrinsic parameters (camera pose)\n: from some arbitrary world\n    coordinate system to the camera's 3D coordinate system\n\nintrinsic parameters\n: From the 3D coordinates in the camera frame\n    to the 2D image plane via projection\n\nExtrinsic Parameters\n\n    The transform \\\\(T\\\\) is a transform that goes from the world to the\n    camera system.\n\n    Translation\n\n        The coordinate \\\\(P\\\\) in \\\\(B\\\\)'s frame is the coordinate \\\\(P\\\\) in frame \\\\(A\\\\),\n        and the location of the camera in frame \\\\(B\\\\).\n\n        \\begin{equation}\n          ^B P = ^A P + ^B O\\_A\n        \\end{equation}\n\n        \\begin{equation}\n          \\begin{bmatrix}\n            ^B P \\\\\\\\\\\\\n            1\n            \\end{bmatrix} = \\begin{bmatrix}\n              I\\{3\\times3} & ^B O\\A \\\\\\\\\\\\\n              0^T & 1\n            \\end{bmatrix} \\begin{bmatrix}\n              ^A P \\\\\\\\\\\\\n              1\n            \\end{bmatrix}\n        \\end{equation}\n\n    Rotation\n\n        We can similarly describe a rotation matrix:\n\n        \\begin{equation}\n        ^B P = ^B \\_A R ^AP\n        \\end{equation}\n\n        \\begin{equation}\n          ^B\\_A R = \\begin{bmatrix}\n            ^B i\\A & ^B j\\A & ^B k\\_A\n          \\end{bmatrix} =\n          \\begin{bmatrix}\n            ^Ai\\_B^T \\\\\\\\\\\\\n            ^Aj\\_B^T \\\\\\\\\\\\\n            ^Ak\\_B^T\n          \\end{bmatrix}\n        \\end{equation}\n\n        Under homogeneous coordinates, rotation can also be expressed as a\n        matrix multiplication.\n\n        \\begin{equation}\n          \\begin{bmatrix}\n            ^B P \\\\\\\\\\\\\n            1\n          \\end{bmatrix} = \\begin{bmatrix}\n            ^B\\_AR & 0 \\\\\\\\\\\\\n            0^T & 1\n          \\end{bmatrix} \\begin{bmatrix}\n            ^A P \\\\\\\\\\\\\n            1\n          \\end{bmatrix}\n        \\end{equation}\n\n        Then, we can express rigid transformations as:\n\n        \\begin{equation}\n          \\begin{bmatrix}\n            ^B P \\\\\\\\\\\\\n            1\n          \\end{bmatrix} = \\begin{bmatrix}\n            1 & ^BO\\_A \\\\\\\\\\\\\n            0^T & 1 \\\\\\\\\\\\\n          \\end{bmatrix} \\begin{bmatrix}\n            ^B\\_AR & 0 \\\\\\\\\\\\\n            0^T & 1 \\\\\\\\\\\\\n          \\end{bmatrix} \\begin{bmatrix}\n            ^A P \\\\\\\\\\\\\n            1\n          \\end{bmatrix} = \\begin{bmatrix}\n            ^B\\AR & ^BO\\A \\\\\\\\\\\\\n            0^T & 1\n          \\end{bmatrix} \\begin{bmatrix}\n            ^A P \\\\\\\\\\\\\n            1\n        \\end{equation}\n\n        And we write:\n\n        \\begin{equation}\n        ^B\\_A T = \\begin{bmatrix}\n            ^B\\AR & ^BO\\A \\\\\\\\\\\\\n            0^T & 1\n          \\end{bmatrix}\n        \\end{equation}\n\n        {{}}\n\n        The world to camera transformation matrix is the extrinsic parameter\n        matrix (4x4).\n\n        {{}}\n\n        The rotation matrix \\\\(R\\\\) has two important properties:\n\n        \\\\(R\\\\) is orthonormal: \\\\(R^T R = I\\\\)\n        \\\\(|R| = 1\\\\)\n\n        One can represent rotation using Euler angles:\n\n        pitch (\\\\(\\omega\\\\))\n        : rotation about x-axis\n\n        yaw (\\\\(\\phi\\\\))\n        : rotation about y-axis\n\n        roll (\\\\(\\kappa\\\\))\n        : rotation about z-axis\n\n        Euler angles can be converted to rotation matrix:\n\n        \\begin{align}\n          R &= R\\x R\\y R\\_z\n        \\end{align}\n\n        Rotations can also be specified as a right-handed rotation by an angle\n        \\\\(\\theta\\\\) about the axis specified by the unit vector \\\\(\\left(\\omega\\_x,\n        \\omega\\y, \\omega\\z \\right)\\\\).\n\n        This has the same disadvantage as the Euler angle representation,\n        where algorithms are not numerically well-conditioned. Hence, the\n        preferred way is to use quarternions. Rotations are represented with\n        unit quarternions.\n\nIntrinsic Parameters\n\n    We have looked at perspective projection, and we obtain the ideal\n    coordinates:\n\n    \\begin{align}\n      u &= f \\frac{X}{Z} \\\\\\\\\\\\\n      v &= f \\frac{Y}{Z}\n    \\end{align}\n\n    However, pixels are arbitrary spatial units, so we introduce an alpha\n    to scale the value.\n\n    \\begin{align}\n      u &= \\alpha \\frac{X}{Z} \\\\\\\\\\\\\n      v &= \\alpha \\frac{Y}{Z}\n    \\end{align}\n\n    However, pixels may not necessarily be square, so we have to introduce\n    a different parameter for \\\\(u\\\\) and \\\\(v\\\\).\n\n    \\begin{align}\n      u &= \\alpha \\frac{X}{Z} \\\\\\\\\\\\\n      v &= \\beta \\frac{Y}{Z}\n    \\end{align}\n\n    We don't know the origin of our camera pixel coordinates, so we have\n    to add offsets:\n\n    \\begin{align}\n      u &= \\alpha \\frac{X}{Z} + u\\_0 \\\\\\\\\\\\\n      v &= \\beta \\frac{Y}{Z} + v\\_0\n    \\end{align}\n\n    We also assume here that \\\\(u\\\\) and \\\\(v\\\\) are perpendicular. To correct for\n    this, we need to introduce skew coefficients:\n\n    \\begin{align}\n      u &= \\alpha \\frac{X}{Z} - \\alpha \\cot \\theta \\frac{Y}{Z} + u\\_0 \\\\\\\\\\\\\n      v &= \\frac{\\beta}{\\sin \\theta} \\frac{Y}{Z} + v\\_0\n    \\end{align}\n\n    We can simplify this by expressing it in homogeneous coordinates:\n\n    {{}}\n\n    The 3x4 matrix is the intrinsic matrix.\n\n    This can be represented in an easier way:\n\n    {{}}\n\n    And if we assume:\n\n    pixels are square\n    there is no skew\n    and the optical center is in the center, then \\\\(K\\\\) reduces to\n\n    \\begin{equation}\n    K = \\begin{bmatrix}\n      f & 0 & 0 \\\\\\\\\\\\\n      0 & f & 0 \\\\\\\\\\\\\n      0 & 0 & 1\n    \\end{bmatrix}\n    \\end{equation}\n\nCombining Extrinsic and Intrinsic Calibration Parameters\n\n    We can write:\n\n    \\begin{equation}\n      p' = K \\begin{bmatrix}\n        ^C\\WR & ^C\\Wt\n      \\end{bmatrix} ^Wp\n    \\end{equation}\n\nPhotometric image formation {#photometric-image-formation}\n\nImages are not composed of 2D features, but of discrete colour or\nintensity values. Where do these values come from, and how do they\nrelate to the lighting in the environment, surface properties and\ngeometry, camera optics and sensor properties?\n\nLighting {#lighting}\n\nTo produce an image, a scene must be illuminated with one or more\nlight sources.\n\nA point light source originates at a single location in space. In\naddition to its location, a point light source has an intensity and a\ncolour spectrum (a distribution over wavelengths).\n\nAn area light source with a diffuser can be modeled as a finite\nrectangular area emitting light equally in all directions. When the\ndistribution is strongly directional, a four-dimensional lightfield\ncan be used instead.\n\nReflectance and shading {#reflectance-and-shading}\n\nWhen light hits an object surface, it is scattered and reflected. We\nlook at some more specialized models, including the diffuse, specular\nand Phong shading models.\n\nThe Bidirectional Reflectance Distribution Function (BRDF)\n\n    Relative to some local coordinate frame on the surface, the BRDF is a\n    four-dimensional function that describes how much of each wavelength\n    arriving at an incident direction \\\\(\\hat{\\mathbf{v}}\\_i\\\\) is emitted in a\n    reflected direction \\\\(\\hat{\\mathbf{v}}\\_r\\\\). The function can be written\n    in terms of the angles of the incident and reflected directions\n    relative to the surface frame as \\\\(f\\r(\\theta\\i, \\phi\\i, \\theta\\r,\n    \\phi\\_r;\\lambda)\\\\).\n\n    BRDFs for a given surface can be obtained through physical modeling,\n    heuristic modeling or empirical observation. Typical BRDFs can be\n    split into their diffuse and specular components.\n\nDiffuse Reflection\n\n    The diffuse component scatters light uniformly in all directions and\n    is the phenomenon we most normally associate with shading. Diffuse\n    reflection also often imparts a strong body colour to the light.\n\n    When light is scattered uniformly in all directions, the BRDF is\n    constant:\n\n    \\begin{equation}\n    f\\d(\\hat{\\mathbf{v}}\\i, \\mathbf{v}}\\r, \\mathbf{n}};\\lambda) = f\\d(\\lambda)\n    \\end{equation}\n\n    and the amount of light depends on the angle between the incident\n    light direction and the surface normal \\\\(\\theta\\_i\\\\).\n\nSpecular Reflection\n\n    The specular reflection component heavily depends on the direction of\n    the outgoing light. Incident light rays are reflected in a direction\n    that is rotated by 180^&deg; around the surface normal\n    \\\\(\\hat{\\mathbf{n}}\\\\).\n\n    {{}}\n\nPhong Shading\n\n    Phong combined the diffuse and specular components of reflection with\n    another term, which he called the ambient illumination. This term\n    accounts for the fact that objects are generally illuminated not only\n    by point light sources but also by a general diffuse illumination\n    corresponding to inter-reflection or distance sources. In the Phong\n    model, the ambient term does not depend on surface orientation, but\n    depends on the colour of both the ambient illumination \\\\(L\\_a(\\lambda)\\\\)\n    and the object \\\\(k\\_a(\\lambda)\\\\),\n\n    \\begin{equation}\n    f\\a(\\lambda) = k\\a(\\lambda) L\\_a(\\lambda)\n    \\end{equation}\n\n    The Phong shading model can then be fully specified as:\n\n    \\begin{equation}\n    L\\r(\\hat{\\mathbf{v}}\\r ; \\lambda) = k\\a(\\lambda) L\\a(\\lambda)\n    k\\d(\\lambda) \\sum\\i L\\i(\\lambda) [\\hat{\\mathbf{v}}\\i \\cdot \\hat{\\mathbf{n}}]^+\n    k\\s(\\lambda) \\sum\\i L\\i(\\lambda) (\\hat{\\mathbf{v}}\\r \\cdot \\hat{\\mathbf{s}}\\i)^{k\\e}\n    \\end{equation}\n\n    The Phong model has been superseded by other models in terms of\n    physical accuracy. These models include the di-chromatic reflection\n    model.\n\nOptics\n\n    Once the light from a scene reaches a camera, it must still pass\n    through the lens before reaching the sensor.\n\nImage Processing {#image-processing}\n\nPoint Operators {#point-operators}\n\nPoint operators are image processing transforms where each output\npixel's value depends only on the corresponding input pixel value.\nExamples of such operators include:\n\nbrightness and contrast adjustments\ncolour correction and transformations\n\nImage Enhancement {#image-enhancement}\n\nHistogram Equalization {#histogram-equalization}\n\nThe underlying math behind histogram equalization involves mapping one\ndistribution (the given histogram of intensity values) to another\ndistribution (a wider and, ideally, uniform distribution of intensity\nvalues).\n\n{{}}\n\nWe may use the cumulative distribution function to remap the original\ndistribution as an equally spread distribution simply by looking up\neach y-value in the original distribution and seeing where it should\ngo in the equalized distribution.\n\n\\begin{equation}\n  g\\{i,j} = \\left\\lfloor \\left( L - 1 \\right) \\sum\\{n = 0}^{f\\_{i,j}}\n  p\\_n  \\right\\rfloor\n\\end{equation}\n\nConvolutions {#convolutions}\n\nConvolution is the process of adding each element of the image to its\nlocal neighbors, weighted by the kernel.\n\nConvolutions can be used to denoise, descratch, blur, unblur and even\nfeature extraction.\n\nMedian filtering is good for removing salt-and-pepper noise, or\nscratches in image\n\n{{}}\n\nColor {#color}\n\nA human retina has 2 kinds of light receptors: rods are sensitive to\namount of light, while cones are sensitive to wavelengths of light\n\nThere are 3 kinds of cones:\n\nshort\n: most sensitive to blue\n\nmedium\n: most sensitive to green\n\nlong\n: most sensitive to red\n\nCones send signals to the brain, and the brain interprets this mixture\nof signals as colours. This gives rise to the RGB colour coding\nscheme. Different coding schemes have different colour spaces.\n\nCones are sensitive to various colours, ranging from wavelengths of\n400nm (violet) to 700nm (red).\n\nThere are some regions that extend beyond the visible region, but are\nstill relevant to image processing:\n\n0.7-1.0\\\\(\\mu m\\\\): Near infrared (NIR)\n1.0-3.0\\\\(\\mu m\\\\): Short-wave infrared (SWIR)\n3.0-5.0\\\\(\\mu m\\\\): Mid-wave infrared (MWIR)\n8.0-12.0\\\\(\\mu m\\\\): Long-wave infrared (LWIR)\n12.0-1000.0\\\\(\\mu m\\\\): Far infrared or very long-wave infrared (VLWIR)\n\nThe range 5-8\\\\(\\mu m\\\\) corresponds to a wavelength spectrum that is\nlargely absorbed by the water in the atmosphere.\n\nColor constancy is the ability of the human visual system to be immune\nto changing illumination in perception of colour. The human colour\nreceptors perceive the overall effect of the mixture of colours, and\ncannot tell its composition.\n\nGamut is the range of colours that can be reproduced with a given\ncolour reproduction system.\n\nIn the RGB colour space, each value is an unsigned 8-bit value from\n0-255.\n\nIn the HSV (Hue Saturation Value) colour space, hue corresponds to\ncolour type from 0 (red) to 360. Saturation corresponds to the\ncolourfulness (0 - 1 full colour), while value refers to the\nbrightness (0 black - 1 white).\n\nThe YCbCRr Colour space is used for TV and video. Y stands for\nluminance, Cb blue difference, and Cr red difference.\n\nThere are several colour conversion algorithms to convert values in\none colour space to another.\n\nPrimary colours are the set of colours combined to make a range of\ncolours. Since human vision is trichromatic, we only need to use 3\nprimary colours. The combination of primary colours can be additive or\nsubtractive.\n\nExamples of additive combinations include overlapping projected lights\nand CRT displays. RGB is commonly used in additive combinatinos.\nExamples of subtracting combinations include mixing of color pigments\nor dyes. The primary colours used in these cases are normally cyan,\nmagenta and yellow.\n\nMeasuring Colour Differences {#measuring-colour-differences}\n\nThe simplest metric is the euclidean distance between colours in the\nRGB space:\n\n\\begin{equation}\n  d(C\\1, C\\2) = \\sqrt{\\left( R\\1 - R\\2 \\right)^2 + \\left( G\\1 - G\\2\n    \\right)^2 + \\left( B\\1 - B\\2 \\right)^2}\n\\end{equation}\n\nHowever, the RGB space is not perceptually uniform, and this is\ninappropriate if one needs to match human perception. HSV, YCbCr are\nalso not perceptually uniform. Some colour spaces that are more\nperceptually uniform are the Munsell, CIELAB and CIELUB colour spaces.\n\nComputing Means {#computing-means}\n\nThe usual formula of computing means \\\\(M = \\frac{1}{n}S =\n\\frac{1}{n}\\sum\\{i=1}^n R\\i\\\\) can lead to overflow even for small \\\\(n\\\\).\nOne way to get around it is to use a floating point representation for\n\\\\(S\\\\). The second method is to do incremental averaging:\n\n\\begin{equation}\n  M\\k = \\frac{k-1}{k}M\\{k-1} + \\frac{1}{k}R\\_k\n\\end{equation}\n\nDigital Cameras sensing colour {#digital-cameras-sensing-colour}\n\nBayer filter\n\nChange Detection {#change-detection}\n\nDetecting change between 2 video frames is straightforward -- compute\nthe differences in pixel intensities across the two frames:\n\n\\begin{equation}\n  D\\_t(x, y) = | I(x,y,t+1)  - I(x,y,t)|\n\\end{equation}\n\nIt is common to use a threshold for \\\\(D\\_t(x,y)\\\\) to declare if a pixel\nhas changed.\n\nTo detect positional changes, the method used must be immune to\nillumination change. This requires motion tracking.\n\nAt the same time, to detect illumination change, the method must be\nimmune to positional change. In the case of a stationary scene and\ncamera, the straightforward method can be used. However, in the non\ntrivial case, motion tracking will be required.\n\nMotion Tracking {#motion-tracking}\n\nThere are two approaches to motion tracking: feature-based and\nintensity-gradient based.\n\nFeature-based {#feature-based}\n\nFeature-based motion tracking utilises distinct features that changes\npositions. For each feature, we search for the matching feature in the\nnext frame, to check if there is a displacement.\n\nGood features are called \"corners\". The two popular corner detectors\nare the Harris corner detector and the Tomasi corner detector.\n\nAlthough corners are only a small percentage of the image, they\ncontain the most important features in restoring image information,\nand they can be used to minimize the amount of processed data for\nmotion tracking, image stitching, building 2D mosaics, stereo vision,\nimage representation and other related computer vision areas.\n\nHarris corner detector {#harris-corner-detector}\n\nCompared to the Kanade-Lucas-Tomasi corner detector, the Harris corner\ndetector provides good repeatability under changing illumination and\nrotation, and therefore, it is more often used in stereo matching and\nimage database retrieval.\n\nInterpreting the eigenvalues:\n\n{{}}\n\nIn flat regions, the eigenvalues are both small, in edges, only one of\nthe eigenvalues are large. On the other hand, in corners, both\neigenvalues are large but the 2 eigenvalues of the same magnitude, the error\n\\\\(E\\\\) increases in all directions.\n\n{{}}\n\nThe Harris corner response function essentially filters out the corners.\n\nProperties\n\n    Harris corner detector is invariant to rotation: Ellipse has the\n        same eigenvalues regardless of rotation.\n    Mostly invariant to additive and multiplicative intensity changes\n        (threshold issue for multiplicative)\n    Not invariant to image scale!\n\nTomasi corner detector {#tomasi-corner-detector}\n\n\\begin{equation}\n  \\frac{1}{N} \\sum\\{u} \\sum\\v \\begin{bmatrix}\n    I\\x^2 & I\\x I\\_y \\\\\\\\\\\\\n    I\\x I\\y & I\\_y^2 \\\\\\\\\\\\\n  \\end{bmatrix}\n\\end{equation}\n\nwhere \\\\(I\\_x = \\frac{\\partial I}{\\partial x}\\\\), \\\\(N\\\\) is the total number\nof pixels in window of interest, \\\\(u\\\\) and \\\\(v\\\\) are the horizontal and\nvertical index of the pixel in the window of interest.\n\nLet the eigenvalues of the above matrix be \\\\(\\lambda\\_{max}\\\\) and\n\\\\(\\lambda\\{min}\\\\). Then the greater \\\\(\\lambda\\{min}\\\\), the more\n\"cornerness\" the feature.\n\nExamples of feature descriptors include SIFT and SURF. The typical\nworkflow involves:\n\nDetecting good features\nBuilding feature descriptors on each of these features\nMatching these descriptors on the second image to establish the\n    corresponding points\n\nGradient-based {#gradient-based}\n\nGradient-based motion tracking makes 2 basic assumptions:\n\nIntensity changes smoothly within image\nPixel intensities of a given object does not change over time\n\nSuppose that an object is in motion. Then the position of the object\nis given by \\\\((dx, dy)\\\\) over time \\\\(dt\\\\). From the brightness constancy\nassumption,\n\n\\begin{equation}\nI(x + dx, y + yd, t + dt) = I(x,y,t)\n\\end{equation}\n\nIf we apply the Taylor series expansion on the left hand side, we get:\n\n\\begin{equation}\n  I(x + dx, y + dy, t + dt) = I(x,y,t) + \\frac{\\partial I}{\\partial\n    x}dx + \\frac{\\partial I}{\\partial y} dy + \\frac{\\partial\n    I}{\\partial t}dt + \\dots\n\\end{equation}\n\nOmitting higher-order terms, we get\n\n\\begin{equation}\n  \\frac{\\partial I}{\\partial\n    x}dx + \\frac{\\partial I}{\\partial y} dy + \\frac{\\partial\n    I}{\\partial t}dt = 0\n\\end{equation}\n\nWe denote this as \\\\(I\\x u + I\\y v + I\\_t = 0\\\\), but this has 2 unknowns, and\nis unsolvable.\n\nLucas-Kanade method {#lucas-kanade-method}\n\nSuppose an object moves by displacement \\\\(\\mathbb{d} = (dx, dy)^T\\\\). Then\n\\\\(J(x+d) = I(x)\\\\), or \\\\(J(x) = I(x-d)\\\\).\n\nDue to noise, there is some error at position \\\\(x\\\\):\n\n\\begin{equation}\ne(x) = I(x - d) - J(x)\n\\end{equation}\n\nWe sum the errors over some window \\\\(W\\\\) at position \\\\(x\\\\):\n\n\\begin{equation}\nE(x) = \\sum\\_{x \\in W} w(x) \\left[ I(x-d) - J(x) \\right]^2\n\\end{equation}\n\nIf \\\\(E\\\\) is small, then the patterns in \\\\(I\\\\) and \\\\(J\\\\) match well. We find\nthe \\\\(d\\\\) that minimises \\\\(E\\\\). If we expand \\\\(I(x-d)\\\\) with Taylor\nexpansion:\n\n\\begin{equation}\nI(x-dx, y-dy) = I(x,y) - dx I\\x(x,y) - dy I\\y (x,y) + \\dots\n\\end{equation}\n\nThen,\n\n\\begin{equation}\n  J(x) = I(x - d) = I(x) - d^T g(x), g(x) = \\begin{bmatrix}\n    I\\_x(x) \\\\\\\\\\\\\n    I\\_y(x)\n  \\end{bmatrix}\n\\end{equation}\n\nWhere g(x) is the intensity gradient. Substituting the above equation,\nand setting \\\\(\\frac{\\partial E}{\\partial d} = 0\\\\):\n\n\\begin{equation}\n\\frac{\\partial E}{\\partial d} = -2 \\sum\\_{x \\in W} w(x) \\left[ I(x) -\n  J(x) - d^T g(x) \\right] g(x)\n\\end{equation}\n\n\\begin{equation}\n\\sum\\{x \\in W} w(x)\\left[ I(x) - J(x) \\right] g(x) = \\sum\\{x \\in W}\nw(x) g(x) g^T(x) d\n\\end{equation}\n\nWe denote this as:\n\n\\begin{equation}\n  Z d = b\n\\end{equation}\n\nwhere\n\n\\begin{equation}\nZ = \\begin{bmatrix}\n  \\sum\\{x \\in W} w I\\x^2 & \\sum\\{x \\in W} w I\\x I\\_y \\\\\\\\\\\\\n  \\sum\\{x \\in W} wI\\x I\\y & \\sum\\{x \\in W} w I\\_y^2\n\\end{bmatrix}, b = \\begin{bmatrix}\n  \\sum\\{x \\in W} w(I-J)I\\x \\\\\\\\\\\\\n  \\sum\\{x \\in W} w(I-J)I\\y\n\\end{bmatrix}\n\\end{equation}\n\nWith 2 unknowns and 2 equations, we can solve for \\\\(d\\\\).\n\nLucas-Kanade algorithm is often used with Harris/Tomasi's corner\ndetectors. First, corner detectors are applied to detect good\nfeatures, then LK method is applied to compute \\\\(d\\\\) for each pixel. \\\\(d\\\\)\nis then accepted only for good features.\n\nThe math of LK tracker assumes \\\\(d\\\\) is small, and would only work for\nsmall displacements. To handle large displacements, the image is\ndownsampled. Usually , the Gaussian filter is used to smoothen the\nimage before scaling down.\n\n{{}}\n\nHomography {#homography}\n\nThe planar homography relates the transformation between 2 planes, up\nto a scale factor:\n\n\\begin{equation}\ns \\begin{bmatrix}\n  x' \\\\\\\\\\\\\n  y' \\\\\\\\\\\\\n  1\n\\end{bmatrix} =\nH \\begin{bmatrix}\n  x \\\\\\\\\\\\\n  y \\\\\\\\\\\\\n  1\n\\end{bmatrix} =\n\\begin{bmatrix}\n  h\\{11} & h\\{12} & h\\_{13} \\\\\\\\\\\\\n  h\\{21} & h\\{22} & h\\_{23} \\\\\\\\\\\\\n  h\\{31} & h\\{32} & h\\_{33} \\\\\\\\\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  x \\\\\\\\\\\\\n  y \\\\\\\\\\\\\n  1\n\\end{bmatrix}\n\\end{equation}\n\nThe homography is a \\\\(3 \\times 3\\\\) matrix with 8 degrees of freedom, as it is\nestimated up to a scale.\n\n{{}}\n\nHomographies are used in:\n\ncamera pose estimation\npanorama stitching\nperspective removal/correction\n\nStructure For Motion {#structure-for-motion}\n\nIn general, a single image cannot provide 3D information. From a set\nof images taken with varying camera positions, we can extract 3D\ninformation of the scene. This requires us to match (associate)\nfeatures in one image with the same feature in another image.\n\nReferences {#references}\n\nComputer Vision\nCS231A: Computer Vision, From 3D Reconstruction to Recognition\nComputer Vision: Linda G. Shapiro, George C. Stockman\n",
        "tags": []
    },
    {
        "uri": "/zettels/conferences",
        "title": "Conferences",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/config_management",
        "title": "Config Management",
        "content": "\n§operating\\_systems\n\nDotfiles {#dotfiles}\n\nFor Nix, there's home-manager\nI currently use GNU stow\nA simple Git repository seems to work: Managing my dotfiles as a git repository | Drew DeVault’s Blog\n",
        "tags": []
    },
    {
        "uri": "/zettels/conor_white_sullivan",
        "title": "Conor White-Sullivan",
        "content": "\nCreator of Roam Research.\n\nsocials\n: Twitter\n",
        "tags": []
    },
    {
        "uri": "/zettels/consciousness",
        "title": "Consciousness",
        "content": "\nTheory of consciousness {#theory-of-consciousness}\n",
        "tags": []
    },
    {
        "uri": "/zettels/control_as_inference",
        "title": "Control As Inference",
        "content": "\ntags\n: Optimal Control and Planning, Reinforcement Learning ⭐\n\n{{}}\n\nWe introduce a binary variable for Optimality \\\\(\\mathcal{O}\\_t\\\\) at each\ntime-step. We want to infer: \\\\(p(\\tau | \\mathcal{O}\\_{1:T})\\\\)\n\nIf we choose \\\\(p(O\\t | s\\t, a\\t) = \\mathrm{exp}(r(s\\t, a\\_t))\\\\), then:\n\n\\begin{align}\n  p(\\tau | \\mathcal{O}\\_{1:T}) &= \\frac{p(\\tau,\n                                \\mathcal{O}\\{1:T})}{p(\\mathcal{O}\\{1:T})}\n  \\\\\\\\\\\\\n                              &\\propto \\prod\\{t} \\mathrm{exp}(r(s\\t,\n                                a\\_t)) \\\\\\\\\\\\\n                              &= p(\\tau) \\mathrm{exp} \\left( \\sum\\_{t}\n                                r(s\\t, a\\t) \\right)\n\\end{align}\n\nWith this Probabilistic Graph Model, we can:\n\nmodel sub-optimal behaviour (important for inverse RL)\ncan apply inference algorithms to solve control and planning problems\nprovides an explanation for why stochastic behaviour may be\n    preferred (useful for exploration and transfer learning)\n\nInference {#inference}\n\ncompute backward messages \\\\(\\beta\\t (s\\t, a\\_t) =\n       p(\\mathcal{O}\\{t:T} | s\\t, a\\_t)\\\\)\ncompute policy \\\\(p(a\\t | s\\t, \\mathcal{O}\\_{1:T})\\\\), the policy of\n    this model under assumption of optimality\ncompute forward messages \\\\(\\alpha\\t(s\\t) = p(s\\t | \\mathcal{O}\\{1:t-1})\\\\)\n    useful for figuring out which states the optimal policy lands\n        in, for the inverse RL problem (not used for forward RL)\n\nBackward Messages {#backward-messages}\n\n\\begin{align}\n  \\beta\\t (s\\t, a\\t) &= p(\\mathcal{O}\\{t:T} | s\\t, a\\t) \\\\\\\\\\\\\n                     &= \\int p(\\mathcal{O}\\{t:T}, s\\{t+1} | s\\t, a\\t)\n                       ds\\_{t+1} \\\\\\\\\\\\\n                     &= \\int p(\\mathcal{O}\\{t+1:T}|s\\{t+1})\n                       p(s\\{t+1}|s\\t,a\\t) p(\\mathcal{O}\\t | s\\t, a\\t)\n                       ds\\_{t+1}\n\\end{align}\n\n\\begin{align}\n  p(\\mathcal{O}\\{t+1:T} | s\\{t+1}) &= \\int p(\\mathcal{O}\\_{t+1:T} |\n                                     s\\{t+1}, a\\{t+1})p(a\\{t+1}| s\\{t+1}) da\\_{t+1} \\\\\\\\\\\\\n                                   &= \\int \\beta\\t(s\\{t+1}, a\\{t+1}) da\\{t+1}\n\\end{align}\n\nwhere we assume actions are likely a priori uniform. From these\nequations, we can get:\n\nFor \\\\(t = T-1 \\mathrm{ to } 1\\\\):\n\n\\begin{equation}\n  \\beta\\t(s\\t, a\\t) = p(\\mathcal{O}\\t | s\\t, a\\t) E\\{s\\{t+1} \\sim\n    p(s\\{t+1},a\\{t+1})} \\left[ \\beta\\{t+1} (s\\{t+1}) \\right]\n\\end{equation}\n\n\\begin{equation}\n  \\beta\\{t}(s\\t) = E\\{a\\t \\sim p(a\\t | s\\t)} \\left[ \\beta\\t(s\\t, a\\_t) \\right]\n\\end{equation}\n\nIf we choose \\\\(V\\t (s\\t) = \\log \\beta\\t (s\\t)\\\\) and \\\\(Q\\t(s\\t, a\\_t) =\n\\log \\beta\\t (s\\t, a\\_t)\\\\):\n\n\\begin{align}\nV\\t(s\\t) &= \\log \\int \\mathrm{exp} (Q\\t(s\\t, a\\t))da\\t \\\\\\\\\\\\\n         &\\rightarrow \\mathrm{max}\\{a\\t} Q\\t(s\\t, a\\_t) \\textrm { as\n           } Q\\t(s\\t, a\\_t) \\textrm { gets bigger }\n\\end{align}\n\nFor \\\\(Q\\\\):\n\n\\begin{equation}\n  Q\\t (s\\t, a\\t) = r(s\\t, a\\t) + \\log E\\left[ \\mathrm{exp} (V\\{t+1}\n    (s\\{t+1},  a\\{t+1})) \\right]\n\\end{equation}\n\nIn a deterministic transition setting, the log and exp cancel out.\nHowever, this otherwise results in an optimistic transition, which is\nnot a good idea!\n\nWhat if the action prior is not uniform? **We can always fold the action\nprior into the reward!**\n\nPolicy computation {#policy-computation}\n\n\\begin{align}\n  p(a\\t | s\\t, \\mathcal{O}\\{1:T}) &= \\pi (s\\t | a\\_t) \\\\\\\\\\\\\n                                  &= p(a\\t | s\\t, \\mathcal{O}\\_{t:T})\n  \\\\\\\\\\\\\n                                  &= \\frac{\\beta\\t(s\\t,\n                                    a\\t)}{\\beta\\t(s\\t)}p(s\\t|a\\_t) \\\\\\\\\\\\\n                                  &= \\frac{\\beta\\t(s\\t,\n                                    a\\t)}{\\beta\\t(s\\_t)}\n\\end{align}\n\nIt turns out the policy is just the ratio between the 2 backward\nmessages. Substituting \\\\(V\\\\) and \\\\(Q\\\\):\n\n\\begin{equation}\n  \\pi(a\\t | s\\t) = \\mathrm{exp}(Q\\t(s\\t, a\\t) - V\\t(s\\t)) = \\mathrm{exp}(A\\t(s\\t, a\\t))\n\\end{equation}\n\nOne can also add a temperature: \\\\(\\pi(a\\t | s\\t) =\n\\mathrm{exp}(\\frac{1}{\\alpha} A\\t(s\\t, a\\_t))\\\\)\n\nForward Messages {#forward-messages}\n\n\\begin{equation}\n  p(s\\t) \\propto \\beta\\t(s\\t) \\alpha\\t(s\\_t)\n\\end{equation}\n\nsame derivations as §hidden\\markov\\model!\n\n{{}}\n\nResolving Optimism with Variational Inference {#resolving-optimism-with-variational-inference}\n\n{{}}\n\nFor more, see (Levine, 2018).\n\nResources {#resources}\n\nCS285 Fa19 10/16/19 - YouTube\n\nBibliography\nLevine, S., Reinforcement learning and control as probabilistic inference: tutorial and review, CoRR, (),  (2018).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/conversation",
        "title": "Conversation",
        "content": "\nWhy study conversation? {#why-study-conversation}\n\nTt seems that everyone can conduct conversation\nThere are basic patterns and techniques\n    People do not adopt a totally new or different system for\n        communicating in any given situation\n    Conversational skill is adopted to other forms of interactive talk\n\nSociology {#sociology}\n\n> \"What I call social facts are the ways of acting, thinking, and\n> feeling external to the individual, and endowed with a power of\n> coercion, by reason of which they control him\"\n>\n> -   Emile Durkheim\n\nGarfinkel's 'breaching' experiment {#garfinkel-s-breaching-experiment}\n\nSpend from 15 minutes to an hour in their homes imagining that you\n    are boarders and act out this assumption. You should conduct\n    yourself in circumspect and polite fashion. Avoid getting personal,\n    to use formal address, to speak only when spoken to.\nClarify everything which may not be immediately understandable for\n    the non-family members\n\nWe obey certain social rules, and make underlying social assumptions\nin all our interactions. This brings about social order.\n\nWhat is a social situation? {#what-is-a-social-situation}\n\n> I would define a social situation as an environment of mutual\n> monitoring possibilities... wherein an individual will find themselves\n> accessible to the naked senses of all others who present -- and\n> similarly, find those others accessible to him.\n>\n> -   Erving Goffman\n\nLinguistics {#linguistics}\n\nTraditional linguistics see the language as an internal cognitive\n    representation of individuals.\nGrammatical categories reflect cultural patterns of thinking and\n    acting -- Boas, Sapir\nEncourage inquiry to speech event, speech situation, speech\n    community -- Gumperz, Hymes\n\nConversation Analysis {#conversation-analysis}\n\nTalk is highly organized; conversation analysis involves understanding\nhow conversational practices fit together in highly intricate ways.\n\nBasic assumptions {#basic-assumptions}\n\nOrder is produced orderliness\nOrder is produced, situated, and occasioned\nOrder is repeatable and recurrent\n\nCharacteristics of Conversation {#characteristics-of-conversation}\n\nInteractive\n    Take turns -- sequential activity\n    Talk is tied to prior and future turns\n    Each turn makes contributions to the conversation\nLocally managed\n    The organization and content of a conversation are not\n        predetermined or planned\nMundane\n    Commonplace and practical\n    Use language\n\nWhat we ask in CA {#what-we-ask-in-ca}\n\nWhat is the speaker doing in saying this and in saying it in this\n    way?\nWe do not ask what it means. We ask what it does.\n\nTurn taking {#turn-taking}\n\nTurn Constructional Units (TCU)\n\nTransition Relevance place (TRP)\n\ngrammatically complete\nintonationally complete\npragmatically complete\n\nWe are very attuned to grammatical intonation, so we know when to\nstart speaking.\n\ntransition Space {#transition-space}\n\nlapse\n: silence that does not belong to any particular speaker\n\nsilence\n: silence that belongs to any particular speaker\n",
        "tags": []
    },
    {
        "uri": "/zettels/copy_editing",
        "title": "Copy Editing",
        "content": "\ntags\n: §writing\n\nseries comma\n: always use the Oxford comma (or series comma)\n\nonly comma\n: use the only comma, where the subject is one of its\n    kind: e.g. His son, Juliet... where Juliet is the only son\n\nthat vs which\n: use that without a comma, where the information presented\n    is important, and which without a comma, where removing such\n    information would do no harm.\n\nReferences {#references}\n\nDreyer’s English: An Utterly Correct Guide to Clarity and Style by Benjamin D...\n",
        "tags": []
    },
    {
        "uri": "/zettels/cplusplus",
        "title": "C++",
        "content": "\nWhat is C++ {#what-is-c}\n\nC++ is a compiled language. For a program to run, its source text has\nto be processed by a compiler, producing object files, which are\ncombined by a linker yielding an executable program. An executable\nprogram is created for a specific hardware/system combination, and is\nnot portable.\n\nThe ISO C++ standard defines 2 kinds of entities:\n\nCore language features, such as built-in types and loops\nStandard-library components, such as containers (vector and map)\n    and IO operations\n\nC++ is statically typed: the type of each entity must be known to the\ncompiler at its point of use.\n\nReferences vs Pointers {#references-vs-pointers}\n\nA reference is similar to a pointer, except that you don't need to use\na prefix \\* to access the value referred to by the reference. In\naddition, a reference cannot be made to refer to a different object\nafter its initialization.\n\nint count_x(const char* p, char x) {\n  if (p == nullptr) return 0;\n  int count = 0;\n  for (; *p != 0; ++p) {\n    if (*p == x) {\n      ++count;\n    }\n  }\n  return count;\n}\n\nC++ Guidelines {#c-guidelines}\n\nFollow the guidelines:\n\nUse General {} style declarations {#use-general-style-declarations}\n\nint i1 = 7.8; // i1 becomes 7\nint i2 {7.8}; // error: floating-point to integer conversion\n\nThe old = style is traditional and dates back to C. Conversions that\nlose information, are allowed and implicitly applied. These are the\nprice paid for C compatibility.\n\nUse non-member begin and end {#use-non-member-begin-and-end}\n\nnon-member begin and end supports more containers, such as arrays.\n\nvector v;\nint a[100];\n\n// C++98\nsort(v.begin(), v.end());\nsort(&a[0], &a[0] + sizeof(a)/sizeof(a[0]));\n\n// C++11\nsort( begin(v), end(v) );\nsort( begin(a), end(a) );\n\nLambda functions {#lambda-functions}\n\nvariables get passed through the square brackets.\n\nconst int factor = 2;\n\nstd::for_each(myVec.begin(), myVec.end(), factor{\n  elem *= factor;\n});\n\nusing {#using}\n\nusing is like typedef, but has less quirks.\n\nusing Month = int;\n\nclass Date {\n    // ...\npublic:\n    Month month() const;  // do\n    int month();          // don't\n    // ...\n};\n\navoid endl {#avoid-endl}\n\nThe endl manipulator is mostly equivalent to '\\n' and \"\\n\"; as most\ncommonly used it simply slows down output by doing redundant flush()s.\nThis slowdown can be significant compared to printf-style output.\n\nPrefer using STL array or vector instead of a C array {#prefer-using-stl-array-or-vector-instead-of-a-c-array}\n\nC arrays are less safe, and have no advantages over array and vector.\nFor a fixed-length array, use std::array, which does not degenerate to\na pointer when passed to a function and does know its size. Also, like\na built-in array, a stack-allocated std::array keeps its elements on\nthe stack. For a variable-length array, use std::vector, which\nadditionally can change its size and handles memory allocation.\n\nOn Namespacing {#on-namespacing}\n\nHow do you properly use namespaces in C++? - Stack Overflow\n\nRAII {#raii}\n\nResource Acquisition Is Initialization or RAII, is a C++ programming\ntechnique which binds the life cycle of a resource that must be\nacquired before use (allocated heap memory, thread of execution, open\nsocket, open file, locked mutex, disk space, database\nconnection—anything that exists in limited supply) to the lifetime of\nan object. (nil, nil)\n\nThe basic idea is that class destructors are always called when a\nparticular instance of an object goes out of scope. This allows for\nautomatic releasing of resources that will never be referenced.\n\nFor shared_ptr, the class object contains a pointer to the object, and\na reference count for the number of pointers with access to the object\nat the pointer. Once this reference count hits 0, the object is\nreleased.\n\nFor unique_ptr, there is no reference count within the class. once\nthis unique\\_ptr goes out of scope, the object at the pointer is\nreleased. For this reason, unique_ptr are more lightweight than\nsharedptr, and cannot be copied. uniqueptr can only be moved.\n\nArray Decaying {#array-decaying}\n\nIt's said that arrays \"decay\" into pointers. A C++ array declared as\nint numbers [5] cannot be re-pointed, i.e. you can't say numbers =\n0x5a5aff23. More importantly the term decay signifies loss of type and\ndimension; numbers decay into int\\* by losing the dimension information\n(count 5) and the type is not int [5] any more.\n\ncin/cout vs scanf/printf {#cin-cout-vs-scanf-printf}\n\ncin/cout is actually faster; but C++ slows it down to sync it with\nC-style io. If using only one style (cin), you can achieve greater IO\nspeed with:\n\nstd::ios::syncwithstdio(false);\n\nSmart pointers {#smart-pointers}\n\nHere's a summary of smart-pointers and their semantics. value\\_ptr is\nnot in the stdlib, but is available as a C++ library.\n\n{{}}\n\nBooks to read {#books-to-read}\n\nLinks {#links}\n\nC++ Patterns\n\nBibliography\nnil,  (nil). Raii - cppreference.com. Retrieved from https://en.cppreference.com/w/cpp/language/raii. Online; accessed 25 January 2019. ↩\n",
        "tags": [
            "proglang",
            "c++"
        ]
    },
    {
        "uri": "/zettels/credit_assignment_snn",
        "title": "Credit Assignment in Spiking Neural Networks",
        "content": "\nThe problem of spatial and temporal credit assignment in RNNs are\nsolved through backpropagating errors in the unrolled RNN.\n\nAlgorithmic solutions to RNNs have 2 challenges in SNNs. First,\nspiking neurons have $S(U(t)) = \\\\(\\Theta(U(t) - \\theta)\\\\). Their\nderivative is zero everywhere except at \\\\(U = \\theta\\\\), where it is\nill-defined. This binary spiking non-linearity stops gradients from\nflowing, and makes gradient-based optimization unsuitable. The same\nissues occur in binary neurons.\n\nSecond, BP is expensive in terms of computation and memory. These\nrestrictions may be poorly suited to the hardware that implements it.\nFor example, non-von Neumann architectures have specific locality\nrequirements. The forward propagation approach may be more favourable.\n\nRelated {#related}\n\n§spiking\\neural\\networks\n",
        "tags": []
    },
    {
        "uri": "/zettels/critical_thinking",
        "title": "Critical Thinking",
        "content": "\nIntroduction {#introduction}\n\nCritical thinking is reasonable and reflective thinking focused on\ndeciding what to believe or do.\n\nIdeal critical thinkers are disposed to:\n\nCare that their beliefs be true, and their decisions be justified\n    Seek alternate hypotheses and explanations\n    Consider seriously other points of view\n    Try to be well informed\nCare to understand and present a position honestly and clearly.\n    Discover and listen to others' view and reasons\n    Be clear about the intended meaning of what is said\n    Determine and focus on the conclusion or question\n    Seek and offer reasons\n    Take into account the total situation\n    Be reflectively aware of basic beliefs\nCare about every person\n    Avoid intimidating or confusing others, account for others'\n        feelings and level of understanding\n\nCritical thinkers are able to do the following:\n\nFocus on a question\nAnalyze arguments\nAsk and answer clarification or challenge questions\nJudge the credibility of a source\nObserve and judge observation reports\nDeduce and judge deduction\nMake material inferences\n    To generalizations, and explanatory hypotheses\nMake and judge value judgments\nDefine terms and judge definitions, using appropriate criteria\nAttribute unstated assumptions that belong under both basic\n    clarification and inference\nConsider and reason from premises\nIntegrate the dispositions in making and defending a decision\nBe sensitive to feelings, level of knowledge and degree of\n    sophistication of others\nEmploy appropriate rhetorical strategies, reacting to fallacy\n    labels in an appropriate manner.\n\nMeta-cognitive Terms {#meta-cognitive-terms}\n\nA posteriori\n: Latin for reasoning or deductions made from known\n    facts or established knowledge that is not a\n    prediction or a hypothesis. Can also be used as an\n    adjective \"a posterior knowledge\", meaning knowledge\n    that is dependent on evidence or fact that has\n    already been established.\n\nA priori\n: Latin for reasoning or deductions made without the need\n    for known facts or knowledge, that is can be a\n    hypothesis or prediction, e.g. stangelets are an example\n    of a priori reasoning. Can also be used as an adjective\n    “a priori knowledge”, meaning a claim that is true\n    without having to be backed by empirical evidence, e.g.\n    all bachelors are unmarried.\n\nAbductive Reasoning\n: One of three types of reasoning, the others\n    being the more familiar deductive and inductive reasoning. It has\n    been described as reason that is most likely to lead to a\n    conclusion because it deals with a subject or knowledge that is\n    complete as not all the facts can be proven. It has been called\n    the type of reasoning that requires a leap of faith.\n\nAlternative Views\n: This is another interpretation of the same set\n    of facts or points of information.\n\nAnalysis\n: How information has been dissected to give meaning, e.g.\n    data analysis. This is the paramount indication of\n    critical thinking. It is the cognitive process of\n    “identifying the intended and actual inferential\n    relationships among statements, questions, concepts…or\n    other forms of representation intended to express\n    beliefs, judgments…information, or opinions.”\n\nAnecdotal Evidence\n: Proof which is possibly only true for some\n    specific instances, not necessarily for all instances. Proof that\n    may not necessarily be replicated by formal experiment.\n\nAppeal\n: An appeal is a technical term for a rhetorical device used\n    to make an argument more persuasive. Some forms of\n    persuasion are listed below, as can be seen some are more\n    rational than others. When unmasking assumptions and\n    evaluating arguments, attention should be paid to the type\n    of appeal being made and whether it is ethical in the\n    context, e.g. an appeal to emotion to raise money may or\n    may not be ethical depending on what pressure is being\n    applied to the emotion of the audience.\n    Appeal to Authority: This is an attempt to persuade an audience\n        that something is correct and should be accepted because it is\n        endorsed by an authority or an organization with a reputation\n        for good governance.\n    Appeal to Celebrity/Fashion: This is an attempt to persuade is\n        made to the audience’s desire to emulate or be like a famous\n        person or to follow the latest social trend, fashion here\n        refers to more than clothes, it may also refer to taking\n        selfies at graduation ceremonies because many people did so\n        this year.\n\nAppeal to Emotion\n: This persuasive technique plays on an\n    audience’s feelings, it uses the Aristotelean notion of pathos.\n    Its use may be valid e.g. in a court case to ask for clemency for\n    a pregnant murderer not be executed before her baby is born,\n    since the baby is innocent of the crime. An appeal to emotion may\n    be suspect when it is used to make an audience feel they are\n    lacking if they do not care about a particular issue, e.g. in an\n    advertisement to allow methadone to be sold as a heroine\n    substitute in mainstream pharmacists.\n\nAppeal to Identity\n: This persuasive technique works on an\n    audience’s sense of group versus self-identity, e.g. “Count on\n    me, Singapore”, “We are Singapore”, “Home” and other National Day\n    Parade songs work on national identity.\n\nAppeal To Logic/Reason\n: This persuasive technique relies on the use of\n    logic or a systematic set of propositions in order to convince\n    the audience of the credibility of the argument. It uses the\n    Aristotelean notion of logos, and is seen as the strongest form\n    of persuasion and therefore also argument. This persuasive\n    technique lends itself to technological and scientific arguments,\n    but there may be flaws in an argument that entirely relies on\n    logos, consider for instance, the sorts of argument that would\n    have had to be debated before the hydrogen bomb was dropped in\n    Hiroshima.\n\nAppeal to Morality\n: This persuasive technique works on the\n    audience’s sense of what is right, moral, or ethical. It uses the\n    Aristotelean notion of ethos, e.g. International pressure on\n    Ukraine to give more access to the crash site of MH17 is based on\n    an appeal to morality.\n\nAppeal to Popular Belief\n: This persuades through asking the\n    audience to consider some common value in humanity, or some\n    belief that many people value, e.g. family values, a problem may\n    arise when the definition of the key word, in this case “family”\n    changes form one section of the population to another, such as\n    traditional, blended and pink family values. An appeal to\n    commonality works when there is a strong association between the\n    audience and the issue at hand, e.g. environmental issues and a\n    sustainable fishery idea.\n\nAppeal to Self-interest\n: This persuasive technique leverages on\n    Maslow’s hierarchy of needs to convince an audience that a\n    specific course of action is for their own good, be it personal\n    security, self-worth, fear of hunger etc. This is most often used\n    in advertising, e.g. “Because you deserve it”.\n\nArgument\n: Any text graphic, written or spoken that uses reasons\n    to support the author’s position or point of view.\n\nAssertion\n: Statements that are made without any supporting\n    evidence of justification.\n\nAuthorial bias\n: The inclination or opinion that a writer or a\n    speaker has for a particular point of view.\n\nAuthorial voice\n: This is the opinion of the writer or the speaker,\n    and not the writer or the speaker quoting from or referring to\n    anyone else’s view. The authorial voice is important in any\n    argument as it shows the author’s critical thinking value add.\n\nBandwagon\n: This term is a metaphor for those who agree or support\n    a particular idea or campaign because it is fashionable\n    or popular. These people are said to have “jumped onto\n    the bandwagon” because they have supported something\n    without knowing why they have done so. They have been\n    caught up with the crowd.\n\nBias\n: Prejudice or propensity towards a particular view.\n\nCeteris Paribus\n: Latin for “all things being equal”. It is used as\n    a condition to preface a statement. E.g. “Ceteris paribus the\n    economy continues to grow at 5% annually, Nagaland will reach its\n    projected sales target by 2015.”\n\nClaim\n: A declarative (information giving) sentence which may be\n    true or false. All arguments have claims, when analyzing\n    arguments, the truth or otherwise of claims have to be\n    assessed. Claim and declarative statements are also term in\n    formal logic and C programming, but the definitions in\n    there are different.\n\nConclusion\n: When writing or speaking, it is the natural or logical\n    end point which all the reasons given in an argument\n    lead to. It can also be a judgment or summative\n    decision made after reading or listening to someone\n    else’s argument.\n\nConstruct an Argument\n: This means putting together a thesis or\n    hypothesis, a line of reason with a selection of reasons that\n    develop or support the thesis or hypothesis and a robust analysis\n    of the points of information,. All this leads to a warranted\n    conclusion. There should be care to ensure there are no unfair\n    instated assumptions, unwarranted inferences or fallacies in the\n    reasons or the line of reasoning. Care must also be given the\n    credibility of sources and the accuracy of data. The entire\n    cognitive process should be expressed cogently, so for instance,\n    care must be taken over word choice and expression of thought.\n\nCounter Argument\n: This refers to an opposing viewpoint or an\n    objection raised against a specific argument or claim.\n\nDeconstruct an Argument\n: This means to take apart an argument in\n    order to assess its component parts and then to evaluate the\n    argument as a whole.\n\nDeductive Reasoning\n: This is the type of reasoning that leads to a\n    confirmed or guaranteed conclusion. This is because all the known\n    facts are given or can be worked out. Mathematics is based\n    largely on deductive reasoning. It also refers to the sort of\n    line of reasoning in an argumentative essay where the author\n    states the thesis at the start of the essay and then sets out\n    his/her reasons and evidence in support of the thesis.\n\nDefinition\n: The meaning of a key word or phrase as used in an\n    argument, which need not be the same as the dictionary\n    meaning. But which has to be specified in the\n    argument.\n\nDisposition\n: Habit or attitude toward a trait of behavior. A\n    critical thinking disposition is specifically the use\n    of a self-regulatory systematic means to analyze\n    information. Such a disposition generally shows\n    diligence in seeking as much information as possible,\n    careful attention to detail, respect for the views of\n    others, consideration in the expressions of one’s own\n    stance, resilience in working out solutions and\n    open-mindedness.\n\nDouble-speak\n: Word or phrase that is deliberately unclear in\n    meaning, could be a political or military euphemism,\n    that is a nice sounding expression for something\n    that is actually unpleasant, e.g. “friendly fire”\n    and “collateral damage” – for being shot at by one’s\n    own side and being killed therefore by accident.\n\nEuphemism\n: A word or phrase that is used to tactfully or politely\n    refer to something unpleasant or socially taboo, e.g.\n    “Sanitation engineer” instead of “rubbish man”. It is\n    not usually meant to be deliberately deceiving unlike a\n    double-speak expression.\n\nEvaluation\n: Assessing the claims, assertions, opinions or other\n    representations of an argument or idea. Evaluation\n    necessitates identifying relevant points for the\n    argument, credibility of sources, logical consistency,\n    valid assumptions and appeals.\n\nEvidence\n: Proof that supports a point or conclusion or line of\n    argument. This may be data, observation, visual\n    representation e.g. photographs or video, artefact, e.g.\n    fossils, books, antiques etc.\n\nExplanation\n: This is the process either written or spoken of\n    justifying or giving reasons for a belief or a\n    decision. This is the verbal representation of one’s\n    thought processes.\n\nFact\n: Proof that is based on something that can be proven by\n    scientific experiment or replicating a social science\n    experiment, or something which is true by popular\n    observation e.g. the sun rises in the east and sets in the\n    west.\n\nFallacy\n: An error in reasoning or a flaw or defect in a reason or\n    a line of reasoning. It can also refer to an argument\n    being entirely fallacious, e.g. the flat Earth theory was\n    proven completely fallacious (adjective for fallacy),\n    once space travel allowed Man to see the planet from\n    above.\n\nFalse Logic\n: This is where the propositions do not add up to a\n    conclusion as stated probably because the context is\n    not fully explained or some conditions are not met,\n    e.g. Birds have wings. Airplanes have wings.\n    Therefore airplanes are birds. False logic is also a\n    term in mathematics, Boolean logic and computer\n    science – but the definitions in those other subjects\n    are different.\n\nGeneralization\n: There are valid generalizations and invalid\n    generalizations. A valid generalization is an\n    overall statement describing a consistently\n    observed pattern or trend in data or some other\n    empirical evidence or social phenomenon. An\n    invalid generalization is a statement purporting\n    to describe a pattern or trend in data, other\n    empirical evidence or social phenomenon that is\n    based on insufficient sampling, lacking\n    confirmatory evidence or using anecdotal evidence.\n    Invalid generalizations contribute to a weak\n    argument.\n\nHabit of Mind\n: Synonym for critical thinking disposition.\n\nInductive Reasoning\n: This is the type of reasoning that will give\n    a conclusion that is probably valid. It also refers to the sort\n    of line of reasoning in an argumentative essay where the author\n    starts with a specific question, then builds up a line of\n    reasoning that leads towards a conclusion.\n\nInference\n: Critical thinking skill used to identify the\n    information that is most pertinent to drawing a\n    reasonable conclusion, hypothesis, theory or educe\n    consequences arising from data, evidence, principles,\n    observations or other forms of representation of\n    information or knowledge.\n\nInterpretation\n: A specific person’s explanation of a phenomenon or\n    an event. Please see also ‘explanation”.\n\nJudicious\n: Adjective to describe when good or sound judgment or\n    reasoning is used. E.g. That argument used the data\n    judiciously to support the inferences and conclusion;\n    it did not make too big a claim\n\nJustification\n: The explanation for holding a particular view.\n\nLine of reasoning\n: This is the order in which the points selected\n    to support an argument are arranged. The usual line of reasoning\n    is that one point should lead into the next point and so build up\n    a convincing case for the argument.\n\nLogical consistency\n: This means that the points do not contradict\n    each other within the argument. Also all the reasons support the\n    conclusion.\n\nMan of straw\n: This is a kind of fallacy in argumentation when an\n    author deliberately uses something that has been\n    distorted or represented absurdly and is thus easy\n    to refute in order to make his/her argument look\n    stronger, e.g. inventing a fictitious person and\n    story to illustrate a point, taking an opponents’\n    words out of context. C.f.\n    . It\n    is sometimes referred to as Aunt Sally.\n\nNon sequitur\n: Latin for “does not follow from”. Used to show that\n    one point is illogical following from the point\n    before.\n\nOpen-minded\n: This is an attitude or a disposition where one is at\n    least receptive to another point of view, and to be\n    willing if necessary to change one’s own view in the\n    light of convincing new evidence or better reasoning.\n\nPosition\n: A point of view supported by reasoning.\n\nPredicate\n: The basis for which an argument is made, a technical term taken from formal logic.\n\nPremise\n: Propositions used that are held to be true and used as a basis for an argument. E.g. Our premise that the rule of law is essential in human society means that law enforcement is integral part of any country.\n\nPrima Facie\n: Latin term meaning “on the surface” or at face value.\n\nPropaganda\n: Information that is transmitted by an organization or government to disseminate a particular view on a topic. This is usually persuasive in nature and is different from public education as it has a more subjective content.\n\nProposition\n: A statement that is presented as true as part of an argument. This may turn out to be false after evaluation, or less valid than originally presented.\n\nPublic Education\n: Information disseminated to inform the general\n    population. Moe objective content than propaganda. E.g. a public\n    education poster on wearing condoms to avoid STD is different to\n    one that says having abortions is murder.\n\nReasoning\n: This is the process of thinking, also known as cognitive process.\n\nReflection\n: Thinking back to an action, event or a process to\n    consider what lessons may be learned in order to make\n    better progress. A critical thinking reflection is one\n    in which one looks back on a specific action, event or\n    process in a systematic way. Critical thinking\n    practitioners have developed a few templates that\n    others may use, or one can develop one’s own.\n\nRefutation\n: In an argument, there is a claim which may be followed\n    by a counter claim. The refutation is the counter\n    claim to the counter claim that reinstates the\n    validity of the original claim. E.g. Argument:\n    Sustained silent reading is a good way of improving\n    one’s language abilities in a language. Counter\n    argument: However, does everyone have enough time to\n    spend on sustained, silent reading every day?\n    Refutation: Stephen Krashen (1982,) has shown that\n    reading for as little as 25 minutes a day is\n    sufficient to build up language skills over time. It’s\n    a method in argumentation that allows the proposition\n    to anticipate an objection and to refute it before the\n    opposition does so. It strengthens the proposition’s\n    stance.\n\nSelf-regulation\n: Synonym for self-discipline, but especially with\n    regard to applying a critical thinking checklist to all\n    information that needs to be processed.\n\nStereotype\n: This is related to a generalization. A stereotype is\n    an unjustified conclusion or judgment about a specific\n    group. That is an unwarranted conclusion, e.g. doctors\n    are rich, Chinese mothers are tiger moms. Stereotypes\n    give rise to weak arguments.\n\nTautology\n: Unnecessary repetition, the author makes the same point\n    but may use different words. Repeating a point does not\n    reinforce it in a logical way.\n\nUnstated Assumption\n: This is a belief that the author of an\n    argument assumes his/her audience shares. It is on this unstated\n    assumption that the author rests his/her case. Unstated\n    assumptions are valid if they are indeed commonly believed e.g.\n    cultural assumption among members of the same culture, but they\n    are not valid if only the author or a select group actually\n    believes this, e.g. Libraries should have all sorts of books.\n    Unstated assumptions are often found in propaganda pieces, but\n    can also be found in most arguments. Most arguments following the\n    western scientific model will have unstated assumptions about the\n    conventions of good argument.\n\nValue Judgment\n: The value that someone places on something, this\n    is essentially subjective. Consider this sentence:\n    “More works of Shakespeare are sold annually than\n    Jane Austen novels”. This is a statement of fact\n    and can be verified with sales figures. However,\n    “Jane Austen’s books are much easier to read and\n    are more enjoyable than Shakespeare’s plays.” This\n    is a value judgment because it is subjective.\n\nWeasel Words\n: Words that make claims sound true but are actually\n    misleading or false. Weasel words are a sort of\n    fallacy, e.g. “many experts …” – this makes it look\n    as if something is endorsed by authorities in the\n    field, but is so vague that there is no possibility\n    of checking the credibility of sources. The use of\n    such words weakens an argument.\n\nCritical Thinking Components {#critical-thinking-components}\n\nSummary of specific information set\nsynthesis of relevant other information\nCT Value add (analysis, hypothesis, counter argument, refutation,\n    conclusion, application, inference)\nnuanced language use\n",
        "tags": []
    },
    {
        "uri": "/zettels/cryptography",
        "title": "Cryptography",
        "content": "\nDiffie-Hellman Algorithm {#diffie-hellman-algorithm}\n\nHow can 2 people who never met agree on a secret key, without a man in\na middle always listening, to figure the key out?\n\nOne-way function\n\nConsider a situation with 3 parties, Alice, Bob (the two parties\ntrying to communicate) and Eve, the thief.\n\nAlice and Bob agree on a public colour. Eve is able to detect the\n    public colour.\nAlice and Bob choose a private colour, and mix their private colours\n    with the public colour and send the mixtures over. Eve is able to\n    collect these mixtures.\nAlice and Bob add their private colour to the mixture sent by the\n    other person, to obtain the secret colour.\n\nEve is unable to obtain the secret colour because it requires the\nprivate colour to obtain.\n\nWe use modular arithmetic as the numerical procedure in place of\ncolours for the Diffie-Hellman algorithm.\n\nWe choose a prime, and a primitive root, such that \\\\(root^x mod prime\\\\)\nis uniformly distributed across all possible modulos. Modulo\narithmetic is a great example on a one-way function, where computation\nis easy to perform in one direction, but difficult to perform in reverse.\n\n{{}}\n",
        "tags": []
    },
    {
        "uri": "/zettels/css",
        "title": "CSS",
        "content": "\ntags\n: §web\\dev, §prog\\lang\n    Adding dark mode : [How To Add CSS Dark Mode To A Website - Kev\n        Quirk](https://kevq.uk/how-to-add-css-dark-mode-to-a-website/)\n\nFlexbox {#flexbox}\n\nAn introduction to CSS Containment - Rego's Everyday Life\n\nCSS Frameworks {#css-frameworks}\n\nTACHYONS - Css Toolkit\nTailwind CSS - A Utility-First CSS Framework for Rapid UI Development\n    Tailwind.css Inbox UI Demo\n\nAdvanced {#advanced}\n\nAn introduction to CSS Containment - Rego's Everyday Life\n",
        "tags": [
            "web"
        ]
    },
    {
        "uri": "/zettels/data_council",
        "title": "Datacouncil.ai Conference Notes",
        "content": "\ntags\n: §conferences, §data\\_science\n\nTaking recommendation technology to the masses - Le Zhang (Microsoft) {#taking-recommendation-technology-to-the-masses-le-zhang--microsoft}\n\nChallenges:\n\nLimited Resource\nFragmented solutions\n\n contains modular functions\nfor model creation, data manipulation, evaluation etc.\n\nSVD, SAR, ALS, NCF, Wide&Deep, xDeepFM, DKN etc.\n\nCollaborative Filtering {#collaborative-filtering}\n\nMemory based method:\n\nMicrosoft Smart Adaptive Recommendation (SAR) algorithm\n\nModel based methods\n\nMatrix factorization methods\n    Singular Value decomposition\n    Spark ALS\n\nNeural network-based methods\n    Restricted Boltzmann Machine (RBM)\n    Neural Collaborative Filtering (NCF)\n\nSAR (ipynb) {#sar--ipynb}\n\nItem-to-item similarity matrix via co-occurence\nUser-to-item affinity matrix via co-occurence of user-item interactions\n    weighted by interaction type and time decay:\n\n\\begin{equation}\n\n\\end{equation}\n\nFree from machine learning and feature collection\nExplainable results\n\nNeural Collaborative Filtering (NCF) {#neural-collaborative-filtering--ncf}\n\nNeural network based architecture to model latent features\nGeneralization of MF based method\n\nContent-based Filtering {#content-based-filtering}\n\n\"Content\" can be user/item features, review comments, knowledge\n    graph etc.\nMitigates cold-start issue\nFeature vector can be highly sparse\n\ne.g. Factorization machines\n\n\\begin{equation}\n  \\hat{y}(\\mathbf{x}) = w\\0 + \\sum\\{i=1}^{n} w\\i x\\i +\n  \\sum\\{i=1}^{n}\\sum\\{j=i+1}^n \\langle v\\i, v\\j \\rangle x\\i x\\j\n\\end{equation}\n\nTODO  xDeepFM\n\n    (Guo et al., 2017), (Lian et al., 2018)\n\nTODO Deep Knowledge-aware Network (Wang et al., 2018) {#deep-knowledge-aware-network}\n\nMulti-channel word-entity aligned knowledge aware CNN\n\nOperationalizing a real-time recommender {#operationalizing-a-real-time-recommender}\n\ncontainerize model serving, use Kubernetes to autoscale\n\nScaling Data Science Teams - Miguel Rios (Twitter) {#scaling-data-science-teams-miguel-rios--twitter}\n\n9-year longitudinal study of scaling data science at Twitter\n\n2 models of DS teams in engineering-driven organizations:\n\nembedded model\n: data scientists part of a smaller team, with other\n    engineers\n    Pros:\n        Dedicated data science resourcing\n        Alignment between DS and the rest of the team\n        One roadmap, fewer dependencies\n        Data science has a more natural \"seat at the table\"\n    Cons:\n        Rigid resourcing (harder to move DS between teams)\n        Barriers for collaboration between data scientists\n        Manager may not have domain knowledge (typically an EM)\n        Risk of Data Science being a support or service to eng. team\n\ncentralized model\n: data scientists manageed by a data science\n    manager, supporting the product teams\n    Pros:\n        Data scientists working together (collaboration and knowledge sharing)\n        DS manager has domain knowledge (between career dev)\n        Resources can be rebalanced to meet customer demand\n        Advocacy for better and consistent tech (tooling, datasets,\n            etc.)\n    Cons:\n        Coordiantion between teams (DS and stakeholder) becomes more complicated\n        In eng. centric orgs the DS teams need to influence org roadmap\n        Risk of data science work not being aligned with product\n        Company needs to support one more function\n\nBest of both worlds: centralized org with embedded teams\n\nE.g.\n\nGrowth Eng (with centralized DS)\nProduct Eng (with centralized DS)\nHealth Eng (with centralized DS)\n\nCentralized proceses, common resources\n\nChallenges:\n\nEveryone has at least 2 teams - centralized DS team, and part of the\n    product team\n    Risk of meeting and planning overload\n    Which is their main team?\nRisk of mismatch of expectation between DS leadership and product leadership\n\nHow to scale this hybrid org structure to ~100 Data Scientists?\n\n~ Create more layers of abstraction:\n\nSplit teams into pillars\n\n\"A product as a system\":\n\nGrowth DS -> Product DS -> Revenue Science\n                ^^^\nInsights, metrics, data enigneering, data visualization\n\nTwitter organizes into:\n\nGrowth\nProduct\nHealth\nFoundational DS\n\nteam charters\nSwimlanes - clear differentiation between teams\nWorking agreement - what to expect from other teams? (e.g.\ninteractions between data engineering & notifications ds team)\n\nHow does the data eng team receive requests?\nWhat is the SLA of a dataset request?\nWhat would be the ownership structure for the request?\nOn what basis this request will be prioritized?\n\nCreate clear communication channels\n\nHave team meetings at all levels\nHave recurrent sessions to review ongoing projects\nHave fun with each other - quarterly offsites and other activities\n\nBuild and strengthen your leadership team\n\nLeadership team is their first team\nHave staff meeting, and keep an open standing agenda\nDo leadership offsites and working sessions (twitter does it monthly\n    on a specific topic)\nMake this reponsible for managing your org's relationship with\n    stakeholders\n\nTLDR: align teams with objectives, build structures of your teams:\nteam charters, working agreements, swimlanes, and strong leadership\nteam\n\nQuestions: Thoughts on self-servicing (end-to-end) data scientists {#questions-thoughts-on-self-servicing--end-to-end--data-scientists}\n\nMoving away from end-to-end\n\nQuestion: How to bridge gap in understanding between data eng and data scientists {#question-how-to-bridge-gap-in-understanding-between-data-eng-and-data-scientists}\n\nstrong overlap in skill set between data eng and scientists e.g.\n    engineers are taught to build data pipelines early when joining\n    Twitter\njob of the DS manager\n\nArgo: Kubernetes Native Workflows and Pipelines - Greg Roodt, Canva {#argo-kubernetes-native-workflows-and-pipelines-greg-roodt-canva}\n\nGithub project\n\nSimilar to airflow\nruns on top of kubernetes\n\nMachine Learning as Code - Youtube - How Kubeflow uses Argo Workflows\nas its core workflow engine and Argo CD to declaratively deploy ML\npipelines and models.\n\nArgo's DAG UI looks nice!\n\nData Architecture 101 for Your Business - Bence Faludi, Independent Consultant {#data-architecture-101-for-your-business-bence-faludi-independent-consultant}\n\n{{}}\n\nHow to handle unclean data?\nHow quick will the transforms be?\n\nTransitioning into a data-driven company\n    Centralized existing datasets\n\nData Collection {#data-collection}\n\nownership and access of data\nnear-real time raw data : access to unfiltered data in minutes\nno data sampling : ensure access to full dataset\nad blockers : responsible for many lost events\npersonal identification information : turn off PII scraping\ndata model : custom events can be sent in nested format\nSDKs with persistent layer: collected logs stored on the offline\n    device\n\nStorage and Flow {#storage-and-flow}\n\nschedulable pipelines with dependencies\n    notifications, SLAs, extendibility\nCollected data transformation\nRaw-level data stored on the storage, accessible on query engine\n\nDatabase Query Engine {#database-query-engine}\n\nread benchmarks\nlook at distributed query engines\nstar schema better for analytics\nflat truth tables\nstore aggregations as cubes\n\nVisualization {#visualization}\n\nself-hosted vs hosted\nnative SQL execution\ninteractive query builder\n\nE.g. stack Kinesis Data Firehose, S3, Airflow, EMR-Presto (Athena for\nlarge jobs), Apache Superset\n\n{#}\n\nBibliography\nGuo, H., Tang, R., Ye, Y., Li, Z., & He, X., Deepfm: a factorization-machine based neural network for ctr prediction, CoRR, (),  (2017).  ↩\n\nLian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., & Sun, G., Xdeepfm: combining explicit and implicit feature interactions for recommender systems, CoRR, (),  (2018).  ↩\n\nWang, H., Zhang, F., Xie, X., & Guo, M., Dkn: deep knowledge-aware network for news recommendation, CoRR, (),  (2018).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/data_science",
        "title": "Data Science",
        "content": "\nTwitter Data Science Event {#twitter-data-science-event}\n\nMaya Hari, Twitter Managing Director APAC {#maya-hari-twitter-managing-director-apac}\n\n> World's largest collection of human thought\n\nIndonesia's flood relief effort empowered by geolocated tweets.\n\nHalf of the world's millennials will be in Asia by 2020. Asia is a\ngreat canvas for solving and innovating on problems, making Asia an\nengine for growth for Twitter.\n\nTransliteration and languages are the next frontier of opportunity.\n\nMiguel Rios, Data Science lead at Twitter {#miguel-rios-data-science-lead-at-twitter}\n\nTwitter #interactive visualizations\nThe Data Science Venn Diagram — Drew Conway\n\n{{}}\n\nData Science Organization in Twitter {#data-science-organization-in-twitter}\n\nFoundational Data Science work\nGrowth Team\nHabits Team\nHealth/Metrics Team\n\nProduct Data Science Lifecycle {#product-data-science-lifecycle}\n\nOpportunity Sizing -> Testable Hypotheses -> Experiment & Design ->\nInstrumentation & Metrics -> Experiment Review -> Post-release\nCheck-ins\n\nExperiment review includes analyzing user behaviour, to come up with\n    new ideas\n\nDiana Macias, Client Engineering Manager {#diana-macias-client-engineering-manager}\n\n140 character limit story {#140-character-limit-story}\n\n2016: Timing wasn't right for going beyond 140\nEmail sent by a Japanese engineer (Iku) previously iOS engineer,\n    now on machine learning team:\n    From personal research project, Japanese characters need 140 characters, but English users need\n        280-300 characters\n    Japanese can contain about 1.9x more information than English in a\n        character, visualized by separating tweets by language\n    Japanese plain tweet length averages around 14 characters\n    Tweet lengths fit a log-normal distribution: extending the\n        log-normal distribution for english, 280 characters is the number found.\nThe 280 character project was born, customizing character limit\n    based on the keyboard.\n9% of tweets hit the 140 limit, 1% of the tweets hit the 280 limit\nPost analysis of 280 project:\n    Less abbreviations\n    More kind words like \"please\", \"thank you\"\n    Same tweet length\n\nAngad Singh, Data Science Team Lead, Twitter SG {#angad-singh-data-science-team-lead-twitter-sg}\n\nTwitter SG Data Science Team {#twitter-sg-data-science-team}\n\nsmall team of 5 data scientists\nfocused on user behaviour and international growth\nexperimentation on Android/iOS/Web\n\n3Vs of data at Twitter {#3vs-of-data-at-twitter}\n\nVelocity: Rate at which data is created\n    Hundreds of millions of Tweets are sent per day. TPS record:\n        one-second peak of 143,199 Tweets per second\n    Order of 100B interaction events per day\nVolume: 100s of petabytes of data\nVariety: Tweets, Users, LIkes, Retweets and many more\n\nProduction Systems\n\nBatch (Hadoop, HDFS, MapReduce)\nReal-time (Eventbus, Kafka Streams)\n\nAnalytics Tools\n\nBatch: Scalding, Spark\nReal-time: Heron\nLambda (Batch + Real-time): Summingbird, TSAR\nInteractive: Presto, Vertica, R\n\nAnalytics Front-ends\n\nHadoop {#hadoop}\n\nSome of the largest Hadoop clusters in the world: > 10k nodes\nStore 100s of peabytes of data\nMore than 100k daily jobs\ntwitter/ambrose for visualizing Hadoop jobs\n\nCore Data Libraries {#core-data-libraries}\n\ntwitter/scalding: DSL on top of Cascading (Java library for MapReduce)\ntwitter/summingbird: Lambda architecture: real-time and batch\n\nInteractive SQL {#interactive-sql}\n\nInteractive means that results of a query are available from the\n    range of seconds to couple of minutes\nSQL still lingua franca of ad-hoc data analysis:\n    Presto\n    HP Vertica\n    Google BigQuery\n\nData Visualization: {#data-visualization}\n\nApache Zepplin\nTableu\n\nData Insights {#data-insights}\n\nAnalytics - Basic Counting\nA/B Testing\nData Science - Exploratory Analysis\nData Science - Machine Learning\n\nBasic Counting {#basic-counting}\n\nDaily/Monthly Acitve Users\nNumber of Tweets\n\nData Science - Custom Analytics {#data-science-custom-analytics}\n\nCause of spikes and dips in main metrics\n\nMachine Learning {#machine-learning}\n\nRecommendations\n    Users: Who to follow\n    Tweets: Algorithmic timeline\nCortex, DL based on Torch framework (now Tensorflow)\n    Identify NSFW images\n    Recognize what is happening in live feeds\n\nIdeal Talent Stack {#ideal-talent-stack}\n\nSystems (Hadoop, Distributed Systems)\nProgramming (Scala, Scalding, SQL)\nMath (Statistics, Linear Algebra)\n",
        "tags": []
    },
    {
        "uri": "/zettels/data_viz",
        "title": "Data Visualization",
        "content": "\nReferences {#references}\n\nCourse website\nThe Grammar of Graphics\nA Layered Grammar Of Graphics\n\nProgram {#program}\n\n{{}}\n\nTools {#tools}\n\nHow do people use visualization? {#how-do-people-use-visualization}\n\nverification\nanalysis\nexploration/discovery\npresentation/storytelling\nart/aesthetics\n\nProvenance {#provenance}\n\nThe steps the user take in the process of visual exploration/analysis\nand the resulting visualizations and findings\n\nWe use provenance for:\n\nrecall\nreuse and replicate\nSharing\nMeta-analysis\n\nCapture:\n\ndatasets\nvisualization and insight\ninteraction\n\nVisual Interaction Techniques {#visual-interaction-techniques}\n\nSelecting/Highlighting/Brushing\nUsing Lasso to update linked views\nsidebars for interactive filtering\n\nA well-designed interactive visualization interface should show the following {#a-well-designed-interactive-visualization-interface-should-show-the-following}\n\nVisualize\n: use effective visual encodings\n\nFilter\n: reduce visible data to relevant items\n\nSelect\n: Retrieve details about interesting items\n\nNavigate\n: Pan, zoom, change view\n\nDeepTree\n\nVisual Design and Encoding - Westermann {#visual-design-and-encoding-westermann}\n\n> The purpose of visualization is insight, not pictures. - Schneiderman\n\nWhy use Interactivity? {#why-use-interactivity}\n\nHandle data complexity\nA single static view can show only one aspect of data\n\n> Overview first, zoom and filter, then details-on-demand\n\nWhy depend on vision? {#why-depend-on-vision}\n\nVisual system is high-bandwidth channel to brain\n\nPre-attentive Processing {#pre-attentive-processing}\n\nSequential vs. Parallel processing (popout)\nCombination of channels usually requires serial search\nDifficult if no unique visual property of the target\n\nGestalt Principles {#gestalt-principles}\n\n{{}}\n\nRepresentations should be correct, accurate and truthful.\n\nTo bring up a change, you must attend to it. (Change blindness)\n\nVisual Design {#visual-design}\n\nA good visualization depends on:\n\ndata types\ncontext of the data\ntasks to perform e.g. identify trends\nquestions to answer\nmessages to deliver\n\n\\begin{equation}\n  \\text{Lie Factor} = \\frac{\\text{Size of effect shown in\n      graphic}}{\\text{Size of effect in data}}\n\\end{equation}\n\nBad visualizations do not allow you to recover original data from the\nvisualization. Keep proportions and relative sizes.\n\nmaximize data-ink ratio\n\nSteven's Psychological Power Law {#steven-s-psychological-power-law}\n\nSteven's psychophysical power law:\n\n\\begin{equation}\n\\text{Perceived sensation} = \\text{Physical Intensity}^T\n\\end{equation}\n\nCompensating for human's over/underestimation:\n\n{{}}\n\nDifficult to focus on one channel when multiple channels are\npresented. (Redudancy is bad!)\n\nVisual mapping - Separable vs integral visual channels\n\nColor + position\nColor + size\nWidth + height\nRed + green\\* Unfiled\n\nReingold-Tilford Algorithm\n\nScientific Data Visualization - Stefan Bruckner {#scientific-data-visualization-stefan-bruckner}\n\nTypes of Visualization {#types-of-visualization}\n\nVolume Visualization\n    Visualization of scalar fields\n    Important in medicine, biology, geoscience, engineering, ...\nFlow Visualization\n    Visualization of Vector Fields\n    Data typically from computational fluid dynamics (CFD)\n        simulations\n\nData Representation {#data-representation}\n\nInherent Spatial Domain?\n    Yes: Do we  recycle data space or not\n    No: Select which representation space\nWhat dimension is used for what?\n    Relationship data space  data attributes\n    Available display space (2D/3D)\n    Where is the focus?\n    Where can you abstract?\n\nGrids {#grids}\n\nCommon way of storing datasets of field type (scalar, vector, tensor\n    fields)\nTypically a high-performance, space-efficient representation\nData is organized in cells which contain samples.\nOften used to define an interpolation function that defines data\n    values between samples leading to a continuous representation.\n\nWhich data orginazation is optimal?\nWhere does the data come from?\nIs there an explicit neighbourhood relationship?\nHow is the neighborhood information stored?\nHow is navigation within the data possible?\nCalculations within the data possible?\nAre the data structured?\n\nRegular Grid {#regular-grid}\n\nOrthogonal, equidistant grid\nSample distances not equal\nImplicit neighborhood-relationship\n\nRectilinear Grid {#rectilinear-grid}\n\nOrthogonal grid\nVarying sample distances (\\\\(x[i], y[j]\\\\) given)\nAllows you to place more samples in areas that are more important to\n    you, not wasting storage in uninterested areas\n\nCurvilinear Grid {#curvilinear-grid}\n\nNon-orthogonal grid\nGrid-points explicitly given (\\\\(x[i,j]\\\\))\nImplicit neighborhood relationship\n\nBlock-structured Grid {#block-structured-grid}\n\nCombination of structured grids\n\nUnstructured Grid {#unstructured-grid}\n\nGrid-points and connections arbitrary\nGrid-points and neighborhood explicitly given\nCells: tetrahedra\n\nTODO Other Grids SUMMARY OF GRID TYPES {#other-grids-summary-of-grid-types}\n\nNon-cartesian Coordinates\n\nScattered Data {#scattered-data}\n\nGrid-free data\n\n> Interesting to look at dimensionality of data space, vs dimensionality\n> of data attributes\n\nData Enhancement {#data-enhancement}\n\nFiltering\nResampling\nData derivation\nData interpolation\n\nData, Visualization, Interaction {#data-visualization-interaction}\n\nCoupling varies considerably\n    Data Generation (data acquisition)\n        Mesaurement, simulation, modelling\n        Can take very long, and be very costly\n    Visualization (rest of visualization pipeline)\n        Data enhancement, viz mapping, rendering\n        Depending on implementation, fast/slow\n    Interaction\n        How can the user intervene, vary parameters\n\nInteractive Steering {#interactive-steering}\n\nSimulation and modelling generate data \"on the fly\"\nAllows real-time insight of the data\nUser can interfere with the simulation, and change the design of the\n    simulations\n\nVolume Visualization {#volume-visualization}\n\nthe visualization of 3D scalar fields\nMapping 3D -> 2D\nProjection (e.g. MIP), slicing, volume rendering\nVolume data is 3Dx1D data\nScalar data, 3D data space, space filling\nUser wants to gain insight into 3D data, find structures of special\n    interest + context\n\nOrganization of Volume Data\n\n    Cartesian or Regular grid\n        CT/MR, often dx=dy color\n\nVisualization Approaches {#visualization-approaches}\n\nSlicing\n: display of 2D cross sections\n\nIndirect Volume Rendering\n: Extraction of an intermediate representation\n\nDirect Volume Rendering\n: Direct display of representation\n\nTODO Isosurface Similarity {#isosurface-similarity}\n\nVisualization in the Spatial Domain {#visualization-in-the-spatial-domain}\n\nSlicing\n\n    Reduce the dimensionality of 3D t o2D by showing a cross section\n    Usually without a transfer function\n    Orthogonal slicing often used to slice along anatomical planes\n        in medical imagery\n    Oblique slicing has arbitrary slice orientation, often used in\n        an multi-planar reformation (MPR) setup.\n    Curved slices often tailored towards specific applications,\n        e.g. visualization of blood vessels.\n\nDirect Volume Rendering\n\n    Dense representation of underlying scalar field: transfer function\n        defines visible structure.\n    Image order (ray casting) fast and easy to implement, and are well\n        supported by current GPUs\n    Object order (splatting, texture slicing) also supported by older\n        GPUs, but difficult to skip non-visible regions. Easy to skip...(?)\n    Nowadays: shading/classification after interpolation/resampling\n    post/pre-interpolative classification order\n\nRay Tracing vs Ray Casting\n\n    Ray tracing\n    : method from image generation, usig ray-object\n        intersection and tracing secondary rays.\n\n    Ray casting\n    : no objects, density values in 3D, only viewing rays.\n\nShading\n\n    lambertian reflection\n    : light reflected equally in all directions\n\n    specular reflection\n    : light reflected more in one direction\n\n    Make structures in volume data sets more realistic by applying an\n    illumination model\n\n    Shade each sample in the volume like a surface: [Blinn-Phong\n        illumination model](https://en.wikipedia.org/wiki/Blinn%25E2%2580%2593Phong%5Fshading%5Fmodel) commonly used.\n    Use normalized gradient vector as estimation for surface normal.\n\nIndirect Volume Rendering {#indirect-volume-rendering}\n\nExtract an intermediate representation from the volume (geometric\n    surface), then use traditional rendering methods\nCuberille regards each xovel as a little cube, classify as either\n    part of the object or not.\n\n{{}}\n\nMarching Cubes is a standard method for the extraction of isosurfaces\nfrom volume data\n\nFlow Visualization {#flow-visualization}\n\nAirplane/ship/car design\nWeather simulation\nMedicine (blood flows etc.)\nGaseous, liquid flow\nFlow models: Differential Equation Systems (ODEs)\nCommon techniques for solving Navier-Stokes equations:\n    Lagrangian approach (particle-based)\n        Treat the fluid as discrete particles, and apply interaction\n            forces.\n        Pros: momentum conservation/more intuitive, and fast, no\n            linear equation solving\n        Cons: connectivity information/surface reconstruction\n    Eulerian approach\n        Discretize the domain using finite differences\n        Use the operator splitting technique to solve each term separately\n        Pros: derivative approximation, adaptive time step/solver\n        Cons: memory usage & speed, grid artifact/resolution limitation.\n\nData Visualization of Text Data - Jaegul Choo {#data-visualization-of-text-data-jaegul-choo}\n\nOverview {#overview}\n\nVector encoding techniques of text\n    Bag-of-words vectors and word embedding\nBasic text visualization techniques\n    Word cloud, wordle, word tree, phrase nets, ThemeRiver\nTopic Modeling\n    Non-negative matrix factorization\n    UTOPIAN and visual analytic systems\nDimension reduction\n    Multidimensional scaling and tSNE\n    Interactive dimension reduction techniques and systems\nInteractive visualization of deep learning\n    Toolkits: Tensorboard, Embedding Projector, Visdom\n    Advanced visual analytics systems: CNNVis, LSTMVis, DeepEyes\n",
        "tags": []
    },
    {
        "uri": "/zettels/databases",
        "title": "Databases",
        "content": "\nHow JOINs work {#how-joins-work}\n\nJOIN are essentially nested loops.\nMultiple techniques are used to improve the performance of JOIN.\nWhen datasets are small or no appropriate index is being maintained,\n    query planners will likely decide to use memory to make queries run\n    faster. The query executor can build hash tables (or sort records in\n    memory, or ...) and use HashJoin or MergeJoin to more efficiently join\n    the two operands according to the join-condition in the query.\nMaintaining index data structures like B-Trees up to date allows\n    Nested Loop algorithm to run much faster even when the dataset\n    doesn’t conveniently fit in memory. Indexed JOINs take time\n    proportional to the size of the result and are not affected by the\n    size of the tables.\n\nUnderstanding the three classical JOIN algorithms – NestedLoop,\nHashJoin, MergeJoin – how they take advantage of different indexes and\nhow they behave when there is no index can give you a lot of insight\non how databases run queries. (Felipe Oliveira Carvalho, 2019)\n\nBibliography\nCarvalho, F. O. (2019). Demystifying Join Algorithms. Retrieved from http://blog.felipe.rs/2019/01/29/demystifying-join-algorithms/. Online; accessed 03 February 2019. ↩\n",
        "tags": [
            "database"
        ]
    },
    {
        "uri": "/zettels/deep_learning",
        "title": "Deep Learning",
        "content": "\nDefinition {#definition}\n\nA class of machine learning algorithm that uses a cascade of multiple\nlayeprs of non-linear processing units for feature extraction and\ntransformation\n\nIntroduction {#introduction}\n\nSensory perception is plagued with large amounts of nuisance\nvariation. Nuisance variables are task-dependent and can be implicit.\nHow do we deal with nuisance variation in the input?\n\nThe holy grail of machine learning is to learn a _disentangled\nrepresentation_ that factors out variation in the sensory input into\nmeaningful intrinsic degrees of freedom.\n\nselective\n: sensitive to task-relevant (target) features\n\ninvariant\n: robust to task-irrelevant (nuisance) features\n\nmulti-task\n: useful for many different tasks\n\nDeep learning solves this central problem in representation learning\nby introducing representations that are expressed in terms of other,\nsimpler representations. Deep learning enables the computer to build\ncomplex concepts out of simpler concepts.\n\nThe key inspiration from neuroscience is to build up feature\nselectivity and tolerance over multiple layers in a hierarchy.\n\nModel/task categorisation {#model-task-categorisation}\n\nsupervised learning\n: input -> otuput, learning the pattern from\n    labeled training data\n\nunsupervised learning\n: learning to differentiate/cluster\n    unlabeled data\n\nreinforcement learning\n: learning from trial-and-error through rewards\n\nBayesian probability {#bayesian-probability}\n\nThe quantification of the expression of uncertainty. We can make\nprecise revisions of uncertainty in light of new evidence.\n\nIn the example of polynomial curve fitting, it seems reasonable to\napply the frequentist notion of probability to the random values of\nthe observed variables. However, we can use probability theory to\ndescribe the uncertainty in model parameters, and the choice of model\nitself.\n\nWe can evaluate the uncertainty in model parameters w after we have\nobserved some data D. \\\\(p(D|w)\\\\) is called the likelihood function.\n\nThe posterior probability is proportional to likelihood x prior.\nModel parameters w are chosen to maximise the likelihood function\n\\\\(p(D|w)\\\\). Because the negative log-likelihood is a monotonically\ndecreasing function, minimising it is equivalent to maximising\nlikelihood.\n\nUniversal Approximation Theorem {#universal-approximation-theorem}\n\nA feed-forward network with a single hidden layer containing a finite\nnumber of neurons can approximate continuous functions on compact\nsubsets of the real space.\n\nMeasure Theory {#measure-theory}\n\nKullback-Leiber (KL) divergence {#kullback-leiber--kl--divergence}\n\nThe KL divergence is 0 iff P and Q are the same distribution in\ndiscrete variables, and equal almost everywhere in continuous\nvariables. Because the KL divergence is non-negative and measures the\ndifference between the two distributions, it is soften conceptualised\nas some sort of distance, but it is not a true measure, because it is\nnot symmetric.\n\nCross Entropy {#cross-entropy}\n\nMinimising the cross entropy with respect to Q is equivalent to\nminimising the KL divergence.\n\nObject Detection {#object-detection}\n\nRegion Proposal Networks find blobby regions that are likely to\n    contain objects, and are relatively fast to run\n    e.g. Selective Search, gives about 2000 region proposals in a few\n        seconds on CPU.\n\nR-CNN uses a RPN like selective search to generate region proposals,\n    followed by a convnet and an SVM on top to predict image labels.\n    There is also a bounding box regression loss, which predicts\n    correction to the proposals generated.\nFast R-CNN passes the image through a convnet, and run the RPN to\n    generate image proposals on a particular feature map of the image.\n    These crops go through a RoI pooling layer.\nFaster R-CNN trains the region proposal network, and is jointly\n    trained with 4 losses:\n    RPN classify object/not object\n    RPN regress box coordinates\n    Final classification score (object classes)\n    Final object coordinates\n\nDetection Without Proposals: YOLO / SSD {#detection-without-proposals-yolo-ssd}\n\nWithin each grid cell:\n\nRegress from each of the B base boxes to a final box with 5 numbers:\n    dx, dy, dh, dw, confidence\nPredict scores for each of C classes (including background as a\n    class)\n\nOutput: \\\\(7 \\* 7 \\(5\\B + C)\\\\)\n\nLearning Rates {#learning-rates}\n\nLearning Rate Annealing {#learning-rate-annealing}\n\nDecay learning rate after several iterations...\n\nAlso: SGDR with cyclic learning rate, restores high learning rate\nafter several iterations to try to find local minima that is largely\nflat, and doesn't change so much in any direction. (Generalises\nbetter)\n\nReinforcement Learning {#reinforcement-learning}\n\nHow Companies use Deep Learning {#how-companies-use-deep-learning}\n\nViSenze {#visenze}\n\nVisenze's primary product is their Visual Search (reverse image\nsearch).\n\nInitially, they started out with a similar model to our approach:\nTrain a CNN, read encodings before the FC layer, use it to perform NN\nsearch.\n\nNow their pipeline is as follows:\nQuery time -> object detection -> Extract Features -> Nearest Neighbours -> Ranked Results\nOffline training -> Detection Model -> Embedding models -> Nearest Neighbours\nIndex time -> Objects -> Extract Features -> Search Index\n(Compression/Hash Model)\n\nObject detection followed the trends in research papers:\n\nR-CNN\nFast-CNN\nFaster-CNN\nYOLO/SSD\nDSOD (Current)\n\nModel performance is based of standard IR metrics: They are using DCG\nscore for evaluating their reverse image search. This requires a lot\nof manual annotation.\n\nModels are trained offline using physical purchased GPUs.\n\nDeployment: Kubernetes on AWS, with generally small CPU servers.\nOverall latency is less than 200ms.\n\nThings they focused on as a company {#things-they-focused-on-as-a-company}\n\nTooling:\n    Annotation System\n        Complete annotation is crucial to detection training\n    Training System\n        Make it easy to change hyperparameters and retrain models\n        Abstract away need for knowing deep learning\n        Platform for tracking metrics\n    Querylog pipeline\n        Take user input as training data\n    Evaluation System\n        Both automatic evaluation via metrics and manual (AB testing)\n            is done before release\n        Visualisations via T-SNE to see if clusters remain the\n            same/make sense, when new learnings are added: **learning\n            without forgetting**\nBusiness:\n    Attend to customer requirements: e.g. if a company wants to sell\n        hats, model needs to be trained to detect hats, and these\n        learnings need to be added to the existing model without\n        affecting data earlier\n\nVisual Embeddings Used {#visual-embeddings-used}\n\nAt Visenze, they use multiple embeddings in different feature spaces\nto measure similarity. The results are then combined before returned.\nThe 4 main embeddings are:\n\nExact matches (same item)\n    Trained with siamese network with batched triplet loss\n        (typically used in face recognition, but seems to work well with\n        product classification)\nSame Category\n    Domain specific labels have been most helpful in achieving\n        state-of-the-art accuracy\n        e.g for fashion, sleeve length, jeans length etc.\nSimilar Categories\n\nLessons Learnt {#lessons-learnt}\n\nTaxonomy Coverage\nTraining Data Coverage\n    Obtaining training data from one source only can lead to severe\n        bias (e.g. detecting watermarks and using it as feature)\nOverfitting\nContinuous Improvement (Learning Without Forgetting)\nBad-case driven development\n    Be quick to identify hard negatives, and add in similar\n        negative samples into training data to improve accuracy\nImage quality, rotation\nRe-ranking based on customer\n\nKey Open Questions about Deep Learning Systems {#key-open-questions-about-deep-learning-systems}\n\nHow and why do they work? Can we derive their structure from first\n    principles? Can we compress/explain the myriad empirical\n    observations/best practices about deep nets?\nCan we shed new light on the hidden representations of objects? Can\n    we generate new theories and testable predictions for both\n    artificial/real neuroscience?\nWhy do they fail? How to improve them? How to alleviate their\n    intrinsic limitations?\nCan we guide the search for better\n    architectures/algorithms/performance in applied DL?\n\nConcrete Theoretical Questions {#concrete-theoretical-questions}\n\nWhat are the implicit modelling assumptions?\nWhat is the inference task and algorithm?\nWhat is the learning algorithm?\nCan we generate new testable predictions for artificial/real nets?\nWhat modeling assumptions are being violated in failures? How can\n    we improve the models, tasks and algorithms?\n\nConvNets from First Principles {#convnets-from-first-principles}\n\nThere are many architectures, but just a few key operations and\nobjectives:\n\n2D (De)Convolution, Spatial max-(un)pooling, ReLu, Skip-connections\nBatch Normalization\nDropOut, Noise Corruption\nData Augmentation\nObjectives: XEnt, NLL, Reconstruction Error, Mutual Information\n\nWe focus on the properties conserved across all species of Convnets.\n\nFinding a generative model {#finding-a-generative-model}\n\nWe reverse engineer a Convnet by building a generative model\n\nDefine a generative model that captures nuisance variation\nRecast feed-forward propogation in a DCN as MAP inference of the\n    full latent configuration -> generative classifier\nApply a discriminative relaxation -> discriminative classifier\nLearn the parameters via Batch Hard EG Algorithm -> SGD-Backprop\n    Training of a DCN\nUse new generative model to address limitations of DCNs: top-down\n    inference, learning from unlabeled data, hyperparameter\n    optimization\n\nVariational Inference and Deep Learning {#variational-inference-and-deep-learning}\n\nOptimizers work well with sums, not products. We want to be able to\ncalculate gradients on a subset of examples and have reasonable\nconfidence for convergence.\n\nLatent variables give our model structure and can make training more\ntractable. With latent variables \\\\(\\bar{z}\\\\), we have:\n\n\\begin{equation}\n  p(x) = \\sum\\z p(x,z) =  \\sum\\z p(x | z)p(z)\n\\end{equation}\n\nPutting log-likelihood and latent variables together, we get:\n\n\\begin{equation}\nL = \\sum\\i \\log \\left( \\sum\\z p(x\\_i, z)\\right)\n\\end{equation}\n\nUsually, we want to sample one example and one value for the latent\nvariable at a time, for tractable optimization. This only works for a sum.\n\nWe compare the above structure with this equation:\n\n\\begin{equation}\nL = \\sum\\i \\left( \\sum\\z \\log p(x\\_i, z)\\right)\n\\end{equation}\n\nIntuitively the new equation says that every latent variable value\nneeds to do a good job of explaining the data\n\nDerivation of the Variational Bound {#derivation-of-the-variational-bound}\n\n\\begin{equation}\n  p(x) = \\sum\\i \\log (\\sum\\z p(x\\_i, z))\n\\end{equation}\n\n\\begin{equation}\np(x) = \\sum\\i \\log(\\sum\\z \\frac{q(z|x\\i) p(x\\i, z)}{q(z|x\\_i)})\n\\end{equation}\n\nBy Jensen's Inequality (concavity of the log function),\n\n\\begin{equation}\np(x) \\ge \\sum\\i \\sum\\z q(z|x\\i) \\log \\frac{p(x\\i, z)}{q(z|x\\_i)}\n\\end{equation}\n\n\\begin{equation}\n  p(x) \\ge \\sum\\i q(z|x\\i) \\sum\\z \\log (p(x\\i | z)) + \\log \\frac{p(z)}{q(z|x\\_i)}\n\\end{equation}\n\n\\begin{equation}\n  p(x) \\ge \\mathbb{E}\\{z \\sim q(z|x\\i)} \\log (p(x\\i | z)) - KL(q(z|x\\i) || p(z))\n\\end{equation}\n\nThe first term is the conditional likelihood of our observation if we\nused a sampled z value from the approximated posterior \\\\(q(z | x)\\\\). The\nsecond term is a divergence between the approximate posterior and the\nprior. This is often well defined.\n\nVariational Autoencoder {#variational-autoencoder}\n\nWe can show that a special type of autoencoder corresponds to this\nlower bound, where \\\\(q(z | x)\\\\) is the encoder and \\\\(p(x|z)\\\\) is the\ndecoder. The first term is the usual reconstruction objective for\nautoencoders. The second term is a special KL term which pulls the\nbottleneck closer to a prior.\n\nIn the simplest case, we make both \\\\(q(z|x)\\\\) and \\\\(p(z)\\\\) independent\nGaussians:\n\n\\begin{equation}\n  KL(q(z|x\\i) || p(z)) = \\sum\\j \\mu\\j^2 + \\sigma\\j^2 - \\log(\\sigma\\_j^2)\n\\end{equation}\n\n\\\\(q(z|x)\\\\) has parameters \\\\(\\mu\\\\) and \\\\(\\sigma\\\\) which are just outputs from our\nencoder. \\\\(p(z)\\\\) is a prior distribution with \\\\(\\mu = 0\\\\) and \\\\(\\sigma = 1\\\\).\n\nThe bottleneck can't remember spatial information, and optimizing for\n\\\\(p(x|z)\\\\) puts heavy emphasis on exact recovery of spatial information.\nSolutions involve removing the bottleneck, or replacing the prior\n\\\\(p(z)\\\\) with one that has more structure.\n",
        "tags": []
    },
    {
        "uri": "/zettels/deep_rl",
        "title": "Deep Reinforcement Learning",
        "content": "\ntags\n: Reinforcement Learning ⭐\n\nWhy Deep Reinforcement Learning? {#why-deep-reinforcement-learning}\n\nReinforcement learning provides a mathematical framework for decision-making\nDeep learning has shown to be extremely successful in unstructured\n    environments (e.g. image, text)\nDeep RL allows for end-to-end training of policies\n    Features are tedious and difficult to hand-design, and are not\n        so transferable across tasks\n    Features are informed by the task\n\nAnatomy of Deep RL algorithms {#anatomy-of-deep-rl-algorithms}\n\n\\begin{equation}\n  \\theta^{\\star}=\\arg \\max \\{\\theta} E\\{\\tau \\sim p\\{\\theta}(\\tau)}\\left[\\sum\\{t} r\\left(\\mathbf{s}\\{t}, \\mathbf{a}\\{t}\\right)\\right]\n\\end{equation}\n\npolicy gradients\n: directly differentiate above objective\n\nvalue-based\n: estimate value/q-function of the optimal policy (no\n    explicit policy)\n\nactor-critic\n: estimate value/q-function of the current policy, use\n    it to improve policy\n\nmodel-based RL\n: estimate the transition model, and then...\n    use it for planning (no explicit policy)\n        Trajectory optimization/optimal control (continuous spaces)\n        Discrete planning in discrete action spaces (§mcts)\n    use it to improve a policy (e.g. via backpropagation, with some tricks)\n    use the model to learn a value function (e.g. through dynamic programming)\n\nWhy so many RL algorithms? {#why-so-many-rl-algorithms}\n\nDifferent tradeoffs\n    sample efficiency\n        is it off policy: can improve policy without generating new\n            samples from that policy?\n        however, are samples cheap to obtain?\n\n{{}}\n\nstability and ease of use (does it converge, and if so to what?)\n    Q-learning: fixed point iteration\n    Model-based RL: model is not optimized for expected reward\nDifferent assumptions\n    fully observable?\n        generally assumed by value function fitting methods (mitigated\n            by adding recurrence)\n        episodic learning\n            generally assumed by pure policy gradient methods\n            assumed by some model-based RL methods\n        continuity or smoothness?\n            assumed by some continuous value function learning methods\n            often assumed by some model-based RL methods\n    stochastic or deterministic?\nDifferent things are easy or hard in different settings\n    easier to represent the policy?\n    easier to represent the model?\n\nChallenges in Deep RL {#challenges-in-deep-rl}\n\nStability and Hyperparameter Tuning {#stability-and-hyperparameter-tuning}\n\nDevising stable RL algorithms is hard\nCan't run hyperparameter sweeps in the real world\n\nWould like algorithms with favourable improvement and convergence\nproperties:\n\nTrust region policy optimization\n    (Schulman et al., 2015)\n\nor algorithms that adaptively adjust parameters:\n\nQ-Prop (Gu et al., 2016)\n\nProblem Formulation {#problem-formulation}\n\nMulti-task reinforcement learning and generalization\nUnsupervised or self-supervised learning\n\nResources {#resources}\n\nCS285 Fall 2019 - YouTube\nWelcome to Spinning Up in Deep RL! — Spinning Up documentation\n    (Tensorflow, Pytorch)\nDavid Silver's Deep RL ICML Tutorial\n\nBibliography\nSchulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P., High-Dimensional Continuous Control Using Generalized Advantage Estimation, CoRR, (),  (2015).  ↩\n\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., & Levine, S., Q-prop: sample-efficient policy gradient with an off-policy critic, CoRR, (),  (2016).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/devops",
        "title": "Dev Ops",
        "content": "\nKubernetes {#kubernetes}\n\nOpen Containers Initiative\n\nFeatures of a COE {#features-of-a-coe}\n\nclustering\nscheduling\nscaling\nload balancing (pets to cattle)\n    monitoring services vs monitoring\nfault tolerance\napp deployment\n\nWhy use kubernetes {#why-use-kubernetes}\n\ngoogle\nbattle hardened\nfeatures\nopen source\n\nDownsides to kubernetes {#downsides-to-kubernetes}\n\ncomplex\nzero to dev\nnon-native\n\n| Feature                 | Concept                               |\n|-------------------------|---------------------------------------|\n| Colocation              | Pods                                  |\n| Scaling/Fault Tolerance | Replication controllers, replica sets |\n| Load Balancing          | Services                              |\n| App Deployment          | Rolling-updates                       |\n\nArchitecture {#architecture}\n\nkubectl\n\nArgues that Kubernetes is affordable and worth the try for personal\nprojects.\n\nThe main arguments are:\n\nKubernetes is reliable and scalable.\nIt's cheap to run: run HTTP proxy for each node, instead of using\n    Google's HTTP load balancer.\nKubernetes makes rollback deployments trivial, since everything is\n    containerised.\n\nA response to the above article. Basically arguing that Kubernetes is\nvery complex and has a lot of moving parts, which is wholly\nunnecessary for side projects.\n",
        "tags": []
    },
    {
        "uri": "/zettels/distributed_rl",
        "title": "Distributed Reinforcement Learning",
        "content": "\nParallelizing Reinforcement Learning ⭐.\n\nHistory of Distributed RL {#history-of-distributed-rl}\n\nDQN (Mnih et al., 2013): §mnih2013\\atari\\deeprl\nGORILA (Nair et al., 2015)\nA3C (Mnih et al., 2016)\nIMPALA (Espeholt et al., 2018)\nApe-X (Horgan et al., 2018)\nR2D3 (Paine et al., 2019)\n\nResources {#resources}\n\nCS285 Fa19 10/30/19 - YouTube\n\nBibliography\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M., Playing atari with deep reinforcement learning, arXiv preprint arXiv:1312.5602, (),  (2013).  ↩\n\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., Maria, A. D., Panneershelvam, V., …, Massively parallel methods for deep reinforcement learning, CoRR, (),  (2015).  ↩\n\nMnih, V., Badia, Adri\\a Puigdom\\enech, Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D., …, Asynchronous methods for deep reinforcement learning, CoRR, (),  (2016).  ↩\n\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., …, Impala: scalable distributed deep-rl with importance weighted actor-learner architectures, CoRR, (),  (2018).  ↩\n\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Hasselt, H. v., & Silver, D., Distributed Prioritized Experience Replay, CoRR, (),  (2018).  ↩\n\nPaine, T. L., Gulcehre, C., Shahriari, B., Denil, M., Hoffman, M., Soyer, H., Tanburn, R., …, Making efficient use of demonstrations to solve hard exploration problems, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/dl_tools",
        "title": "Deep Learning Tools",
        "content": "\ntags\n: §deep\\_learning\n\nTensorflow {#tensorflow}\n\nWhat is Tensorflow {#what-is-tensorflow}\n\nTensorFlow is open-source software library for numerical computation.\nA graph of computations is defined, and TensorFlow builds an optimized\ngraph from that.\n\nTensorFlow breaks up the graph into several chunks and run them in\nparallel across multiple CPUs or GPUs where possible, and also\nsupports distributed computing.\n\n{{}}\n\nGlossary {#glossary}\n\ntensor\n: A tf.Tensor object represents a partially defined\n    computation that will eventually produce a value.\n    TensorFlow programs work by first building a graph of\n    tf.Tensor objects, detailing how each tensor is computed\n    based on the other available tensors and then by running\n    parts of this graph to achieve the desired results.\n\nTF Slim {#tf-slim}\n\narg_scope\n\n    Example of how to use tf.contrib.framework.arg\\_scope:\n\n        from third_party.tensorflow.contrib.layers.python import layers\n\n    argscope = tf.contrib.framework.argscope\n\n    with arg_scope([layers.conv2d], padding='SAME',\n                   initializer=layers.variancescalinginitializer(),\n                   regularizer=layers.l2_regularizer(0.05)):\n      net = layers.conv2d(inputs, 64, [11, 11], 4, padding='VALID', scope='conv1')\n      net = layers.conv2d(net, 256, [5, 5], scope='conv2')\n\n    The first call to conv2d will behave as follows:\n\n        layers.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\n                  initializer=layers.variancescalinginitializer(),\n                  regularizer=layers.l2_regularizer(0.05), scope='conv1')\n\n    The second call to conv2d will also use the arg\\_scope's default for\n    padding:\n\n        layers.conv2d(inputs, 256, [5, 5], padding='SAME',\n                  initializer=layers.variancescalinginitializer(),\n                  regularizer=layers.l2_regularizer(0.05), scope='conv2')\n\n    Example of how to reuse an arg\\_scope:\n\n        with arg_scope([layers.conv2d], padding='SAME',\n                   initializer=layers.variancescalinginitializer(),\n                   regularizer=layers.l2_regularizer(0.05)) as sc:\n        net = layers.conv2d(net, 256, [5, 5], scope='conv1')\n...\n\n    with arg_scope(sc):\n        net = layers.conv2d(net, 256, [5, 5], scope='conv2')\n\n    Example of how to use tf.contrib.framework.add\\arg\\scope to enable your\n    function to be called within an arg\\_scope later:\n\n        @tf.contrib.framework.addargscope\n    def conv2d(args, *kwargs)\n\nTF Serve {#tf-serve}\n\nTF Estimator {#tf-estimator}\n\n{{}}\n\nReferences:\n\nAt the heart of our framework is Estimator, a class that both provides\nan interface for downstream infrastructure, as well as a convenient\nharness for developers. Te interface for users of Estimator is loosely\nmodeled afer Scikit-learn and consists of only four methods: train\ntrains the model, given training data. evaluate computes evaluation\nmetrics over test data, predict performs inference on new data given a\ntrained model, and fnally, export_savedmodel exports a SavedModel, a\nserialization format which allows the model to be used in TensorFlow\nServing, a prebuilt production server for TensorFlow models.\n\n| Estimator method | Mode parameter set |\n|------------------|--------------------|\n| train()          | ModeKeys.TRAIN     |\n| evaluate()       | ModeKeys.EVAL      |\n| predict()        | ModeKeys.PREDICT   |\n\nEstimators receive a configuration object called RunConfig which\ncommunicates everything that the Estimator needs to know about the\nenvironment in which the model will be run: how many workers are\navailable,, how to save intermediate checkpoints etc.\n\n{{}}\n\ntrain, evaluate and predict take an input function, which is expected\nto produce two dictionaries: one containing Tensors with inputs\n(features), and one containing Tensors with labels.\n\nPredict\n\n     When model_fn is called with mode == ModeKeys.PREDICT, the model\n    function must return a tf.estimator.EstimatorSpec containing the\n    following information:\n\n    the mode, which is tf.estimator.ModeKeys.PREDICT\n    the prediction\n\nclass_ids will be the model prediction for the class (Iris flower type)\nThe output node with the highest value is our prediction\n    predictions = { 'class_ids': tf.argmax(input=logits, axis=1) }\n\nReturn our prediction\n    if mode == tf.estimator.ModeKeys.PREDICT:\n       return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\nEval\n\n    When model_fn is called with mode == ModeKeys.EVAL, the model function must evaluate the model, returning loss.\n\nTo calculate the loss, we need to convert our labels\nOur input labels have shape: [batch_size, 1]\n    labels = tf.squeeze(labels, 1)          # Convert to shape [batch_size]\n    loss = tf.losses.sparsesoftmaxcross_entropy(labels=labels, logits=logits)\n\n    We can also compute and return additional metrics.\n\nCalculate the accuracy between the true labels, and our predictions\n    accuracy = tf.metrics.accuracy(labels, predictions['class_ids'])\n\n        if mode == tf.estimator.ModeKeys.EVAL:\n       return tf.estimator.EstimatorSpec(\n           mode,\n           loss=loss,\n           evalmetricops={'my_accuracy': accuracy})\n\nTrain\n\n    When model_fn is called with mode == ModeKeys.TRAIN, the model\n    function must train the model.\n\n        optimizer = tf.train.AdagradOptimizer(0.05)\n    train_op = optimizer.minimize(\n       loss,\n       globalstep=tf.train.getglobal_step())\n\nSet the TensorBoard scalar my_accuracy to the accuracy\n    tf.summary.scalar('my_accuracy', accuracy[1])\n\n    return tf.estimator.EstimatorSpec(\n       mode,\n       loss=loss,\n       trainop=trainop)\n\nTF Feature Columns {#tf-feature-columns}\n\nReference:\n\n{{}}\n\nFeature columns bridge raw data with the data your model needs.\n\nThere are nine functions in the tf.feature_column api.\n\nNumeric column\n\n        numericfeaturecolumn = tf.featurecolumn.numericcolumn(key=\"SepalLength\",\n                                                              dtype=tf.float64)\n\n    vectorfeaturecolumn = tf.featurecolumn.numericcolumn(key=\"Bowling\",\n                                                             shape=10)\n\n    matrixfeaturecolumn = tf.featurecolumn.numericcolumn(key=\"MyMatrix\",\n                                                             shape=[10,5])\n\nBucketized column\n\n    Often, you don't want to feed a number directly into the model, but\n    instead split its value into different categories based on numerical\n    ranges. Consider the following bucketing scheme:\n\n    {{}}\n\n    We create the bucketized column from a numeric column:\n\nA numeric column for the raw input.\n    numericfeaturecolumn = tf.featurecolumn.numericcolumn(\"Year\")\n\nBucketize the numeric column on the years 1960, 1980, and 2000\n    bucketizedfeaturecolumn = tf.featurecolumn.bucketizedcolumn(\n        sourcecolumn = numericfeature_column,\n        boundaries = [1960, 1980, 2000])\n\nCategorical Identity Column\n\n    Categorical identity columns are a special case of bucketized columns.\n    In a categorical identity column, each bucket represents a single,\n    unique integer.\n\n    {{}}\n\n    This is a one-hot encoding, not a binary numerical encoding.\n\nCreate a categorical output for input \"featurenamefrominputfn\",\nwhich must be of integer type. Value is expected to be >= 0 and ...\n        return ({ 'Integer1':[values], ...., 'Integer2':[values] },\n                [Label_values])\n\nCategorical vocabulary column\n\n    {{}}\n\n    We cannot input strings directly to a model. Instead, we must first\n    map strings to numeric or categorical values. Categorical vocabulary\n    columns provide a good way to represent strings as a one-hot vector.\n\nGiven input \"featurenamefrominputfn\" which is a string,\ncreate a categorical feature to our model by mapping the input to one of\nthe elements in the vocabulary list.\n    vocabularyfeaturecolumn =\n        tf.featurecolumn.categoricalcolumnwithvocabulary_list(\n            key=\"featurenamefrominputfn\",\n            vocabulary_list=[\"kitchenware\", \"electronics\", \"sports\"])\n\nGiven input \"featurenamefrominputfn\" which is a string,\ncreate a categorical feature to our model by mapping the input to one of\nthe elements in the vocabulary list.\n    vocabularyfeaturecolumn =\n        tf.featurecolumn.categoricalcolumnwithvocabulary_list(\n            key=\"featurenamefrominputfn\",\n            vocabulary_list=[\"kitchenware\", \"electronics\", \"sports\"])\n\n    In many cases, the number of categories is large, and we can limit it\n    via hashing:\n\n    {{}}\n\nCreate categorical output for input \"featurenamefrominputfn\".\nCategory becomes: hashvalue(\"featurenamefrominputfn\") % hashbucket_size\n    hashedfeaturecolumn =\n        tf.featurecolumn.categoricalcolumnwithhash_bucket(\n            key = \"featurenamefrominputfn\",\n            hashbucketssize = 100) # The number of categories\n\nFeature Crosses\n\n    Combining features allows the model to learn separate weights\n    specifically for whatever that feature combination means.\n\nIn our input_fn, we convert input longitude and latitude to integer values\nin the range [0, 100)\n    def input_fn():\nUsing Datasets, read the input values for longitude and latitude\n        latitude = ...   # A tf.float32 value\n        longitude = ...  # A tf.float32 value\n\nIn our example we just return our latint, longint features.\nThe dictionary of a complete program would probably have more keys.\n        return { \"latitude\": latitude, \"longitude\": longitude, ...}, labels\n\nAs can be see from the map, we want to split the latitude range\n[33.641336, 33.887157] into 100 buckets. To do this we use np.linspace\nto get a list of 99 numbers between min and max of this range.\nUsing this list we can bucketize latitude into 100 buckets.\n    latitude_buckets = list(np.linspace(33.641336, 33.887157, 99))\n    latitudefc = tf.featurecolumn.bucketized_column(\n        tf.featurecolumn.numericcolumn('latitude'),\n        latitude_buckets)\n\nDo the same bucketization for longitude as done for latitude.\n    longitude_buckets = list(np.linspace(-84.558798, -84.287259, 99))\n    longitudefc = tf.featurecolumn.bucketized_column(\n        tf.featurecolumn.numericcolumn('longitude'), longitude_buckets)\n\nCreate a feature cross of fclongitude x fclatitude.\n    fcsanfranciscoboxed = tf.featurecolumn.crossed_column(\n        keys=[latitudefc, longitudefc],\n        hashbucketsize=1000) # No precise rule, maybe 1000 buckets will be good?\n\nIndicator and Embedding columns\n\n    Indicator columns treat each category as an element in a one-hot\n    vector, where the matching category has value 1 and the rest have 0s.\n\n    {{}}\n\n        indicatorcolumn = tf.featurecolumn.indicatorcolumn(categoricalcolumn)\n\n    An embedding column represents data as a lower-dimensional, ordinary\n    vector in which each cell can contain any number.\n\n    {{}}\n\n    As a guideline, the embedding vector dimension should be the 4th root\n    of the number of categories.\n\n        categorical_column = ... # Create any categorical column\n\nRepresent the categorical column as an embedding column.\nThis means creating a one-hot vector with one element for each category.\n    embeddingcolumn = tf.featurecolumn.embedding_column(\n        categoricalcolumn=categoricalcolumn,\n        dimension=dimensionofembedding_vector)\n\nHooks {#hooks}\n\nHooks are useful for custom processing that has to happen alongside\nthe main loop. For example, we can use hooks for recordkeeping,\ndebugging, monitoring or reporting. Hooks are activated by passing\nthem to the train call. Estimators use hooks internally to implement\ncheckpointing, summaries and more.\n\nclass TimeBasedStopHook(tf.train.SessionRunHook):\n    def begin(self):\n        self.started_at = time.time()\n\n    def afterrun(self, runcontext, run_values):\n        if time.time() - self.startedat >= TRAINTIME:\n            runcontext.requeststop()\n\nExperiment {#experiment}\n\nThe core of the distributed execution support is provided with the\nExperiment class.\n\n{{}}\n\nIn each TensorFlow cluster, there are several parameter servers, and\nseveral worker tasks. Most workers are handling the training process,\nwhich basically calls the Estimator train method with the training\ninput_fn.\n\nThe primary mode of replica training in Estimators is between-graph\nreplication and asynchronous training.\n\nPartial Run {#partial-run}\n\nCame across this when working with Reinforcement Learning\n(Knowledge-base Completion). The problem required the network to take\nevaluate and obtain an action.\n\na = array_ops.placeholder(dtypes.float32, shape=[])\nb = array_ops.placeholder(dtypes.float32, shape=[])\nc = array_ops.placeholder(dtypes.float32, shape=[])\nr1 = math_ops.add(a, b)\nr2 = math_ops.multiply(r1, c)\n\nh = sess.partialrunsetup([r1, r2], [a, b, c])\nres = sess.partialrun(h, r1, feeddict={a: 1, b: 2})\nres = sess.partialrun(h, r2, feeddict={c: res})\n\nPartial run continues the execution of a session with more feeds and\nfetches.\n\nTo use partial execution, a user first calls partialrunsetup() and then a sequence of partialrun(). partialrunsetup specifies the list of feeds and fetches that will be used in the subsequent partialrun calls.\n\nThe optional feed_dict argument allows the caller to override the value of tensors in the graph.\n\nDebugging Memory Leaks {#debugging-memory-leaks}\n\nFinalize the session graph\n\n    Finalizing the graph ensures that no new nodes are being added to the\n    graph on each session run, by marking the graph as read-only. Anything\n    that tries to modify the graph will raise an Exception.\n\nUse tcmalloc\n\n    tcmalloc suffers less from fragmentation when allocating and\n    deallocating large objects (such as tensors). Some memory-intensive\n    Tensorflow programs have been known to leak heap address space (while\n    freeing individual objects they use) with the default malloc.\n\n    tcmalloc also has a heap profiler, which can be analyzed with the\n    google-pprof tool.\n\nTODO What happens when a session is created? {#what-happens-when-a-session-is-created}\n\nContext: created a dataflow job that created one session each run,\nwhich was really slow: 200 workers 1 prediction/s.\n\nFigure out what a tf.Session contains, and how they are initialized\nwith/without graphs.\n\nOptimizing the Input Pipeline {#optimizing-the-input-pipeline}\n\nUse prefetch to overlap the work of a producer and consumer\nCache the dataset into memory if it can fit\n\nTransitioning to TF 2.0 {#transitioning-to-tf-2-dot-0}\n\nIf you are working on custom architectures, we suggest using tf.keras\nto build your models instead of Estimator. (Tensorflow, 2018)\n\nKeras {#keras}\n\nKeras is a high-level neural networks API, compatible with multiple\nbackends. (Keras, 2019) As of Tensorflow 2.0, Keras will be\nthe primary API for building neural networks.\n\nThe Sequential model is a simple model consisting of a linear stack of\nlayers.\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(units=64, activation='relu', input_dim=100))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n\nxtrain and ytrain are Numpy arrays --just like in the Scikit-Learn API.\nmodel.fit(xtrain, ytrain, epochs=5, batch_size=32)\n\nEvaluate performance:\nlossandmetrics = model.evaluate(xtest, ytest, batch_size=128)\n\nGenerate predictions on new data:\nclasses = model.predict(xtest, batchsize=128)\n\nBibliography\n    Tensorflow,  (2018). Standardizing on keras: guidance on high-level apis in tensorflow 2.0. Retrieved from https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a. Online; accessed 07 January 2019. ↩\n\n    Keras,  (2019). Home Keras Documentation. Retrieved from https://keras.io/. Online; accessed 08 January 2019. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/docker",
        "title": "Docker 101",
        "content": "\ntags\n: Linux, Operating Systems\n\nWhy Containers {#why-containers}\n\nContainers are Cheap {#containers-are-cheap}\n\n{{}}\n\nContainers are Cheap\n\n    Docker provides lightweight virtualization with almost zero\n        overhead.\n    Start and stop containers within seconds.\n\nContainers are Portable {#containers-are-portable}\n\nContainers contain all the required software dependencies to run your\napplication.\n\nOnce a container is built, you can be sure it runs \"the same way\" on any\nhost operating system with the same Linux kernel.\n\nContainers in OSes {#containers-in-oses}\n\n| Year |                                                                |\n|------|----------------------------------------------------------------|\n| 2006 | Process Containers, later renamed to cgroups                   |\n| 2008 | LXC: implemented using cgroups and kernel namespaces           |\n| 2013 | Docker: initially built on LXC, but now runs on libcontainer |\n\n2006: Process Containers {#2006-process-containers}\n\n{{}}\n\n2008: LXC Containers, and 2013: Docker {#2008-lxc-containers-and-2013-docker}\n\n{{}}\n\nWhat is Docker? {#what-is-docker}\n\nDocker is a toolchain for managing containers.\n\n{{}}\n\nWhat You'll Do Today {#what-you-ll-do-today}\n\nYou'll be packaging a simple application with Docker.\n\nIf time permits, we'll cover additional things, such as passing\nenvironment variables.\n\nDocker Basics {#docker-basics}\n\nPulling images {#pulling-images}\n\ndocker pull alpine\n\nAlpine?\n\n    alpine is an image for a tiny Linux distribution, used by Docker for\n    most of its official images.\n\n    A container image is a lightweight, stand-alone, executable package of\n    a piece of software that includes everything needed to run it: code,\n    runtime, system tools, system libraries, settings.\n\n    alpine is pulled from the Docker Registry, where hundreds of\n    thousands of images are hosted.\n\nStarting a container {#starting-a-container}\n\ndocker run alpine\n\nWhat is happening here?\n\nRunning a command {#running-a-command}\n\ndocker run alpine echo \"Hello from alpine!\"\n\nDocker Status {#docker-status}\n\nList all docker images:\n\ndocker images\n\nList all running docker containers:\n\ndocker ps\n\nList all docker containers (including stopped containers):\n\ndocker ps -a\n\nEntering a Container Interactively {#entering-a-container-interactively}\n\ndocker run -it alpine sh\n\nuname -r                        # 4.9.41-moby\nhostname                        # container_id\nwhoami                          # root\nid                              # uid=0(root) gid=0(root) groups=0(root),1(bin)...\n\nInside alpine {#inside-alpine}\n\nDocker images consist of multiple layers:\n\n{{}}\n\nLayers for the ubuntu image {#layers-for-the-ubuntu-image}\n\nObserve the output for the following:\n\ndocker pull ubuntu:15.04\ndocker history ubuntu\n\nDeclare layers with a Dockerfile {#declare-layers-with-a-dockerfile}\n\nFROM ubuntu\nMAINTAINER Kimbro Staken\n\nRUN apt-get install -y software-properties-common python\nRUN add-apt-repository ppa:chris-lea/node.js\nRUN echo \"deb http://us.archive.ubuntu.com/ubuntu/ precise universe\" >> /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get install -y nodejs\n#RUN apt-get install -y nodejs=0.6.12~dfsg1-1ubuntu1\nRUN mkdir /var/www\n\nADD app.js /var/www/app.js\n\nCMD [\"/usr/bin/node\", \"/var/www/app.js\"]\n\nDockerfile Cheatsheet {#dockerfile-cheatsheet}\n\n| Command    | Info                                                                                    |\n|------------|-----------------------------------------------------------------------------------------|\n| FROM       | Sets the Base Image for subsequent instructions.                                        |\n| RUN        | execute any commands in a new layer on top of the current image and commit the results. |\n| CMD        | provide defaults for an executing container.                                            |\n| EXPOSE     | informs Docker that the container listens on the specified network ports at runtime.    |\n| ENV        | sets environment variable.                                                              |\n| COPY       | copies new files or directories to container.                                           |\n| ENTRYPOINT | configures a container that will run as an executable.                                  |\n| VOLUME     | creates a mount point for externally mounted volumes or other containers.               |\n| WORKDIR    | sets the working directory.                                                             |\n| ARG        | defines a build-time variable.                                                          |\n| LABEL      | apply key/value metadata to your images, containers, or daemons.                        |\n\nDocker Networking {#docker-networking}\n\nDocker creates three networks by default. We're mostly concerned with\nbridge.\n\n$ docker network ls\n\nNETWORK ID          NAME                DRIVER\n7fca4eb8c647        bridge              bridge\n9f904ee27bf5        none                null\ncf03ee007fb4        host                host\n\nHow bridge works {#how-bridge-works}\n\nUnless specified otherwise, the docker container will connect to this\ndefault bridge network. This provides a means of Docker containers\nto access the outside world. This is achieved through rules on the\nkernel's iptable.\n\nBy default, none of the ports are published and the outside world has\nno access to the docker containers.\n\nRunning a Simple Webserver {#running-a-simple-webserver}\n\nnc -ll -p 8080 -e /bin/echo -e \"HTTP/1.1 200 OK\\n\\n$(date)\\n\"\n\nAllowing Ingress {#allowing-ingress}\n\ndocker run -p 5001:8080 alpine \\\n       nc -ll -p 8080 -e /bin/echo -e \"HTTP/1.1 200 OK\\n\\n$(date)\\n\"\n\nAs a daemon:\n\ndocker run -p 5001:8080 -d alpine \\\n       nc -ll -p 8080 -e /bin/echo -e \"HTTP/1.1 200 OK\\n\\n$(date)\\n\"\n\nExercise: package a Node.js Server with Docker {#exercise-package-a-node-dot-js-server-with-docker}\n\ncurl -i http://localhost:5001/\n\nHTTP/1.1 200 OK\nX-Powered-By: Express\nContent-Type: text/html; charset=utf-8\nContent-Length: 14\nETag: W/\"e-ZohVPp9YwmNT/yh3111KJ3ZG6Uk\"\nDate: Fri, 13 Oct 2017 18:34:46 GMT\nConnection: keep-alive\n\nHello world!!\n\nServer code {#server-code}\n\nhttps://git.io/vdXLC\n\nGeneral Instructions {#general-instructions}\n\nInstall the Node.js runtime (use Ubuntu)\nCopy the files into the container\nGet the node package manager npm\nRun npm install\nRun npm start to start server\n\nBut... I want to change my files! {#but-dot-dot-dot-i-want-to-change-my-files}\n\ndocker run -d my/webserver\nCreate file locally\ndocker exec -it container_name sh\nls #WAT\n\nMounting Volumes {#mounting-volumes}\n\nThere are three types of mounts:\n\nVolumes are managed by docker. Volumes also support the use of\n    volume drivers, which allow you to store your data on remote hosts\n    or cloud providers, among other possibilities.\nBind Mounts may be stored anywhere on the host filesystem.\ntmpfs mounts are stored in the host system's memory only.\n\nMounting the /src directory {#mounting-the-src-directory}\n\ndocker run -p 5000:8080 -v ~/path/to/directory:/usr/src/app/src my/webserver\n\nBeyond Docker {#beyond-docker}\n\nDocker is the building block for many devops solutions.\n\nContainer Orchestration: Kubernetes, Cloud Foundry etc.\nMonitoring: cAdvisor, InfluxDB etc.\nReverse Proxies and Load Balancers: fabio etc.\n\nDocker makes microservices manageable and scalable.\n",
        "tags": []
    },
    {
        "uri": "/zettels/documentation_generators",
        "title": "Documentation Generators",
        "content": "\nWe use these to generate documentation for projects.\n\nSoftware {#software}\n\nDocusaurus\nGatsby.js\nGitbook - Only has a limited number of free spaces\nSphinx\nMkDocs\n\nBy far my favourite now is MkDocs, and I'm using it for Org-Roam's\ndocumentation page. It is built using mkdocs with:\n\nMaterial theme for MkDocs\nAdmonition\nPyMdown\n",
        "tags": []
    },
    {
        "uri": "/zettels/documentation",
        "title": "Documentation",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/ds_algo",
        "title": "Data Structures and Algorithms",
        "content": "\nJava {#java}\n\nAccess Modifier {#access-modifier}\n\nC: Class; P: Package; SC: Subclass; W: World\n\n| Modifier  | C | P | SC | W |\n|-----------|---|---|----|---|\n| public    | Y | Y | Y  | Y |\n| protected | Y | Y | Y  | N |\n| none      | Y | Y | N  | N |\n| private   | Y | N | N  | N |\n\nComparable {#comparable}\n\nImplement Comparable:\n\nint compareTo(T o)\n\nhashCode {#hashcode}\n\nIf two objects are equal, hashCode must return the same result\nhashCode must return the same result hen invoked on the same object\n    more than once\n\nint hashCode() {}\nboolean equals(Object o) {}\n\nCommon Issues {#common-issues}\n\nArray out of bounds; divide by zero; infinite loop; exception thrown\nbut not caught; access member variable in static method\n\nAlgorithmic Analysis {#algorithmic-analysis}\n\nAmortized Cost\n: Algo has amortized cost \\\\(T(n)\\\\) if &forall; k\n    &isin; \\mathbb{Z}, cost of k operations is \\\\(\\leq\n                        kT(n)\\\\)\n\nMaster's Theorem {#master-s-theorem}\n\n\\\\(T(n) = aT(\\frac{n}{b}) + f(n)\\\\) where a &ge; 1, b > 1\n\n\\\\(f(n) \\in O(n^c)\\text{, } c  \\log\\_ba \\text{ and } \\exists k \\\\ \\text{ st }\n      af(\\frac{n}{b})\\le kf(n) \\text{ then } T(n) \\in \\Theta(n^{\\log\\_ba})\\\\)\n\nCommon Ones {#common-ones}\n\n\\\\(T(n) = T(n/2) + \\Theta(1) = O(\\log n)\\\\) (Binary Search)\n\\\\(T(n) = 2T(n/2) + \\Theta(1) = O(n)\\\\) (Binary Tree Traversal)\n\\\\(T(n) = 2T(n/2) + \\Theta(\\log n) = O(n)\\\\) (Optimal Sorted Matrix\n    Search)\n\\\\(T(n) = 2T(n/2) + O(n) = O(n \\log n)\\\\) (Merge Sort)\n\nAbstract Data Types {#abstract-data-types}\n\nBag\n: Insert(i); Draw()\n\nStack (LIFO)\n: empty(); peek(); pop(); push(i)\n\nQueue (FIFO)\n: add(); offer(i); peek(); poll(); remove()\n\nDequeue\n: double-ended queue\n\nSearching {#searching}\n\nBinary Search \\\\(O(\\log n)\\\\)\n\nint binarySearch(int[] arr, int key) {\n    int start = 0, end = arr.length - 1;\n    int found = -1;\n    while(start  key) {\n            end = mid - 1;\n        } else {\n            found = mid;\n            // if we want first instance\n            end = mid - 1;\n            // if we want last instance\n            // start = mid + 1;\n        }\n    }\n    return found;\n}\n\nOne sided Binary Search\n: Suppose one side is bounded, eg [1,\n    &infin;). Use the sequence [1,2,4,8,16..., \\\\(2^k\\\\)...] If it works for\n    \\\\(2^k\\\\), then search on [\\\\(2^{k-1}, 2^k\\\\)]\n\nPeak Finding\n: A[j] in array A is peak if (i) A[j] > A[j-1] (ii)\n    A[j] > A[j+1]. If only one item in array, vacously true\n\n1D Peak Finding \\\\(O(\\log n)\\\\)\n: D&C\n\nif a[n/2] }}\n\ninsert \\\\(O(\\log n)\\\\):\n    insert key in BST\n    walk up, perform max 2 rotations if out-of-balance\ndelete(v): (\\\\(\\log n\\\\) rotations)\n    If v has 2 children, swap with successor\n    delete v, and reconnect children\n    for every ancestor of deleted node\n        rotate if out-of-balance\nSplay Trees: Rotate nodes that are accessed to root. consider using\n    where operations are non-random.\n\nAugmented Trees {#augmented-trees}\n\nRank Tree (Order Statistics) {#rank-tree--order-statistics}\n\nstore weight of tree in each node:\n\\\\(w(v) = w(v.left) + w(v.right) + 1\\\\)\nselect(k) \\\\(O(\\log n)\\\\): finds node with rank \\\\(k\\\\)\n\nrank = left.weight + 1;\n  if (k == rank)\n    return v\n  else if (k  max endpoint in c.left\n  search in right subtree\nelse search in left subtree\n\nfindAll(x) \\\\(O(k \\log n)\\\\) for k overlapping intervals\n\nsearch(x)\nstore it somewhere else\nremove interval\nrepeat until no intervals found\n\nOrthogonal Range Searching {#orthogonal-range-searching}\n\n1D\n\n    use a binary tree search tree\n    store all points in the leaves of the tree, internal nodes store\n        only copies\n    each internal node v stores the max of any leaf in the left subtree\n    Query Time: \\\\(O(k + \\log n)\\\\)\n    Building Tree: \\\\(O(n \\log n)\\\\)\n\nk-dim Tree\n\n    each node in the x-tree has a set of points in its subtree\n    store the y-tree at each x-node containing all points\n    Query Time: \\\\(O(k + \\log^d n)\\\\)\n    Building Tree: \\\\(O(n \\log^{d-1}n)\\\\)\n    Space: \\\\(O(n \\log^{d-1}n)\\\\)\n\nCustom Augmentations {#custom-augmentations}\n\nAverage height of people taller: augment nodes to include the\n    count of the number of nodes in that sub-tree, along with the sum\n    of the heights of all the people in that sub-tree. To return the\n    desired average, first search for the name in the hash table; assume\n    it is at node v; then find the sum of the heights of: the right-child\n    of v, and if w is on the path from v to the root and v is in w’s\n    left-subtree, then w’s right-subtree and w.\n\nHash Tables {#hash-tables}\n\nn: #items, m: #buckets\nSimple Uniform Hashing: Keys are equally likely to map to every\n    bucket, and are mapped independently\n    \\\\(load(ht) = \\frac{n}{m}\\\\)\n    \\\\(E\\_\\text{search} = 1 + \\frac{n}{m}\\\\)\n    Assume \\\\(m=\\Omega(n)\\\\), \\\\(E\\_\\text{search} = O(1)\\\\)\n\nHash Functions {#hash-functions}\n\nDivision {#division}\n\n\\\\(h(k) = k \\text{ mod } m\\\\), choose m prime\n\nMultiplication {#multiplication}\n\nfix table size: \\\\(m=2^r\\\\), for some \\\\(r\\\\)\nfix word size: \\\\(w\\\\), size of key in bits\nfix odd constant \\\\(A\\\\), \\\\(A > 2^{w-1}\\\\)\n\\\\(h(k) = (Ak) \\text{ mod } 2^w >> (w - r)\\\\)\n\nRolling Hash {#rolling-hash}\n\nWhen key changes by single character\n\nChaining {#chaining}\n\nbucket stores linked list, containing (object, value)\nWorst insert \\\\(O(1 + cost(h))\\\\)\nExpected search = \\\\(1 + \\frac{n}{m} = O(1)\\\\)\nWorst search \\\\(O(n)\\\\)\n\nOpen Addressing {#open-addressing}\n\nOne item per slot, probe sequence of buckets until find only one\n\\\\(h(key, i) : U \\mapsto {1..m}\\\\), \\\\(i\\\\) is no. of collisions\nsearch: keep probing until empty bucket, or exhausted entire table\ndelete: set key to tombstone value, so probe sequence still works\ninsert: on deleted cell, overwrite, else find next available slot\ngood hash function:\n    \\\\(h(key, i)\\\\) enumerates all possible buckets\n    Simple Uniform Hashing\nLinear: \\\\(h(k,i) = h(k) + i\\\\), Clustering\nDouble: \\\\(h(k,i) = f(k) + i \\cdot g(k) \\text{ mod } m\\\\)\nInsert, Search: \\\\(\\frac{1}{1-\\alpha}\\\\) where \\\\(\\alpha = \\frac{n}{m} \\le\n      1\\\\)\ngood: saves space, rare mem alloc, better cache perf\nbad: sensitive to hash, load\n\nCuckoo Hashing {#cuckoo-hashing}\n\nResolving hash collisions with worst-case constant lookup time\nLookup: inspection of just two locations in the hash table\nInsertion: Insert into first table if empty; else kick out other\n    key to second location.\nIf infinite loop, hash function is rebuilt in place\n\nTable resizing {#table-resizing}\n\nScan old table \\\\(O(m\\1)\\\\), create new table \\\\(O(m\\2)\\\\), insert each\n    element \\\\(O(1)\\\\), total \\\\(O(m\\1 + m\\2 + n)\\\\)\n\\\\(O(n)\\\\) amor: if \\\\(n == m\\\\), \\\\(m = 2m\\\\), if \\\\(n  priority of child\ninsert: create new leaf, bubbleUp\ndecreaseKey: update priority, bubbleDown\ndelete: swap with leaf, delete, and then bubble\nstore in array:\n    \\\\(left(x) = 2x + 1\\\\)\n    \\\\(right(x) = 2x + 2\\\\)\n    \\\\(parent(x) = \\lfloor(x-1)/2\\rfloor\\\\)\n\nHeap Sort {#heap-sort}\n\nHeapify (insert n items)  O(n log n)\nExtract from heap n times (O(n log n))\n\nImprovement: recursively join 2 heaps and bubble root down (base\n    case single node) O(n)\nO(n log n) worst case, deterministic, in-place\n\nUFDS (weighted) {#ufds--weighted}\n\nunion(p,q) \\\\(O(\\log n)\\\\)\n    find parent of p and q\n    make root of smaller tree root of larger tree\nfind(k) \\\\(O(\\log n)\\\\)\n    search up the tree, return the root\n    (PC): update all traversed nodes parent to root\n\nWU with PC, union and find \\\\(O(\\alpha(m,n))\\\\)\n\nMST {#mst}\n\nacyclic subset of edges that connects all nodes, and has minimum\n    weight\n\nProperties {#properties}\n\nCutting edge in MST results in 2 MSTs\nCycle Poperty: \\\\(\\forall\\\\) cycle, max weight edge is not in MST\nCut Property: \\\\(\\forall\\\\) partitions, min weight edge\n    across cut is in MST\n\nPrim's \\\\(O(E \\log V)\\\\) {#prim-s--oe-log-v}\n\nUses cycle property\n\nT = {start}\nenqueue start's edges in PQ\nwhile PQ not empty\n  e = PQ.dequeue()\n  if (vertex v linked with e not in T)\n    T = T U {v, e}\n  else\n    ignore edge\nMST = T\n\nKruskal's \\\\(O(E\\log V)\\\\) {#kruskal-s--oe-log-v}\n\nUses UFDS\nIt is possible that some edge in the first \\\\(V-1\\\\) edges will form a\n    cycle with pre-existing MST solution\n\nSort E edges by increasing weight\nT = {}\nfor (i = 0; i  T\\_1/p\\\\)\n\\\\(T\\p > T\\\\infty\\\\), cannot run slower on more processors\nGoal: \\\\(T\\p = (T\\1/p) + T\\\\infty\\\\), \\\\(T\\1/p\\\\) is the parallel part,\n    \\\\(T\\_\\infty\\\\) is the sequential part\n\nMatrix Addition {#matrix-addition}\n\nBefore:\n• Work analysis: \\\\(T\\_1(n) = O(n^2)\\\\)\n• critical path analysis: \\\\(T\\_\\infty(n) = O(n^2)\\\\)\nAfter:\n\npMatAdd(A,B,C,i,j,n)\n  if(n == 1)\n    C[i,j] = A[i,j] + B[i,j];\n  else:\n    spawn pMatAdd(A,B,C,i,j,n/2);\n    spawn pMatAdd(A,B,C,i,j + n/2,n/2);\n    spawn pMatAdd(A,B,C,i + n/2,j,n/2);\n    spawn pMatAdd(A,B,C,i + n/2,j + n/2,n/2);\n    sync;\n\nWork Analysis: \\\\(T\\1(n) = 4T\\1(n/2) + O(1) = O(n^2)\\\\)\nCritical Path Analysis: \\\\(T\\\\infty(n) = T\\\\infty(n/2) + O(1) = O(\\log n)\\\\)\n\nParallelized Merge Sort \\\\(O(\\log^3n)\\\\) {#parallelized-merge-sort--o-log-3n}\n\npMerge(A[1..k], B[1..m], C[1..n])\n  if (m > k) then pMerge(B, A, C);\n  else if (n==1) then C[1] = A[1];\n  else if (k==1) and (m==1) then\n    if (A[1] <= B[1]) then\n      C[1] = A[1]; C[2] = B[1];\n    else\n      C[1] = B[1]; C[2] = A[1];\n  else\n    // binary search for j where\n    // B[j] <= A[k/2] <= B[j+1]\n    spawn pMerge(A[1..k/2],\n                 B[1..j],\n                 C[1..k/2+j])\n    spawn pMerge(A[k/2+1..l],\n                 B[j+1..m],\n                 C[k/2+j+1..n])\n    synch;\n\npMergeSort(A, n)\n  if (n=1) then return;\n  else\n    X = spawn pMergeSort(A[1..n/2], n/2)\n    Y = spawn pMergeSort(A[n/2+1, n], n/2)\n    synch;\n    A = spawn pMerge(X, Y);\n",
        "tags": []
    },
    {
        "uri": "/zettels/ekf_localization",
        "title": "EKF Localization",
        "content": "\nEKF localization {#ekf-localization}\n\nEKF localization assumes that the map is represented by a collection\nof features. At any point in time \\\\(t\\\\), the robot gets to observe a\nvector of ranges and bearings to nearby features:\n\\\\(z\\{t}=\\left\\\\{z\\{t}^{1}, z\\_{t}^{2}, \\ldots\\right\\\\}\\\\).\n\nThe easiest localization algorithm assumes all features are uniquely\nidentifiable, although this assumption can be corrected for. This\nproblem is called the unknown correspondence problem. One solution is\nthe maximum likelihood correspondence, where one determines the most\nlikely value of the correspondence and uses that value.\n\nMaximum likelihood correspondence is brittle when there are many\nequally likely hypotheses for the correspondence variable. This is\nskirted around by choosing landmarks that are sufficiently distinct or\nfar apart, and making sure that the robot's uncertainty is small.\n\nPractical Considerations {#practical-considerations}\n\nefficient search\n: it is computationally expensive to loop through\n    all landmarks \\\\(k\\\\) in the map. There are often simple tests to\n    identify plausible candidate landmarks.\n\nmutual exclusion\n: a key assumption is the independence of feature\n    noise, which enabled us to process individual features sequentially,\n    avoiding an exponential search. This resulted in being able to\n    assign the same landmark in the map with multiple values. However,\n    this is invalid by default in some sensors, such as cameras. Knowing\n    this mutual exclusion can significantly reduce the search space.\n\noutliers\n: Outliers are not addressed in the algorithm.\n\ninformation\n: EKF localization uses a subset of all available\n    information (processed features) to localize. In addition EKF is\n    unable to process negative information (lack of a feature).\n\nThese methods are furthermore unable to solve global localization and\nthe kidnapped robot problem.\n\nRelated {#related}\n\n§robot\\_localization\n§markov\\_localization\n§grid\\mc\\localization\n",
        "tags": []
    },
    {
        "uri": "/zettels/em",
        "title": "Expectation Maximization and Mixture Models",
        "content": "\nIntroduction {#introduction}\n\nIf we define a joint distribution over observed and latent variables,\nthe corresponding distribution of the observed variables alone is\nobtained by marginalization. This allows relatively complex marginal\ndistributions over observed variables to be expressed in terms of more\ntractable joint distributions over the expanded space of observed and\nlatent variables.\n\nMixture distributions, such as the Gaussian mixture, can be\ninterpreted in terms of discrete latent variables. These mixture\nmodels are also useful in clustering data. We first discuss the\nproblem of finding clusters using a non-probabilistic technique called\nK-means clustering. Then we introduce the latent variable view of\nmixture distributions where the discrete latent variables can be\ninterpreted as defining assignments of data points to specific\ncomponents of the mixture.\n\nThe EM algorithm is the general approach to finding maximum likelihood\nestimators in latent variable models. We will see that K-means\nclustering is the non-probabilistic version of the EM algorithm.\n\nK-means Clustering {#k-means-clustering}\n\nWe formulate the K-means clustering problem as follows:\n\nSuppose we have a datset \\\\(\\\\{\\mathbf{x\\1}, \\dots, \\mathbf{x}\\N\\\\}\\\\)\nconsisting of \\\\(N\\\\) observations of a random D-dimensional Euclidean\nvariable \\\\(\\mathbf{x}\\\\). Our goal is to partition the data set into a\nfixed number of clusters \\\\(K\\\\). We formalize this by introducing\n\\\\(\\mathbf{\\mu}\\_k\\\\), that represents the center of the clusters. Our goal\nis then to minimize the objective function:\n\n\\begin{equation}\n  J = \\sum\\{n=1}^{N} \\sum\\{k=1}^{N} r\\{nk} \\lvert \\mathbf{x}\\n -\n  \\mathbf{\\mu}\\_k^2 \\rvert\n\\end{equation}\n\nThe K-means algorithm works as follows:\n\nChoose some initial values of $&mu;$\\_k.\nMinimize \\\\(J\\\\) wrt \\\\(r\\{nk}\\\\), keeping \\\\(\\mu\\k\\\\) fixed.\nMinimize \\\\(J\\\\) wrt \\\\(\\mu\\k\\\\), keeping \\\\(r\\{nk}\\\\) fixed.\nRepeat steps 2 and 3 until convergence.\n\nWe shall see that updating \\\\(r\\{nk}\\\\) and \\\\(\\mu\\k\\\\) respectively\ncorrespond to the E-step and M-step of the EM algorithm.\n\nConsider first the determination of \\\\(r\\_{nk}\\\\). Because \\\\(J\\\\) is a ilnear\nfunction of \\\\(r\\_{nk}\\\\), this optimization can be performed easily to\ngive a closed-form solution:\n\n\\begin{equation}\n  r\\_{nk} = \\begin{cases}\n    1 & \\text{if } k = \\textrm{argmin}\\j \\lvert \\mathbf{x}\\n -\n    \\mathbf{\\mu}\\_j \\rvert ^2 \\\\\\\\\\\\\n    0 & \\text{otherwise.}\n  \\end{cases}\n\\end{equation}\n\nNow, consider the optimization of \\\\(\\mu\\k\\\\) with \\\\(r\\{nk}\\\\) held fixed.\nThe objective function \\\\(J\\\\) is quadratic of \\\\(\\mu\\_k\\\\), and can be\nminimized by setting the derivative wrt to \\\\(\\mu\\_k\\\\) to 0. We solve it\nto be:\n\n\\begin{equation}\n  \\mathbf{\\mu}\\k = \\frac{\\sum\\{n} r\\{nk}\\mathbf{x}\\n}{\\sum\\{n} r\\{nk}}\n\\end{equation}\n\nThis result can be interpreted as setting \\\\(\\mu\\_k\\\\) equal to the mean of\nall the datapoints \\\\(\\mathbf{x}\\_n\\\\) assigned to cluster \\\\(k\\\\).\n\nBecause each step reduces the objective function \\\\(J\\\\), the algorithm is\nguaranteed to converge to some minimum. This minimum may be a local\nminimum, instead of a global one.\n\nOne way of choosing the cluster centers \\\\(\\mu\\_k\\\\) is to choose a random\nsubset of \\\\(K\\\\) data points as the centers. K-means is often used to\ninitialize parameters in a Gaussian mixture model before applying the\nEM algorithm.\n\nThe K-means algorithm described can be generalized to datasets by\nusing a dissimilarity measure \\\\(\\mathcal{V}(x, x')\\\\) (instead of\nEuclidean distance), and minimizing the distortion measure:\n\n\\begin{equation}\n  \\tilde{J} = \\sum\\{n=1}^{N} \\sum\\{k=1}^{K} r\\{nk}\\mathcal{V}(\\mathbf{x}\\n, \\mathbf{x}\\_k)\n\\end{equation}\n\nwhich gives the K-medoids algorithm.\n\nThe computational cost of the E-step is O(KN), while the M-step\ninvolves \\\\(O(N\\_k^2)\\\\) evaluations of \\\\(\\mathcal{V}(\\dot, \\dot)\\\\).\n\nOne feature of the K-means algorithm is that it assigns each datapoint\n\\\\(\\mathbf{x}\\_i\\\\) to uniquely one of the clusters. There may be\ndatapoints that lie roughly halfway between two cluster centers. By\nadopting a probabilistic approach, we can obtain 'soft' assignments\nthat reflect the level of uncertainty over the most appropriate\nassignment.\n\nThe Gaussian Mixture Model {#the-gaussian-mixture-model}\n\nThe Gaussian mixture model allows modelling more complex\ndistributions. We can formulate the Gaussian mixture model in terms of\ndiscrete latent variables. the Gaussian mixture distribution can be\nwritten as a linear superposition of Gaussians of the form:\n\n\\begin{equation}\n  p(\\mathbf{x}) = \\sum\\{k=1}^{K} \\pi\\k \\mathcal{N}(\\mathbf{x} |\n  \\mathbf{\\mu}\\k, \\mathbf{\\Sigma}\\k)\n\\end{equation}\n\nWe can introduce a K-dimensional binary random variable \\\\(\\mathbf{z}\\\\)\na 1-of-K representation such that all elements but one of them are 0s,\nand \\\\(p(z\\k = 1) = \\pi\\k\\\\). We can then define the joint distribution\n\\\\(p(\\mathbf{x}, \\mathbf{z})\\\\) corresponding to the following graphical\nmodel.\n\n{{}}\n\nThe parameters \\\\(\\pi\\_k\\\\) must satisfy:\n\n\\begin{equation}\n  0 \\le \\pi\\k \\le 1, \\sum\\{k=1}^{K}\\pi\\_k = 1\n\\end{equation}\n\nwe can then represent \\\\(p(\\mathbf{z})\\\\) as:\n\n\\begin{equation}\n  p(\\mathbf{z}) = \\prod\\{k=1}^{K}\\pi\\k^{z\\_k}\n\\end{equation}\n\nThe conditional distribution can also be written as:\n\n\\begin{equation}\n  p(\\mathbf{x} | \\mathbf{z}) = \\prod\\_{k=1}^{K} \\mathcal{N}(\\mathbf{x}\n  | \\mathbf{\\mu}\\k, \\mathbf{\\Sigma}\\k)^{z\\_k}\n\\end{equation}\n\nSince the joint distribution is given by\n\\\\(p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z})\\\\), the marginal distribution of\n\\\\(\\mathbf{x}\\\\) is given by summing the joint distribution over all\npossible states of \\\\(\\mathbf{z}\\\\):\n\n\\begin{equation}\np(\\mathbf{x}) = \\sum\\_{z} p(\\mathbf{z}) p(\\mathbf{x} | \\mathbf{z}) =\n\\sum\\{k=1}^{K} pi-k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}\\k, \\mathbf{\\Sigma}\\_k)\n\\end{equation}\n\nWe define another quantity \\\\(\\gamma(z\\k) = p(z\\k = 1 | \\mathbf{x})\\\\), whose\nquantity can be found via Bayes theorem:\n\n\\begin{equation}\n  \\gamma(z\\k) = \\frac{\\pi\\k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}\\_k,\n    \\mathbf{\\Sigma}\\k)}{\\sum\\{j=1}^{K} \\pi\\_j \\mathcal{N}(\\mathbf{x} |\n    \\mathbf{\\mu}\\j, \\mathbf{\\Sigma}\\j)}\n\\end{equation}\n\nWe shall view \\\\(\\pi\\k\\\\) as the prior probability that \\\\(z\\k = 1\\\\), and\n\\\\(\\gamma(z\\k)\\\\) is the responsibility_ that component \\\\(k\\\\) takes for\nexplaining the observation \\\\(\\mathbf{x}\\\\).\n\nSuppose we have a dataset of observations \\\\(\\\\{\\mathbf{x}\\_1, \\dots,\n\\mathbf{x}\\_k\\\\}\\\\), and wish to model this data using a mixture of\nGaussians. We can find the maximum likelihood estimates of the mixture\nmodel parameters. Before proceeding, it is worth noting that the\nmaximum likelihood approach has severe problems. In particular, the\nmaximimizing the log likelihood function is not a well-posed problem,\nand there are singularities that can cause the log-likelihood to\nbecome infinity in the limit where the variance is zero. This occurs\nwhen a Gaussian component collapses onto a single data point. This can\nbe interpreted as overfitting. We can hope to avoid these\nsingularities by using suitable heuristics, using the MAP or Bayesian\napproach.\n\nEM for Gaussian Mixture Model {#em-for-gaussian-mixture-model}\n\nWe can find maximum likelihood solutions for models with latent\nvariables using the EM algorithm. We demonstrate this for the case of\nthe Gaussian mixture model.\n\nFirst, we find the derivatives of \\\\(\\ln p(\\mathbf{X} | \\mathbf{\\pi},\n\\mathbf{\\mu}, \\mathbf{\\Sigma})\\\\) wrt to \\\\(\\mu\\_k\\\\) to 0:\n\n\\begin{equation}\n  0 = \\sum\\{n=1}^{N} \\underbrace{\\frac{\\pi\\k \\mathcal{N}(\\mathbf{x}\\_n |\n    \\mathbf{\\mu}\\k, \\mathbf{\\Sigma}\\k)}{\\sum\\{j} \\pi\\j \\mathcal{N}\n    (\\mathbf{x}\\n| \\mathbf{\\mu}\\j \\mathbf{\\Sigma}\\j)}}\\{\\gamma (z\\_{nk})}\n  \\mathbf{\\Sigma}^{-1}\\k \\left( \\mathbf{x}\\n - \\mathbf{\\mu}\\_k \\right)\n\\end{equation}\n\nMultiplying by \\\\(\\mathbf{\\Sigma}\\_k^{-1}\\\\) (which we assume to be\nnon-singular), we get:\n\n\\begin{equation}\n\\mathbf{\\mu}\\k = \\frac{1}{N\\k} \\sum\\_{n=1}^{N}\n\\gamma(z\\{nk})\\mathbf{x}\\n, \\text{ where } N\\k =\\sum\\{n=1}^{N}\\gamma(z\\_{nk})\n\\end{equation}\n\nWe can interpret \\\\(N\\_k\\\\) as the effective number of points assigned to\ncluster \\\\(k\\\\).\n\nIf we set the derivative of \\\\(\\ln p(\\mathbf{X} | \\mathbf{\\pi},\n\\mathbf{\\mu}, \\mathbf{\\Sigma})\\\\) wrt to \\\\(\\Sigma\\_k\\\\) to 0, and work it\nout, we also get that:\n\n\\begin{equation}\n  \\pi\\k = \\frac{N\\k}{N}\n\\end{equation}\n\nThese results are not closed form solutions of the parameters of the\nmixture model, since they depend on \\\\(\\gamma(z\\_{nk})\\\\). However, the\niterative procedure in the EM algorithm, allows us to choose some\ninitial values and perform E and M-steps to converge to a solution.\n\nIn the expectation step (E-step), we use the current values for the\nparameters to evaluate the posterior probabilities, or\nresponsibilities. We then use these probabilities in the maximization\nstep (M-step), to re-estimate the means, covariances and mixing\ncoefficients.\n\nGeneral EM {#general-em}\n\nThe goal of the EM algorithm is to find maximum likelihood solutions\nfor models having latent variables. We denote the set of all observed\ndata by \\\\(\\mathbf{X}\\\\), in which the nth row represents\n\\\\(\\mathbf{x}\\_n^T\\\\), and similarly we denote the set of all latent\nvariables by \\\\(Z\\\\), with a corresponding row $\\mathbf{z}\\_n^T4. The set\nof all model parameters is denoted by \\\\(\\mathbf{\\theta}\\\\). The\nlog-likelihood function is given by:\n\n\\begin{equation}\n  \\ln p(\\mathbf{X} | \\mathbf{\\theta}) = \\ln \\left\\\\{ \\sum\\_{\\mathbf{z}}\n    p(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta} ) \\right\\\\}\n\\end{equation}\n\nA key observation is that the summation occurs within the\nlogarithm. Even if the joint distribution belongs to the exponential\nfamily, the marginal \\\\(p(\\mathbf{X} | \\mathbf{\\theta})\\\\) generally does\nnot because of this summation. The presence of the sum prevents the\nlogarithm from acting directly on the joint distribution, resulting in\ncomplicated expressions for the maximum likelihood solution.\n\nSince we are in general not given the complete dataset \\\\(\\\\{\\mathbf{X},\n\\mathbf{Z}\\\\}\\\\), but only the incomplete data \\\\(\\mathbf{X}\\\\), our state of\nknowledge of the values of the latent variables is given only by a\nposterior distribution \\\\(p(\\mathbf{Z} | \\mathbf{X}, \\mathbf{\\theta})\\\\).\nInstead, we consider the expected value under the posterior\ndistribution of the latent variable, which corresponds to the E-step\nof the EM algorithm. In the subsequent M-step, we maximize this\nexpectation.\n\nIn the E-step, we use the current parameters \\\\(\\theta^{\\text{old}}\\\\) to\nfind the posterior distribution of the latent variables given by\n\\\\(p(\\mathbf{Z} | \\mathbf{X}, \\mathbf{\\theta}^{\\text{old}})\\\\). We use the\nposterior distribution to fin the expectation of the complete-data log\nlikelihood evaluated for some general parameter \\\\(\\theta\\\\). This\nexpectation, denoted \\\\(Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{\\text{old}}\n)\\\\), is given by:\n\n\\begin{equation}\n  Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{\\text{old}}) = \\sum\\_{\\mathbf{Z}}\n  p(\\mathbf{Z} | \\mathbf{X}, \\mathbf{\\theta}^{\\text{old}})\\ln\n  p(\\mathbf{X} , \\mathbf{Z} | \\mathbf{\\theta})\n\\end{equation}\n\nIn the M-step, we revise the parameter estimate\n\\\\(\\mathbf{\\theta}^{\\text{new}}\\\\) by maximizing the Q function:\n\n\\begin{equation}\n  \\mathbf{\\theta}^{\\text{new}} = \\textrm{argmax}\\_{\\theta}\n  Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{\\text{old}})\n\\end{equation}\n\nIn the definition of \\\\(Q\\\\), the logarithm acts directly on the joint\ndistribution, making the M-step tractable.\n\nIn general, we suppose that the direct optimization of \\\\(p(\\mathbf{X} |\n\\mathbf{\\theta})\\\\) is difficult, and the optimization of \\\\(p(\\mathbf{X},\n\\mathbf{Z} | \\mathbf{\\theta})\\\\) is significantly easier.\n\nWe introduce a distribution \\\\(q(\\mathbf{Z})\\\\) over the latent variables,\nand we observe that for any choice of \\\\(q(\\mathbf{Z})\\\\), the following\ndecomposition holds:\n\n\\begin{equation}\n  \\ln p(\\mathbf{X} | \\mathbf{\\theta}) = \\mathcal{L}(q,\n  \\mathbf{\\theta}) + KL(q || p)\n\\end{equation}\n\nwhere\n\n\\begin{equation}\n  \\mathcal{L} (q, \\mathbf{\\theta}) = \\sum\\_{\\mathbf{Z}} q(\\mathbf{Z})\n  \\ln \\left\\\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta})}{q(\\mathbf{Z})} \\right\\\\}\n\\end{equation}\n\nand\n\n\\begin{equation}\n  KL(q||p) = - \\sum\\_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\left\\\\{\n    \\frac{p(\\mathbf{Z} | \\mathbf{X}, \\mathbf{\\theta})}{q(\\mathbf{Z})} \\right\\\\}\n\\end{equation}\n\n{{}}\n\nThe EM algorithm involves alternatingly computing a lower bound on the\nlog likelihood for the current parameter values, and then maximizing\nthis bound to obtain the new parameter values.\n\nFor complex models, the E-step and M-step can still be intractable.\nThe Generalized EM (GEM) algorithm addresses the problem on the\nintractable M-step. Instead of maximizing \\\\(L(q, \\mathbf{\\theta})\\\\) wrt\n\\\\(\\mathbf{\\theta}\\\\), it seeks to change the parameters such that the\nvalue is increased. Similarly, one can address the intractable E-step\nby seeking to partially optimize \\\\(L(q, \\mathbf{\\theta})\\\\) wrt\n\\\\(q(\\mathbf{Z})\\\\).\n\nReferences {#references}\n\n(Borman, 2004),(Bishop, 2006)\n\nBibliography\nBorman, S., The expectation maximization algorithm-a short tutorial, Submitted for publication, 41(),  (2004).  ↩\n\nBishop, C. M., Pattern recognition and machine learning (2006), : springer. ↩\n",
        "tags": [
            "machine-learning"
        ]
    },
    {
        "uri": "/zettels/emacs_lisp",
        "title": "Emacs Lisp",
        "content": "\ntags\n: Programming Languages\n\nIn preparation for submission to MELPA, one should run package-lint on\nthe code.\n\nGitHub - chrisdone/elisp-guide\n",
        "tags": []
    },
    {
        "uri": "/zettels/emacs",
        "title": "Emacs",
        "content": "\nMuch of Emacs is written in Emacs Lisp.\n\nIconic Blog Posts {#iconic-blog-posts}\n\ntour-de-babel - steveyegge2\n",
        "tags": []
    },
    {
        "uri": "/zettels/emtiyaz_khan",
        "title": "Emtiyaz Khan",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/entropy",
        "title": "Entropy",
        "content": "\ntags\n: Information Theory, Gibbs' Inequality\n\nDefinitions {#definitions}\n\nThe Shannon information content of an outcome \\\\(x\\\\), measured in bits,\nis defined to be:\n\n\\begin{equation}\n  h(x) = \\log\\_2 \\frac{1}{P(x)}\n\\end{equation}\n\nThe entropy of an ensemble \\\\(X\\\\) is defined to be the average Shannon\ninformation content of an outcome:\n\n\\begin{equation}\n  H(X)\\equiv \\sum\\{x \\in \\mathcal{A}\\X} P(x) \\log \\frac{1}{P(x)}\n\\end{equation}\n\nEntropy is 0 when the outcome is deterministic, and maximized with\nvalue \\\\(\\log(|\\mathcal{A}\\_X|)\\\\) when the outcomes are uniformly\ndistributed.\n\nThe joint entropy of two ensembles \\\\(X, Y\\\\) is:\n\n\\begin{equation}\n  H(X,Y) \\equiv \\sum\\{x,y \\in \\mathcal{A}\\x \\mathcal{A}\\_y} P(x,y) \\log \\frac{1}{P(x,y)}\n\\end{equation}\n\nEntropy is additive if the ensembles are independent:\n\n\\begin{equation}\n  H(X,Y) = H(X) + H(Y)\n\\end{equation}\n\nEntropy is decomposable.\n",
        "tags": []
    },
    {
        "uri": "/zettels/erm",
        "title": "Empirical Risk Minimization",
        "content": "\nIn Machine Learning, the training set error is often called the\nempirical error or empirical risk, andn this is the error the\nclassifier incurs over the sample:\n\n\\begin{equation}\nL\\S(h) = \\frac{|\\\\{i \\in [m] : h(x\\i) \\ne y\\_i\\\\}}{m}\n\\end{equation}\n\nGiven a hypothesis class \\\\(H\\\\), finding the hypothesis \\\\(h \\in H\\\\) that\nminimizes the empirical risk is a simple learning strategy.\n",
        "tags": []
    },
    {
        "uri": "/zettels/evolving_connectionist_systems",
        "title": "Evolving Connectionist Systems",
        "content": "\nEvolving Connectionist Systems (ECoS) learn local models from data\nthrough clustering the data and associating a local output function\nfor each cluster represented in a connectionist structure\n(Stefan Schliebs \\& Nikola Kasabov, 2013). These clusters are\ncreated based on similarity between data samples either in the input\nspace, or both in the input space and output space. These functions\nare represented in their connection weights.\n\nECoS traditionally uses the simple sigmoid model of a neuron. Evolving\nSpiking Neural Networks (§evolving\\_snn) architectures use a spiking neuron\nmodel.\n\nBibliography\nSchliebs, S., & Kasabov, N., Evolving Spiking Neural Network-A Survey, Evolving Systems, 4(2), 87–98 (2013).  http://dx.doi.org/10.1007/s12530-013-9074-9 ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/evolving_snn",
        "title": "Evolving Spiking Neural Networks",
        "content": "\ntags\n: §spiking\\neural\\networks\n\nEarly eSNN architectures use the §leaky\\integrate\\and\\_fire model,\nand rank-order encoding. eSNNs can be used for classification. Given\nan input sample, the spike train is propagated through the SNN,\nresulting in the firing of output neurons. If no output neuron is\nactivated, the classification result is undetermined. If one or more\noutput neurons emit a spike, the label of the neuron with the shortest\nresponse time represents the classification result.\n\nTraining an eSNN {#training-an-esnn}\n\nFor each class label an individual repository is evolved. A new output\nneuron is created and fully connected to the previous layer of\nneurons. Input spikes are propagated through the network, and the\nweight vector for the neuron is computed, along with its firing\nthreshold. This weight vector is compared to existing neurons in the\nrepository. If neurons are too similar (e.g. small Euclidean distance\nbetween weight vectors), they are merged.\n\nSee (Stefan Schliebs \\& Nikola Kasabov, 2013) for a\ncomprehensive review.\n\nBibliography\nSchliebs, S., & Kasabov, N., Evolving Spiking Neural Network-A Survey, Evolving Systems, 4(2), 87–98 (2013).  http://dx.doi.org/10.1007/s12530-013-9074-9 ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/expectation_maximization",
        "title": "Expectation Maximization and Mixture Models",
        "content": "\ntags\n: §machine\\learning\\algorithms\n\nIntroduction {#introduction}\n\nIf we define a joint distribution over observed and latent variables,\nthe corresponding distribution of the observed variables alone is\nobtained by marginalization. This allows relatively complex marginal\ndistributions over observed variables to be expressed in terms of more\ntractable joint distributions over the expanded space of observed and\nlatent variables.\n\nMixture distributions, such as the Gaussian mixture, can be\ninterpreted in terms of discrete latent variables. These mixture\nmodels are also useful in clustering data. We first discuss the\nproblem of finding clusters using a non-probabilistic technique called\nK-means clustering. Then we introduce the latent variable view of\nmixture distributions where the discrete latent variables can be\ninterpreted as defining assignments of data points to specific\ncomponents of the mixture.\n\nThe EM algorithm is the general approach to finding maximum likelihood\nestimators in latent variable models. We will see that K-means\nclustering is the non-probabilistic version of the EM algorithm.\n\nK-means Clustering {#k-means-clustering}\n\nWe formulate the K-means clustering problem as follows:\n\nSuppose we have a datset \\\\(\\\\{\\mathbf{x\\1}, \\dots, \\mathbf{x}\\N\\\\}\\\\)\nconsisting of \\\\(N\\\\) observations of a random D-dimensional Euclidean\nvariable \\\\(\\mathbf{x}\\\\). Our goal is to partition the data set into a\nfixed number of clusters \\\\(K\\\\). We formalize this by introducing\n\\\\(\\mathbf{\\mu}\\_k\\\\), that represents the center of the clusters. Our goal\nis then to minimize the objective function:\n\n\\begin{equation}\n  J = \\sum\\{n=1}^{N} \\sum\\{k=1}^{N} r\\{nk} \\lvert \\mathbf{x}\\n -\n  \\mathbf{\\mu}\\_k^2 \\rvert\n\\end{equation}\n\nThe K-means algorithm works as follows:\n\nChoose some initial values of $&mu;$\\_k.\nMinimize \\\\(J\\\\) wrt \\\\(r\\{nk}\\\\), keeping \\\\(\\mu\\k\\\\) fixed.\nMinimize \\\\(J\\\\) wrt \\\\(\\mu\\k\\\\), keeping \\\\(r\\{nk}\\\\) fixed.\nRepeat steps 2 and 3 until convergence.\n\nWe shall see that updating \\\\(r\\{nk}\\\\) and \\\\(\\mu\\k\\\\) respectively\ncorrespond to the E-step and M-step of the EM algorithm.\n\nConsider first the determination of \\\\(r\\_{nk}\\\\). Because \\\\(J\\\\) is a ilnear\nfunction of \\\\(r\\_{nk}\\\\), this optimization can be performed easily to\ngive a closed-form solution:\n\n\\begin{equation}\n  r\\_{nk} = \\begin{cases}\n    1 & \\text{if } k = \\textrm{argmin}\\j \\lvert \\mathbf{x}\\n -\n    \\mathbf{\\mu}\\_j \\rvert ^2 \\\\\\\\\\\\\n    0 & \\text{otherwise.}\n  \\end{cases}\n\\end{equation}\n\nNow, consider the optimization of \\\\(\\mu\\k\\\\) with \\\\(r\\{nk}\\\\) held fixed.\nThe objective function \\\\(J\\\\) is quadratic of \\\\(\\mu\\_k\\\\), and can be\nminimized by setting the derivative wrt to \\\\(\\mu\\_k\\\\) to 0. We solve it\nto be:\n\n\\begin{equation}\n  \\mathbf{\\mu}\\k = \\frac{\\sum\\{n} r\\{nk}\\mathbf{x}\\n}{\\sum\\{n} r\\{nk}}\n\\end{equation}\n\nThis result can be interpreted as setting \\\\(\\mu\\_k\\\\) equal to the mean of\nall the datapoints \\\\(\\mathbf{x}\\_n\\\\) assigned to cluster \\\\(k\\\\).\n\nBecause each step reduces the objective function \\\\(J\\\\), the algorithm is\nguaranteed to converge to some minimum. This minimum may be a local\nminimum, instead of a global one.\n\nOne way of choosing the cluster centers \\\\(\\mu\\_k\\\\) is to choose a random\nsubset of \\\\(K\\\\) data points as the centers. K-means is often used to\ninitialize parameters in a Gaussian mixture model before applying the\nEM algorithm.\n\nThe K-means algorithm described can be generalized to datasets by\nusing a dissimilarity measure \\\\(\\mathcal{V}(x, x')\\\\) (instead of\nEuclidean distance), and minimizing the distortion measure:\n\n\\begin{equation}\n  \\tilde{J} = \\sum\\{n=1}^{N} \\sum\\{k=1}^{K} r\\{nk}\\mathcal{V}(\\mathbf{x}\\n, \\mathbf{x}\\_k)\n\\end{equation}\n\nwhich gives the K-medoids algorithm.\n\nThe computational cost of the E-step is O(KN), while the M-step\ninvolves \\\\(O(N\\_k^2)\\\\) evaluations of \\\\(\\mathcal{V}(\\dot, \\dot)\\\\).\n\nOne feature of the K-means algorithm is that it assigns each datapoint\n\\\\(\\mathbf{x}\\_i\\\\) to uniquely one of the clusters. There may be\ndatapoints that lie roughly halfway between two cluster centers. By\nadopting a probabilistic approach, we can obtain 'soft' assignments\nthat reflect the level of uncertainty over the most appropriate\nassignment.\n\nThe Gaussian Mixture Model {#the-gaussian-mixture-model}\n\nThe Gaussian mixture model allows modelling more complex\ndistributions. We can formulate the Gaussian mixture model in terms of\ndiscrete latent variables. the Gaussian mixture distribution can be\nwritten as a linear superposition of Gaussians of the form:\n\n\\begin{equation}\n  p(\\mathbf{x}) = \\sum\\{k=1}^{K} \\pi\\k \\mathcal{N}(\\mathbf{x} |\n  \\mathbf{\\mu}\\k, \\mathbf{\\Sigma}\\k)\n\\end{equation}\n\nWe can introduce a K-dimensional binary random variable \\\\(\\mathbf{z}\\\\)\na 1-of-K representation such that all elements but one of them are 0s,\nand \\\\(p(z\\k = 1) = \\pi\\k\\\\). We can then define the joint distribution\n\\\\(p(\\mathbf{x}, \\mathbf{z})\\\\) corresponding to the following graphical\nmodel.\n\n{{}}\n\nThe parameters \\\\(\\pi\\_k\\\\) must satisfy:\n\n\\begin{equation}\n  0 \\le \\pi\\k \\le 1, \\sum\\{k=1}^{K}\\pi\\_k = 1\n\\end{equation}\n\nwe can then represent \\\\(p(\\mathbf{z})\\\\) as:\n\n\\begin{equation}\n  p(\\mathbf{z}) = \\prod\\{k=1}^{K}\\pi\\k^{z\\_k}\n\\end{equation}\n\nThe conditional distribution can also be written as:\n\n\\begin{equation}\n  p(\\mathbf{x} | \\mathbf{z}) = \\prod\\_{k=1}^{K} \\mathcal{N}(\\mathbf{x}\n  | \\mathbf{\\mu}\\k, \\mathbf{\\Sigma}\\k)^{z\\_k}\n\\end{equation}\n\nSince the joint distribution is given by\n\\\\(p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z})\\\\), the marginal distribution of\n\\\\(\\mathbf{x}\\\\) is given by summing the joint distribution over all\npossible states of \\\\(\\mathbf{z}\\\\):\n\n\\begin{equation}\np(\\mathbf{x}) = \\sum\\_{z} p(\\mathbf{z}) p(\\mathbf{x} | \\mathbf{z}) =\n\\sum\\{k=1}^{K} pi-k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}\\k, \\mathbf{\\Sigma}\\_k)\n\\end{equation}\n\nWe define another quantity \\\\(\\gamma(z\\k) = p(z\\k = 1 | \\mathbf{x})\\\\), whose\nquantity can be found via Bayes theorem:\n\n\\begin{equation}\n  \\gamma(z\\k) = \\frac{\\pi\\k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}\\_k,\n    \\mathbf{\\Sigma}\\k)}{\\sum\\{j=1}^{K} \\pi\\_j \\mathcal{N}(\\mathbf{x} |\n    \\mathbf{\\mu}\\j, \\mathbf{\\Sigma}\\j)}\n\\end{equation}\n\nWe shall view \\\\(\\pi\\k\\\\) as the prior probability that \\\\(z\\k = 1\\\\), and\n\\\\(\\gamma(z\\k)\\\\) is the responsibility_ that component \\\\(k\\\\) takes for\nexplaining the observation \\\\(\\mathbf{x}\\\\).\n\nSuppose we have a dataset of observations \\\\(\\\\{\\mathbf{x}\\_1, \\dots,\n\\mathbf{x}\\_k\\\\}\\\\), and wish to model this data using a mixture of\nGaussians. We can find the maximum likelihood estimates of the mixture\nmodel parameters. Before proceeding, it is worth noting that the\nmaximum likelihood approach has severe problems. In particular, the\nmaximimizing the log likelihood function is not a well-posed problem,\nand there are singularities that can cause the log-likelihood to\nbecome infinity in the limit where the variance is zero. This occurs\nwhen a Gaussian component collapses onto a single data point. This can\nbe interpreted as overfitting. We can hope to avoid these\nsingularities by using suitable heuristics, using the MAP or Bayesian\napproach.\n\nEM for Gaussian Mixture Model {#em-for-gaussian-mixture-model}\n\nWe can find maximum likelihood solutions for models with latent\nvariables using the EM algorithm. We demonstrate this for the case of\nthe Gaussian mixture model.\n\nFirst, we find the derivatives of \\\\(\\ln p(\\mathbf{X} | \\mathbf{\\pi},\n\\mathbf{\\mu}, \\mathbf{\\Sigma})\\\\) wrt to \\\\(\\mu\\_k\\\\) to 0:\n\n\\begin{equation}\n  0 = \\sum\\{n=1}^{N} \\underbrace{\\frac{\\pi\\k \\mathcal{N}(\\mathbf{x}\\_n |\n    \\mathbf{\\mu}\\k, \\mathbf{\\Sigma}\\k)}{\\sum\\{j} \\pi\\j \\mathcal{N}\n    (\\mathbf{x}\\n| \\mathbf{\\mu}\\j \\mathbf{\\Sigma}\\j)}}\\{\\gamma (z\\_{nk})}\n  \\mathbf{\\Sigma}^{-1}\\k \\left( \\mathbf{x}\\n - \\mathbf{\\mu}\\_k \\right)\n\\end{equation}\n\nMultiplying by \\\\(\\mathbf{\\Sigma}\\_k^{-1}\\\\) (which we assume to be\nnon-singular), we get:\n\n\\begin{equation}\n\\mathbf{\\mu}\\k = \\frac{1}{N\\k} \\sum\\_{n=1}^{N}\n\\gamma(z\\{nk})\\mathbf{x}\\n, \\text{ where } N\\k =\\sum\\{n=1}^{N}\\gamma(z\\_{nk})\n\\end{equation}\n\nWe can interpret \\\\(N\\_k\\\\) as the effective number of points assigned to\ncluster \\\\(k\\\\).\n\nIf we set the derivative of \\\\(\\ln p(\\mathbf{X} | \\mathbf{\\pi},\n\\mathbf{\\mu}, \\mathbf{\\Sigma})\\\\) wrt to \\\\(\\Sigma\\_k\\\\) to 0, and work it\nout, we also get that:\n\n\\begin{equation}\n  \\pi\\k = \\frac{N\\k}{N}\n\\end{equation}\n\nThese results are not closed form solutions of the parameters of the\nmixture model, since they depend on \\\\(\\gamma(z\\_{nk})\\\\). However, the\niterative procedure in the EM algorithm, allows us to choose some\ninitial values and perform E and M-steps to converge to a solution.\n\nIn the expectation step (E-step), we use the current values for the\nparameters to evaluate the posterior probabilities, or\nresponsibilities. We then use these probabilities in the maximization\nstep (M-step), to re-estimate the means, covariances and mixing\ncoefficients.\n\nGeneral EM {#general-em}\n\nThe goal of the EM algorithm is to find maximum likelihood solutions\nfor models having latent variables. We denote the set of all observed\ndata by \\\\(\\mathbf{X}\\\\), in which the nth row represents\n\\\\(\\mathbf{x}\\_n^T\\\\), and similarly we denote the set of all latent\nvariables by \\\\(Z\\\\), with a corresponding row $\\mathbf{z}\\_n^T4. The set\nof all model parameters is denoted by \\\\(\\mathbf{\\theta}\\\\). The\nlog-likelihood function is given by:\n\n\\begin{equation}\n  \\ln p(\\mathbf{X} | \\mathbf{\\theta}) = \\ln \\left\\\\{ \\sum\\_{\\mathbf{z}}\n    p(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta} ) \\right\\\\}\n\\end{equation}\n\nA key observation is that the summation occurs within the\nlogarithm. Even if the joint distribution belongs to the exponential\nfamily, the marginal \\\\(p(\\mathbf{X} | \\mathbf{\\theta})\\\\) generally does\nnot because of this summation. The presence of the sum prevents the\nlogarithm from acting directly on the joint distribution, resulting in\ncomplicated expressions for the maximum likelihood solution.\n\nSince we are in general not given the complete dataset \\\\(\\\\{\\mathbf{X},\n\\mathbf{Z}\\\\}\\\\), but only the incomplete data \\\\(\\mathbf{X}\\\\), our state of\nknowledge of the values of the latent variables is given only by a\nposterior distribution \\\\(p(\\mathbf{Z} | \\mathbf{X}, \\mathbf{\\theta})\\\\).\nInstead, we consider the expected value under the posterior\ndistribution of the latent variable, which corresponds to the E-step\nof the EM algorithm. In the subsequent M-step, we maximize this\nexpectation.\n\nIn the E-step, we use the current parameters \\\\(\\theta^{\\text{old}}\\\\) to\nfind the posterior distribution of the latent variables given by\n\\\\(p(\\mathbf{Z} | \\mathbf{X}, \\mathbf{\\theta}^{\\text{old}})\\\\). We use the\nposterior distribution to fin the expectation of the complete-data log\nlikelihood evaluated for some general parameter \\\\(\\theta\\\\). This\nexpectation, denoted \\\\(Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{\\text{old}}\n)\\\\), is given by:\n\n\\begin{equation}\n  Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{\\text{old}}) = \\sum\\_{\\mathbf{Z}}\n  p(\\mathbf{Z} | \\mathbf{X}, \\mathbf{\\theta}^{\\text{old}})\\ln\n  p(\\mathbf{X} , \\mathbf{Z} | \\mathbf{\\theta})\n\\end{equation}\n\nIn the M-step, we revise the parameter estimate\n\\\\(\\mathbf{\\theta}^{\\text{new}}\\\\) by maximizing the Q function:\n\n\\begin{equation}\n  \\mathbf{\\theta}^{\\text{new}} = \\textrm{argmax}\\_{\\theta}\n  Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{\\text{old}})\n\\end{equation}\n\nIn the definition of \\\\(Q\\\\), the logarithm acts directly on the joint\ndistribution, making the M-step tractable.\n\nIn general, we suppose that the direct optimization of \\\\(p(\\mathbf{X} |\n\\mathbf{\\theta})\\\\) is difficult, and the optimization of \\\\(p(\\mathbf{X},\n\\mathbf{Z} | \\mathbf{\\theta})\\\\) is significantly easier.\n\nWe introduce a distribution \\\\(q(\\mathbf{Z})\\\\) over the latent variables,\nand we observe that for any choice of \\\\(q(\\mathbf{Z})\\\\), the following\ndecomposition holds:\n\n\\begin{equation}\n  \\ln p(\\mathbf{X} | \\mathbf{\\theta}) = \\mathcal{L}(q,\n  \\mathbf{\\theta}) + KL(q || p)\n\\end{equation}\n\nwhere\n\n\\begin{equation}\n  \\mathcal{L} (q, \\mathbf{\\theta}) = \\sum\\_{\\mathbf{Z}} q(\\mathbf{Z})\n  \\ln \\left\\\\{ \\frac{p(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta})}{q(\\mathbf{Z})} \\right\\\\}\n\\end{equation}\n\nand\n\n\\begin{equation}\n  KL(q||p) = - \\sum\\_{\\mathbf{Z}} q(\\mathbf{Z}) \\ln \\left\\\\{\n    \\frac{p(\\mathbf{Z} | \\mathbf{X}, \\mathbf{\\theta})}{q(\\mathbf{Z})} \\right\\\\}\n\\end{equation}\n\n{{}}\n\nThe EM algorithm involves alternatingly computing a lower bound on the\nlog likelihood for the current parameter values, and then maximizing\nthis bound to obtain the new parameter values.\n\nFor complex models, the E-step and M-step can still be intractable.\nThe Generalized EM (GEM) algorithm addresses the problem on the\nintractable M-step. Instead of maximizing \\\\(L(q, \\mathbf{\\theta})\\\\) wrt\n\\\\(\\mathbf{\\theta}\\\\), it seeks to change the parameters such that the\nvalue is increased. Similarly, one can address the intractable E-step\nby seeking to partially optimize \\\\(L(q, \\mathbf{\\theta})\\\\) wrt\n\\\\(q(\\mathbf{Z})\\\\).\n\nReferences {#references}\n\n(Borman, 2004), (Bishop, 2006)\n\nBibliography\nBorman, S., The expectation maximization algorithm-a short tutorial, Submitted for publication, 41(),  (2004).  ↩\n\nBishop, C. M., Pattern recognition and machine learning (2006), : springer. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/experience_replay",
        "title": "Experience Replay",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/exploration_in_rl",
        "title": "Exploration In Reinforcement Learning",
        "content": "\nIn Reinforcement Learning ⭐, exploration is important where rewards\nare sparse, and not a direct indication of how good an action is. Some\nenvironments where good exploration is necessary is Montezuma's\nrevenge, where finishing a game only weakly correlates with rewarding\nevents.\n\nKey Questions:\n\nHow can an agent discover high-reward strategies that require a\n    temporally-extended sequence of complex behaviours that,\n    individually, are not rewarding?\nHow can an agent decide whether to attempt new behaviours or\n    continue to do the best thing it knows so far?\nIs there an optimal exploration strategy?\n\nIn order of theoretical tractability (tractable to intractable):\n\nmulti-armed bandits (1-step stateless RL problems) : can be\n    formailized as POMDP identification\ncontextual bandits (1-step RL problems) : policy learning is\n    trivial even with POMDP\nsmall, finite MDPs (tractable planning, model-based RL setting) :\n    can frame as Bayesian model identification, reason explicitly about\n    value of information\nlarge, infinite MDPs, continuous spaces : optimal methods don't work\n\nGeneral Themes in Exploration {#general-themes-in-exploration}\n\nRequires some form of uncertainty\nAssumes:\n    Unknown is good (optimism)\n    Sample = Truth\n    information gain is good\n\nExploration in Bandits {#exploration-in-bandits}\n\nOptimistic Exploration {#optimistic-exploration}\n\nKeep track of average reward \\\\(\\hat{\\mu}\\_a\\\\) for each action \\\\(a\\\\), and\nchoose $a = \\mathrm{argmax} \\hat{\\mu}\\a + C&sigma;\\a$for some variance\nestimate \\\\(\\sigma\\_a\\\\). This method is model-free.\n\nPosterior/Thompson Sampling {#posterior-thompson-sampling}\n\nHere, we assume $r(a\\i) &sim; p&theta;\\i(r\\_i), defining a POMDP with\n\\\\(s = \\left[\\theta\\1, \\dots, \\theta\\n \\right]\\\\), and we have a belief\nover the states.\n\nThompson sampling does this:\n\nsample \\\\(\\theta\\1, \\dots, \\theta\\n \\sim \\hat{p}(\\theta\\1, \\dots, \\theta\\n)\\\\)\npretend the model \\\\(\\theta\\1, \\dots, \\theta\\n\\\\) is correct\ntake the optimal action\nupdate the model\n\nThompson sampling is hard to analyze theoretically, but can work well\nempirically.\n\nInformation Gain {#information-gain}\n\n\\begin{equation}\n  IG(z, y|a) = E\\_y\\left[ \\mathcal{H}(\\hat{p}(z)) - \\mathcal{H}(\\hat{p}(z)|y)|a \\right]\n\\end{equation}\n\nis how much we learn about \\\\(z\\\\) from action \\\\(a\\\\), given current beliefs\n\nIf we have \\\\(\\Delta(a) = E[r(a^\\star) - r(a)]\\\\), the expected\nsuboptimality of \\\\(a\\\\), and \\\\(g(a) = IG(\\theta\\a, r\\a | a)\\\\), then we can\nchoose \\\\(a\\\\) according to \\\\(\\mathrm{argmin}\\_a \\frac{\\Delta(a)^2}{g(a)}\\\\).\n\nUpper Confidence Bound {#upper-confidence-bound}\n\n\\begin{equation}\n  a = \\mathrm{argmax} \\hat{\\mu}\\_a + \\sqrt{\\frac{2 \\ln T}{N(a)}}\n\\end{equation}\n\nExtending Exploration to RL {#extending-exploration-to-rl}\n\nCount-based exploration (Bellemare et al., 2016) {#count-based-exploration}\n\nUse pseudo-counts:\n\n\\begin{equation}\nr\\i^+ = r\\i + \\mathcal{B}(\\hat{N}(s))\n\\end{equation}\n\nThere are many choices for the bonus.\n\nBibliography\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R., Unifying count-based exploration and intrinsic motivation, In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 29 (pp. 1471–1479) (2016). : Curran Associates, Inc. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/exponential_family",
        "title": "Exponential Family",
        "content": "\ntags\n: §statistics\n\nA one-parameter exponential family model is any model whose density\ncan be expressed as:\n\n\\begin{equation}\n  p(y | \\theta)=h(y) g(\\theta) \\exp \\\\{\\eta(\\theta) t(y)\\\\}\n\\end{equation}\n\nwhere \\\\(\\theta\\\\) is the parameter of the family, and \\\\(t(y)\\\\) is the\nsufficient statistic for \\\\(\\theta\\\\).\n\nWhen a model belongs to the one-parameter exponential family, a family\nof conjugate prior distributions is given by:\n\n\\begin{equation}\n  p(\\theta) \\propto g(\\theta)^{\\nu} \\exp \\\\{\\eta(\\theta) \\tau\\\\}\n\\end{equation}\n\nwhere \\\\(\\nu\\\\) and \\\\(\\tau\\\\) are parameters of the prior, such that\n\\\\(p(\\theta)\\\\) is a well-defined pdf.\n\nCombining this prior with a sampling model \\\\(Y \\sim p(y|\\theta)\\\\) yields\nthe posterior:\n\n\\begin{align} p(\\theta | y) & \\propto p(y | \\theta) p(\\theta) \\\\ & \\propto g(\\theta) \\exp \\\\{\\eta(\\theta) t(y)\\\\} \\cdot g(\\theta)^{\\nu} \\exp \\\\{\\eta(\\theta) \\tau\\\\} \\\\ & \\propto g(\\theta)^{\\nu+1} \\exp \\\\{\\eta(\\theta)[\\tau+t(y)]\\\\} \\end{align}\n\nwhich belongs to the same family as the prior distribution, with\nparameters \\\\(\\nu + 1\\\\) and \\\\(\\tau + t(y)\\\\).\n",
        "tags": []
    },
    {
        "uri": "/zettels/extended_kalman_filter",
        "title": "Extended Kalman Filter",
        "content": "\ntags\n: Bayes Filter, Kalman Filter, Information Filter\n\nKey Idea {#key-idea}\n\nRemove linearity assumption from the Kalman Filter:\n\n\\begin{align}\n  x\\t &= g(u\\t, x\\{t-1}) + \\epsilon\\t \\\\\\\\\\\\\n  z\\t = h(x\\t) + \\gamma\\_t\n\\end{align}\n\nWhere function \\\\(g\\\\) and replaces \\\\(A\\t, B\\t\\\\), and function \\\\(h\\\\) replaces\n\\\\(C\\_t\\\\) respectively.\n\nThe belief remains approximated by a Gaussian, represented by mean\n\\\\(\\mu\\t\\\\) and covariance \\\\(\\Sigma\\t\\\\). This belief is approximate, unlike\nin Kalman filters.\n\nLinearization is key to EKFs. EKFs use first-order Taylor expansion\nfor \\\\(g\\\\) to construct a linear approximation to a function \\\\(g\\\\) from its\nvalue and slope. The slope is given by the partial derivative:\n\n\\begin{equation}\n  g' (u\\t, x\\{t-1}) := \\frac{\\partial g(u\\t, x\\{t-1})}{\\partial x\\_{t-1}}}\n\\end{equation}\n\nBoth \\\\(g\\\\) and the slope depend on the argument of \\\\(g\\\\). We choose the\nmost likely argument: the mean of the posterior \\\\(\\mu\\_{t-1}\\\\), giving:\n\n\\begin{align}\n  g(u\\t, x\\{t-1}) \\approx g(u\\t, \\mu\\{t-1}) + g'(u\\t, \\mu\\{t-1})\n  (x\\{t-1} - \\mu\\{t-1})\n\\end{align}\n\nWhere we can define \\\\(g'(u\\t, \\mu\\{t-1}) := G\\t\\\\). \\\\(G\\t\\\\) is the Jacobian\nmatrix, with dimensions \\\\(n \\times n\\\\), where \\\\(n\\\\) is the dimensions of\nthe state.\n\nSimilarly, \\\\(h\\\\) is linearized as:\n\n\\begin{equation}\n  h(x\\t) \\approx h(\\overline{\\mu}\\t) + H\\t (x\\t - \\overline{\\mu}\\_t)\n\\end{equation}\n\nAlgorithm {#algorithm}\n\n\\begin{algorithm}\n  \\caption{Extended Kalman Filter}\n  \\label{ekf}\n  \\begin{algorithmic}[1]\n    \\Procedure{ExtendedKalmanFilter}{$\\mu\\{t-1}, \\Sigma\\{t-1}, \\mu\\t, \\z\\t$}\n    \\State $\\overline{\\mu}\\t = g(u\\t, \\mu\\_{t-1})$\n    \\State $\\overline{\\Sigma}\\t = G\\t \\Sigma\\{t-1} G\\t^T + R\\_t$\n    \\State ${K}\\t = \\overline{\\Sigma}\\t H\\t^T (H\\t \\overline{\\Sigma}\\t H\\t^T + Q\\_t)^{-1}$\n    \\State $\\mu\\t = \\overline{\\mu}\\t + K\\t(z\\t - h(\\overline{\\mu}\\_t))$\n    \\State $\\Sigma\\t = (I - K\\t H\\t) \\overline{\\Sigma}\\t$\n    \\State \\Return $\\mu\\t, \\Sigma\\t$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\nCons {#cons}\n\nSince the belief is modelled as a multi-variate Gaussian, it is\nincapable of modelling multimodal beliefs. One extension is to\nrepresent posteriors as a mixture of Gaussians. These are called\nmulti-hypothesis Kalman filters.\n\nExtensions {#extensions}\n\nThere are multiple ways for linearization. The unscented Kalman filter\nprobes the function to be linearized at selected points, and\ncalculates a linearized approximation based on the outcomes of the\nprobes. Moments matching linearizes while preserving the true mean and\ntrue covariance of the posterior distribution.\n",
        "tags": []
    },
    {
        "uri": "/zettels/fast_nn_training",
        "title": "Fast Neural Network Training",
        "content": "\ntags\n: §machine\\learning\\algorithms\n",
        "tags": []
    },
    {
        "uri": "/zettels/feedback_alignment_random_error_bp",
        "title": "Feedback Alignment and Random Error Backpropagation",
        "content": "\ntag\n: §machine\\learning\\algorithms, §spiking\\neural\\networks,\n    §neuroscience\n\nBackpropagation is not biologically plausible because it the error\nsignals to update the weights of the hidden layers need to be\npropagated back from the top layer.\n\nFeedback alignment side-steps this problem by replacing the weights in\nthe backpropagation rule with random ones:\n\n\\begin{equation}\n  \\delta\\{i}^{(l)}=\\sigma^{\\prime}\\left(a\\{i}^{(l)}\\right) \\sum\\{k} \\delta\\{k}^{(l+1)} G\\_{k i}^{(l)}\n\\end{equation}\n\nwhere \\\\(G^{(l)}\\\\) is a fixed, random matrix with the same dimensions as\n\\\\(W\\\\). The replacement of \\\\(W^{T,(l)}\\\\) with \\\\(G^{(l)}\\\\) breaks the\ndependency of the backward phase on \\\\(W\\\\), enabling the rule to be more\nlocal. Another variation is to replace the backpropagation of the\nerrors in each layer with a random propagation of errors to each\nlayer:\n\n\\begin{equation}\n  \\delta\\{i}^{(l)}=\\sigma^{\\prime}\\left(a\\{i}^{(l)}\\right) \\sum\\{k} \\delta\\{k}^{(L)} H\\_{k i}^{(l)}\n\\end{equation}\n\nRandom BP applied to SNNs do not account for the temporal dynamics of\nneurons and synapses. SuperSpike solves this problem.\n",
        "tags": []
    },
    {
        "uri": "/zettels/feeds",
        "title": "RSS Feeds",
        "content": "\nFeeds {#feeds}\n\nTech {#tech}\n\nEmacs {#emacs}\n\nAI/ML {#ai-ml}\n\nProgramming Languages {#programming-languages}\n\nGeneral {#general}\n\nCompany Blogs {#company-blogs}\n\nMath {#math}\n\nVocab {#vocab}\n",
        "tags": []
    },
    {
        "uri": "/zettels/finance",
        "title": "Statistical Methods for Finance",
        "content": "\nReturns {#returns}\n\nNet Returns:\n\n\\begin{equation}\nR\\t = \\frac{P\\t}{P\\{t-1}} - 1 = \\frac{P\\t - P\\{t-1}}{P\\{t-1}}\n\\end{equation}\n\n\\\\(R\\_t \\ge -1\\\\): a 100% loss occurs if the asset becomes totally\nworthless.\n\nGross Returns:\n\n\\begin{equation}\n  \\frac{P\\t}{P\\{t-1}} = 1 + R\\_t\n\\end{equation}\n\nThe gross return over the most recent \\\\(k\\\\) periods is:\n\n\\begin{equation}\n  1 + R\\t(k) = \\frac{P\\t}{P\\{t-k}} = (1 + R\\t)\\dots (1 + R\\_{t-k+1})\n\\end{equation}\n\nLog Returns:\n\n\\begin{equation}\n  r\\t = \\log (1 + R\\t) = \\log \\frac{P\\t}{P\\{t-1}} = p\\t - p\\{t-1}\n\\end{equation}\n\n\\begin{equation}\nr\\t(k) = r\\t + r\\{t-1} + \\dots + r\\{t-k+1}\n\\end{equation}\n\nlog returns are similar for daily returns, less similar for yearly\nreturns, and  not necessarily similar for multi-year returns.\n\nRandom Walk Model {#random-walk-model}\n\nIn the random walk model, single-period log returns are assumed to be\nindependent:\n\n\\begin{align} \\label{eq:rw}\n  1 + R\\t(k) &= (1 + R\\t)\\dots(1 + R\\_{t-k+1}) \\\\\\\\\\\\\n             &= \\textrm{exp}(r\\t) \\dots \\textrm{exp}(r\\{t-k+1}) \\\\\\\\\\\\\n             &= \\textrm{exp}(r\\t + \\dots + r\\{t-k+1})\n\\end{align}\n\nIt is also sometimes assumed that log returns are \\\\(N(\\mu,\\sigma^2)\\\\)\nfor some constant mean and variance. Then \\\\(k\\\\) period log returns are\n\\\\(N(k\\mu, k\\sigma^2)\\\\).\n\nGeometric Random Walks {#geometric-random-walks}\n\nFrom eq:rw, we have that\n\n\\begin{equation}\n  \\frac{P\\t}{P\\{t-k}} = 1 + R\\t(k) = \\textrm{exp}(r\\t + \\dots + r\\_{t-k+1})\n\\end{equation}\n\nA process whose logarithm is a random walk is called a geometric\nrandom walk. If \\\\(r\\1\\\\), \\\\(r\\2\\\\), \\\\(\\dots\\\\) are i.i.d \\\\(N(\\mu, \\sigma^2)\\\\),\nthen \\\\(P\\_t\\\\) is lognormal for all \\\\(t\\\\) and the process is a lognormal\ngeometric random walk. \\\\(\\mu\\\\) is called the log-mean, and \\\\(\\sigma^2\\\\)\nthe log-standard deviation of thet lognormal distribution of\n\\\\(exp(r\\_t)\\\\).\n\nValidity of the Random Walk Model {#validity-of-the-random-walk-model}\n\nIn the lognormal geometric random walk model we assume that:\n\nlog returns are normally distributed\nlog returns are mutually independent\n",
        "tags": []
    },
    {
        "uri": "/zettels/fitness",
        "title": "Fitness",
        "content": "\nPosture {#posture}\n\nYour current posture is the result of your body's adaptations to the\ndemands you place on it. Your body builds strength and flexibility\nwhere demaind is placed on it, and loses strength and flexibility when\nno stressors are applied.\n\nThe factors that lead to increased incidence of back pain are\ndecreased muscle endurance and weakness rather than structural\nfactors.\n\nThe key to resolving pain and discomfort is improving strength and\nendurance of the appropriate musculature.\n\nUnderstanding Pain {#understanding-pain}\n\nOne of the first models of pain was the gate ocntrol theory. This\nquickly became outdated, and 2 primary models remain: The neuromatrix\ntheory of pain, and the biopsychosocial model.\n\nThe biopsychosocial model of pain asserts that there are biological,\npsychological and social factors that influence pain within the body.\n\nbiological factors\n: pain from repetitive stress, trauma, damage\n\npsychological factors\n: effect of emotions and thoughts, mood, attention, sleep etc.\n\nsocial factors\n: Social activities, work and occupation resulting in more isolation\n\nThe neuromatrix model looks at six areas, namely:\n\ncognitive issues\n: memories of past experiences\n\nsensory issues\n: nociceptive inputs from cutaneous, visceral and musculature senses\n\nemotional issues\n: limbic system and stress mechanism\n\npain perception\n: sensory, affectieve and cognitive dimensions\n\nactions\n: voluntary and involuntary actions\n\nstress\n: immune system, cortisol, and other stress hormones\n\nPain is a protective mechanism, not necessarily a symptom of damage.\nNot avoiding various activities due to pain, and not changing your\nlifestyle around the pain are critical during a rehabilitation\nprocess.\n\nThe three most common causes of tightness in the back are pain,\ninstability, or weakness. Your muscles may become tight to protect you\nfrom injury. Exercises that are aimed at strengthening and stabilizing\nthe back tend to clear up such instances of back pain and tightness.\n\nPosture in real life {#posture-in-real-life}\n\nPosture is a very complex interplay between the neurological and\nmusculoskeletal systems within the body, which all come together to\nmake up a specific position.\n\nAligning the joints into a better position would allow more muscular\nforce to be used, yielding both improved performance, and less risk of\ninjury.\n\nWe need to think about posture in the context of moving well, and the\nability to adapt effectively to your environment while avoiding\ninjury-prone positioning.\n\nCorrecting Posture {#correcting-posture}\n\nConsistent effort, over time, is one of the cornerstones of physical\nchange, and everything you can do to encourage this is beneficial.\n\nUpper Crossed Syndrome (UCS) and Lower Crossed Syndrome (LCS)\ndescribes patterns of weakness and tightness in the upper and lower\nbody respectively, in the common desk job culture.\n\nUCS: Head-forward posture: weak neck flexors, weak scapular\nretractors, tight upper cervical muscles/suboccipitals, tight chest\npec/pec minor/rounded shoulders\n\nLCS: Anterior pelvic tilt: weak glutes, weak abs, tight hip flexors,\ntight back\n\nThese are symptoms of the underlying cause. Any stretching and\nexercise is like a bandage. It will help cover a wound and protect it,\nbut the real healing comes from your body engaging in the healing\nprocess. Mindfulness and sustained habits that aid this process are\nkey.\n\nCorrective Exercises {#corrective-exercises}\n\nMobility and stability exercises improve your range of motion through\neither dynamic movement or static holds. The strength and endurance\nmovements contribute most to your ability to maintain good postural\npositioning in your daily activities.\n",
        "tags": []
    },
    {
        "uri": "/zettels/formulation",
        "title": "Formulation",
        "content": "\nIn this experiment, the training and test examples are generated with\nthe function \\\\(y=x\\\\) with Gaussian noise added. We fit a linear function\nand a 10th degree polynomial.\n\nFor the 10th degree polynomial, we fit using polynomial regression and\nthen with ridge regression. In scikit learn, ridge regression finds\n\\\\(\\min\\w ||Xw - y||\\2^2 +\\alpha||w||\\_2^2\\\\).\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import meansquarederror\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nSet seed for random number generator to make results reproducible\nnp.random.seed(1)\n\nNumber of data points in train and test sets\ndata_size = 100\ndatainterval = 10.0/datasize\n\nreg = 0.01\nif data_size == 10:\n    reg = 0.2\n\nLinear regression model\nlinear = LinearRegression(fit_intercept=True,normalize=True)\nPolynomial regression model, degree the same as data size\npoly = Pipeline([('poly', PolynomialFeatures(degree=10)),\n                 ('lin', LinearRegression(fit_intercept=True, normalize=True))])\n\nPolynomial regression model with ridge regression, degree the same as data size\nridgepoly = Pipeline([('poly', PolynomialFeatures(degree=10)),\n                      ('ridgereg', Ridge(alpha = reg, fit_intercept=True,normalize=True))])\n\nConstruct training set\nOutput is y = x + noise\nxtrain = np.arange(datainterval/2, 10, datainterval)\ntrainnoise = np.random.normal(0, 1, datasize)\nytrain = xtrain + train_noise\n\nFit the models\nlinear = linear.fit(xtrain[:, np.newaxis], ytrain)\npoly = poly.fit(xtrain[:, np.newaxis], ytrain)\nridgepoly = ridgepoly.fit(xtrain[:, np.newaxis], ytrain)\n\nConstruct test set, interleaved with training set\nxtest = np.arange(datainterval,10 + datainterval/2, data_interval)\ntestnoise = np.random.normal(0, 1, datasize)\n\nDo predictions\nlinear_pred = linear.predict(xtest[:,np.newaxis])\npoly_pred = poly.predict(xtest[:,np.newaxis])\nridgepoly_pred = ridgepoly.predict(xtest[:,np.newaxis])\n\nMeasure mean squared error\nytest = xtest + test_noise\nlinerror = meansquarederror(ytest, linear_pred)\npolyerror = meansquarederror(ytest, poly_pred)\nridgepolyerror = meansquarederror(ytest, ridgepoly_pred)\n\nPlotting\nx_plot = np.linspace(0, 10, 100)\n\nfig = plt.figure(1, figsize=(12, 13.5))\nfig.clf()\n\nsub1 = fig.add_subplot(3,2,1)\nsub1.set_title('Lin Reg Train Set')\nsub1.scatter(xtrain, ytrain,  color='red')\nsub1.plot(xplot, linear.predict(xplot[:,np.newaxis]), color='green',linewidth=3)\n\nsub2 = fig.add_subplot(3,2,2)\nsub2.set_title('Lin Reg Test Set')\nsub2.scatter(xtest, ytest,  color='red')\nsub2.plot(xplot, linear.predict(xplot[:,np.newaxis]), color='green',linewidth=3)\n\nsub3 = fig.add_subplot(3,2,3)\nsub3.set_title('Poly Reg Train Set')\nsub3.scatter(xtrain, ytrain,  color='red')\nsub3.set_ylim([-2,12])\nsub3.plot(xplot, poly.predict(xplot[:,np.newaxis]), color='green',linewidth=3)\n\nsub4 = fig.add_subplot(3,2,4)\nsub4.set_title('Poly Reg Test Set')\nsub4.scatter(xtest, ytest,  color='red')\nsub4.set_ylim([-2,12])\nsub4.plot(xplot, poly.predict(xplot[:,np.newaxis]), color='green',linewidth=3)\n\nsub5 = fig.add_subplot(3,2,5)\nsub5.set_title('Ridge Poly Reg Train Set')\nsub5.scatter(xtrain, ytrain,  color='red')\nsub5.set_ylim([-2,12])\nsub5.plot(xplot, ridgepoly.predict(xplot[:,np.newaxis]), color='green',linewidth=3)\n\nsub6 = fig.add_subplot(3,2,6)\nsub6.set_title('Ridge Poly Reg Test Set')\nsub6.scatter(xtest, ytest,  color='red')\nsub6.plot(xplot, ridgepoly.predict(xplot[:,np.newaxis]), color='green',linewidth=3)\n\nfig.show()\nprint(\"Linear test set error: \" + \"{0:.2f}\".format(linerror))\nprint(\"Poly test set error: \" + \"{0:.2f}\".format(polyerror))\nprint(\"Ridge poly test set error: \" + \"{0:.2f}\".format(ridgepolyerror))\n\n{{}}\n",
        "tags": [
            "machine_learning",
            "ipython"
        ]
    },
    {
        "uri": "/zettels/free_energy_rl",
        "title": "Free-Energy Reinforcement Learning",
        "content": "\ntags\n: Reinforcement Learning ⭐\n\nThis is a framework proposed by Sallans and Hinton in 2004\n(Sallans \\& Hinton, 2004). The key insight is that a product of experts\nallows for model parameters to be learnt efficiently, because values\nand derivatives for the product of experts can be efficiently computed.\n\n{{}}\n\nThe weights of the RBM are tweaked such that the free energy of a\nnetwork configuration equals to the reward signal for the given\nstate-action pair.\n\nAn action is selected by performing Gibbs sampling, holding the state\nvariables fixed. The action with the lowest free energy is produced,\ncorresponding to the highest expected reward for the given state.\n\nSpiking neural networks can be used to implement RBMs, hence used for\nFERL (Takashi Nakano \\& Makoto Otsuka, 2011).\n\nBibliography\nSallans, B., & Hinton, G., Reinforcement learning with factored states and actions., Journal of Machine Learning Research, 5(), 1063–1088 (2004).  ↩\n\nNakano, T., & Otsuka, M., Spiking neural network model of free-energy-based reinforcement learning, BMC Neuroscience, 12(S1), 244 (2011).  http://dx.doi.org/10.1186/1471-2202-12-s1-p244 ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/game_api_design",
        "title": "Game API Design",
        "content": "\ntags\n: §software\\_engineering\n\nCore Tenets from Handmade (Handmade, 2019) {#core-tenets-from-handmade}\n\nMaximize portability {#maximize-portability}\n\nWrite in C99 if possible\nTry to avoid:\n    compiler extensions\nDo:\n    Use the C standard library\n    Undef macros that should not be exposed to the end user\n    Prefix names to avoid collisions\n    Write the interface in C\n\nBe easy to build {#be-easy-to-build}\n\nDon't use a custom build system\nMake build system optional\nAllow people to compile from source\nMinimize dependencies\ndon't allocate memory or handle resources for the user\nbe const correct\nalways ask for the size of buffers\n\nBe easy to integrate {#be-easy-to-integrate}\n\nConsider error codes or result structs that must be handled at\n    runtime\nKeep error code/reason in struct\n\nParsePNGFileResult result = ParsePNGFile(pngfiledata);\nif (result.error) { /* handle error */ }\n\nBibliography\nHandmade,  (2019). How to write better (game) libraries | handmade.network wiki. Retrieved from https://handmade.network/wiki/7138-howtowritebettergame_libraries. Online; accessed 12 December 2019. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/gaussian_filter",
        "title": "Gaussian Filter",
        "content": "\nGaussian Filters is a tractable implementation of the Bayes filter\n(Bayes Filter) for continuous spaces.\n\nKey Idea {#key-idea}\n\nBeliefs are represented by a multi-variate normal distribution.\n\n\\begin{equation}\n  p(x) = \\text{det}(2 \\pi \\Sigma)^{-\\frac{1}{2}} \\text{exp} \\left( -\n    \\frac{1}{2} (x -\\mu)^T \\Sigma^{-1} (x- \\mu) \\right)\n\\end{equation}\n\nThe density of variable \\\\(x\\\\) is characterized by mean \\\\(\\mu\\\\) and\ncovariance \\\\(\\Sigma\\\\).\n\nRamifications {#ramifications}\n\nSince beliefs are represented by a multi-variate normal distribution,\nthis means that beliefs are uni-modal. This is suitable for many\ntracking problems. However, this is a poor match for many global\nestimation problems with multiple hypotheses that should give rise to\ntheir own modes in the posterior.\n\nRepresentations {#representations}\n\nmoments representation\n: The Gaussian is represented by its mean\n    and covariance (first and second moments)\n\ncanonical representation\n:\n\nThese representations have a bijective mapping, and are functionally\nequivalent, but give rise to different algorithms.\n\nUsing the moments representation gives rise to the Kalman Filter.\n",
        "tags": []
    },
    {
        "uri": "/zettels/gaussian_processes",
        "title": "Gaussian Processes",
        "content": "\nIn supervised learning, we are given training data \\\\(\\mathcal{D}\\\\), and\nwe need to learn a function \\\\(f\\\\) that can make predictions for all\npossible input values. To do this, assumptions need to be made,\nbecause there are infinitely many function consistent with the\ntraining data. In general, there are 2 options:\n\nRestricting the class of functions considered\nAssigning a prior probability to every possible function\n\nRestricting the class has several issues. First, if the class is too\nrestrictive, we might not find a function that matches.  If a class is\nnot restrictive enough, we might overfit the training data.\n\nAssigning a prior probability also has problems, because there are an\ninfinite set of possible functions. This is where Gaussian processes\ncome in.\n\nA Gaussian process is a generalization of the Gaussian probability distribution.\nWhereas a probability distribution describes random variables which\nare scalars or vectors, a stochastic process governs the properties of\nfunctions. One can think of a function as an extremely long vector,\nwith each entry specifying the function value \\\\(f(x)\\\\) at that input\n\\\\(x\\\\). If one asks only for properties of the function at a finite\nnumber of points, Gaussian processes yield the same answer ignoring\nthe infinitely many other  points.\n\nTODO Gaussian Process, not quite for dummies - Yuge Shi {#gaussian-process-not-quite-for-dummies-yuge-shi}\n\nReferences {#references}\n\nGaussian Processes for Machine Learning\n",
        "tags": []
    },
    {
        "uri": "/zettels/gcc",
        "title": "GCC",
        "content": "\ntags\n: Compilers, The C Language\n\nBuilding GCC {#building-gcc}\n\nInstead of downloading the tarball, we clone from the Git mirror,\nbecause the tarball didn't contain the full GCC distribution.\n\ngit clone https://github.com/gcc-mirror/gcc\ncd gcc\n./contrib/download_prerequisites\nmkdir build && cd build\n../configure                                           \\\n    --prefix=/usr                                      \\\n    --disable-multilib                                 \\\n    --with-system-zlib                                 \\\n    --enable-languages=c,c++,d,fortran,go,objc,obj-c++\nmake\n\nCompiler Flow {#compiler-flow}\n\nTo scope this discussion, we shall focus on the compilation flow for\nC, mentioning features involving other languages where necessary.\n\nThe GCC compiler can be split into 2 components, the front-end and the\nback-end. Compilation is initiated at gcc-main.c, where a driver is\ninitialized. This driver collects required information about the\noutput it needs to create (for example, the language specifications).\n\nThe driver calls toplevmain, which in turn calls docompile.\ndo_compile initializes the compiler, performing the following things:\n\ncalls process_options, which sets global states based on the flags\nSet up the back-end if requested with backend_init()\nInitialize language-dependent structures, such as the symbol table.\nCalls compile_file() to compile the file.\nCalls finalize() to shut down.\n\nFront-end {#front-end}\n\nThe front-end is responsible for preprocessing, lexing, and parsing\nthe input source program into an intermediate representation. This\nintermediate representation is common across all the languages that\nGCC supports, including C, C++, Go and Fortran.\n\nMultiple intermediate representations are available. For C, the\nfront-end uses the GENERIC tree representation, which is able to\nrepresent entire functions in trees, and is defined in gcc/tree.def\nusing C macros. The GENERIC tree representations hold types,\nattributes, and operations.\n\nEach language is contained in the ./gcc/language folder, e.g. gcc/c.\nIn each language folder, there are 2 important files:\n\nconfig-lang.in\n: a shell script containing important variables\n    concerning the language\n\nMake-lang.in\n: a Makefile for building documentation, and\n    installing the front-end\n\nInvoking the Front-end\n\n    The front-end is invoked once to parse the entire input, via\n    langhooks.parsefile() in the compile_file function in\n    gcc/toplev.c:450.\n\n    For C, compile_file does various things:\n\n    Initializes the call graph (cgraph)\n    Parses the file using the parser defined in gcc/c/c-parser.c.\n        This parser is a recursive-descent parser, with the callback\n            finish_function() being called after each recursive-descent\n            function completes\n\n    finish_function() is declared in gcc/c/c-decl.c, which does multiple\n    things:\n\n    Converts C code to the GENERIC tree representation by calling\n        c_genericize\n    Calls cgraphnode::finalizefunction(fndecl, false) at the end (if\n        nested, creates a call-graph node otherwise)\n\ncgraphunit\n\n    This cgraph\\node finalizefunction is defined in gcc/cgraphunit.c,\n    which acts as the interface between tree-based front-ends like GENERIC\n    and the backend.\n\n    As mentioned earlier, finalize_function is called when the front-end\n    is done parsing the body. It queues nodes for processing in the\n    enqueued_nodes linked-list, for processing later.\n\n    In the compile_file function described earlier, after\n    langhooks.parsefile(), symtab->finalizecompilationunit() is\n    called. This calls cgraphnode::analyzefunctions, which loops\n    through the enqueue_nodes and:\n\n    Lowers representation into GIMPLE (gimplify)\n    build callgraph edges and references for all trivially needed\n        symbols and all symbols referred by them.\n    lowers thunks\n    calls compile(), which runs IPA passes (interprocedural\n        optimization). IPA uses information in the call graph to perform\n        transformations across function boundaries. IPA passes include\n        computation of reachability, and inlining functions.\n        The GIMPLE representation is further lowered into SSA form, and\n            optimization techniques are done there, including:\n            Dead code elimination\n            Building the control flow graph\n            Alias analysis\n            Copy Renaming\n        calls expandallfunctions() which further lowers to RTL form by\n            calling initfunctionstart (decl). The RTL form generated is\n            target-dependent.\n\n    All passes (optimization or otherwise) are managed by a pass\n    manager to ensure they are executed in the correct order. The passes\n    are defined in gcc/passes.def. Depending on the optimization level,\n    different passes are run.\n\n    RTL generation is done in gcc/emit-rtl.c. Some RTL optimization passes\n    are run over the RTL form, including:\n\n    common subexpression elimination\n    global subexpression elimination\n    web construction\n    LRA (local register allocation): virtual registers are converted\n        into physical registers, with spilling where necessary\n    basic-block reordering\n    peephole optimizations\n\n    The files for backends are located in directories under gcc/config,\n    e.g. gcc/config/aarch64.\n\n    The final pass converts RTL code into assembly code for output. The\n    source files are final.c plus insn-output.c. Finally, code for the\n    target host is output.\n\nThe C Parser {#the-c-parser}\n\nThe C parser is currently a handwritten recursive-descent parser. The\nreasons for handwriting the parser include:\n\nSimplicity\n: Recursive-descent parsers are easy to read and debug\n\nPerformance\n: Handwriting the parser enables for handwritten\n    optimization\n\nError Recovery\n: We can handwrite rules for common syntatic errors\n    and recover from them.\n\nThe C parser used to be a generated parser via Bison, but extending\nthe parser was difficult. Historically, Objective-C and OpenMP support\nwas difficult to achieve with a generated parser.\n\nIn addition, the parser for C is relatively simple, in comparison to\nother portions in GCC, such as optimization, so it is reasonable to\nhandwrite the parser to ensure that the parse trees obtained are\ndeterministic and easy to debug.\n\nThe Intermediate Code Formats {#the-intermediate-code-formats}\n\nWe list the intermediate code formats in descending order of level.\n\nGENERIC\n: The purpose of GENERIC is to represent functions in a\n    tree representation that is language-independent. The\n    transition point is c_genericize in gcc/c-decl.c\n\nGIMPLE\n: GIMPLE is derived from GENERIC, by converting it into a\n    three-address representation. The three-address\n    representation allows for several higher-level\n    optimization passes. The transition point is\n    gimplifyfunctiontree in cgraphunit.c:669. Some\n    optimization passes include:\n    vectorization\n    empty loops\n    loop parallelization\n\nRTL\n: The Register Transfer Language is lowest level IR, where\n    instructions are output one-by-one. RTL is closest to the\n    machine language, and more optimizations can be done at this\n    level. This also includes machine-specific optimizations, as\n    different machines have different instruction sets. The\n    entry-point to RTL generation happens in the CFG expansion\n    pass, defined in gcc/cfgexpand.c. The source files for RTL\n    generation include stmt.c, calls.c, expr.c, explow.c,\n    expmed.c, function.c, optabs.c and emit-rtl.c. Some\n    optimization passes include:\n    loop optimization\n    (global) common subexpression elimination\n    Instruction scheduling\n    Register allocation\n\nGCC's RTL representation {#gcc-s-rtl-representation}\n\nRTL is inspired by Lisp lists. It has both an internal form, made up\nof structures that point at other structures, and a textual form that\nis used in the machine description and in printed debugging dumps. The\ntextual form uses nested parentheses to indicate the pointers in the\ninternal form.\n\nConsider the code for simple.c:\n\n#include\n\nint main() {\n  int a = 0;\n  return 0;\n}\n\nWe compile with GCC dumping the RTL code:\n\ngcc -fdump-rtl-all-all /home/jethro/Dropbox/NUS/CS4212/assignments/simple.c\n\nWe get the list of RTLs at different RTL passes:\n\n+-- a.out\n+-- simple.c.229r.expand\n+-- simple.c.230r.vregs\n+-- simple.c.231r.into_cfglayout\n+-- simple.c.232r.jump\n+-- simple.c.244r.reginfo\n+-- simple.c.264r.outof_cfglayout\n+-- simple.c.265r.split1\n+-- simple.c.267r.dfinit\n+-- simple.c.268r.mode_sw\n+-- simple.c.269r.asmcons\n+-- simple.c.273r.ira\n+-- simple.c.274r.reload\n+-- simple.c.278r.split2\n+-- simple.c.282r.proandepilogue\n+-- simple.c.285r.jump2\n+-- simple.c.298r.stack\n+-- simple.c.299r.alignments\n+-- simple.c.301r.mach\n+-- simple.c.302r.barriers\n+-- simple.c.306r.shorten\n+-- simple.c.307r.nothrow\n+-- simple.c.308r.dwarf2\n+-- simple.c.309r.final\n\\-- simple.c.310r.dfinish\n\nEach instruction has the form (type id prev next n (statement)). We\nlook at a instruction generated from the program:\n\n(insn 5 2 6 2 (set (mem/c:SI (plus:DI (reg/f:DI 82 virtual-stack-vars)\n                                      (const_int -4 [0xfffffffffffffffc])) [1 aD.2249+0 S4 A32])\n                   (const_int 0 [0])) \"/home/jethro/Dropbox/NUS/CS4212/assignments/simple.c\":4 -1\n                   (nil))\n\n(mem/c:SI (plus:DI (reg/f:DI 82 virtual-stack-vars)\n                   (const_int -4 [0xfffffffffffffffc])) [1 aD.2249+0 S4 A32])\n\nObtains a from an offset from the virtual stack, and loads it into\nmemory. set is the assign operation in int a = 0. (const_int 0 [0])\nrepresents 0.\n\nIn (insn 5 2 6 2 ...), 5 is the current instruction, the first 2 is the previous\ninstruction, 6 is the next instruction and the final 2 is the basic\nblock ID.\n\nPeephole Optimizations {#peephole-optimizations}\n\nPeephole optimizations in GCC are defined in markdown files in\ndifferent target machines. For example, we look at the\ngcc/config/arm/arm.md. These contain Lisp expressions of the form:\n\n(define_peephole2\n  [insn-p1\n  insn-p2\n  ...]\n  \"condition\"\n  [new-insn-p1\n  new-insn-p2\n  ...]\n  \"preparation statements\")\n\nThis follows some form of pattern matching. Common matching functions\nare found in\n. For\nexample, match_operand constrains the operands allowed for that\ninstruction. and captures it into group 1.\n\nmatch_dup assumes that operand number n has already been determined by\na match\\_operand appearing earlier in the recognition template, and it\nmatches only an identical-looking expression.\n\nAll of the peephole examples below are machine-dependent:\nspecifically, the instruction set of the machine is an important\nfactor.\n\nExample 1: gcc/config/arm/arm.md:L9208 {#example-1-gcc-config-arm-arm-dot-md-l9208}\n\n(define_peephole2\n  [(set (reg:CC CC_REGNUM)\n        (compare:CC (matchoperand:SI 1 \"registeroperand\" \"\")\n                    (const_int 0)))\n  (condexec (ne (reg:CC CCREGNUM) (const_int 0))\n             (set (matchoperand:SI 0 \"registeroperand\" \"\") (const_int 0)))\n  (condexec (eq (reg:CC CCREGNUM) (const_int 0))\n             (set (matchdup 0) (constint 1)))\n  (match_scratch:SI 2 \"r\")]\n  \"TARGET32BIT && peep2regnodeadp (3, CC_REGNUM)\"\n  [(parallel\n    [(set (reg:CC CC_REGNUM)\n          (compare:CC (constint 0) (matchdup 1)))\n    (set (matchdup 2) (minus:SI (constint 0) (match_dup 1)))])\n  (set (match_dup 0)\n       (plus:SI (plus:SI (matchdup 1) (matchdup 2))\n                (geu:SI (reg:CC CCREGNUM) (constint 0))))]\n  )\n\nHere we look for instructions of the form: `Rd = (eq (reg1)\n(const_int0))`. We substitute it for ARM instructions of the form:\n\nnegs Rd, reg1\nadc  Rd, Rd, reg1\n\nwhich is shorter and more efficient. We do it where the target machine\nis 32-bits.\n\nExample 2: gcc/config/i386/i386.md:L12671 {#example-2-gcc-config-i386-i386-dot-md-l12671}\n\n(define_peephole2\n  [(set (matchoperand:W 0 \"registeroperand\")\n        (matchoperand:W 1 \"memoryoperand\"))\n  (set (pc) (match_dup 0))]\n  \"!TARGET_X32\n   && !TARGETINDIRECTBRANCH_REGISTER\n   && peep2regdead_p (2, operands[0])\"\n  [(set (pc) (match_dup 1))])\n\nCombines the simple jump instruction into a single instruction.\n\nExample 3: gcc/config/aarch64/aarch64.md:L1852 {#example-3-gcc-config-aarch64-aarch64-dot-md-l1852}\n\n(define_peephole2\n  [(match_scratch:GPI 3 \"r\")\n  (set (matchoperand:GPI 0 \"registeroperand\")\n       (plus:GPI\n        (matchoperand:GPI 1 \"registeroperand\")\n        (matchoperand:GPI 2 \"aarch64pluslongstrictimmedate\")))]\n  \"aarch64moveimm (INTVAL (operands[2]), mode)\"\n  [(set (matchdup 3) (matchdup 2))\n  (set (matchdup 0) (plus:GPI (matchdup 1) (match_dup 3)))]\n  )\n\nIf there's a free register, and a constant can be loaded in with a\nsingle instruction, we set it directly.\n",
        "tags": []
    },
    {
        "uri": "/zettels/generalization_in_rl",
        "title": "Generalization In Reinforcement Learning",
        "content": "\ntags\n: Reinforcement Learning ⭐\n\nGeneralization using successor features (Dayan 1993).\n\nAdapt to new reward structure (Barreto 2018)\n\nHow many tasks are needed before modern approaches generalize?\n    (Cobbe 2019)\n\nGeneralization with selective noise injection and information\nbottleneck\n\nInsights:\n\nSelective noise injection for gradient update but not behaviour\n    (rollout) policy speeds learning\nRegularization with Information bottleneck is particularly\n    effective\n\n\\begin{equation}\n  \\nabla\\{\\theta} J\\left(\\pi\\{\\theta}\\right)=\\widehat{\\mathbb{E}}\\{\\pi\\{\\theta}^{r}\\left(a\\{t} | x\\{t}\\right)}\\left[\\sum\\{t}^{T} \\frac{\\pi\\{\\theta}\\left(a\\{t} | x\\{t}\\right)}{\\pi\\{\\theta}^{r}\\left(a\\{t} | x\\{t}\\right)} \\nabla\\{\\theta} \\log \\pi\\{\\theta}\\left(a\\{t} | x\\{t}\\right) \\hat{A}\\{t}\\right]\n\\end{equation}\n\nBenchmarks {#benchmarks}\n\nMulti-Room (Chevalier 2018)\n    No room is seen twice\nCoinRun (Cobbe 2019)\nopenai/procgen\n",
        "tags": []
    },
    {
        "uri": "/zettels/generalized_value_functions",
        "title": "Generalized Value Functions",
        "content": "\n(TODO)\n\nSuccessor representation is the discounted occupancy of a state.\nThe value function for any reward can be computed in one shot if the\n    successor representation is given.\nIt is an example of a vectorized representation of a generalized\n    value function\n",
        "tags": []
    },
    {
        "uri": "/zettels/ger1000",
        "title": "Quantitative Reasoning",
        "content": "\nChapter 1: Design {#chapter-1-design}\n\nWhat is a study? {#what-is-a-study}\n\nA study seeks to establish whether there is an association between a\ndependent and independent variable.\n\nStatisticians use the method of comparison to find the effect of\ntreatment/exposure on a disease/response.\n\n| exposure      | response      |\n|---------------|---------------|\n| vaccine       | polio         |\n| heart disease | polio         |\n| smoking       | heart disease |\n| obesity       | depression    |\n\nCompare the responses of a treatment group to a control group.\n\nConfounders {#confounders}\n\nIf the control group is similar to the treatment group, apart from\n    the treatment, the differences in response are likely to be due to\n    the effect of the exposure\nIf not, then other effects could be \"confounded\" with the results of\n    the treatment. These are called confounders.\nConfounders must be associated with both the exposure and the response\nMinimized through randomized-control.\n\nRandomized-controlled experiments {#randomized-controlled-experiments}\n\nObjective: ensure similarity between treatment and control group\n\nPut subjects into treatment and control at random\nIf possible, give control placebo:\n    neutral, but resembles treatment.\n    Response should be treatment itself and not idea of it\nDouble-blind:\n    Subjects and evaluators do not know whether subject is in\n        treatment or control group.\n    Prevents bias in analysis\n\nObservational Studies {#observational-studies}\n\nIn controlled experiments, investigators decide who will be in the\ntreatment group and who will be in the control group.\n\nIn observational studies, subjects assign themselves to the different\ngroups. To see if confounding is a problem, look at how the exposed\nand non-exposed groups are selected.\n\nOne way to control for confounders is to make comparisons for smaller\nand more homogeneous grups (eg. by age, sex). This is called\n\"slicing\" (not an official term).\n\nObservational studies can establish association. But association\ndoes not imply causation.\n\nVariable Types {#variable-types}\n\nDiscrete {#discrete}\n\nSmoking is an example of a discrete variable (a.k.a. categorical\nvariable).\n\nEg. Smoking has two categories (binary categorical): you smoke or you\ndon't.\n\nContinuous {#continuous}\n\na.k.a. numerical, measurement\n\n2x2 contingency table {#2x2-contingency-table}\n\n|       | Y | not Y |\n|-------|---|-------|\n| A     | a | c     |\n| not A | b | d     |\n\nAssociation {#association}\n\nA and Y are associated if\n\n(1) rate(A|Y) != rate(A|!Y) OR\n(2) rate(Y|A) != rate(Y|!A)\n\nConsistency rule states than (1) iff (2), and vice-versa.\n\na/(a+c) != b/(b+d)\na/(a+b) != c/(c+d)\n\nAdherence {#adherence}\n\nAssignment may be random, but adherence is not\nClues in to success of blinding (eg. drug has negative side effects)\n\nSimpsons Paradox {#simpsons-paradox}\n\nRelationships between percentages in subgroups can be reversed if\nsubgroups are combined.\n\nDesign {#design}\n\nExperimental\n    Controlled\n        Randomized\n    Not controlled\nObservational\n\nRandomized and controlled studies minimize confounding.\n\nTheorem {#theorem}\n\nSuppose units are randomly assigned to be exposed or not. If the\nsample size is very large, then the likelihood that a given variable C\nis not associated to exposure x tends to almost certainty.\n\nRisk Ratio {#risk-ratio}\n\n|       | A | not A | row Total |\n|-------|---|-------|-----------|\n| B     | x | y     | x + y     |\n| not B | a | b     | a + b     |\n\nrisk (A | B) = x / (x+y)\nrisk (A | !B) = a / (a+b)\n\nRR = risk(A|B) / risk(A|!B)\nRR = 1 means no association\n\nRR > 1 => first group has higher risk\nPopulation risk cannot be estimated in case-control studies, even\n    with random samples.\n\nOdds Ratio {#odds-ratio}\n\n|       | A | not A |\n|-------|---|-------|\n| B     | x | y     |\n| not B | a | b     |\n\nodds(A|B) = x/y\nodds(A|!B)= a/b\n\nOR = bx/ay\nodds = risk/(1-risk)\n\nPopulation vs Estimated RR {#population-vs-estimated-rr}\n\npopulation sample size too large, calculation done based on samples.\n\n| Study        | Samples from | Advantage                    |\n|--------------|--------------|------------------------------|\n| Cohort       | Exposure     | Risk and RR can be estimated |\n| Case-control | Response     | Good for rare diseases       |\n\nChapter 2: Association {#chapter-2-association}\n\nDeterministic Relationship\n    Value of variable can be determined if we know the value of the\n        other variable\nStatistical Relationship\n    Natural variability exists in measurements\n    Average pattern of one variable can be described given the value\n        of the other variable\n\nCategorical Variables {#categorical-variables}\n\nData that consists of group or category names. Measurements can be\ngrouped too.\n\nMeasurements of Association: RR and OR {#measurements-of-association-rr-and-or}\n\nRR and OR can be accurately estimated to a cohort study\nRR is intuitively clearer and can only be estimated from cohort\n    studies\nOR applies to both cohort and case-control studies\n\nMeasurement Variables {#measurement-variables}\n\nBivariate data and Scatter Diagram {#bivariate-data-and-scatter-diagram}\n\nExploring relationship {#exploring-relationship}\n\nAverage: eg. son's average height is taller than dad\nassociation: positive gradient?\nlinear or exponential relationship?\nStandard deviation: spread or variability of data\n\nCorrelation Coefficient {#correlation-coefficient}\n\nSummarizes direction and strength of linear association: -1  0 positive association\nr  Due to range restriction in one variable, the correlation coefficient\n> obtained tends to understate the strength of association between two\n> variables.\n\nRange restriction: bivariate data set formed based on criteria on one\nvariable data for the other variable is only available for a limited\nrange.\n\nRange restriction tends to have diminishing influence on the strength\nof the association, called the attenuation effect.\n\nRegression fallacy {#regression-fallacy}\n\n> In virtual test-retest situations, the bottom group on the first test\n> will on average show some improvement on the second test, and the top\n> group will, on average, fall back.\n\nPrediction with linear regression {#prediction-with-linear-regression}\n\nY = a + bX\n\nslope and intercept determined using least-square-method. Predicting\n\"average\", not exact. Also dangerous to predict beyond observed range.\n\nChapter 3: Sampling {#chapter-3-sampling}\n\nDefinitions {#definitions}\n\nUnit: Object/Individual\nPopulation: Collection of units\nSample: Subset of a population\nSampling frame: list of sampling units intended to identify all\n    units in the population\n    Good Coverage\n    Up-to-date and complete\n\nSampling methods {#sampling-methods}\n\nProbability Sampling\n    Every unit must have a known probability of being sampled\n    Simple random sampling: all units have equal probability\nSystematic sampling\n    Selecting units from a list through the application of a\n        selection interval K, so every Kth unit following a random start\n        is included in the sample\n    treated as simple random when sampling units are arranged randomly\n    might obtain undesirable sample if sampling units and K have\n        cyclical effect\n    can use when # sampling units unknown\nStratified\n    first divide population of units into strata, take a probability\n        sample from each group\nMulti-stage\n\nDifficulties in Sampling {#difficulties-in-sampling}\n\nImperfect sample frame\n    Perfect sampling frame consists of all units in population\n    otherwise, might include unwanted units (increased cost of\n        study), or exclude desired units (need to redefine target population).\nNon-response\n    not all units are contactable, willing to take part.\n        Non-respondents typically differ from respondents, and this\n        effect needs to be studied.\nVolunteer sample (biased)\nConvenience sample (biased)\nJudgement sample (uses own discretion, biased)\nQuota sample (Having proportions of categories dose not make\n    extension of results to population better)\n\nChapter 4: Probability {#chapter-4-probability}\n\nInterpretations {#interpretations}\n\n| Relative Frequency                        | Personal Probability                            |\n|-------------------------------------------|-------------------------------------------------|\n| Will you win the lottery                  | Will you be working overseas once you graduate? |\n| Can be quantified exactly                 | Cannot be quantified exactly                    |\n| Based on repeated observation of outcomes | Based on personal belief                        |\n\nOdds of having disease = P(disease) / P(no disease)\n\nAverage value = expected value\n\nP-values {#p-values}\n\np-value = the probability of obtaining an outcome _equivalent to or\n    more extreme_ than the observed\nnull hypothesis: assumption used to calculate p-value (eg. coin is fair)\nif p-value is small, unlikely for observed to occur by chance, and\n    unlikely for null hypothesis to be true. Converse for large.\np-value > 0.05 : do not reject NH at 5% significance level. Cannot\n    conclude that it is not fair. Observed effect in sample is likely\n    to reflect effect in population.\n\nTesting rare events (Medical screening) {#testing-rare-events--medical-screening}\n\nBase rate: P(disease)\nSensitivity: P(positive | disease)\nSpecificity: P(negative | no disease)\n\n| To test                                       | Not to test                    |\n|-----------------------------------------------|--------------------------------|\n| no alternative test                           | Alternative more reliable test |\n| Test is inexpensive & more expensive 2nd test | Test is expensive              |\n| Goo chance of successful treatment            | Unreliable treatment           |\n\nChapter 5: Networks {#chapter-5-networks}\n\nCollection of objects and well-defined relations between objects\n\nDefinitions {#definitions}\n\nDegree: number of other vertices in the network a node is adjacent to\nOrder: number of vertices\nSize: number of edges\nDistance d(X,Y) = distance between X and Y\n\nCentrality {#centrality}\n\nn vertices\n\n| Centrality | Formula                      |\n|------------|------------------------------|\n| Closeness  | Ccen(u) = sum[d(u,vi)/ n-1 ] |\n| Degree     | Dcen(u) = deg(u) / n-1       |\n\nBetweeness: For a vertex Z in any graph, how many shortest paths are\nthere, between any pair of 2 vertices, passing through Z?\n\nIf 2 shortest paths between a,b, only 1 pass through z, add 1/2.\n\nAppendix: Answering Questions {#appendix-answering-questions}\n\nexposure (potential cause)\nresponse (potential effect)\ndesign\nsampling\nunit\n",
        "tags": []
    },
    {
        "uri": "/zettels/ges1028",
        "title": "Singapore Society",
        "content": "\nIntroduction to Sociology {#introduction-to-sociology}\n\nSociological Perspective {#sociological-perspective}\n\nDefined as seeing the general in the particular\n\nChild birth: Education, use of contraception\nSuicide: Strength of social ties vs individualistic societies\n\nGlobal Perspective {#global-perspective}\n\nStudy of the larger world and our society's place in it. Logical extension of sociological perspective.\n\nHigh-income countries: 64 nations, including USA.\n    These generate the majority of goods and services, people here own the most wealth.\nMiddle-income countries: 73 nations\n    People as likely to live in rural villages as developed countries. Considerable social inequality.\nLow-income countries: 57 nations\n    Low standards of living. Majority struggle to get by: hunger, unsafe water etc.\n\nWhy we care:\n\nWhere we live shapes the lives we lead\nSocieties globally become increasingly interconnected\n\nBenefits of the sociological perspective {#benefits-of-the-sociological-perspective}\n\nAssesses the truth of common sense\nHelps us see opportunities and constraints in our lives\nEmpowers us to be active participants in our society\nHelps us live in a diverse world\n\nOrigins of sociology {#origins-of-sociology}\n\nChanges in Europe during 18th nadn 19th century:\n\nFactory-based economy\n\nWorkers becoming part of an anonymous labour force, weakening traditions that had guided community life.\n\nGrowth of cities\n\n\"Enclosure movement\": fencing off of areas for diferent purposes. Social problems such as air pollution.\n\nPolitical changes\n\nNew ways of thinking: Hobbes, Adam Smith and John Locke.\nPhilosophers now spoke of things like personal liberty and individual rights.\n\nAugust Comte coined the term sociology in 1838, and saw it as a product of 3 phases:\n\nTheological\n\nReligious view that society expressed God's will\n\nMetaphysical (Renaissance: 15th Century)\n\nSociety was a natural rather than supernatural phenomenon\n\nScientific\n\nRise of science, headed by Newton, Einstein and other scientists.\nScientific approach used to analyze society.\n\nPositivism: Use of \"positive\" facts as opposed to mere speculation\n\nSociological Theory {#sociological-theory}\n\nTranslating observations into understanding\nTheory: Statement of how and why specific facts are related\nThree theoretical approaches\n\nStructural-Functional Approach\n\n    Framework for building the theory that sees society as a complex system whose parts work together to promote solidarity and stability.\n    Social structure: any stable societal pattern.\n    Analyze these structures and their consequences for the operation of society as a whole.\n    Robert K Merton distinguished between:\n\n    Manifest functions: recognized and intended consequences of social patterns\n    Latent functions: unrecognized and unintended consequences\n\n    Social dysfunction: social patterns that disrupt the operation of society\n\nSocial-conflict approach\n\n    Framework for building the theory that sees society as an arena of inequality that generates conflict and change.\n    Used to reason about social/wealth/racial inequality etc.\n\nGender-conflict approach\n\n    Feminism, gender inequaltiy\n\nRace-conflict approach\n\n    POV that focuses on inequality and conflict between people of different racial and ethnical categories\n\nSymbolic-interaction approach\n\n    Framework for building the theory that sees society as a product of the everyday interactions of individuals\n    We live in a world of symbols, and attach meaning to everything\n\n    Summary in last two pages of reading week 1\n\nHistory in Singapore {#history-in-singapore}\n\nPAP hostile towards history in 1960s and 1970s.\n‘Before you discuss your future,’ Lee Kuan Yew exhorted the citizens of\n\nSingapore in 1998, ‘remember how we got here.’\n\nnostalgia for the 1970s was an inherent criticism of the fast pace of change and goals of the state.\nPAP's rule under threat: but capitalized it, harnessing history for nationalist aims\nHistory is a trope of knowledge, an established way of thinking about society and as such is critical to understanding a society.\nThe failure of Singaporean society to theorize, or come to terms with, its own past, constitutes opaqueness in the study of Singapore.\n\nSingaporean Histography {#singaporean-histography}\n\nFirst written by colonizers\n    Sir Thomas Stanford Raffles(1781 - 1826) autobiography: A memoir of the Life of and Public Service of Sir Thomas Stanford Raffles. Published 1824.\n    Expansion of historical work happened closer to national independence (around 1960s)\n    Similar to HK, where waning of colonialism results in the exploration of their history.\n\nElite Representations {#elite-representations}\n\nLarge part of Singapore's history told in terms of the life of LKY.\nTexts are attempts to build a \"national mythology\", conflates national history with personal.\nBiography and autobiography provide an incomplete picture of a national history.\nPolitical biographies and autobiographies of LKY reinforce the national narrative.\nInclusion in national education curriculum\nThe function of Lee’s autobiographies is broader than the writing of a national narrative, because the autobiography actively participates in the construction of a national imaginary.\n'Even from my sick bed, even if you are going to lower me into the grave and I feel that something is going wrong, I will get up.' - LKY\nLee is very much a product of colonialism, educated in a British tradition. He is often described as ‘a Chinese mirror of the perfect Anglo leader’.\nS Rajaratnam and Devan Nair \"midwives of the Singaporean nation-state\". Set the template for writing of history in the future.\nNair\n    Looking to the past for inspiration was both dangerous and backward.\n    Industrialization/modernization divided people into those who look to the past and those who look to the future.\nSourcing for historical material is hard: bias, difficulty of dating pre-colonial material.\nCensorship by Singapore Government\nLow profile makes it hard to attract academics to work in historiography\nThe way in which a nation constructs its history can be integral to the construction of the nation itself. 78 The relationship between history and the nation in Singapore is polemical because of the state’s changing attitude towards the past.\nRajaratnam’s 1970s statement that ‘knowing where you are going is more important than knowing where you came from,’ 79 and Lee Kuan Yew’s aforementioned proclamation of 1998, ‘Before you discuss your future, remember how we got here’\nEducation policies concerned with mother-tongue language learning and the inclusion of Confucian values in the moral education curriculum are part of this desire to cultivate Singapore as an inherently ‘Asian’ nation.\nThreat and struggle are the two dominant themes the PAP promotes in its understanding of Singapore’s history, and events and issues are manipulated to fit with this model\n\nNostalgia {#nostalgia}\n\nRather than censoring such responses, the Singaporean state responded to the rise in nostalgia by trying to co-opt it for nationalist purposes. In\ntransforming nostalgia from something that could potentially undermine the policies and rhetoric of development, to a positive part of a broader and\nmultilayered nation building project, the state is acting in a typically adaptive mode. In mainstreaming nostalgia, the state effectively moved nostalgia away from the 1970s and broadened its meaning.\n ‘Friends and Family: A Singapore Album collection’ is both a virtual interactive web-based exhibition and a traditional museum exhibition. Encouraged to contribute to the collective Singaporean Identity\n\nSummary {#summary}\n\nWith numerous strategies the Singaporean state attempts to control the meaning of history in Singapore. The past is presented by methods of display, content and absences, as uncontested and unproblematic. The historiography mirrors this. The history that the state constructs and manipulates ratifies the construction of cultural knowledge in Singapore, even when it is authored by. The more active role Singaporeans are playing as creators of historical knowledge has returned Singapore to a more traditional relationship with history. That is, the state is utilizing history as a form of nation building and as a way of negotiating a multiracial society. Instead of history presenting a threat to the fragile balance of a multiracial society, it is now a tool for bringing people together. In the shared experience of life in Singapore as well as the shared experiences of lives—births, marriages, celebrations, etc.—history has become a part of the national story. The state has been effective in widening the focus of nostalgia beyond the dangerous period of the 1970s, but as with other actions of adaptive regimes, as more is given to citizens the greater their potential demands become, and the greater the need for adaptation. In attempting to focus attention towards sites of nostalgia that are less problematic, especially the physical manifestation of buildings, the Singaporean state is still seeking to control the meaning of the past. An emphasis on heritage and the built environment freezes a historical moment and strips it of context. That is, the state is making heritage an object of the present and not the past. Likewise, ‘A Singapore Album’ and blog sites turn nostalgia into something that is contemporary not historical. In so doing the Singaporean state is simultaneously negotiating the production of historical knowledge and seeking to de-politicise history. If, as L. P. Hartley suggested ‘the past is a foreign country,’ 121 then making the past the present makes it less foreign.\n\nWhat is Singapore society? {#what-is-singapore-society}\n\nDefining Characteristics {#defining-characteristics}\n\nPragmatic {#pragmatic}\n\nNational Education (NE) and Social Studies textbooks\nEmphasis of several core values and recurring themes\n\nWhat do you know of Singapore's history and manner which it is presented? {#what-do-you-know-of-singapore-s-history-and-manner-which-it-is-presented}\n\nNational Society of Singapore (NSS) {#national-society-of-singapore--nss}\n\nConcept of Resistance {#concept-of-resistance}\n\nHo (2002) Internet as the guidance\n\nGestural Politics {#gestural-politics}\n\nGoh Chok Tong: political scientists started talking about GP\nJust trying to project a liberal image\nmaintain power relations (keep the people happy), prevent regression\nto oppressive society\n\nLee (2008) : How the state uses rhetoric (\"openness\", \"inclusiveness\")\nas liberal gestures\nThey are gestures because words lack substance\n\"Civic\" vs \"civil\": Duties vs rights\n\nBackground {#background}\n\nNGO, non-profit\nMission: promote nature awareness and appreciation\n1986: MNS:S discovered area around Sungei Buloh\nMNS-S prepared its first conservation proposal\nState planned for an agro-technology park in the area\n1989: Sungei Buloh Nature park was founded\n\nSungei Buloh isolated success\n\nGovernment planned to build a golf course at Lower Pierce Reservoir,\nwhich was ultimately void\n\n25,000 strong petition could not change decision to build housing area\n\n2011, Bukit Brown cemetery. 8-lane highway to be constructed\ndiagonally across the Bukit Brown cemetery\n\nEconomic Pragmatism vs Conservation\n\nGestural Politics {#gestural-politics}\n\n(Rodan 2006)\n\nPromises of openness does not amount to democratic developments\nDefine the parameters of politics and political participation in\n    Singapore under LHL's charge.\n    mediate the meanings of political transformation in Singapore via\n        creative institutional and gestural initiatives.\nGoh Chok Tong promised a kinder and gentler style of rule when he\n    took office in 1990.\n\"liberal gestures\" continue to reproduce and mediate an illiberal regime.\nGoh's rule as prime minister (from 1990 to 2004) arguably became\n    best known for the institution and entrenchment of the infamous\n    out-of-bound markers\n    Golfing terminology intended to demarcate the parameters of\n        political debate and dissent in Singapore\n    OB markers remain the most cited reason for political apathy among\n        its citizens and the corresponding lack of public discourse on\n        civil society and political issues in SG.\nPM Lee's decleration of greater openness under his premiership could\n    be seen as the continuation of PAP's \"regime reproduction\"\n    initiatives that are intended to steer \"change in Singapore down a\n    preferred path of political co-option rather than political contestation\".\nSpeech peppered with \"openness\" nothing more than a public relations\n    statement to project Singapore as a mature, progressive and creative\n    society to the rest of the world.\ndisplay of readiness to engage with \"diverse views\"\nLHL's rule since August 2004 typified by use of language and\n    buzzwords that \"seem long on rhetoric  but short on content\"\n\"openness\" and \"inclusiveness\" are terms that can be invoked\n    repetitively and as liberally as desired.\n\nExamples {#examples}\n\n9 March 2004, Singapore Tourism Board (STB) launched \"Uniquely\n    Singapore\"\n    Comprising a range of media advertisements for different global markets\n    Developed in the wake of SG's recovery from its economically\n        crippling encounter with the Severe Acute Respiratory Syndrome (SARS).\n    Coincided with the circulation of broad rhetoric that speaks of\n        a \"more open\" and \"creative\" Singapore.\n    Vibrant place where locals and foreign talents can \"live, work\n        and play\"\n    Time Magazine, 1999, Singapore as \"competitive, creative, even\n        funky\"; such inscriptions mark a form of radicalness that is\n        intended to displace old mindsets about Singapore's colorless\n        cultural landscape.\nFashionable rhetoric of \"creativity\", popularized by the Singapore\n    government's decleration in 2002 that it would embrace the global\n    \"creative industries\" project.\n    overarching intention of creativity and innovation is to boost\n        Singapore's economic capital by attracting talented individuals\n    Productive energies of such \"bohemian-creative\" individuals\n        would \"rub off\" on Singaporean workers.\nPublicized changes have included:\n    a declared willingness to appoint openly gay public servants to\n        sensitive positions in civil service\n    Legalization of \"bar-top\" dancing in pubs and nightclubs\n    Granting of permits for extreme sports such as skydiving\n2003, government has been busy liberalizing the city's nocturnal\n    entertainment scene by allowing 24-hour \"party zones\" in night\n    spots, along with a host of established hotels and clubs\nTolerance to difference, diversity and \"acceptance\" of alternative lifestyles\n    LHL - panoramic vision of Singapore included \"an expanded space\"\n        for Singaporeans to \"live, laugh, grow and be themselves\".\n    Dimension and make-up of newly liberated space (like the\n        invisible boundaries of the OB-markers), are likely to remain\n        cryptic and ambiguous\n    Policy changes affect minority of Singaporeans, but give PAP\n        enough substance to push their \"rhetoric\"\n\"Great Casino Debate\"\n    Under GCT's rule, proposals for a casino in Singapore have been\n        rejected since the 1970s\n    Pushing for casino not straightforward, necessary to engage the\n        citizenry by gathering feedback and guaging opinions\n    13 march 2004, Singapore's Trade and Industry Minister George\n        Yeo delcared Singapore's new state of openness when he said in\n        Parliament that the government would \"keep an open mind\" on\n        whether to build a casino in Singapore.\n    Religious groups dismayed at proposal, voiced strong objections\n    Group of citizens calling themselves \"Families Against the\n        Casino Threat in Singapore\" (FACTS)\n    FACTS collected 20000 signatures through an Internet petition,\n        to be submitted to President of Singapore.\n    Most believed debate amounted to nothing but talkfest\n    By Dec 2004, MTI received more than 700 letters, emails and\n        faxes on the issue\n\nRace {#race}\n\nDemographics {#demographics}\n\nChinese 74.1%, Malays 13.4%, Indians 9.2%, Others (3.3%) - Singapore Census 2010\nCMIO classification\n\nRace as Colonial Legacy {#race-as-colonial-legacy}\n\nSingapore is a product of Western colonial capitalism\n\nThe idea of meritocracy is never enforceable in practice beacuse\nsocial advantage is not equally distributed\n\nCultural Logic of SG's Multiracialism {#cultural-logic-of-sg-s-multiracialism}\n\nHeightened racial consciousness\nStereotypical thinking to reinforce racial differences\nTop-down\n\nLecture Summary {#lecture-summary}\n\nLecture 1 Topic: Introduction - Studying Singapore Society {#lecture-1-topic-introduction-studying-singapore-society}\n\nKnowledge production and dissemination {#knowledge-production-and-dissemination}\n\nGiven that we are social actors born in a specific socio-political and\nhistorical context, we tap upon a range of sources of knowledge when\nwe attempt to explain social issues or phenomena (e.g. low fertility\nrates; academic underperformance; etc.) These can be organised broadly\nas lay perspectives; and disciplined perspectives. However, there are\napproaches which are regarded more dominant than others. We considered\nhow and why a particular perspective may emerge as dominant in a\nparticular context and the key stakeholders interested in sustaining\nsuch dominance.\n\nDisciplined perspectives and the sociological imagination {#disciplined-perspectives-and-the-sociological-imagination}\n\nTo understand Singapore society beyond lay perspectives and the\n“commonsense”, the lecture introduced the framework of the\n“sociological imagination” (C.W. Mills). This broadly means we are\nable to go beyond the individual and connect “private” troubles to\n“public” issues (By extension, it is a quality of mind in making the\nconnection between individual and society; biography and history; and\nself and the world.\n\nThe global perspective {#the-global-perspective}\n\nThe global condition is an extension of the sociological imagination,\nwhich is important in view of the connectedness our lives to the world\nand the global structures that govern our lives.\n\nBlaming the victim {#blaming-the-victim}\n\nTherefore, when we extend our analysis beyond the individual and look\ntoward more holistic and long-term systemic conditions to explain\nspecific social phenomena, we move away from a limiting approach which\nsolely “blames the victim” (Ryan).\n\nLecture 2 Topic: Histories and the Past {#lecture-2-topic-histories-and-the-past}\n\nRepresentations and politics of “the past” {#representations-and-politics-of-the-past}\n\nOur lecture stressed that we are not overly preoccupied whether the\npast (both pre-colonial and colonial) is historically “accurate” or\notherwise, but rather how particular narratives of the “past” have\nbeen represented as dominant. Therefore, the lecture also considered\nwho produces history (the “authors”) and its relationship to power,\nideology and the state. In other words, these narratives are produced\nto reinforce specific interests of groups who control power and\nresources.\n\nHistorical consciousness {#historical-consciousness}\n\nThe engagement of “historical consciousness” directs our attention to\nfocus on the multiplicity of interpretations of “the past” to push the\nboundaries of historical knowledge. This means moving beyond history\nas a tool for propaganda to appreciate historical complexities and\ndiversity/pluralism.\n\nLecture 3 Topic: State and Civil Society {#lecture-3-topic-state-and-civil-society}\n\nCivil society and citizen participation {#civil-society-and-citizen-participation}\n\nOur lecture attempted to critically analyse the extent to which active\ncitizenship engagement on a range of different issues (e.g. the\nenvironment, women’s issues, LGBT and human rights, migrant workers,\netc.) is rendered possible in Singapore. We differentiated between\n“civic” and “civil” society, and closely interrogated the role of the\nstate in establishing legal, social and cultural boundaries pertaining\nto citizen participation.\n\nGestural politics and “resistance” {#gestural-politics-and-resistance}\n\nWe also closely discussed the relevance of Lee’s concept of “gestural\npolitics” in which he refers as essentially “pseudo-politics”\nprimarily aimed at sustaining the appearance of a liberal democracy\nwhile simultaneously extending the power of the authoritarian state\nthrough legal and extra-legal mechanisms, despite calls to\n“inclusiveness” and active citizenship.\n\nGlobalisation and the internet: The final part of the lecture\nemphasised the role of the internet and transnational alliances with\ncivil society organisations beyond Singapore in an attempt to not only\nincrease its reach to a wider audience, but more crucially to\nstrengthen the authority and legitimacy of the different claims local\ncivil society organisations were making.\n\nLecture 5 Topic: Race and Ethnic Relations {#lecture-5-topic-race-and-ethnic-relations}\n\nRace and ethnicity as socially constructed categories {#race-and-ethnicity-as-socially-constructed-categories}\n\nOur lectures made the distinction between race (perceived\nphysical/genetic attributes considered socially significant to a\ncollective) and ethnicity (perceived cultural attributes and practices\nrendered salient to a group). The lecture further demonstrated how\nrace and ethnicity are not fixed or immutable categories, but rather\nhow the meanings of such categories are very much dependent on the\ncontext and temporal dimensions (In the case of Singapore, the lecture\nshowed the role of colonialism and how this shaped our understanding\nof ‘race’ in Singapore). At the same time, the lecture addressed how\nthese markers were not inherently ‘natural’ especially given the fact\nthat ‘race’ as a category has been disputed by scientists. Rather,\nthese markers only carried weight and significance when specific\ngroups ascribe these physical and/or cultural markers as important.\n\nPrejudice and discrimination {#prejudice-and-discrimination}\n\nAt the same time, we also made an important distinction between\nprejudice (rigid and unfair cognitive attitudes and emotions about a\ncategory of people) and discrimination (unfair and unequal treatment,\nbehaviour, action and practice enacted against a category of people).\n\nPrivilege and access to resources {#privilege-and-access-to-resources}\n\nThe lecture emphasised the importance of group membership and how\nthese are linked to the allocation of rights, privileges, obligations\nas well as sanctions and disadvantages. These very much pertain to how\nrace and ethnicity are employed as organising principles to govern\nsocial relations and the distribution of resources.\n\nThe politics of “difference” {#the-politics-of-difference}\n\nOur lecture then focused on how the state in Singapore makes sense of\ndifference, how such differences include “race” and the broader\nimplications of such differences. We attempted to explain how “race”\nhas been defined by the Singapore state, why “race” has been so\nprominent and salient in Singapore, and how these have configured\ndifferent dimensions of our social life.\n\nMulticulturalism and difference {#multiculturalism-and-difference}\n\nWe interrogated how the management of such differences has been\ntranslated into multiculturalist state policies and programmes that\nincluded housing, political representation and education. The lecture\nexplained the different dimensions of multiculturalism which included\nthe accordance of “equality” to each community and other principles.\nAt a more critical level and through different areas of social and\npolitical life in Singapore, we appraised the implications, problems\nand contradictions embedded in the ideology of multiculturalism,\nparticularly in relation to resource allocation, life chances and\nexperiences of everyday racisms.\n\n“Racial harmony” as repressive {#racial-harmony-as-repressive}\n\nThe lectures finally appraised how multiculturalism provides the state\nthe legitimacy to regulate and police race and ethnicity. We further\ninterrogated how the discourse of “racial harmony” has been\npoliticised to legitimise and reproduce state power and intervention,\nas well as the allocation and distribution of specific rights,\nprivileges and resources.\n\nLecture 6 Topic: Gender and Sexuality {#lecture-6-topic-gender-and-sexuality}\n\nUnpacking key concepts {#unpacking-key-concepts}\n\nIn this lecture, we made the distinction between “sex” (biological and\nphysical distinctions between male and female) and “gender” (socially\nand culturally produced differences between men and women). We also\nexplained three other important concepts central to our lecture:\nsexuality; patriarchy; and heteronormativity. Membership in these\ngroup categories of sex/gender and sexuality are pertinent, given that\nthese are tied to privileges, rights and resource allocation. These\ntherefore revisit the primary topic of this section which addresses\nsocial inequalities.\n\nState patriarchy and policy-making {#state-patriarchy-and-policy-making}\n\nThe second part of the lectures addressed the issue of gender\ninequalities and analysed the social, economic, and political\nconditions in Singapore which facilitated the ‘naturalisation’ of\ndominant gender norms, values, and practices privileging male\nexperiences. These included our critical appraisal of population and\nfamily policies, and political representation.\n\nSexuality and heteronormative interests {#sexuality-and-heteronormative-interests}\n\nThe final part of the lecture critically discussed the inequalities\npremised on differences of sexuality and the social implications of\ninvoking heteronormative discourses pertaining to “conservatism”,\nwhich again legitimises a particular configuration of the patriarchal\nand heteronormative familial form as well as acceptable and\npermissible gender and sexual behaviour, which at a broader level, is\nin line with the state’s economic and productivist interests. The\nconcept of sexual citizenship was explained to bring to the forefront\nagain questions of rights and privileges of sexual minorities; as well\nas issues pertaining to belonging and emotional attachment to the\nnation-state.\n\nLecture 7 Topic: Class and Meritocracy {#lecture-7-topic-class-and-meritocracy}\n\nSocial stratification and meritocracy {#social-stratification-and-meritocracy}\n\nIn this lecture, we attempted to explain meritocracy as a system of\nstratification and rewards grounded on the basis of merit and\nnon-discrimination (ethnicity, gender, sexuality, family, etc.). We\ndiscussed the key characteristics of meritocracy and how such a system\nwhich has been institutionalised and normalised in Singapore has\nshaped different dimensions in Singapore society. These include the\nfields of education and politics. We further appraised the challenges\nand contradictions in the ideology of meritocracy and unpacked the\nimpetus of the ruling elite to sustain such an ideology, especially in\nrelation to the justification of the given distribution of resources,\nrights and rewards. These also obscure how life chances and success\nare intimately shaped and connected to cultural capital, social\nconnections, and other considerations. The lectures show further how\nthe potentially egalitarian characteristics of meritocracy may clash\nwith its emphasis on talent allocation, competition and reward,\nthereby transforming this into an ideology of elitism and inequality.\n\nClass inequalities and the politics of welfare {#class-inequalities-and-the-politics-of-welfare}\n\nThe lectures addressed the implications of widening income disparities\nand class inequalities. These were connected to provisions have been\norganised along racial self-help groups and away from state welfare,\nand the consequences and problems of such a system that has been\ninstitutionalised in Singapore. These have been rationalised to steer\nattention away from an overdependence on the state for help and\nobscuring inequalities which have been produced through widening\nincome gap and other state policies.\n\n“Blaming the victim” {#blaming-the-victim}\n\nWe also demonstrated how the ideology of meritocracy takes attention\naway from structural conditions and the role of the state in\nallocating resources, placing the onus of accountability to the\nindividual instead. At the same time, the repetition of meritocracy\ndenounces the presence of discrimination, arguing that success and\nfailure are contingent on the basis of merit.\n\nLecture 8 Topic: Religion and Secularisation {#lecture-8-topic-religion-and-secularisation}\n\nUnpacking key concepts {#unpacking-key-concepts}\n\nIn this lecture, we attempted to conceptualise “religion” by using\nfunctional definitions (what religion does) and substantive\ndefinitions (what religion is); as well as introduce the concept of\n“living religions” in Singapore.\n\nLiving religions and mixing-and-matching {#living-religions-and-mixing-and-matching}\n\nThe second part of the lectures elaborated on the concept of “living\nreligions” and outlined the religious landscape in Singapore. The\nfocus of living religions stresses on the level of religious practice\nand everyday religiosity. We employed the case study of Hinduism to\nfurther show how living religions may not necessarily fit within\nstrict frames of religious behaviour. We used the concept of\n“mixing-and-matching” (process of selecting and enacting different\nstyles of religiosity preferred by a practitioner without these\ntraditions merging and becoming a unitary whole).\n\nSecularisation as a process {#secularisation-as-a-process}\n\nIn our lecture, we first discussed the characteristics and processes\ninvolved in secularisation and to critically assess whether the\nseparation between religion and politics was possible, especially\nduring occasions where religious beliefs and practice contravene\nnational interests and security. In this section we focused on the\nrole of the state, state intervention and religion in Singapore; and\nthe implications of possible competing interests between religious\nadherents/practitioners and the state.\n\nLecture 10 Topic: Population and Health {#lecture-10-topic-population-and-health}\n\nPopulation and the “demographic crisis” {#population-and-the-demographic-crisis}\n\nThe lecture first considered the broader global and local conditions\nthat organised health care provision and financing in Singapore. These\nincluded the dominant patterns in population affecting Singapore –\nlarge ageing population, low fertility rates, high old-age dependency\nratio, increased morbidity and life expectancy, rise of chronic\nillnesses, and wider income disparities and inequalities, contributing\nto escalating health care costs.\n\nHealth System in Singapore {#health-system-in-singapore}\n\nWe subsequently examined the different principles structuring the\nhealth care system and policies in Singapore (both in terms of health\ncare financing and coverage) and critically appraised the shortcomings\nof health care in Singapore. These can be categorised as ‘spectacular’\nand ‘systemic’ failures, based on the prescribed reading.\n\nVulnerable groups and the politicisation of health care {#vulnerable-groups-and-the-politicisation-of-health-care}\n\nAt the same time, we identified several groups which may be more\nvulnerable than others such as HIV patients – as well as the problems\nthese groups potentially faced. The provision of health care\ninfrastructure to its citizens is also intimately tied to broader\nstate interests and legitimacy to rule the nation.\n\nLecture 11 Topic: Popular Culture and the Arts {#lecture-11-topic-popular-culture-and-the-arts}\n\nThe power of the state and the arts {#the-power-of-the-state-and-the-arts}\n\nThe lecture primarily outlined the relationship between the state,\npopular culture and the arts, as well as the contesting visions and\ninterests of the state pertaining to the arts (instrumental, pragmatic\nand political gains), as compared to the interests of arts\npractitioners.\n\nCultural hegemony {#cultural-hegemony}\n\nWe introduced and applied the concept of “cultural hegemony” and its\ndifferent dimensions to demonstrate how the ruling elites have been\nable to naturalise a set of dominant ideas, beliefs and practices as\n“universal” and “normal”. These ideologies can be potentially conveyed\nthrough popular culture and the arts.\n\nIdeology and resistance {#ideology-and-resistance}\n\nDifferent forms of popular culture (films, TV, music, plays, etc.) can\nbe readily appropriated by the state to normalise state ideologies\nsuch as multiculturalism and meritocracy, but at the same time such\nforms can also be deployed by producers of popular culture and the\narts to critically respond to issues and problems in Singapore\nsociety.\n\nLecture 12 Topic: Migration and Globalisation {#lecture-12-topic-migration-and-globalisation}\n\nMigrant workers in Singapore: {#migrant-workers-in-singapore}\n\nIn our lecture, we screened the film “Ilo Ilo” which allows us to\nrecalibrate our understanding of migrant workers in our everyday\nlives. At the same time, the film also afforded us glimpses to\nextrapolate relevant concepts we have employed in our module\npertaining to the meanings of difference and its concomitant\nprivileges, rights and access to resources; as well as concepts such\nas prejudice, discrimination, power and inequality embedded through\nrace/ethnicity, class and gender as organising principles.\n\nState regulation of migrant workers {#state-regulation-of-migrant-workers}\n\nIn the next and final lecture (Week 13), it will briefly contextualise\nand elaborate the position of migrant workers as reflected in the\nfilm, how categories of migrants are established (foreign “worker” vs.\nforeign “talent”) and the implications of these distinctions in a\n“global city” like Singapore.\n",
        "tags": []
    },
    {
        "uri": "/zettels/getting_things_done",
        "title": "Getting Things Done (GTD)",
        "content": "\nGTD is a work-life management system created by David Allen. This\nproductivity is summarized nicely in this article.\n\nOrg-Mode has first-class support for this technique. There are several\npeople who have documented their GTD workflow using Org-Mode. Here is\na list:\n\nOrg Mode - Organize Your Life In Plain Text! ⭐\nOrg-mode Workflow: A Preview - Jethro Kuan ⭐\n",
        "tags": []
    },
    {
        "uri": "/zettels/gibbs_inequality",
        "title": "Gibbs' Inequality",
        "content": "\nThe relative entropy or §kl\\_divergence between two probability\ndistributions \\\\(P(x)\\\\) and \\\\(Q(x)\\\\) defined over the same alphabet\n\\\\(\\mathcal{A}\\_X\\\\) is:\n\n\\begin{equation}\n  D\\{\\textrm{KL}}(P||Q) = \\sum\\{x} P(x) \\log \\frac{P(x)}{Q(x)}\n\\end{equation}\n\nGibb's Inequality states that:\n\n\\begin{equation}\n  D\\_{\\textrm{KL}}(P||Q) \\ge 0\n\\end{equation}\n\nfor any \\\\(P\\\\) and \\\\(Q\\\\).\n",
        "tags": []
    },
    {
        "uri": "/zettels/gibbs_sampling",
        "title": "Gibbs Sampling",
        "content": "\nGibbs sampling is a special case of the Metropolis-Hastings method,\nwhere a sequence of proposal distributions \\\\(q\\\\) is defined in terms of\nthe conditional distributions of the joint distribution\n\\\\(p(\\mathbf{x})\\\\), and proposals are always accepted.\n\nIn the general case of a system with \\\\(K\\\\) variables, a single iteration\ninvolves sampling one parameter at a time:\n\n\\begin{equation}\n\\begin{array}{l}{x\\{1}^{(t+1)} \\sim P\\left(x\\{1} | x\\{2}^{(t)}, x\\{3}^{(t)}, \\ldots, x\\{K}^{(t)}\\right)} \\\\ {x\\{2}^{(t+1)} \\sim P\\left(x\\{2} | x\\{1}^{(t+1)}, x\\{3}^{(t)}, \\ldots, x\\{K}^{(t)}\\right)} \\\\ {x\\{3}^{(t+1)} \\sim P\\left(x\\{3} | x\\{1}^{(t+1)}, x\\{2}^{(t+1)}, \\ldots, x\\_{K}^{(t)}\\right), \\text { etc. }}\\end{array}\n\\end{equation}\n\nPros and Cons {#pros-and-cons}\n\nSuffers the same defects as Metropolis-Hastings methods\nNo adjustable parameters, so it's easy to start with\n",
        "tags": []
    },
    {
        "uri": "/zettels/git",
        "title": "Git",
        "content": "\ntags\n: Unix, Version Control\n\nLinks {#links}\n\nTODO Understanding Git Conceptually {#understanding-git-conceptually}\n\nTODO GitHub - pluralsight/git-internals-pdf: PDF on Git Internals {#github-pluralsight-git-internals-pdf-pdf-on-git-internals}\n\nGit Internals {#git-internals}\n\nContent of .git folder {#content-of-dot-git-folder}\n\nGit stores snapshots (exact content of the files) at the point of a commit.\n\nls .git\n\nThe content is stored in the objects sub-directory. There are 4 kinds\nof objects:\n\nblob\n: used for storing the contents of a single file\n\ntree\n: contains references to other blobs or subtrees\n\ncommit\n: contains reference to another tree object and some other\n    information (author, committer etc.)\n\ntag\n: Another reference to a commit object\n\nGit Branching Models {#git-branching-models}\n\nA successful Git branching model » nvie.com\n",
        "tags": []
    },
    {
        "uri": "/zettels/google_cloud_platform",
        "title": "Google Cloud Platform",
        "content": "\nCloud ML {#cloud-ml}\n\nExactly one replica is designated the master. This task manages the\n    others and reports status for the job as a whole. The training\n    service runs until your job succeeds or encounters an unrecoverable\n    error. In the distributed case, it is the status of the master\n    replica that signals the overall job status.\nIf you are running a single-process job, the sole replica is the\n    master for the job.\nOne or more replicas may be designated as workers. These replicas do\n    their portion of the work as you designate in your job\n    configuration.\nOne or more replicas may be designated as parameter servers. These\n    replicas coordinate shared model state between the workers.\n\nFor more on the distributed training flow, see\n",
        "tags": []
    },
    {
        "uri": "/zettels/gpipe",
        "title": "Gpipe",
        "content": "\nGpipe is a scalable pipeline parallelism library published by Google\nBrain, which allows for efficient training of large, memory-consuming\nmodels (Huang et al., 2018). Pipeline parallelism allows for\n§fast\\nn\\training.\n\nIn Gpipe, neural networks with sequential layers are partitioned\nacross accelerators. The pipeline parallelism divides each input\nmini-batch into smaller micro-batches, enabling different accelerators\nto work on different micro-batches simultaneously. This is especially\nuseful in §large\\batch\\training.\n\n{{}}\n\nBibliography\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., Lee, H., …, Gpipe: efficient training of giant neural networks using pipeline parallelism, CoRR, (),  (2018).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/grid_mc_localization",
        "title": "Grid & Monte Carlo Localization",
        "content": "\nGrid & Monte Carlo Localization methods are able to solve global\nlocalization problems (in comparison to §ekf\\_localization and\n§markov\\_localization).\n\nThey also:\n\nprocess raw sensor measurements\nare non-parametric: not bound to uni-modal distributions\n\nGrid Localization {#grid-localization}\n\nThe grid localization algorithm uses a histogram filter to represent\nthe posterior belief. Coarseness of the grid is an accuracy,\ncomputational-complexity tradeoff. A grid too coarse might prevent the\nfilters from working altogether.\n\n\\begin{algorithm}\n  \\caption{Grid Localization}\n  \\label{grid\\_localization}\n  \\begin{algorithmic}[1]\n    \\Procedure{Grid Localization}{$\\\\{p\\{k, t-1}\\\\}, u\\t, z\\_t, m$}\n    \\ForAll{$k$}\n    \\State $\\overline{p}\\{k,t} = \\sum\\i p\\_{i, t-1}\n    \\mathbf{\\mathrm{motion model}}(\\mathrm{mean}(x\\k), u\\t, \\mathrm{mean}(x\\_i))$\n    \\State $p\\{k,t} = \\eta \\textbf{measurement model}(z\\t,\n    \\mathrm{mean}(x\\_k), m)$\n    \\EndFor\n    \\State \\Return $p\\_{k,t}$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\nMonte-Carlo Localization {#monte-carlo-localization}\n\nMC localization uses the particle filter (§particle\\_filter) to\nrepresent the posterior belief. The accuracy is determined by the size\nof the particle set.\n\n{{}}\n\nThis algorithm is unable to recover when the pose is incorrect, hence\nis, without modification, unsuitable for the kidnapped robot problem.\nThis problem is particularly important when the number of particles is\nsmall, and when particles are spread over a large volume (with global\nlocalization).\n\nThis problem can be easily solved by injecting random particles into\nthe particle set. This can be seen as having a small probability of\nthe robot being kidnapped. One can add a fixed number of random\nparticles per iteration, or use an estimate correlated with the\nlocalization accuracy, which can be estimated from data.\n\nAnother limitation is the proposal mechanism. The particle filter uses\nthe motion model as a proposal distribution, but it seeks to\napproximate a product of this distribution and the perceptual\nlikelihood. The larger the difference between the proposal and target\ndistribution, the more samples required.\n\nIn MCL, this induces a failure mode. A perfect, noiseless sensor would\nalways inform the robot of its correct pose, but MCL would fail. A simple\ntrick that works is to artificially inflate the amount of noise in the\nsensor. An alternative is to modify the sampling process, by reversing\nthe role of the measurement and motion model for a small number of\nparticles. This results in an algorithm called the mixture MCL.\n",
        "tags": []
    },
    {
        "uri": "/zettels/hacking",
        "title": "Hacking",
        "content": "\nSearch engines for Hackers {#search-engines-for-hackers}\n\n(trimstray, 2019)\n\nBibliography\ntrimstray,  (2019). Trimstray on twitter: &quot;search engines for hackers:&#10;&#10;https://t.co/awr3x88xu1&#10;https://t.co/03trswurnp&#10;https://t.co/b9ihx23mec&#10;https://t.co/uo1ofjb7eb&#10;https://t.co/ne7fsoqspl&#10;https://t.co/s2wg7coga5&#10;https://t.co/ubqtz7quud&#10;https://t.co/izx4b82wlq&#10;https://t.co/oa04gvdxtp&#10;https://t.co/tkjuuvu9il&#10;&#10;#it #tech&quot;. Retrieved from https://twitter.com/trimstray/status/1086705742793658369. Online; accessed 09 February 2019. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/hadoop",
        "title": "Hadoop",
        "content": "\ntags\n: §data\\_science\n\nHadoop provides tools for working with big data, by raising the _level\nof abstraction_.\n\nMost of this content comes from Hadoop: The Definitive Guide (White, 2009)\n\nData Storage and Analysis {#data-storage-and-analysis}\n\nAlthough the storage capacities of hard drives have increased\nmassively over the years, access speeds -- the rate at which data can\nbe read from drives -- have not kept up. The obvious way to reduce the\ntime is to read from multiple drives at once.\n\nThe idea is to provide a shared system where access is shared, in\nreturn for shorter analysis times. Analysis jobs tend to be spread\nover time, so interference is minimal.\n\nSeveral issues need to be overcome. For example, with the usage of\nmany pieces of hardware, the chance of hardware failure is much\nhigher. Hadoop's distributed file system addresses this issue. Second,\nmany analysis tasks require combination of data in ome way, from\nmultiple sources. The MapReduce model provides a programming model\nthat abstracts the problem from disk reads and writes, transforming\nit into a computation over sets of keys and values.\n\nHadoop has evolved beyond batch processing. For example, HBase is a\nkey-value store that uses HDFS for its underlying storage, providing\nonline read/write access of individual rows and batch operations for\nreading and writing data in bulk.\n\nYARN (Yet another Resource Negotiator) was introduced in Hadoop 2,\nthat enabled new processing models. YARN is a cluster resource\nmanagement system, which allows any distributed program to run on data\nin a Hadoop Cluster. These include:\n\nInteractive SQL: Impala and Hive enable low-latency responses for\n    SQL queries on Hadoop using a distributed query engine.\nIterative processing: Spark enables an exploratory style of working\n    with datasets.\nStream processing: Storm, Spark streaming or Samza allow running\n    real-time, distributed computations of unbounded streaming data\nSearch: Solr can index documents as they are added to HDFS\n\nHadoop works well on unstructured or semi-structured data, because it\nis designed to interpret data at processing time (schema-on-read).\n\nHadoop tries to co-locate data with the compute nodes, so data access\nis fast because it's local. Data locality is one of the key reasons\nfor Hadoop's good performance. Hadoop explicitly models network\ntopology, making a best attempt to avoid network saturation.\n\nOn the other hand, high-performance computing (HPCs) provides APIs as\nthe Message Passing Interface (MPI), which requires that users\nexplicitly handle the mechanics of data flow. Processing in Hadoop\noperates at a higher level.\n\nData Flow and MapReduce {#data-flow-and-mapreduce}\n\nA MapReduce job is a unit of work that the client wants to be\nperformed: it consists of input data, the MapReduce program, and\nconfiguration information.\n\nHadoop runs the job by dividing it into tasks, of which there are two\ntypes: map tasks and reduce tasks. These tasks are scheduled by YARN,\nand run on nodes in the cluster. If a task fails, they will be\nautomatically rescheduled to run on a different node in the cluster.\n\nHadoop divides the input to a MapReduce job into fixed-size pieces\ncalled input splits, or splits. Hadoop creates one map task for each\nsplit, which runs the user-defined map function for each record in the\nsplit.\n\nHaving amny splits means the time taken to process each split is small\ncompared to the time to process the whole input. However, there is\noverhead in managing splits. Hence, there is a tradeoff between\nparallelizing and managing this overhead. As a rule of thumb, a good\nsplit size is the size of a HDFS file block, which is 128MB by\ndefault.\n\nHadoop attempts to run the map task on a node where the input data\nresides in HDFS, because it doesn't use valuable cluster bandwidth.\nThis is called the data locality optimization.\n\nMap tasks write their intermediate output to the local disk, not to\nHDFS. If a node running the map task fails before the map output has\nbeen consumed by the reduce task, then Hadoop will automatically rerun\nthe map task on another node to re-create the map output.\n\n{{}}\n\nReduce tasks don't have the advantage of data-locality; the input to a\nsingle reduce task is often the output from all mappers. Therefore the\nsorted map outputs have to be transferred across the network to the\nnode where the reduce task is running, where they are merged and then\npassed to the user-defined reduce function.\n\nThe number of reduce tasks is not governed by the size of the input,\nand is specified independently. Twitter does reducer count estimation\nusing hraven.\n\nHadoop also allows users to specify a combiner function to be run on\nthe map output, to allow users to form input to the reduce function.\nTHis is an optimization: there is no guarantee of how many times it\nwill call the combiner function for a particular map output record.\nHence, it should be idempotent.\n\nFor example, suppose weather readings for the year 1950 was produced\nby two maps:\n\n(1950, 0)\n(1950, 20)\n(1950, 10)\n\n(1950, 25)\n(1950, 15)\n\nThe reduce function (in our case max) would be called with the list:\n\n(1950, [0, 20, 10, 25, 15])\n\nand finally return (1950, 25).\n\nTo optimize this, we may have a combiner function that performs max\nsuch that the reduce function would be called with:\n\n(1950, [20, 25])\n\nto produce the same output.\n\nHadoop uses Unix standard streams as the interface between Hadoop and\nprograms, so any language that can read and write to standard output\ncan be used to write the MapReduce program.\n\nThe Hadoop Distributed Filesystem (HDFS) {#the-hadoop-distributed-filesystem--hdfs}\n\nHDFS is designed for storing very large files with streaming data\naccess patterns, running on clusters of commodity hardware. Hadoop is\nbuilt around the write-once, read-many-times pattern. Time to read the\nwhole dataset is optimized, over time to read the first record. HDFS\nis optimized for delivering data at high-throughput, sometimes at the\nexpense of latency. Hence, HDFS is ill-suited for:\n\nLow-latency file access\nLots of small files\n\nHDFS Concepts {#hdfs-concepts}\n\nBlock Size\n\n    The block size is the minimum amount of data a disk can read or write.\n    HDFS uses a relatively large block size (128MB by default). Unlike a\n    filesystem for a single disk, a file in HDFS that is smaller than a\n    single block does not occupy a full block's worth of underlying\n    storage. HDFS block sizes are large compared to regular disk blocks to\n    minimize the cost of seeks.\n\n    Having a block abstraction for a distributed filesystem brings several\n    benefits. First, it allows a file to be larger than any single disk in\n    the network, since blocks can be stored in any disk. Second, it\n    simplifies the storage subsystem. Third, it fits well with\n    replication, for providing fault tolerance and availability. A block\n    that is unavailable can be replicated from alternative locations.\n\n        hadoop fs fsck / -files -blocks\n\n    will list the blocks that make up each file in the filesystem.\n\nNamenodes and Datanodes\n\n    The HDFS cluster has two types of nodes operating in a master-worker\n    pattern: a namenode (the master) and a number of datanodes (workers).\n    The namenode manages the filesystem namespace. It maintains the\n    filesystem tree, and the metadata for all the files and directories in\n    the tree. This information is persisted in the form of 2 files: the\n    namespace image and the edit log. The namenode knows the datanodes on\n    which all the blocks for a given file are located. This data is not\n    persisted; it is reconstructed from datanodes when the system starts.\n\n    A client accesses the filesystem on behalf of the user by\n    communicating with the namenode and datanodes. The filesystem\n    interface is similar to a Portable Operating System Interface (POSIX).\n\n    Datanodes store and retrieve blocks when they are told to (by clients\n    or the namenode), and report back to the namenode periodically about\n    the blocks they are storing.\n\n    If the namenode is obliterated, all files on the filesystem would be\n    lost, since there is no way to reconstruct the original files, given\n    that this information was stored on the main node. To make the\n    namenode resilient to failure, the files are backed up onto multiple\n    filesystems. A secondary namenode that merges the namespace image with\n    the edit log (to prevent the edit log from growing too large), runs on\n    a separate machine. The state of the secondary namenode always lags\n    behind the primary. Hence, it case of total primary namenode failure,\n    the usual action is to copy the namenode's metadata files that are on\n    a NFS to secondary, and run the secondary node as the primary.\n\nBlock Caching\n\n    Frequently accessed blocks may be explicitly cached in the datanode's\n    memory, in an off-heap block cache. By default, a block is cached only\n    in one datanode's memory, but this can be configured on a per-file\n    basis. Job schedulers can take advantage of the cached blocks by\n    running tasks on the datanode where the block is cached. A small\n    lookup table used in a join is a good candidate for caching.\n\nHDFS Federation\n\n    Introduced in the 2.x release series, HDFS federation allows a custer\n    to scale by adding namenodes. This is to scale namenodes, which grow\n    quickly in size because it has to keep a reference to every file and\n    block in the filesystem. To access a federated HDF cluster, clients\n    use client-side mount tables to map file paths to namenodes. This is\n    managed in configuration using ViewFileSystem and the viewfs:// URLs.\n\n    Under federation, each namenode manages a namespace volume, which is\n    made up of the metadata for the namespace, and a block pool containing\n    all the blocks for the files in the namespace. Namespace volumes are\n    independent of each other, which means namenodes do not communicate\n    with one another, and failure of one namenode does not affect teh\n    availibility of the namespaces managed by other namenodes. Datanodes\n    register with each namenode in the cluster and store blocks from\n    multiple block pools.\n\nHDFS High Availability\n\n    When a namenode fails, recovery can take a long time: an administrator\n    needs to start a new primary namenode, load the namespace image,\n    replay the edit log, and receive block reports from the datanodes.\n\n    Hadoop 2 added HDFS high availibility. A pair of namenodes are in\n    active-standby configuration. In the event of failure, the standby\n    namenode takes over as the primary namenode without service\n    interruption.\n\n    For this to happen, architectural changes were needed:\n\n    The namenodes must use highly-available shared storage to share the\n        edit log\n    Datanodes must send block reports to both namenodes\n    Clients must be configured to handle namenode failover\n    The secondary namenode's role is subsumed by the standby, which\n        takes periodic checkpoints of the active namenode's namespace\n\n    There are 2 choices for highly-available shared storage: an NFS filer,\n    or a quorum journal manager (QJM). the QJM is a dedicated HDFS\n    implementation, designed for the sole purpose of a highly available\n    edit log, and is the recommended choice. The QJM runs as a group of\n    journal nodes, and each edit must be written to a majority of the\n    journal nodes. Typically, there are 3 journal nodes, so the system can\n    tolerate the loss of 1 of them. This is similar to the way ZooKeeper\n    works, but QJM does not use ZooKeeper underneath.\n\nFailover and Fencing\n\n    The transition from the active namenode to the standby is managed by a\n    new entity in the system called the failover controller. There are\n    various failover controllers but the system called the failover\n    controller. There are varoious failover controllers, but the default\n    implementation uses ZooKeeper to ensure that only one namenode is\n    active. Each namenode runs a lightweight failover controller process\n    whose job is to monitor its namenode for failures and trigger a\n    failover should a namenode fail.\n\n    The QJM only allows one namenode to write to the edit log at one time;\n    however, it is still possible for the previously active namenode to\n    serve stale read requests to clients, so setting up an SSH fencing\n    command that will kill the namenode's process is a good idea. Stronger\n    fencing methods are required with the NFS filer, since it is not\n    possible to only allow one namenode to write at a time.\n\n    Client failover is handled transparently by the client library. The\n    simplest implementation uses client-side configuration to control\n    failover.\n\nHadoop FileSystem Abstractions\n\n    HDFS is just one implementation of the filesystem abstraction. There\n    are several implementations, examples of which are listed below:\n\n    {{}}\n\nFile writes\n\n    {{}}\n\nHadoop distcp\n\n    distcp is implemented as a MapReduce job where the work of copying is\n    done by the maps that run in parallel across the cluster. It is an\n    efficient, distributed copy program.\n\nYARN {#yarn}\n\nApache YARN is Hadoop's cluster resource management system. YARN\nprovides APIs for requesting and working with cluster resources, but\nthese APIs are not typically used directly by user code. Instead,\nusers write to higher-level APIs provided by distributed computing\nframeworks, such as Spark and MapReduce.\n\n{{}}\n\nYARN provides its core services via two types of long-running daemon:\na resource manager (one per cluster) to manage the use of resources\nacross the cluster, and node managers on all the nodes in the cluster\nto launch and monitor containers. Depending on how YARN is configured,\na container may be a Unix process, or a linux cgroup.\n\n{{}}\n\nTo run an application on YARN, a client contacts the resource manager\nand asks it to run an application master process. The resource manager\nfinds a node manager that can launch an application master in a\ncontainer. The application could request more containers from the\nresource managers, and use them to run a distributed computation. This\nis what a MapReduce application does. Most non-trivial YARN\napplications use some form of remote communication to pass status\nupdates and results around, but these are application specific.\n\nData Serialization {#data-serialization}\n\nData is often represented differently in-memory, and requires\nserialization before being written to disk. For evolvability,\nserialization formats should provide forward compatibility. This\nallows schemas to change without affecting data that was written\npreviously. Some serialization formats include:\n\nThrift\nProtocol Buffers\nAvro\n\nThrift and Protocol Buffers are highly similar projects. Thrift is\nrelatively more mature, with generated serialization classes for many\ndifferent languages, and ships with an RPC framework. Protocol Buffers\nand gRPC were developed simulataneously, but ship as separate\nprojects.\n\nAvro is designed from the ground-up for the Hadoop filesystem by Doug\nCutting, the author of Hadoop. In contrast with Thrift and Protocol\nBuffers, it provides a dynamic schema. The data format is also\nsplittable by default.\n\nTo allow for more efficient reads, Twitter uses Parquet, a project\nthat came out of a collaboration between Twittera and Cloudera.\nInstead of storing Thrift structures in the Thrift binary format,\nParquet uses a data converter to convert Thrift structures into\nParquet format, a compressed, columnar data representation.\n\n(Martin Kleppmann, 2016)\n\nParquet's Columnar Storage {#parquet-s-columnar-storage}\n\nParquet's columnar representation is inpired by Google's Dremel.\n(Sergey Melnik et al., 2010)\n\nThrift and Protocol Buffer's binary representations are field values\nlaid out sequentially. Using a columnar-striped representation enables\nqueries on just a few columns to read less data from storage.\n\nA key challenge is the natural occurrence of nested records in web and\nscientific computing. Normalizing these nested records are often\ncomputationally too expensive. The approach Dremel takes is storing\nnested records with their values, and _repetition and definition\nlevels_.\n\nRepetition levels\n: repetition levels tell us at what repeated\n    field in the field's path the value has repeated.\n\nDefinition levels\n: definition levels tell us how many fields in\n    \\\\(p\\\\) could be undefined, are actually present.\n\n{{}}\n\n{{}}\n\nEach column is stored as a set of blocks, each block containing the\nrepetition and definition levels, and compressed field values.\n\nRecord shredding is performed by creating a tree of field writers,\nwhose structure matches the file hierarchy in the schema. Field\nwriters update only when they have their own data, and do not try to\npropagate the parent state down the tree unless absolutely necessary.\n\nRecord assembly is performed by constructing an optimal FSM that reads\nfield values and levels for each field, and append the values\nsequentially to the output records.\n\nEfficient algorithms for record shredding and assembly are provided in\nAppendix A of the Dremel paper. (Sergey Melnik et al., 2010)\n\nBeyond MapReduce {#beyond-mapreduce}\n\nMapReduce -\nDataflow model -\nFlumeJava -\nMillWheel -\n\nBeam is an open source, unified model for defining and executing data\nprocessing workflows.\n\n{#}\n\nBibliography\nWhite, T., Hadoop: the definitive guide (2009), : O'Reilly Media, Inc. ↩\n\nKleppmann, M., Designing data-intensive applications: the big ideas behind reliable, scalable, and maintainable systems (2016), : O'Reilly. ↩\n\nMelnik, S., Gubarev, A., Long, J. J., Romer, G., Shivakumar, S., Tolton, M., & Vassilakis, T., Dremel: interactive analysis of web-scale datasets, In , Proc. of the 36th Int'l Conf on Very Large Data Bases (pp. 330–339) (2010). : . ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/haskell",
        "title": "Haskell",
        "content": "\ntags\n: §prog\\_lang\n\nIntroduction {#introduction}\n\nExpressions {#expressions}\n\nExpressions include concrete values, variables, and also functions\nFunctions are expressions that are applied to an argument, and hence\n    can be reduced or evaluated\n\nInfix/Prefix {#infix-prefix}\n\ndiv (prefix) -> div (infix)\n(infix) -> (+) (prefix)\n\nLet vs Where {#let-vs-where}\n\nLet introduces an expression, so it can be used wherever you can have\nan expression, but where is a declaration and is bound to a\nsurrounding syntactic construct.\n\nTypeclasses {#typeclasses}\n\nTypeclasses are a way of adding additional functionality that is\nreusable across all the types that have instances of that typeclass.\nNum is a typeclass for most numeric types, that provide the basic\noperators (+), (-), (*) , (/) etc.\n\nDatatype declaration {#datatype-declaration}\n\nA datatype declaration defines a type constructor and data\nconstructors. Data constructors are the values of a particular type,\nand are also functions that let us create data, or values, of a\nparticular type.\n\nSectioning {#sectioning}\n\nRefers to the partial application of infix operators.\n\nlet x = 5\nlet y = (2^)\nlet z = (^2)\n\ny x                             -- 32\nz x                             -- 25\n\nlet celebrate = (++ \" woot!\")\ncelebrate \"naptime\" -- \"naptime woot!\"\ncelebrate \"dogs\" -- \"dogs woot!\"\n\nTypes {#types}\n\nPolymorphism\n\n    Parametric polymorphism\n        Refers to type variables, or parameters, that are fully\n            polymorphic\n        When unconstrained by a typeclass, the final concrete type could\n            be anything\n    Constrained polymorphism\n        Puts typeclass constraints on the variable, decreasing the number\n            of concrete types it could be, but increasing what you can\n            actually do with it by defining and bringing into scope a set of\n            operations\n\n    Numeric literals are polymorphic and stay so until given a more\n    specific type.\n\nParametricity\n\n    parametricity means that the behaviour of a function with respect to\n    the types its (parametric polymorphic) arguments is uniform. The\n    behaviour cannot change just because it was applied to an argument of\n    a different type.\n\nMaking things more polymorphic\n\n        -- fromIntegral :: (Num b, Integral a) => a -> b\n    -- e.g.\n    6 / fromIntegral (length [1,2,3])\n\nLaziness and Performance {#laziness-and-performance}\n\nLaziness can be a useful tool for improving performance, but more\noften than not it reduces performance by adding a constant overhead to\neverything. Because of laziness, the compiler can't evaluate a\nfunction argument and pass the value to the function, it has to record\nthe expression in the heap in a suspension (or thunk) in case it is\nevaluated later. Storing and evaluating suspensions is costly, and\nunnecessary if the expression was going to be evaluated anyway.\n\nOne can force eager evaluation by prepending a bang(!) in front of\nthe expression.\n\nType classes {#type-classes}\n\nWhere a declaration of a type defines how that particular type is\ncreated, a declaration of a typeclass defines how a set of types are\nconsumed or used in computations.\n\nAs long as a type implements, or instantiates a typeclass, then\nstandard functions implemented on the typeclass can be used.\n\ndata DayOfWeek =\n  Mon | Tue | Wed | Thu | Fri | Sat | Sun\n\n-- day of week and numerical day of month\n\ndata Date =\n  Date DayOfWeek Int\n\nBecause Eq is not derived in the typeclass, we need to instantiate one\nof our own:\n\ninstance Eq DayOfWeek where\n  (==) Mon Mon = True\n  (==) Tue Tue = True\n  (==) Wed Wed = True\n  (==) Thu Thu = True\n  (==) Fri Fri = True\n  (==) Sat Sat = True\n  (==) Sun Sun = True\n  (==) _ _ = False\n\ninstance Eq Date where\n  (==) (Date weekday dayOfMonth) (Date weekday' dayOfMonth') =\n    weekday == weekday' && dayOfMonth = dayOfMonth'\n\nTypeclass instances are unique parings of the typeclass and a type.\nThey define the ways to implement the typeclass methods for that type.\n\nIO {#io}\n\nAn IO action is an action that, when performed, has side effects,\nincluding reading from input and printing to the screen, and will\ncontain a return value.\n\nIn IO (), () denotes an empty tuple, referred to as a unit. A\nunit is both a value and a type, that has only one inhabitant.\n\nSummary {#summary}\n\nA typeclass defines a set of functions and/or values;\nTypes have instances of that typeclass\nThe instances specify the ways that type uses the functions of the typeclass\n\nLists {#lists}\n\ndata [] a = [] | a : [a]\n\nExtracting portions of lists {#extracting-portions-of-lists}\n\ntake :: Int -> [a] -> [a]\ndrop :: Int -> [a] -> [a]\nsplitAt :: Int -> [a] -> ([a], [a])\n\ntakeWhile :: (a -> Bool) -> [a] -> [a]\ndropWhile :: (a -> Bool) -> [a] -> [a]\n\nTransforming lists of values {#transforming-lists-of-values}\n\nmap :: (a -> b) -> [a] -> [b]\nfmap :: Functor f => (a -> b) -> f a -> f b\n\nmap (+1) [1,2,3,4] -- [2,3,4,5]\nmap (1-) [1,2,3,4] -- [0,-1,-2,-3]\n\nfilter :: (a -> Bool) -> [a] -> [a]\nfilter _ [] = []\nfilter pred (x:xs)\n  | pred x = x : filter pred xs\n  | otherwise = filter pred xs\n\nzip :: [a] -> [b] -> [(a,b)]\nzip [1,2] [3,4] -- [(1,3), (2,4)]\n\nzipWith (+) [1,2,3] [10,11,12] -- [11,13,15]\n\nFolding lists {#folding-lists}\n\nFolds as a general concept are called catamorphisms.\nCatamorphisms are a means of deconstructing data. If the spine of\nthe list is the structure of a list, then a fold is what can reduce\nthat structure.\n\nfoldr :: (a -> b -> b) -> b -> [a] -> b\nfoldr f z xs =\n  case xs of\n    [] -> z\n    (x:xs) -> f x (foldr f z xs)\n\nAlgebraic Datatypes {#algebraic-datatypes}\n\nA type can be thought of as an enumeration of constructors that have\nzero or more arguments.\n\nHaskell offers sum types, product types, product types with record\nsyntax, type aliases, and a special datatype called a newtype that\noffers a different set of options and constraints from either type\nsynonyms or data declarations.\n\n  data Bool = False | True\n-- [1] [2] [3] [4] [5] [6]\n\n  data [] a = [] | a : [a]\n--     [7]   [8]    [9]\n\nKeyword data to signal that what follows is a data declaration,\n    or a declaration of a datatype\nType constructor (with no arguments)\nEquals sign divides the type constructor from the data constructor\nData constructor. In this case, a data constructor that takes no\n    arguments, so is called a nullary constructor.\nPipe denotes a sum type, which indicates a logical disjunction\n    (colloquially or) in what values can have that type\nConstructor for the value True, another nullary constructor\nType constructor with an argument. The argument is a polymorphic\n    type variable, so the list's argument can be of different types\nData constructor for the empty list\nData constructor that takes two arguments, an a and also a [a]\n\nData and type constructors {#data-and-type-constructors}\n\nType constructors are used only at the type level, in type signatures\nand typeclass declarations and instances. Types are static and resolve\nat compile time.\n\nData constructors construct the values at term level, values you can\ninteract with at runtime.\n\nType and data constructors that take no arguments are constants. They\ncan only store a fixed type and amount of data.\n\nType constructors and kinds {#type-constructors-and-kinds}\n\nKinds are types of types, or types one level up. We represent kinds in\nHaskell with *. We know something is a fully applied, concrete type\nwhen it is represented as `. When it is  -> *`, it is still\nwaiting to be applied.\n\n-- :k Bool\nBool :: *\n\n-- :k [Int]\n[Int] :: *\n\n-- :k []\n[] :: * -> *\n\nBoth Bool and [Int] are fully applied, concrete types, so their kind\nsignatures have no function arrows.\n\nTypes vs Data {#types-vs-data}\n\nWhen data constructors take arguments, those arguments refer to other\ntypes.\n\ndata Price =\n  --  (a)\n  Price Integer deriving (Eq, Show)\n-- (b)  [1]\n-- type constructor a\n-- data constructor b\n-- type argument [1]\n\nWhat makes these datatypes algebraic? {#what-makes-these-datatypes-algebraic}\n\nAlgebraic datatypes are so, because we can describe the patterns of\nargument structures using two basic operations: sum and product.\n\nThe cardinality of a datatype is the number of possible values it\ndefines. Knowing how many possible values inhabit a type can help\nreason about programs.\n\nThe cardinality of Bool is 2, only being to take on True or False.\n\nDatatypes that only contains a unary constructor always have the same\ncardinality as the type they contain.\n\ndata Goats = Goats Int deriving (Eq, Show)\n\nHere, Goats has the cardinality of Int.\n\nSum Types {#sum-types}\n\nCardinality is obtained through summation. Example, Bool:\n\ndata Bool = True | False\n\nIn this case, the cardinality of Bool is the sum of the cardinality\nof True and False.\n\nRecord syntax {#record-syntax}\n\ndata Person =\n  Person { name :: String\n         , age :: Int }\n         deriving (Eq, Show)\n\nSignaling Adversity {#signaling-adversity}\n\nMaybe {#maybe}\n\ndata Maybe = Just a | Nothing\n\ntype Name = String\ntype Age = Integer\n\ndata Person = Person Name Age Deriving (Eq, Show)\n\nmkPerson :: Name -> Age -> Maybe Person\nmkPerson name age\n  | name /= \"\" && age >=0 = Just $ Person name age\n  | otherwise = Nothing\n\nmkPerson is a smart constructor. It allows us to construct values\nonly if it meets a certain criteria.\n\nEither {#either}\n\nWe use an either to figure out which criteria is not met:\n\ndata Either a b = Left a | Right b\n\ndata Person Invalid = NameEmpty | AgeTooLow deriving (Eq, Show)\n\nmkPerson :: Name -> Age -> Either PersonInvalid Person\nmkPerson name age\n  | name /= \"\" && age >=0 - Right $ Person name age\n  | name == \"\" = Left PersonInvalid\n  | otherwise = Left AgeTooLow\n\nLeft is used as the invalid or error constructor. Functor will not\nmap over the left type argument because it has been applied away.\n\nSignalling Multiple errors\n\n        type Name = String\n    type Age = Integer\n    type ValidatePerson a = Either [PersonInvalid] a\n\n    data Person = Person Name Age deriving Show\n\n    data PersonInvalid = NameEmpty | AgeTooLow deriving (Eq, Show)\n\n    ageOkay :: Age -> Either [PersonInvalid] Age\n    ageOkay age = case age >= 0 of\n      True -> Right age\n      False -> Left [AgeTooLow]\n\n    nameOkay :: Name -> Either [PersonInvalid] Name\n    nameOkay name = case name == \"\" of\n      True -> Left [NameEmpty]\n      False -> Right name\n\n    mkPerson :: Name -> Age -> ValidatePerson Person\n    mkPerson name age =\n      mkPerson' (nameOkay name) (ageOkay age)\n\n    mkPerson' :: ValidatePerson Name\n              -> ValidatePerson Age\n              -> ValidatePerson Person\n\n    mkPerson' (Right nameOk) (Right ageOk) = Right (Person nameOk ageOk)\n    mkPerson' (Left badName) (Left badAge) = Left (badName ++ badAge)\n    mkPerson' (Left badName) _ = Left badName\n    mkPerson' _ (Left badAge) = Left badAge\n\nAnamorphisms {#anamorphisms}\n\nAnamorphisms are the dual of catamorphisms. Catamorphisms, or\nfolds, break data structures down, anamorphisms builds up data\nstructures.\n\n-- iterate is like a very limited unfold that never ends\niterate :: (a -> a) -> a -> [a]\n\ntake 10 $ iterate (+1) 0\n[0,1,2,3,4,5,6,7,8,9]\n\n--unfoldr is more general\nunfoldr :: (b -> Maybe (a,b)) -> b -> [a]\n\ntake 10 $ unfoldr (\\b -> Just (b, b+1)) 0\n[0,1,2,3,4,5,6,7,8,9]\n\nMonoids {#monoids}\n\nIn Haskell, algebras are implemented with typeclasses; the typeclasses\ndefine the set of operations. When we talk about operations over a\nset, the set is the type the operations are for.\n\nOne of those algebras we use in Haskell is Monoid.\n\nA monoid is a binary associative pattern with an identity.\n\nA monoid is a function that takes two arguments and follows two laws:\nassociativity and identity.\n\nAssociativity: arguments can be regrouped or paranthesised in\n    different orders and give the same result\nIdentity: there exists some value such that when it is passed as\n    input to the function, the operation is rendered moot and the other\n    value is returned. E.g. adding 0, multiplying by 1\n\nMonoids are the pattern of summation, multiplication and list\nconcatenation, among other things.\n\nclass Monoid m where\n  mempty :: m\n  mappend :: m -> m -> m\n  mconcat :: [m] -> m\n  mconcat = foldr mappend mempty\n\nmappend is how any two values that inhabit the type can be joined\ntogether. mempty is the identity value for that mappend operation.\n\nExamples of Monoids {#examples-of-monoids}\n\nList\n\n        mappend [1,2,3] [4,5,6]\n    -- [1,2,3,4,5,6]\n    mconcat [[1..3], [4..6]]\n    -- [1,2,3,4,5,6]\n    mappend \"Trout\" \" goes well with garlic\"\n    -- \"Trout goes well with garlic\"\n\n    instance Monoid [a] where\n      mempty = []\n      mappend = (++)\n\nIntegers\n\n    Integers form a monoid under summation and multiplication. Because it\n    is unclear which rule is to be followed, there is no Monoid class\n    under Integer, but there is the Sum and Product types that signal\n    which Monoid instance is wanted.\n\nNewtype {#newtype}\n\nUsing newtype constrains the datatype to having a single unary data\nconstructor, and newtype guarantees no additional runtime overhead\nin \"wrapping\" the original type. The runtime representation of newtype\nand what it wraps are always identical.\n\n() :: Monoid m => m -> m -> m\n\n` is the infix version of mappend`.\n\nMonoid instances must abide by the following laws:\n\n-- left identity\nmappend mempty x = x\n\n-- right identity\nmappend x mempty = x\n\n-- associativity\nmappend x (mappend y z) = mappend (mappend x y) z\n\nmconcat = foldr mappend mempty\n\nMonoid instances in Bool {#monoid-instances-in-bool}\n\nAll True  All True\n-- All {getAll = True}\n\nAll True  All False\n-- All {getAll = False}\n\nAny True  Any False\n-- Any {getAny = True}\n\nAny False  Any False\n-- Any {getAny = False}\n\nAll represents boolean conjuction, while Any represents boolean disjunction.\n\nFor Maybe, First returns the \"first\" or leftmost non-Nothing\nvalue. Last returns the \"last\" or rightmost non-Nothing value.\n\n(First (Just 1))  (First (Just 2))\n-- First {getFirst = Just 1}\n\ninstance Monoid b => Monoid (a -> b)\ninstance (Monoid a, Monoid b) => Monoid (a,b)\ninstance (Monoid a, Monoid, b, Monoid c) => Monoid (a,b,c)\n\nSemigroups {#semigroups}\n\nSemigroups are like monoids, but without the identity constraint. The\ncore operation remains binary and associative.\n\nclass Semigroup a where\n  () :: a -> a -> a\n\n(a  b)  c = a  (b  c)\n\ndata NonEmpty a = a :| [a] deriving (Eq, Ord, Show)\n\nFunctors {#functors}\n\nA functor is a way to apply a function over or around some structure\nthat we don't want to alter. That is, we want to apply the function to\nthe value that is \"inside\" some structure, and leave the structure\nalone.\n\nIntuitively, a Functor represents a \"container\" of some sort, along\nwith the ability to apply a function uniformly to every element in the\ncontainer. Another intuition of a Functor is that it represents some\nsort of \"computational context\".\n\nThis is why functors are generally introduced by way of fmapping over\nlists. No elements are removed or added, only transformed.\n\nThe typeclass Functor generalises this pattern, so that this basic\nidea can be used across different structures.\n\nclass Functor f where\n  fmap :: (a -> b) -> f a -> f b\n\nThe argument f a is a Functor f that takes a type argument a.\nThat is, the f is a type that has an instance of the Functor\ntypeclass.\n\nThe return value is f b. It is the same f from f a, while the\ntype argument b possibly but not necessarily refers to a different type.\n\nfmap specialises to different types as such:\n\nfmap :: (a -> b) -> f a -> f b\nfmap :: (a -> b) -> [] a -> [] b\nfmap :: (a -> b) -> Maybe a -> Maybe b\nfmap :: (a -> b) -> Just a -> Just b\nfmap :: (a -> b) -> Either a -> Either b\nfmap :: (a -> b) -> (e,) a -> (e,) b\nfmap :: (a -> b) -> Identity a -> Identity b\n\nFunctor Laws {#functor-laws}\n\nIdentity\n\n        fmap id == id\n\n    If we fmap the identity function, it should have the same result as\n    passing our value to identity.\n\nComposition\n\n        fmap (f . g) == fmap f . fmap g\n\nStructure Preservation\n\n        fmap :: Functor f => (a -> b) -> f a -> f b\n\n    The f is constrained by the typeclass Functor, but that is all we\n    know about its type from this definition. Because the f persists\n    through the type of fmap, whatever the type is, we know it must be a\n    type that can take an argument, as in f a and f b and that it will\n    be the \"structure\" we're lifting the function over when we apply it to\n    the value inside.\n\nExamples {#examples}\n\ndata WhoCares a =\n  ItDoesnt\n  | Matter a\n  | WhatThisIsCalled\n  deriving (Eq, Show)\n\nIn the above datatype, only Matter can be fmapped over, because\nthe others are nullary, and there is no value to work with inside the\nstructure.\n\nHere is a law-abiding instance of Functor.\n\ninstance Functor WhoCares where\n  fmap _ ItDoesnt = ItDoesnt\n  fmap _ WhatThisIsCalled = WhatThisIsCalled\n  fmap f (Matter a) = Matter (f a)\n\nThis is a law-breaking instance:\n\ninstance Functor WhoCares where\n  fmap _ ItDoesnt = WhatThisIsCalled\n  fmap f WhatThisIsCalled = ItDoesnt\n  fmap f (Matter a) = Matter (f a)\n\nIn this instance, the structure -- not the values wrapped or contained\nwithin the structure -- change.\n\nMaybe and Either Functors {#maybe-and-either-functors}\n\ndata Two a b = Two a b\n\nNotice Two has the kind * -> * -> *, however, functors are of kind\n* -> *, and hence functors on the type Two would be invalid. we can\nreduce the kindness by doing the following:\n\ninstance Functor (Two a) where\n  fmap f (Two a b) = Two a (f b)\n\nNotice that we didn't apply f to a, because a is now part of the\nFunctor structure, and is untouchable.\n\nIgnoring possibilities {#ignoring-possibilities}\n\nThe Functor instances for the Maybe and Either datatypes are useful if\nyou tend to ignore the left cases, which are typically the error or\nfailure cases. Because fmap doesn't touch those cases, you can map\nyour function right to the values that you intend to work with and\nignore failure cases.\n\nMaybe\n\n        incIfJust :: Num a => Maybe a -> Maybe a\n    incIfJust (Just n) = Just $ n + 1\n    incIfJust Nothing = Nothing\n\n    incMaybe :: Num a => Maybe a -> Maybe a\n    incMaybe = fmap (+1)\n\nEither\n\n        incIfRight :: Num a => Either e a => Either e a\n    incIfRight (Right n) = Right $ n + 1\n    incIfRight (Left e) = Left e\n\n    -- can be simplified to\n    incEither :: Num a => Either e a => Either e a\n    incEither = fmap (+1)\n\nSummary {#summary}\n\nFunctor is a mapping between categories. In Haskell, this manifests\nas a typeclass which lifts a function between to types over two new\ntypes. This conventionally implies some notion of a function which can\nbe applied to a value with more structure than the unlifted function\nwas originally designed for. The additional structure is represented\nby the use of a higher kinded type f, introduced by the definition\nof the Functor typeclass.\n\nTo lift over, and later in Monad, to bind over, is a metaphor. One\nway to think about it is that we can lift a function into a context.\nAnother is that we lift a function over some layer of structure to\napply it.\n\nfmap (+1) $ Just 1 -- Just 2\nfmap (+1) [1,2,3] -- [2,3,4]\n\nIn both cases, the function we're lifting is the same. In the first\ncase, we lift that function into a Maybe context in order to apply it,\nin the second case, into a list context.\n\nThe context determines how the function will get applied: the context\nis the datatype, the definition of the datatype, and the Functor\ninstance we have for that datatype.\n\nApplicative {#applicative}\n\nMonoid gives us a means of hashing two values of the same type\ntogether.\n\nFunctor is for function application over some structure we don't want\nto have to think about.\n\nThe Applicative typeclass is a Monoidal Functor. The Applicative\ntypeclass allows for function application lifted over structure (like\nFunctor). But with Applicative the function we're applying is also\nembedded in some structure. Because the function and the value it's\nbeing applied to both have structure, we have to smash those\nstructures together.\n\nclass Functor f => Applicative f where\n  pure :: a -> f a\n  () :: f (a -> b) -> f a -> f b\n\nThe pure function embeds something into functorial (applicative)\nstructure.\n\n`` is an infix operation called 'apply'. This is very similar to\nthe types of fmap.\n\n-- fmap\n() :: Functor f => (a -> b) -> f a -> f b\n() :: Applicative f => f (a -> b) -> f a -> f b\n\nthe Control.Applicative library provides some convenience functions:\nliftA, liftA2 and liftA3:\n\nliftA :: Applicative f => (a -> b) -> f a -> f b\nliftA2 :: Appplicative f => (a -> b -> c) -> f a -> f b -> f c\nliftA2 :: Appplicative f => (a -> b -> c -> d) -> f a -> f b -> f c -> f d\n\nliftA is just fmap with an Applicative typeclass constraint as\nopposed to a Functor typeclass constraint.\n\nIn pure, the left type is handled differently from the right:\n\npure 1 :: ([a], Int) -- ([], 1)\npure 1 :: Either a Int -- Right 1\n\nThe left type is part of the structure, and the structure is not\ntransformed by the function application.\n\nIn a sense, Applicative is Monoid bolted onto a Functor to be able to\ndeal with functions embedded in additional structure. In another,\nwe're enriching function application with the very structure we were\npreviously merely mapping over with Functor.\n\n[(2), (3)]  [4,5] -- [24, 25, 34, 35]\n= [8,10,12,15]\n\n`` takes a functor that has a function in it, and another functor\nand applies the function inside the functor. `` is left associative.\n\ninstance Applicative Maybe where\n  pure = Just\n  Nothing  _ = Nothing\n  (Just f)  something = fmap f something\n\ninstance Applicative [] where\n  pure x = [x]\n  fs  xs = [f x | f >=) :: (Monad m) => m a -> (a -> m b) -> m b\n\nIn Prelude, IO, lists, and Maybe are members of the monadic\nclasses.\n\n-- infixl 1 >>=, >>\nclass Applicative m => Monad (m :: * -> *) where\n  (>>=) :: m a -> (a -> m b) -> m b\n  (>>) :: m a -> m b -> m b\n  return :: a -> m a\n  fail :: String -> m a\n\n>>, referred, to as bind, combines a Monad containing values of type\na, and a function which operates on a and returns a monad of type\nb.\n\n>>, also sometimes called Mr. Pointy, is used when the function\ndoes not need the value of the first Monadic operator.\n\nThe precise meaning of bind depends on the monad. For example, in the\nIO monad, x>>=y performs two actions sequentially, passing the\nresult of the first into the second. For the lists and Maybe type,\nthese monadic operations can be understood in terms of passing zero or\nmore values from one calculation to the next.\n\nThe do syntax provides a simple shorthand for chains of monadic\noperations:\n\ndo e1 ; e2 = e1 >> e2\ndo p >= (\\v -> case v of p -> e2; _ -> fail \"s\")\n\nThe laws which govern >>= and return are:\n\nreturn a >>= k          = k a\nm >>= return            = m\nxs >>= return . f       = fmap f xs\nm >>= (\\x -> k x >>= h) = (m >>= k) >>= h\n\nBuilt in Monads {#built-in-monads}\n\nMaybe\n\n        -- treating Maybe as unctors\n    fmap (++\"!\") (Just \"wisdom\") -- Just \"wisdom!\"\n    fmap (++\"!\") Nothing -- Nothing\n\n    -- treating Maybe as Applicatives\n    Just (+3)  Just 3 -- Just 6\n    Nothing  Just \"greed\" -- Nothing\n    max  Just 3  Just 6 -- Just 6\n    max  Just 3  Nothing -- Nothing\n\n    -- Upgrading to Monads\n    (\\x -> Just (x + 1)) 1 -- Just 2\n    applyMaybe :: Maybe a -> (a -> Maybe b) -> Maybe b\n    applyMaybe Nothing f = Nothing\n    applyMaybe (Just x) f = f x\n\n    Just 3 applyMaybe \\x -> Just (x + 1) -- Just 4\n    Nothing applyMaybe \\x -> Just (x + 1) -- Nothing\n\n    -- applyMaybe is >>= for the Maybe monad\n\nLists\n\n    The monadic aspects of lists bring non-determinism into code in a\n    clear and readable manner.\n\n        instance Monad [] where\n      return x = [x]\n      xs >>= f = concat (map f xs)\n      fail _ = []\n\n        [3,4,5] >> = \\x -> [x, -x]\n    -- [3, -3, 4, -4, 5, -5]\n\n    Non-determinism also includes support for failure. Here, the empty\n    list [] is the equivalent of Nothing, because it signifies the\n    absence of a result.\n\n    Just like with Maybe values, we can chain several lists with >>=:\n\n        [1,2] >>= \\n -> ['a', 'b'] >>= \\ch -> return (n,ch)\n    -- [(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n\n    do\n      n >=` becomes:\n\n        (>>=) :: [a] -> (a -> [b]) -> [b]\n\n    Given a list of a's and a function that maps an a onto a list of\n    b's, >>= applies this function to each of the a's in the input\n    and returns the generated b's concatenated into a list. The return\n    function creates a singleton list.\n\n    The following two expressions are equivalent:\n\n        [(x,y) | x  (a, S)) -- The monadic type\n\ninstance Monad SM where\n  -- defines state propogation\n  SM c1 >>= fc2 = SM (\\s0 -> let (r, s1) = c1 s0\n                                 SM c2 = fc2 r\n                             in\n                               c2 s1)\n  return k = SM (\\s -> (k, s))\n\n-- extracts the state from the Monad\nreadSM :: SM S\nreadSM = SM (\\s -> (s, s))\n\n-- updates the state of the monad\nupdateSM :: (S -> S) -> SM () -- alters the state\nupdateSM f = SM (\\s -> ((), f s))\n\n-- run a computation in the SM monad\nrunSM :: S -> SM a -> (a, S)\nrunSM s0 (SM c) = c s0\n\nSM is defined to be a computation that implicitly carries a type\ns. SM consists of functions that take a state and produce two\nresults: a returned value (of any type) and an updated state.\n\nThe instance declaration defines the 'plumbing' of the monad: how to\nsequence two computations and the definition of an empty computation.\n\nSequencing (>>=) defines a computation (denoted by the constructor\nSM) that passes the initial state, s0 into c1, then passes the\nvalue coming out of this computation, r, to the function that\nreturns the second computation, c2. Finally, the state coming out of\nc1 is passed into c2 and the overall result is the result of c2.\n\nHere return doesn't change the state at all; it only serves to\nbring a value into the monad.\n\nreadSM brings the state out of the monad for observation while\nupdateSM allows the user to alter the state in the monad.\n\nDo Notation {#do-notation}\n\nHaskell's do notation supports an imperative style of programming by\nproviding syntactic sugar for chains of monadic expressions.\n\na >>= \\x ->\nb >>\nc >>= \\y ->\nd\n\n-- becomes:\n\ndo { x > do { stmts }\ndo { v >= \\v -> do { stmts }\ndo { let decls; stmts} → let decls in do { stmts }\n\nroutine :: Maybe Pole\nroutine = do\n  start <- return (0,0)\n  first <- landLeft 2 start\n  second <- landRight 2 first\n  landLeft 1 second\n",
        "tags": []
    },
    {
        "uri": "/zettels/hidden_markov_model",
        "title": "Hidden Markov Model",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/histogram_filter",
        "title": "Histogram Filter",
        "content": "\nThe state space is decomposed into finitely many regions, and the\ncumulative posterior for each region is represented by a single\nprobability value. In discrete spaces, these are known as discrete\nBayes filters, while in continuous spaces, as histogram filters.\n\nDiscrete Bayes Filter {#discrete-bayes-filter}\n\nThe discrete Bayes filter is directly obtained from the Bayes filter,\nreplacing the integral with a sum. The input is a discrete probability\ndistribution \\\\(\\\\{p\\_{k,t}\\\\}\\\\), and most recent control and measurement\n\\\\(u\\t\\\\) and \\\\(z\\t\\\\).\n\n\\begin{algorithm}\n\\caption{Discrete Bayes Filter}\n\\label{discrete\\bayes\\filter}\n  \\begin{algorithmic}[1]\n    \\Procedure{DiscreteBayesFilter}{$\\left\\\\{p\\{k,t-1}\\\\}, u\\t, z\\_t$}\n    \\ForAll{$k$}\n    \\State $\\overline{p}\\{k,t} = \\sum\\i p(X\\t = x\\k | u\\t, XK\\{t-1} =\n    x\\i) p\\{i, t-1}$\n    \\State $p\\{k,t} = \\eta p(z\\t | X\\t = x\\k)\\overline{p}\\_{k, t}$\n    \\EndFor\n    \\State \\Return $\\\\{p\\_{k,t}\\\\}$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\nHistogram Filter {#histogram-filter}\n\nHistogram filters decompose a continuous state space into finitely\nmany regions:\n\n\\begin{equation}\n  \\text{range}(X\\t) = x\\{1,t} \\cup x\\{2,t } \\dots x\\{K, t}\n\\end{equation}\n\nwhere \\\\(X\\_t\\\\) is the random variable describing the robot at time \\\\(t\\\\).\nEach \\\\(x\\_{k,t}\\\\) is a convex region, and form a partitioning of the\nstate space. A simple decomposition is a multi-dimensional grid.\n\nDecomposition Techniques {#decomposition-techniques}\n\nStatic Techniques\n\n    Static techniques rely on a fixed decomposition that is chosen in\n    advance. These are easier to implement, but can be computationally wasteful.\n\nDynamic Techniques\n\n    Dynamic techniques adapt the decomposition to the shape of the\n    posterior distribution.\n\n    Density trees decompose the state space recursively, adapting the\n    resolution to the posterior probability mass.\n\n    Selective updating chooses a subset of grid cells to update for the\n    posterior. These are the grid cells whose posterior probability\n    exceeds some user-set threshold.\n\nRelated {#related}\n\n§bayes\\_filter\n§particle\\_filter\n",
        "tags": []
    },
    {
        "uri": "/zettels/hopfield_network",
        "title": "Hopfield Network",
        "content": "\nA Hopfield network is a fully connected Ising model with a symmetric\nweight matrix, \\\\(\\mathbf{W} = \\mathbf{W^T}\\\\). These weights plus the\nbias term \\\\(\\mathbf{b}\\\\), can be learned from training data, using\n(approximate) maximum likelihood.\n\nHopfield networks can be used as an associative memory, or content\naddressable memory.\n\nSuppose we train on a set of fully observed bit vectors, corresponding\nto patterns we want to memorize. At test time, we present a partial\npattern to the network. We would like to estimate the missing\nvariables; this is called pattern completion.\n\nSince exact inference is intractable in this model, it is standard to\nuse a coordinate descent algorithm known as iterative conditional\nmodes (ICM), which just sets each node to its most likely (lowest\nenergy) state, given all its neighbours. The full conditional can be\nshown to be:\n\n\\begin{equation}\n  p(y\\s = 1 | \\mathbf{y\\{-s}}, \\mathbf{\\theta}) =\n  \\textrm{sigm}(\\mathbf{w\\{s,:}}^T y\\{-s} + b\\_s)\n\\end{equation}\n\nPicking the most probable state amounts to using the rule \\\\(y\\_s^\\* = 1\\\\)\nif \\\\(\\sum\\{t} w\\{st}y\\t > b\\s\\\\) and \\\\(y\\_s^ = 0\\\\) otherwise.\n\nBoltzmann machines generalize the Hopfield/Ising model by including\nsome hidden nodes, which makes the model representationally more\npowerful. Inference in such models often uses Gibbs sampling, which is\na stochastic version of ICM.\n\nBinary Hopfield network {#binary-hopfield-network}\n\nOur convention in general will be that \\\\(w\\_{ij}\\\\) denotes the connection\nfrom neuron \\\\(j\\\\) to neuron \\\\(i\\\\).\n\nA Hopfield network consists of \\\\(I\\\\) neurons. They are fully connected\nthrough symmetric, bidirectional connections with weights \\\\(w\\_{ij} =\nw\\{ji}\\\\). \\\\(w\\{ii} = 0\\\\) for all \\\\(i\\\\). The activity of neuron \\\\(i\\\\) is\ndenoted by \\\\(x\\_i\\\\).\n\na Hopfield network's activity rule is for each neuron to update its\nstate as if it were a single neuron with the threshold activation\nfunction:\n\n\\begin{equation}\n  x(a) = \\Theta(a) = \\begin{cases}\n    1 & a \\ge 0 \\\\\\\\\\\\\n    -1 & a < 0\n  \\end{cases}\n\\end{equation}\n\nSince there is feedback in a Hopfield network (every neurons' output\nis an input to all other neurons), we will have to specify an order\nfor the updates to occur. These updates may be synchronous or\nasynchronous.\n\nFor synchronous updates, all neurons compute their activations:\n\n\\begin{equation}\n  a\\i = \\sum\\{j} w\\{ij} x\\j\n\\end{equation}\n\nand update their states simultaneously to \\\\(x\\i = \\Theta(a\\i)\\\\).\n\nFor asynchronous updates, one neuron at a time computes its\nactivations and updates its state. The sequence of updates may be a\nfixed or random sequence.\n\nThe learning rule is intended to make a set of desired memories \\\\(\\\\{\n\\mathbf{x^{(n)}}\\\\}\\\\) be stabel states of the Hopfield network's\nactivity rule. Each memory is a binary pattern, with \\\\(x\\_i \\in \\\\{ -1,\n+1\\\\}\\\\).\n\nThe Hebb's rule of learning corresponds to the sum of outer products:\n\n\\begin{equation}\n  w\\{ij} = \\eta \\sum\\{n} x\\i^{(n)}x\\j^{(n)}\n\\end{equation}\n\nwhere \\\\(\\eta\\\\) is a constant, commonly chosen to be \\\\(\\frac{1}{n}\\\\).\n",
        "tags": [
            "machine-learning"
        ]
    },
    {
        "uri": "/zettels/how_to_know_kidd",
        "title": "How To Know - Chelsea Kidd",
        "content": "\nLink: Celeste Kidd | How to Know · SlidesLive\n\nWhy are we curious about some things but not others? What are the core\ncognitive systems that people use to guide their learning about the\nworld?\n\n5 Key things in ML that we should know about humans {#5-key-things-in-ml-that-we-should-know-about-humans}\n\nHumans continuously form beliefs {#humans-continuously-form-beliefs}\n\nUpon seeing a bowl, we update our belief about what a bowl is. This\nalso influences what we wish to sample next. Infants look away when\nevents are predictable, but also when they are surprising!\n\n{{}}\n\n**When present data to learning algorithms, we should show data that is\nat ideal region on the \"surprising\" spectrum.**\n\nCertainty diminishes interest {#certainty-diminishes-interest}\n\nWe are not curious about things that we already know. If we think we\nknow an answer, when presented with the objective truth, we are still\nunlikely to change our beliefs.\n\nCertainty is driven by feedback {#certainty-is-driven-by-feedback}\n\nWhen feedback isn't available, our estimates about our certainty may\nnot be accurate. When feedback is readily available, we can sometimes\nbe certain, when we should not be.\n\nLess feedback may encourage overconfidence {#less-feedback-may-encourage-overconfidence}\n\nHumans form beliefs quickly {#humans-form-beliefs-quickly}\n",
        "tags": []
    },
    {
        "uri": "/zettels/how_to_take_smart_notes",
        "title": "How To Take Smart Notes",
        "content": "\nauthor\n: Sonke Ahrens\n\ntags\n: Writing, Note-taking, Books, Productivity\n\nrecommended by\n: Conor White-Sullivan\n\nMotivations {#motivations}\n\nThere's a ton of books covering the art of writing, and very little\n    on note-taking.\nThese books don't cover the connections between note-taking and\n    writing well.\n\n> Notes aren't a record of my thinking process. They are my thinking\n> process. -- Richard Feynman\n\nThe mind is extremely reliant of external scaffolding:\n\n> Notes on paper, or on a computer screen ... do not make contemporary\n> physics or other kinds of intellectual endeavour easier, they make it\n> possible -- Neil Levy (Neuroethics and the Extended Mind)\n\nHence, use the Zettelkasten method. To get around this idea of\nnote-taking, it is important to understand that:\n\nNote sequences are meant for developing ideas, not storing them\nLinks and indices are helpful, but not central features\nThe workflow is streamlined to writing\n\nIn Zettelkasten, the most time-consuming portion is determining _the\norder_ for the notes in which to write about.\n\nConcrete Changes {#concrete-changes}\n\nInstead of highlighting passages, manually create notes of the\n    ideas you get as you read. These notes should be relevant to the\n    contexts important to you, not just related to the book you read.\n    (Nat Eliason, 2020)\nAlways reference the source. Cite, or indicate the page number.\nWhen copying notes over, try to make the ideas standalone.\nWhen filing notes, think instead about: _in which context will I\n    want to stumble on it again?_ Use tags for this.\n\nResources {#resources}\n\nSönke Ahrens - How to take smart notes on Vimeo\nHow to Take Smart Notes: A Step-by-Step Guide - Nat Eliason\n\nBibliography\nEliason, N. (2020). How to take smart notes: a step-by-step guide - nat eliason. Retrieved from https://www.nateliason.com/blog/smart-notes. Online; accessed 14 February 2020. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/how_to_write_a_technical_paper",
        "title": "How To Write a Technical Paper",
        "content": "\ntags\n: Writing, Papers\n\npaper\n: (James Whiteside, 2017)\n\nAbstract {#abstract}\n\nThe abstract is what the reader reads to determine if the paper is\nworthy of merit for further study. It should contain:\n\nBrief introduction describing the discipline\nA concise statement of the problem\nA brief explanation of the solution and its key ideas\nA brief description of the results obtained and their impacts\n\nIntroduction {#introduction}\n\nGives background on and motivation for research, establishing its\nimportance. Consider technological trends, recent promising\ndevelopments.\n\nThe summary should include a problem description, which is more\ndetailed than the abstract. Close with a description of the paper\noutline, what sections it contains and what the reader will find in each.\n\nRelated Work {#related-work}\n\nShows what has happened in the field\nProvides a critique of the approaches in the literature\n\nSystem Model {#system-model}\n\nDescribes all hypotheses and assumptions of the environment on which\nthe problem will be stated. Realize both explicit and implicit\nassumptions.\n\nProblem Statement {#problem-statement}\n\nState the problem clearly, being as exact as possible.\n\nSolution {#solution}\n\nFirst, provide an overview of the solution. Give rationale, explain\nconcepts and mechanisms. Next, provide a detailed description of the\nsolution and its functionality. Figures are often helpful.\n\nAnalysis {#analysis}\n\nPerform qualitative and quantitative analysis on the solution. This\nincludes proof of correctness, and performance analysis.\n\nSimulation and Experimentation {#simulation-and-experimentation}\n\nEstablish that the experimental setup is statistically stable. Explain\neach experiment and caption each figure appropriately.\n\nConclusion {#conclusion}\n\nThe conclusion sections elaborates on the impacts of using your\napproach. It also states limitations or disadvantages of your\nsolution, and enables you to provide directions for future research in\nthe field.\n\nBibliography\nWhiteside, J. D., How to write a technical paper, In ,  (pp. ) (2017). : . ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/http",
        "title": "HTTP",
        "content": "\ntags\n: §web\\_dev\n\nGET Requests and Request Body {#get-requests-and-request-body}\n\n> Yes. In other words, any HTTP request message is allowed to contain a\n> message body, and thus must parse messages with that in mind. Server\n> semantics for GET, however, are restricted such that a body, if any,\n> has no semantic meaning to the request. The requirements on parsing\n> are separate from the requirements on method semantics.\n>\n> So, yes, you can send a body with GET, and no, it is never useful to\n> do so.\n>\n> This is part of the layered design of HTTP/1.1 that will become clear\n> again once the spec is partitioned (work in progress).\n>\n> -- Roy Fielding\n\nHaving servers return content based on the value of the request body\nin the GET request is a bad practice.\n",
        "tags": []
    },
    {
        "uri": "/zettels/human_behaviour_as_optimal_control",
        "title": "Human Behaviour As Optimal Control",
        "content": "\nDoes Reinforcement Learning ⭐ and Optimal Control provide a\nreasonable model of human behaviour? Is there a better explanation?\n\nFor example, is the gait of a human being optimizing for a certain\nobjective?\n\nIf we assume that the policy maximizes the expectation of total\nreward under some given dynamics, can we learn the human reward\nfunction from the data?\n\nWe can model sub-optimal behaviour using techniques from graphical\nmodels (Control As Inference), and use this framework to derive new\n\"soft\" RL algorithms (keywords: soft optimality).\n\nResources {#resources}\n\nCS285 Fa19 10/16/19 - YouTube\n",
        "tags": []
    },
    {
        "uri": "/zettels/i-diagrams",
        "title": "I-Diagrams",
        "content": "\ntags\n: §information\\_theory\n\nIf we view random variables as sets, different information theory\nconcepts can be represented using an information diagram.\n\n{{}}\n\nThis can be used to visualize more complex expressions.\n",
        "tags": []
    },
    {
        "uri": "/zettels/ideas",
        "title": "Ideas",
        "content": "\nRealtime plotting of safe bike routes given traffic data {#realtime-plotting-of-safe-bike-routes-given-traffic-data}\n\nCANCELLED App to enable deep focus, by blocking websites temporarily {#app-to-enable-deep-focus-by-blocking-websites-temporarily}\n\nState \"CANCELLED\"  from              [2017-01-14 Sat 10:29]\nNote taken on [2017-01-14 Sat 10:29]\n    Solved by Leechblock in Firefox\n\nOrg-mode as a service {#org-mode-as-a-service}\n\nNUS Street view {#nus-street-view}\n",
        "tags": []
    },
    {
        "uri": "/zettels/imitation_learning",
        "title": "Imitation Learning",
        "content": "\nBehavioural Cloning {#behavioural-cloning}\n\nBehavioural cloning is a fancy name for supervised learning. We\ncollect tuples of actions and observations from demonstrations, and\nused supervised learning to learn a policy \\\\(\\pi\\{\\theta}(a\\t | o\\_t)\\\\).\n\nThe problem with behavioural cloning is that the errors accumulate,\nand the state trajectory will change dramatically. When we evaluate\nthe algorithm, can we make \\\\(p\\{data}(o\\t) = p\\{\\pi\\\\theta}(o\\_t)\\\\)?\n\nDAgger: Dataset Aggregation {#dagger-dataset-aggregation}\n\nGoal: collect training data from \\\\(p\\{\\theta\\\\pi}(o\\t)\\\\) instead of \\\\(p\\{data}(o\\_t)\\\\)\nhow? run \\\\(\\pi\\\\theta (a\\t | o\\t)\\\\), but need labels \\\\(a\\t\\\\)!\n\ntrain \\\\(\\pi\\\\theta(a\\t | o\\_t)\\\\) from human data \\\\(\\mathcal{D}\\\\)\nrun \\\\(\\pi\\\\theta(a\\t|o\\t)\\\\) to get dataset \\\\(\\mathcal{D\\\\pi}\\\\)\nAsk human to label \\\\(D\\\\pi\\\\) with actions \\\\(a\\t\\\\)\nAggregate: \\\\(\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\mathcal{D}\\_\\pi\\\\)\n\nProblem: have to ask humans to label large datasets iteratively, and\n    can be unnatural (resulting in bad labels)\nBehavioural cloning may still work when we model the expert very\n    accurately (no distributional \"drift\")\n\nWhy might we fail to fit the expert? {#why-might-we-fail-to-fit-the-expert}\n\nnon-Markovian behaviour\n    Our policy assumes that the action depends only on the current\n        observation.\n    Perhaps a better model is to account for all observations.\n    Problem: history exacerbates causal confusion\n        (Haan et al., 2019)\n\nMultimodal behaviour\n    Solutions:\n        output mixture of Gaussians (easy to implement, works well in\n            practice)\n        Latent Variable models (additional latent variable as part of\n            input)\n        Autoregressive discretization\n\n{{}}\n\nWhat's the problem with imitation learning? {#what-s-the-problem-with-imitation-learning}\n\nHumans need to provide data, which is typically finite. Deep models\n    typically require large amounts of data.\nHuman are not good at providing some kinds of actions\nHumans can learn autonomously (from experience)\n\nImitation Learning in the RL context {#imitation-learning-in-the-rl-context}\n\nReward function:\n\n\\begin{equation}\n  r(\\mathbf{s}, \\mathbf{a})=\\log p\\left(\\mathbf{a}=\\pi^{\\star}(\\mathbf{s}) | \\mathbf{s}\\right)\n\\end{equation}\n\nCost function:\n\n\\begin{equation}\n  c(\\mathbf{s}, \\mathbf{a})=\\left\\\\{\\begin{array}{l}{0 \\text { if } \\mathbf{a}=\\pi^{\\star}(\\mathbf{s})} \\\\ {1 \\text { otherwise }}\\end{array}\\right.\n\\end{equation}\n\nThe number of mistakes go up quadratically in the worst case:\n\nAssuming: \\\\(\\pi\\_{\\theta}\\left(\\mathbf{a} \\neq \\pi^{\\star}(\\mathbf{s}) | \\mathbf{s}\\right) \\leq \\epsilon\\\\)\n\n{{}}\n\nBibliography\nHaan, P. d., Jayaraman, D., & Levine, S., Causal confusion in imitation learning, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/importance_sampling",
        "title": "Importance Sampling",
        "content": "\nImportance sampling does not allow us to generate samples from \\\\(p(x)\\\\),\nbut allows us to estimate the expectation of a function \\\\(f(x)\\\\).\n\nSuppose we cannot sample from \\\\(p(x)\\\\)\n\n\\begin{aligned}\n  E\\_{x \\sim p(x)}[f(x)] &=\\int p(x) f(x) d x \\\\ &=\\int\n  \\frac{q(x)}{q(x)} p(x) f(x) d x \\\\ &=\\int q(x) \\frac{p(x)}{q(x)}\n  f(x) d x \\\\ &=E\\_{x \\sim q(x)}\\left[\\frac{p(x)}{q(x)} f(x)\\right]\n\\end{aligned}\n\n\\\\(R\\\\) samples are generated from \\\\(q(x)\\\\). Values of \\\\(x\\\\) where \\\\(q(x)\\\\) is\ngreater than that of \\\\(p(x)\\\\) are over-represented, and vice-versa.\nHence, importance weights are introduced:\n\n\\begin{equation}\n  w\\_r \\equiv \\frac{p^\\star(x^{( r)})}{q^\\star(x^{( r)})}\n\\end{equation}\n\nwhich adjust the importance of each point in the estimator:\n\n\\begin{equation}\n  \\hat{\\Phi} = \\frac{\\sum\\{r} w\\r f(x^{( r)})}{\\sum\\r w\\r}\n\\end{equation}\n\nDifficulties {#difficulties}\n\nIt difficult to estimate how reliable the estimator \\\\(\\hat{\\phi}\\\\) is.\nIn the case where the proposal density \\\\(q(x)\\\\) is small in a region\n    where \\\\(|f(x)p^\\star(x)|\\\\) is large, the estimate would be greatly\n    wrong, and it is possible that even after many samples are\n    generated, none fall into this region.\nIn high-dimensional problems:\n    It will take a long time to acquire samples that lie in the\n        typical set of \\\\(p\\\\), unless \\\\(q\\\\) is a good approximation of \\\\(p\\\\).\n    Even if able to obtain samples in the typical set, the weights\n        associated with these samples are likely to vary by large\n        factors because probabilities of points, despite being in the\n        typical set, still differ by factors of order\n        \\\\(\\mathrm{exp}(\\sqrt{N})\\\\), where \\\\(N\\\\) is the number of dimensions.\n",
        "tags": []
    },
    {
        "uri": "/zettels/information_bottleneck_dnn",
        "title": "Information Bottleneck in Deep Neural Networks",
        "content": "\nThe information bottleneck theory was recently used to study Deep\nNeural Networks. Shwartz-Ziv and Tishby proposed that the information\nbottleneck expresses the tradeoff between the mutual information\nmeasures \\\\(I(X,T)\\\\) and \\\\(I(T,Y)\\\\)\n(Shwartz-Ziv \\& Tishby, 2017).\n\n\\\\(I(X,T)\\\\) and \\\\(I(X,Y)\\\\) quantifies the amount of information that the\nlayer contains about the input and output respectively.\n\nKey Findings from (Shwartz-Ziv \\& Tishby, 2017) {#key-findings-from}\n\nMost of the training epochs are spent on compression of the input\n    to efficient representation, and not on fitting the training labels\nTh representation compression phase begins when training errors\n    become small, and SGD epochs change from a fast drift to a smaller\n    training error into a stochastic relaxation or random diffusion,\n    constrained by the training error value.\nThe converged layers lie on or very close to the Information\n    Botteneck (IB) theoretical bound, and the maps from the input to\n    any hidden layer and from this hidden layer to the output satisfy\n    the IB self-consistent equations.\nThe generalization-through-noise mechanism is unique to DNNs\nThe training time is dramatically reduced when adding more layers,\n    hence the main advantage of adding layers is computational??\n\nTL;DR: SGD has 2 phases: ERM, and representation compression. The\nlatter phase is an explanation for the lack of overfitting in DL.\n\nBibliography\nShwartz-Ziv, R., & Tishby, N., Opening the black box of deep neural networks via information, CoRR, (),  (2017).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/information_filter",
        "title": "Information Filter",
        "content": "\ntags\n: Gaussian Filter, Bayes Filter\n\nKey Idea {#key-idea}\n\nThe multi-variate Gaussians are represented in their canonical\nrepresentation, by precision/information matrix \\\\(\\Omega\\\\) and the\ninformation vector \\\\(\\xi\\\\), where \\\\(\\Omega = \\Sigma^{-1}\\\\), and \\\\(\\xi =\n\\Sigma^{-1} \\mu\\\\).\n\nThe Gaussian can be redefined as follows:\n\n\\begin{equation}\n  p(x) = \\eta \\text{exp} \\left\\\\{ - \\frac{1}{2} x^T \\Omega x + x^T \\xi \\right\\\\}\n\\end{equation}\n\nwhere \\\\(\\eta\\\\) has been redefined to subsume a constant. The reason they\nare called information matrix and vectors is because \\\\(- \\log p(x)\\\\) is\nquadratic in \\\\(\\Omega\\\\) and \\\\(\\xi\\\\).\n\nFor Gaussians, \\\\(\\Omega\\\\) is positive semi-definite, so \\\\(- \\log p(x)\\\\) is\na quadratic distance function with mean \\\\(\\mu = \\Omega^{-1} \\xi\\\\). The\nmatrix \\\\(\\Omega\\\\) determines the rate at which the distance function\ninccreases is different dimensions of the variable \\\\(x\\\\). A quadratic\ndistance that is weighted by a matrix \\\\(\\Omega\\\\) is called Mahalanobis\ndistance.\n\nAlgorithm {#algorithm}\n\n\\begin{algorithm}\n  \\caption{Information Filter}\n  \\label{information\\_filter}\n  \\begin{algorithmic}[1]\n    \\Procedure{InformationFilter}{$\\xi\\{t-1}, \\Omega\\{t-1}, \\mu\\t, \\z\\t$}\n    \\State $\\overline{\\Omega}\\t = (A\\t \\Omega\\{t-1}^{-1} A\\t^T + R\\_t)^{-1}$\n    \\State $\\overline{\\xi}\\t = \\overline{\\Omega\\t}\\left( A\\_t\n      \\Omega\\{t-1}^{-1} \\xi\\{t-1} + B\\t u\\t  \\right)$\n    \\State $\\Omega\\t = C\\t^T Q\\t^{-1} C\\t + \\overline{\\Omega}\\_t$\n    \\State $\\xi\\t = C\\t^T Q\\t^{-1}z\\t + \\overline{\\xi}\\_t$\n    \\State \\Return $\\xi\\t, \\Omega\\t$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\nPros {#pros}\n\nRepresenting global uncertainty is simple: \\\\(\\Omega = 0\\\\). With\n    moments, global uncertainty amounts to covariance of infinite magnitude.\nMore numerically stable for many applications.\nNatural fit for multi-robot problems, where sensor data is\n    collected decentrally. Information integration is additive and\n    achieved by summing information from multiple robots. This is\n    because the canonical parameters represent a probability in log\n    form.\nInformation matrix may be sparse, lending itself to algorithms that\n    are computationally efficient.\n\nCons {#cons}\n\nThe update step requires the recovery of a state estimate,\n    inverting the information matrix. Matrix inversion is\n    computationally expensive.\n",
        "tags": []
    },
    {
        "uri": "/zettels/information_theoretic_rl",
        "title": "Information-Theoretic Reinforcement Learning",
        "content": "\nCan we learn without any reward function at all?\n\nIdentities {#identities}\n\nentropy\n: \\\\(\\mathcal{H}(p(x)) = - E\\_{x \\sim p(x)}[\\log p(x)]\\\\)\n\nmutual information\n: \\\\(\\mathcal{I}(x;y) = D\\_{KL}(p(x,y) || p(x)p(y))\\\\)\n\nInformation theoretic quantities in RL {#information-theoretic-quantities-in-rl}\n\n\\\\(\\pi(s)\\\\)\n: state marginal distribution of policy \\\\(\\pi\\\\)\n\n\\\\(\\mathcal{H}(\\pi(s))\\\\)\n: state marginal entropy of policy \\\\(\\pi\\\\)\n\nempowerment\n: \\\\(\\mathcal{I}(s\\{t+1};a\\t) = \\mathcal{H}(s\\{t+1}) - \\mathcal{H}(s\\{t+1}|a\\_t)\\\\)\n\nPapers {#papers}\n\nSkew-Fit (Pong et al., 2019)\nDiversity is All your Need (Eysenbach et al., 2018)\n\nBibliography\nPong, V. H., Dalal, M., Lin, S., Nair, A., Bahl, S., & Levine, S., Skew-fit: state-covering self-supervised reinforcement learning, CoRR, (),  (2019).  ↩\n\nEysenbach, B., Gupta, A., Ibarz, J., & Levine, S., Diversity is all you need: learning skills without a reward function, CoRR, (),  (2018).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/information_theory",
        "title": "Information Theory",
        "content": "\ntags\n: Machine Learning, Statistics\n\nIntroduction {#introduction}\n\nHow can we achieve perfect communication over an imperfect noisy communication channel? {#how-can-we-achieve-perfect-communication-over-an-imperfect-noisy-communication-channel}\n\nThe physical solution is to improve the characteristics of the\ncommunication channel to reduce its error probability. For example, we\ncan use more reliable components in the communication device's\ncircuitry.\n\nInformation theory and coding theory offer an alternative: we accept\nthe given noisy channel as it is, and add communication systems to it\nto detect and correct errors introduced by the channel.\n\nAn encoder encodes the source message s into a transmitted message\nt, adding redundancy to the original message in some way. The\nchannel adds noise to the transmitted message, yielding a received\nmessage r. The decoder uses the known redundancy introduced by the\nencoding system to infer both the original signal s and the added\nnoise.\n\nInformation theory is concerned of the theoretical limitations and\npotentials of these systems. Coding theory is concerned with the\ncreation of practical encoding and decoding systems.\n\nError-correcting codes for binary symmetric channels {#error-correcting-codes-for-binary-symmetric-channels}\n\nRepetition codes\n\n    Key idea: repeat every bit of the message a prearranged number of\n    times, and pick the bit with the majority vote.\n\n    We can describe the channel as adding a sparse noise vector n to the\n    transmitted vector, adding in a modulo 2 arithmetic.\n\n    One can show that this algorithm is optimal by considering the maximum\n    likelihood function of s.\n\n    The repetition code \\\\(R\\_3\\\\) (repeat 3 times) has reduced the probability\n    of error, but has also reduced the rate of information by a factor\n    of 3.\n\nBlock codes - the (7,4) Hamming Code\n\n    Key idea: add redundancy to blocks of data instead of encoding one bit\n    at a time.\n\n    A block code is a rule for converting a sequence of source bits s,\n    of length K, into  transmitted sequence t of length N > K bits.\n    The Hamming code transmits N=7 bits for every K=4 bits.\n\n    Because the Hamming code is a linear code, it can be written compactly\n    as a matrix:\n\n    \\begin{equation\\*}\n      \\text{transmitted} = G^T \\text{source}\n    \\end{equation\\*}\n\n    where \\\\(G\\\\) is the generator matrix of the code.\n\n    Decoding for linear codes: syndrome decoding\n\n        The decoding problem for linear codes can also be described in terms\n        of matrices. We evaluate 3 parity-check bits for the received bits\n        \\\\(r\\1r\\2r\\3r\\4\\\\), and see whether they match the three received bits\n        \\\\(r\\5r\\6r\\_7\\\\). The differences (mod 2) between these 2 triplets are\n        called the syndrome of the received vector. If the syndrome is 0,\n        then the received vector is a code word, and the most probable\n        decoding is given by reading its first four bits.\n\n        \\begin{align\\*}\n          G^T &=\n          \\begin{bmatrix}\n            I\\_4 \\\\\\\\\\\\\n            P\n          \\end{bmatrix}, \\\\\\\\\\\\\n          H &=\n              \\begin{bmatrix}\n                -P & I\\_3\n              \\end{bmatrix}\n                     =\n              \\begin{bmatrix}\n                P & I\\_3\n              \\end{bmatrix}\n                    =\n              \\begin{bmatrix}\n            1 & 1 & 1 & 0 & 1 & 0 & 0 \\\\\\\\\\\\\n            0 & 1 & 1 & 1 & 0 & 1 & 0 \\\\\\\\\\\\\n            1 & 0 & 1 & 1 & 0 & 0 & 1\n            \\end{bmatrix}\n        \\end{align\\*}\n\nWhat performance can the best codes achieve?\n\n    We consider the (R, p_b) plane, where \\\\(R\\\\) is the rate,\n    and \\\\(p\\_b\\\\) is the decoded bit-error probability, Claude Shannon proved that the boundary between achievable and\n    non-achievable points meets the \\\\(R\\\\) axis at a non-zero value \\\\(R = C\\\\).\n    For any channel, there exist codes that make it possible to\n    communicate with arbitrarily small probability of error $p\\_b= at\n    non-zero rates. This theorem is called the _noisy-channel coding\n    theorem_.\n\n    The maximum rate at which communication is possible with arbitrarily\n    small \\\\(p\\_b\\\\) is called the capacity of the channel.\n\n    \\begin{equation\\*}\n      C(f) = 1 - H\\2(f) = 1 - \\left[f\\log\\2\\frac{1}{f} + (1-f)\\log\\_2\\frac{1}{1-f}\\right]\n    \\end{equation\\*}\n\nMeasuring Information Content {#measuring-information-content}\n\nWe view information content as the \"degree of surprise\" on learning\nthe value of \\\\(x\\\\), for some random variable \\\\(x\\\\). This content will\ntherefore depend on \\\\(p(x)\\\\), and we're looking for a monotonic function\n\\\\(h(x)\\\\) that expresses information content.\n\nWe would also like some desirable properties from our function \\\\(h(x)\\\\):\n\\\\(h(x, y) = h(x) + h(y)\\\\) if random variables \\\\(x\\\\) and \\\\(y\\\\) are\nstatistically independent, since the information gained from the\nrealization of both random variables must be additive. Since \\\\(p(x, y) =\np(x)p(y)\\\\), it's easy to see that \\\\(h(\\cdot)\\\\) must be given by the\nlogarithm of \\\\(h(x)\\\\). Thus, we have:\n\n\\begin{equation} \\label{eqn:inf\\_content}\n  h(x) = -\\log\\_2 p(x)\n\\end{equation}\n\nThen the average information a random variable transmits in the\nprocess is obtained by taking the expectation of Eq. eqn:inf_content\nwith respect to the distribution \\\\(p(x)\\\\):\n\n\\begin{equation} \\label{eqn:entropy}\n H[x] = - \\sum\\{x} p(x) \\log\\2p(x)\n\\end{equation}\n\nSource Coding Theorem {#source-coding-theorem}\n\nWe can compress N outcomes from a source \\\\(X\\\\) into roughly \\\\(NH(X)\\\\)\nbits.\n\nThis is provable by counting the typical set.\n\nShannon's Noisy-Channel Coding Theorem {#shannon-s-noisy-channel-coding-theorem}\n\n> Information can be communicated over a noisy channel at a non-zero\n> rate with arbitrarily small error probability\n\nArticles {#articles}\n\nAndreas Kirsch | Better intuition for information theory\nVisual Information Theory -- colah's blog\n",
        "tags": []
    },
    {
        "uri": "/zettels/interval_estimation_bayesian",
        "title": "Interval Estimation in Bayesian Statistics",
        "content": "\ntags\n: §bayesian\\_statistics\n\nSuppose instead of §point\\estimation\\bayesian, we'd like to identify a\nregion that is likely to contain the true value of parameter \\\\(\\theta\\\\).\nIn Bayesian inference, this region is called the credible set, or\nthe Bayesian confidence interval.\n\nA \\\\(100(1-\\alpha)%\\\\) credible set for \\\\(\\theta\\\\) is subset \\\\(\\mathcal{C}\\\\)\nof \\\\(\\Theta\\\\) such that:\n\n\\begin{equation}\n  P(\\theta \\in \\mathcal{C} | y)=\\int\\_{\\mathcal{C}} p(\\theta | y) \\mathrm{d} \\theta \\geq 1-\\alpha\n\\end{equation}\n\nInterpreting credible sets is different in Bayesian statistics,\ncompared to frequentist confidence intervals.\n\nIn Bayesian statistics, the unknown parameters \\\\(\\theta\\\\) is regarded as\na random variable, and the interval is fixed once data is observed.\nThat is, we can make direct probabilistic statements like:\n\n> The probability that \\\\(\\theta\\\\) lies in \\\\(\\mathcal{C}\\\\) given observed\n> data \\\\(y\\\\) is \\\\((1-\\alpha)\\\\).\n\nIn frequentist statistics, \\\\(Y\\\\) is regarded as random, giving rise to a\nrandom interval which has probability \\\\((1-\\alpha)\\\\) of containing the\nfixed but unknown \\\\(\\theta\\\\). The corresponding statement is:\n\n> If we could recompute \\\\(\\mathcal{C}\\\\) for a large number of datasets\n> collected the same way, then about \\\\(100(1-\\alpha)%\\\\) of them will\n> contain the true value of \\\\(\\theta\\\\)\n\nAnother way to view this is that frequentist and Bayesian notions of\ncoverage describe pre- and post-experimental coverage respectively.\nResearchers have shown that Bayesian credible sets constructed via\nsome methods will also have almost the correct frequentist coverage.\n\nQuantile/equal-tails intervals {#quantile-equal-tails-intervals}\n\nWe find two numbers \\\\(\\theta\\{\\alpha / 2}\\theta\\{1-\\alpha / 2} | y\\right)=\\alpha / 2\n\\end{equation}\n\nThe \\\\(100(1-\\alpha)%\\\\) quantile-based CI is \\\\(\\left[\\theta\\_{\\alpha / 2},\n\\theta\\_{1-\\alpha / 2}\\right]\\\\).\n\n{{}}\n\nHighest Posterior Density (HPD) region {#highest-posterior-density--hpd--region}\n\nThe HPD credible set is defined as the set:\n\n\\begin{equation}\n  \\mathcal{C}=\\\\{\\theta \\in \\Theta: p(\\theta | y) \\geq k(\\alpha)\\\\}\n\\end{equation}\n\nwhere \\\\(k(\\alpha)\\\\) is the largest constant satisfying:\n\n\\begin{equation}\n  P(\\theta \\in \\mathcal{C} | y) \\geq 1-\\alpha\n\\end{equation}\n\nAll points in a HPD region have higher posterior density than points\noutside the region.\n\nTo visualize this, imagine drawing a horizontal line across the graph\nat the mode of the posterior distribution, and the pushing it down\nuntil the corresponding values on the \\\\(\\theta\\\\) axis contains the\nappropriate probability.\n\n{{}}\n\nComputing HPD requires numerical methods. HPD might not be an interval\nif the distribution is multimodal. Some packages like coda assumes\nthat the distribution is not severely multimodal.\n\nGenerally, the quantile-based CI will be equal to the HPD region if\nthe posterior is symmetric and uni-modal, but wider otherwise. For\nunimodal posterior densities, the HPD interval has the shortest length\nfor the same level of coverage.\n",
        "tags": []
    },
    {
        "uri": "/zettels/inverse_rl",
        "title": "Inverse Reinforcement Learning",
        "content": "\ntags\n: Reinforcement Learning ⭐\n\nStandard Imitation Learning copies actions performed by the expert,\nand do not reason about the outcomes of the actions. However, humans\ncopy the intent of the actions, which result in vastly different\nactions.\n\nWe want to learn the reward function because the reward function is\noften hard to specify.\n\nInverse Reinforcement Learning is about learning reward functions.\nThis problem is, however, ill-specified: **there are infinitely many\nreward functions that can explain the same behaviour**. Formally:\n\nInverse RL is the setting where we are given:\n\nstates \\\\(s \\in S\\\\), actions \\\\(a \\in A\\\\)\n(sometimes) transitions \\\\(p(s' | s, a)\\\\)\nsamples \\\\(\\\\{\\tau\\_i\\\\}\\\\) sampled from \\\\(\\pi^\\star (\\tau)\\\\)\n\nand we would like to learn \\\\(r\\_{\\psi}(s,a)\\\\), where \\\\(\\psi\\\\) are the\nparameters of the reward functions. A common choice is a linear reward\nfunction:\n\n\\begin{equation}\n  r\\\\psi (s,a) = \\sum\\{i} \\psi\\i f\\i(s,a) = \\psi^T f(s,a)\n\\end{equation}\n\nFeature Matching IRL {#feature-matching-irl}\n\nIdea: if features \\\\(f\\\\) are important, what if we match their\nexpectations? Let \\\\(\\pi^{r\\\\psi}\\\\) be the optimal policy for \\\\(r\\\\psi\\\\),\nthen we pick \\\\(\\psi\\\\) such that \\\\(E\\\\pi r\\\\psi [f(s,a)]= E\\_{\\pi^\\star}[f(s,a)]\\\\)\n\nWe can approximate the RHS using expert samples, and the LHS is the\nstate-action marginal under \\\\(\\pi^{r\\_\\psi}\\\\). This is still ambiguous,\nand a solution inspired from SVMs is to use the _maximum margin\nprinciple_:\n\n\\begin{equation}\n  \\mathrm{min}\\_\\psi \\frac{1}{2} |\\psi|^2 \\text{ such that } \\psi^T\n  E\\{\\pi^\\star}[f(s,a)] \\ge \\mathrm{max}\\{\\psi \\in \\Pi} \\psi^T\n  E\\_{\\pi}[f(s,a)] + D(\\pi, \\pi^\\star)\n\\end{equation}\n\nwhere \\\\(D\\\\) could be the difference in expectations.\n\nIssues with the maximum margin principle:\n\nMaximizing margin is an arbitrary choice\nNo clear model of sub-optimality\n\nMaximum likelihood learning {#maximum-likelihood-learning}\n\nThe IRL partition function is:\n\n\\begin{equation}\n  \\mathrm{max}\\{\\psi}\\frac{1}{N} \\sum\\{i=1}^{N} r\\\\psi (\\tau\\i) - \\log Z\n\\end{equation}\n\nwhere \\\\(Z\\\\) is the integral over all trajectories: \\\\(Z = \\int p(\\tau) \\mathrm{exp}(r\\_\\psi(\\tau))d\\tau\\\\)\n\n\\begin{equation}\n  \\nabla\\\\psi L = \\frac{1}{N}\\sum\\{i=1}^{N}\\nabla\\\\psi r\\\\psi(\\tau\\_i)\n  \\frac{1}{Z} \\int p(\\tau) \\mathrm{exp}(r\\\\psi(\\tau))\\nabla\\\\psi\n  r\\_\\psi(\\tau) d\\tau\n\\end{equation}\n\n\\begin{equation}\n  \\nabla\\\\psi L = E\\{\\tau \\sim \\pi^\\star (\\tau)} [\\nabla\\_\\psi\n  r\\\\psi(\\tau\\i)] - E\\{\\tau \\sim p(\\tau | \\mathcal{O}\\{1:T},\n    \\psi)}[\\nabla\\\\psi r\\\\psi (\\tau)]\n\\end{equation}\n\nfirst term is estimated with expert samples, and the second with the\nsoft optimal policy under current reward.\n\nMaxEntropy Inverse RL (Ziebart et al., 2008) {#maxentropy-inverse-rl}\n\nGiven \\\\(\\psi\\\\), compute backward message \\\\(\\beta(s\\t, a\\t)\\\\)\nGiven \\\\(\\psi\\\\), compute forward message \\\\(\\alpha(s\\_t)\\\\)\nCompute \\\\(\\mu\\t(s\\t, a\\t) \\propto \\beta(s\\t, a\\t) \\alpha(s\\t)\\\\)\nEvaluate:\n\n\\begin{equation}\n  \\nabla\\\\psi L = \\frac{1}{N}\\sum\\{i=1}^{N}\\sum\\{t=1}^{T} \\nabla\\\\psi\n  r\\\\psi (s\\{i,t},a\\{i,t}) - \\sum\\{t=1}^{T} \\int \\int\n  \\mu\\t(s\\t,a\\t)\\nabla\\\\psi r\\\\psi(s\\t, a\\t)ds\\t da\\_t\n\\end{equation}\n\n\\\\(\\psi \\leftarrow \\psi + \\eta \\nabla\\_\\psi L\\\\)\n\nIn the case where the reward function is linear, we can show that it optimizes\nto maximize entropy in the policy under the constraint that the\nexpectations of the rewards for the policy and the expert are equal.\n\nMaxEnt IRL requires:\n\nSolving for soft optimal policy in the inner loop\nEnumerating all state-action tuples for visitation frequency and\n    gradient\n\nSample-based Updates {#sample-based-updates}\n\nThis handles unknown dynamics, or large/continuous state-action\nspaces. This works under the assumption that we can sample from the\nenvironment.\n\n\\begin{equation}\n  \\nabla\\\\psi L \\approx \\frac{1}{N} \\sum\\{i=1}^{N} \\nabla\\\\psi r\\\\psi\n  (\\tau\\i) - \\frac{1}{M} \\sum\\{j=1}^{M} \\nabla\\\\psi r\\\\psi(\\tau\\_j)\n\\end{equation}\n\nWe learn \\\\(p(a\\t | s\\t, \\mathcal{O}\\_{1:T}, \\psi)\\\\) using any max-ent RL\nalgorithm like soft Q-learning, then run this policy to sample\n\\\\(\\tau\\_j\\\\). But this is expensive, so make a small improvement to\n\\\\(p(a\\t | s\\t, \\mathcal{O}\\_{1:T}, \\psi)\\\\) instead, and use importance\nsampling to account for the distribution mismatch. Each policy update\nw.r.t \\\\(r\\_\\psi\\\\) brings us closer to the target distribution.\n\nResources {#resources}\n\nCS285 Fa19 10/21/19 - YouTube\n\nPapers {#papers}\n\n(Abbeel \\& Ng, 2004)\n(Ratliff et al., 2006)\n\nBibliography\nZiebart, B., Maas, A., Bagnell, J., & Dey, A., Maximum Entropy Inverse Reinforcement Learning., In ,  (pp. 1433–1438) (2008). : . ↩\n\nAbbeel, P., & Ng, A. Y., Apprenticeship learning via inverse reinforcement learning, In , Proceedings of the twenty-first international conference on Machine learning (pp. 1) (2004). : . ↩\n\nRatliff, N. D., Bagnell, J. A., & Zinkevich, M. A., Maximum margin planning, In , Proceedings of the 23rd international conference on Machine learning (pp. 729–736) (2006). : . ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/investing_in_etfs",
        "title": "Investing In ETFs",
        "content": "\nIdentifying good ETFs (Joshua Giersch, Rich By Retirement) {#identifying-good-etfs--joshua-giersch-rich-by-retirement}\n\nGood ETFs satisfy the following conditions:\n\nLow fees (<0.3% per annum)\nPlenty of assets in the fund (above $100 million)\n\"Cash\" or \"physical\", not \"synthetic\"\n    This is because if the counterparty collapses, the synthetic ETF\n        will start tracking the bunch of miscellaneous stocks\n",
        "tags": []
    },
    {
        "uri": "/zettels/investment",
        "title": "Investment",
        "content": "\n§investing\\in\\etfs\n§portfolio\\_composition\n",
        "tags": []
    },
    {
        "uri": "/zettels/ios",
        "title": "iOS",
        "content": "\nSwift {#swift}\n\nThe Swift Programming Language book\n",
        "tags": []
    },
    {
        "uri": "/zettels/is1103",
        "title": "IS1103: Computing and Society",
        "content": "\nMorality {#morality}\n\nDirectives that guide our conduct as individuals\nSocial policies framed at the macrolevel\n\nMoral systems are evaluated against standards called principles.\nMorality is:\n\nPublic\n: Rules are public; Everyone is obligated to partake in a moral system\n\nInformal\n: Has no formal authoritative judges presiding over it\n\nImpartial\n: Moral rules are ideally designed to apply equitably to all participants\n\nRational\n: The system is based on principles of logical reason\n\nValues {#values}\n\nInstrumental\n: Provide external benefit (eg. computers etc)\n\nIntrinsic\n: Valued for their own sake (eg. life, happiness)\n\nEthical Theories {#ethical-theories}\n\nConsequence-based (Utilitarianism) {#consequence-based--utilitarianism}\n\nSome have argued that the primary goal of a moral system is to produce\ndesirable consequences or outcomes for its members. For these\nethicists, the consequences (i.e., the ends achieved) of actions and\npolicies provide the ultimate standard against which moral decisions\nmust be evaluated\n\nUtilitarianism {#utilitarianism}\n\n> An individual act (X) or a social policy (Y) is morally permissible if\n> the consequences that result from (X) or (Y) produce the greatest\n> amount of good for the greatest number of persons affected by the act\n> or policy.\n\nSocial utility is superior to alternative criteria for evaluating\n    moral systems.\nSocial utility can be measured by the amount of happiness\n    produced.\n\nAssumes:\n\nAll people desire happiness.\nHappiness is an intrinsic good that is desired for its own sake.\n\nAct Utilitarianism:\n\n> An act, X, is morally permissible if the consequences produced by\n> doing X result in the greatest good for the greatest number of persons\n> affected by Act X.\n\nRule Utilitarianism:\n\n> An act, X, is morally permissible if the consequences of following the\n> general rule, Y, of which act X is an instance, would bring about the\n> greatest good for the greatest number.\n\nCritic against Utilitarianism:\n\nMorality is basically tied to the production of happiness or pleasure.\nMorality can ultimately be decided by consequences (of either acts\n    or policies).\n\nCritics of utilitarianism argue that morality can be grounded neither\nin consequences\n\nDuty-based (Deontology) {#duty-based--deontology}\n\nKant points out that, in some instances, performing our duties may\nresult in our being unhappy and may not necessarily lead to\nconsequences that are considered desirable.\n\nKant bases deontology on two premises:\n\nOur nature as rational creatures\n    Rationality is what separates us from other kinds of creatures. If\n        our primary nature were to merely seek happiness or pleasure, we\n        would be indistinguishable from other creatures.\n    Rational nature reveals certain duties or obligations to each\n        other as \"rational beings\"\nHuman beings are ends-in-themselves\n    A genuinely moral system would never permit some humans to be\n        treated as means to the ends of others\n    We have a duty to treat fellow humans as ends; each individual\n        has the same moral worth\n\nCategorical Imperative:\n\n> Act always on that maxim or principle (or rule) that can be\n> universally binding, without exception, for all human beings.\n\nThis forms a system of universality and impartiality. The objective\nrule to be followed—that is, the litmus test for determining when an\naction will have moral worth—is whether the act complies with the\ncategorical imperative, and whether it is universal and impartial.\n\nAct Deontology:\n\nRoss argues that when two or more moral duties\nclash, we have to look at individual situations in order to determine\nwhich duty will override another.\n\nRoss believes that we have certain prima facie (or self-evident)\nduties, which, all things being equal, we must follow. He provides a\nlist of prima facie duties such as honesty, benevolence, justice, and\nso forth.\n\nReflect on the competing prima facie duties.\nWeigh the evidence at hand to determine which course of action\n    would be required in a particular circumstance.\n\nContract-based {#contract-based}\n\nIn his classic work Leviathan, Hobbes describes an original\n\"premoral\" state that he calls the \"state of nature.\" It is premoral\nbecause there are no moral (or legal) rules yet in existence. In this\nstate, each individual is free to act in ways that satisfy his or her\nown natural desires. According to Hobbes, our natural (or physical)\nconstitution is such that in the state of nature we act in ways that\nwill enable us to satisfy our desires (or appetites) and to avoid what\nHobbes calls our \"aversions.\"\n\nHobbes believes that we are willing to surrender some of our\n\"absolute\" freedoms to a sovereign. In return, we receive many\nbenefits, including a system of rules and laws that are designed and\nenforced to protect individuals from being harmed by other members of\nthe system.\n\nWe see that it is in our individual self-interest to develop a moral\nsystem with rules.\n\nCriticisms {#criticisms}\n\nSome critics, such as Pojman (2006), point out that contract-based\ntheories provide the foundation for only a minimalist morality. They\nare minimalist in the sense that we are obligated to behave morally\nonly where an explicit or formal contract exists. So if I have no\nexpress contract with you, or if a country such as the United States\nhas no explicit contract with a developing nation, there is no moral\nobligation for me to help you or for the United States to come to the\naid of that developing nation.\n\nRights-Based {#rights-based}\n\nhumans possess some natural rights\nTwo kinds of legal rights: positive rights and negative rights.\n    Having a negative right to something simply means that one has the\n    right not to be interfered with in carrying out the privileges\n    associated with that right.\nPositive rights more rare and harder to justify.\n\nCharacter-based {#character-based}\n\nBecause virtue ethics focuses primarily on character development and\nmoral education, it does not need to rely on a system of formal\nrules.\n\nCharacter-based ethical systems would most likely flourish in cultures\nwhere the emphasis placed on community life is stronger than that\naccorded to the role of individuals themselves.\n\n| Type of Theory                  | Advantages                                         | Disadvantages                                                 |\n|---------------------------------|----------------------------------------------------|---------------------------------------------------------------|\n| Consequence-based (Utilitarian) | Stresses promotion of happiness and utility        | Ignores concerns of justice for the minority population       |\n| Duty-based (deontology)         | Stresses the role of duty and respect for persons  | Underestimates the importance of happiness and social utility |\n| Contract-based (rights)         | Provides a motivation for morality                 | Ofers only a minimal morality                                 |\n| Character-based (virtue)        | Stresses character development and moral education | Depends on homogeneous standards for morality                 |\n\nMoor's Just Consequentialist Framework {#moor-s-just-consequentialist-framework}\n\nDeliberate over various policies from an impartial point of view to\n    determine whether they meet the criteria for being ethical\n    policies. A policy is ethical, if it\n    does not cause any unnecessary harms to individuals and groups,\n        and\n    supports individual rights, the fulfilling of duties, etc.\nSelect the best policy from the set of just policies arrived at in\n    the deliberation stage by ranking ethical policies in terms of\n    benefits and (justifiable) harms. In doing this, be sure to:\n    weigh carefully between the good consequences and bad\n        consequences in the ethical policies, and\n    distinguish between disagreements about facts and disagreements\n        about principles and values, when deciding which particular\n        ethical policy should be adopted. (Knowledge about the facts\n        surrounding a particular case should inform the decision-making\n        process.)\n\nProfessional Ethics {#professional-ethics}\n\nProfessionals are experts in a field, which provides them an advantage\nover the lay person and that professional’s work has the potential to\nimpact—either positively or negatively—the general public at large.\n\nBroadly speaking, a computer/IT professional is anyone employed in the\ncomputing and IT fields—from software and hardware engineers, to\nspecialists such as support person- nel, network administrators, and\ncomputer repair technicians.\n\nAs IT professionals we have significant opportunities to:\n\ndo good or cause harm,\nenable others to do good or cause harm,\ninfluence others to do good or cause harm.\n\nMoral Responsibility\n: determined by looking at causality and\n    intent. X is responsible for Y if X caused Y, regardless of\n    intention of outcome, or regardless of the outcome of an\n    intention.\n\nLegal Liability\n: usually a legal concept, determines compensation\n    for harmful consequences, rather than blame. May be legally\n    liable, though not morally responsible\n\nAccountability\n: Broader concept than liability that finds\n    answerable — to superiors, authorities, or public.\n    Helps because responsibility is hard to pinpoint\n    to individuals in large software developments.\n\nWhistleblowing {#whistleblowing}\n\nMorally permitted to blow the whistle:\n\nThe product will do \"serious and considerable harm\" to the public.\nThe engineer(s) have reported the \"serious threat\" to their\n    immediate supervisor.\nThe engineer(s) have \"exhausted the internal procedures and\n    possibilities\" within the company, including going to the board of\n    directors, having received no support from their immediate\n    supervisor.\n\nTo have a strict moral obligation to blow the whistle, De George\n   believes that two additional conditions must be satisfied:\n\nThe engineer(s) have \"accessible, documented evidence that would\n    convince a reasonable, impartial, observer that one’s view of the\n    situation is correct.\"\nThe engineer(s) have \"good reasons to believe that by going public\n    the necessary changes will be brought about.\"\n\nPrivacy {#privacy}\n\nConsider the impact that changes involving this technology have had on\nprivacy with respect to the:\n\namount of personal information that can be collect,\nspeed at which personal information can be transmitted,\nduration of time that the information can be retained,\nkind of information that can be acquired and exchanged.\n\nDefinitions {#definitions}\n\nAccessibility privacy\n: Privacy is defined as one’s physically\n    being let alone, or being free from intrusion into one’s physical\n    space.\n\nDecisional privacy\n: Privacy is defined as freedom from\n    interference in one’s choices and decisions.\n\nInformational privacy\n: Privacy is defined as control over the flow\n    of one’s personal information, including the transfer and\n    exchange of that information.\n\nMoor's Account of Privacy {#moor-s-account-of-privacy}\n\n> An individual [has] privacy in a situation with regard to others if\n> and only if in that situation the individual [is] protected from\n> intrusion, interference, and information access by others.\n\nNaturally private vs normatively private {#naturally-private-vs-normatively-private}\n\nIn a naturally private situation, individuals are protected from\naccess and interference from others by natural means, for example,\nphysical boundaries such as those one enjoys while hiking alone in the\nwoods. In this case, privacy can be lost but not violated, because\nthere are no norms—conventional, legal, or ethical—according to which\none has a right, or even an expectation, to be protected. In a\nnormatively private situation, on the other hand, individuals are\nprotected by conventional norms (e.g., formal laws and informal\npolicies) because they involve certain kinds of zones or contexts that\nwe have determined to need normative protection.\n\nContextual Integrity {#contextual-integrity}\n\nNissenbaum’s privacy framework requires that the processes used in\ngathering and disseminating information (a) are \"appropriate to a\nparticular context\" and (b) comply with norms that govern the flow of\npersonal information in a given context. She refers to these two types\nof informational norms as follows:\n\nNorms of appropriateness.\nNorms of distribution.\n\nWhereas norms of appropriateness determine whether a given type of\npersonal information is either appropriate or inappropriate to divulge\nwithin a particular context, norms of distribution restrict or limit\nthe flow of information within and across contexts. When either norm\nhas been \"breached,\" a violation of privacy occurs; conversely, the\ncontextual integrity of the flow of personal information is maintained\nwhen both kinds of norms are \"respected.\"\n\nThe Value of Privacy {#the-value-of-privacy}\n\nFried suggests that unlike most instrumental values that are simply\none means among others for achieving a desired end, privacy is also\nessential, that is, necessary to achieve some important human ends,\nsuch as trust and friendship. We tend to associate intrinsic values\nwith necessary conditions and instrumental values with contingent, or\nnonnecessary conditions; so while privacy is instrumental in that it\nis a means to certain human ends, Fried argues that it is also a\nnecessary condition for achieving those ends.\n\nMoor believes that privacy is an articulation, or \"expression\" of the\n\"core value\" security, which in turn is essential across cultures, for\nhuman flourishing.\n\nBased on the insights of DeCew and others, one might infer that\nprivacy is a value that simply benefits individuals. However, some\nauthors have pointed out the social value that privacy also provides,\nnoting that privacy is essential for democracy.\n\nSecurity {#security}\n\n| Type    | What                                                                                    |\n|---------|-----------------------------------------------------------------------------------------|\n| Data    | Securing data that resides in computer databases; transmitted between computer systems  |\n| System  | Securing hardware and operating system resources; application software and programs     |\n| Network | Securing the infrastructure of privately owned networks; infrastructure of the Internet |\n\nData security\n: Concerned with vulnerabilities pertaining to\n    unauthorized access to data, as well as with\n    threats to the confidentiality, integrity, and\n    availability of data that resides in computer\n    storage devices or is exchanged between computer\n    systems. Data can either be sensitive, or proprietary,\n    or both.\n    Confidentiality: Preventing unauthorized persons from gaining\n        access to unauthorized information\n    Integrity: Preventing an attacker from modifying data.\n    Accessibility: Making sure that resources are available for\n        authorized users.\n\nSystem security\n: Concerned with attacks on system resources (such\n    as computer hardware, operating system software, and application\n    software) by malicious programs.\n\nNetwork security\n: Concerned with attacks on computer networks,\n    including the infrastructure of privately owned networks as well\n    as the Internet itself.\n\nDifference between privacy and security {#difference-between-privacy-and-security}\n\nPrivacy\n: Personal info accessed by organisations claiming to have legitimate need\n\nSecurity\n: Obtaining of information by unauthorised personnel\n\nPrivacy can obscure the identity of security violaters\n\nCloud Computing {#cloud-computing}\n\n> a model for enabling ubiquitous, convenient, on-demand network access\n> to a shared pool of configurable computing resources (e.g., networks,\n> servers, storage, applications and services).\n\nMajor Concerns {#major-concerns}\n\nHow users can control their data stored in the cloud—currently,\n    users have very little \"control over or direct knowledge about how\n    their information is transmitted, processed, or stored\".\nIntegrity of the data —- for example, if the host company goes out\n    of business, what happens to the users’ data?\nAccess to the data; i.e., can the host deny a user access to\n    his/her own data?\nAnd a fourth concern has to do with who actually \"owns\" the data\n    that is stored in the cloud\n\nHacking and Hacker Ethic {#hacking-and-hacker-ethic}\n\nAccording to Simpson (2006), a hacker is anyone who \"accesses a\ncomputer system or network without authorization from the owner.\"\n\nSteven Levy (2001)\n\nAccess to computers should be unlimited and total.\nAll information should be free.\nMistrust authority–promote decentralization.\nHackers should be judged by their hacking (not by bogus criteria\n    such as degrees, age, race, or position).\nYou can create art and beauty on a computer.\nComputers can change your life for the better.\n\nCyberterrorism {#cyberterrorism}\n\nDorothy Denning (2004) defines it as the “convergence of terrorism and\ncyberspace.” As such, cyberterrorism covers politically motivated\nhacking operations intended to cause grave harm—that is, resulting in\neither loss of life or severe economic loss, or both.\n\nHacktivism {#hacktivism}\n\n\"Electronic Civil Disobedience\" (ECD). Criteria for CD:\n\nNo damage done to persons or property. (Debatable, depending on context)\nNonviolent.\nNot for personal profit.\nEthical motivation—the strong conviction that a law is unjust, or\n    unfair, to the extreme detriment of the common good.\nWillingness to accept personal responsibility for the outcome of\n    actions.\n\nHacktivism\n: The convergence of political activism and computer\n    hacking techniques to engage in a new form of civil\n    disobedience.\n\nCyberterrorism\n: The convergence of cybertechnology and terrorism\n    for carrying out acts of terror in (or via)\n    cyberspace.\n\nInformation Warfare\n: Using malware in cyberattacks designed to\n    mislead the enemy and disrupt/damage an opponent’s military\n    defense system and its critical infrastructure.\n\nCybercrime {#cybercrime}\n\nBy thinking about cybercrimes in terms of their unique or special\nfeatures—i.e., conditions that separate them from ordinary crimes—we\ncould distinguish authentic or “genuine” cybercrimes from other crimes\nthat merely involve the use or the presence of cybertechnology. We\npropose a definition of a genuine cybercrime as a crime in which\n\n> the criminal act can be carried out only through the use of\n> cybertechnology and can take place only in the cyberrealm.\n\nCyberpiracy\n: using cybertechnology in unauthorized ways to (a)\n    reproduce copies of proprietary information, or (b)\n    distribute proprietary information (in digital form)\n    across a computer network.\n\nCybertrespass\n: using cybertechnology to gain unauthorized access\n    to (a) an individual’s or an organization’s\n    computer system, or (b) a password-protected Web\n    site.\n\nCybervandalism\n: using cybertechnology to unleash one or more\n    programs that (a) disrupt the transmission of\n    electronic information across one or more computer\n    networks, including the Internet, or (b) destroy\n    data resident in a computer or damage a computer\n    system’s resources, or both.\n\nPre-emptive hacking {#pre-emptive-hacking}\n\n> The goal of the ethical hacker is to help the organization take\n> preemptive measures against malicious attacks by attacking the system\n> himself; all the while staying within legal limits ... an Ethical\n> Hacker is very similar to a Penetration Tester ... When it is done\n> by request and under a contract between an Ethical Hacker and an\n> organization, it is legal.\n\nJustified through utilitarian/consequentialist means\n: Less harm\n    overall, society gain benefits from a more robust system\n\nNot justified as innocent individuals are used as means to an end\n: The\n    systems of innocent users are used for pre- emptive hacking as\n    'host computers', which is morally unjustifiable under\n    deontological theory, as nobody should be used as an means to an\n    end\n\nCyber-assisted crimes {#cyber-assisted-crimes}\n\nCyberexacerbated\n: Cyberstalking; Internet pedophilia; Internet\n    pornography\n\nCyberassisted\n: Online tax fraud; Physical assault with a computer\n    (e.g., hitting someone over the head with a\n    computer monitor); Property damage using a\n    computer hardware device (e.g., throwing a CPU\n    through a window)\n\nIdentity Theft {#identity-theft}\n\nCyberexcerbated; a crime in which an imposter obtains key pieces of\npersonal information, such as social security or driver’s license\nnumbers, in order to impersonate someone else. The information can be\nused to obtain credit, merchandise, and services in the name of the\nvictim, or to provide the thief with false credentials.\n\nVigilante {#vigilante}\n\nDefinition: a civilian or organization acting in a law enforcement\ncapacity (or in the pursuit of self-perceived justice) without legal\nauthority.\n\nVigilantism – reaction (often punishment) to real/perceived\n    deviance.\nInvolves planning and premeditation by those engaging in it\nIts participants are private citizens whose engagement is voluntary\nForm of autonomous citizenship that constitutes a social movement\nUses or threatens the use of force\nArises when established order is under threat from the\n    transgression, potential transgression or imputed transgression of\n    institutionalized norms\nAims to control crime or other social infractions by offering\n    reassurances (or ‘guarantees’) of security both to the participants\n    and to others.\n\nInternet technology facilitates the following:\n\nAllows the average person to play the role of the report/journalist\n    to chronicle objectionable act (i.e. become reporters)\nAllows the average person to read/watch the footage of the act and\n    take active actions in response (i.e. become vigilantes)\nFootage of the act could be easily circulated to a global audience\nEasier for the poster and the reader to remain anonymous\nHarder for the offenders to “erase” their label. The stigma may be\n    permanent.\n\nBig Data {#big-data}\n\nParadoxes {#paradoxes}\n\nTransparency {#transparency}\n\nBig data promises to use this data to make the world more transparent,\nbut its collection is invisible, and its tools and techniques are\nopaque, shrouded by layers of physical, legal, and technical privacy\nby design. If big data spells the end of privacy, then why is the big\ndata revolution occurring mostly in secret?\n\nIdentity {#identity}\n\nBig data seeks to identify, but it also threatens identity. This is\nthe Identity Paradox. We instinctively desire sovereignty over our\npersonal identity. Whereas the important right to privacy harkens from\nthe right to be left alone, the right to identity originates from\nthe right to free choice about who we are.\n\nIf we lack the power to individually say who “I am,” if filters and\nnudges and personalized recommendations undermine our intellectual\nchoices, we will have become identified but lose our identities as we\nhave defined and cherished them in the past.\n\nPower {#power}\n\nBig data will create winners and losers, and it is likely to benefit\nthe institutions who wield its tools over the individuals being mined,\nanalyzed, and sorted. Not knowing the appropriate legal or technical\nboundaries, each side is left guessing. Individuals succumb to denial\nwhile governments and corporations get away with what they can by\ndefault, until they are left reeling from scandal after shock of\ndisclosure. The result is an uneasy, uncertain state of affairs that\nis not healthy for anyone and leaves individual rights eroded and our\ndemocracy diminished.\n\nPrivacy {#privacy}\n\nCompanies and governments increase use of big data to improve\n    products and services, including defending against terrorist and\n    cybersecurity attacks\nPublic increases questions about privacy and how data is used as\n    realisation of invasions is brought to awareness\n\nPrivacy needs new focus. Instead of focusing on the collection of\ninformation, our new focus should be on the rules that govern\nhow personal information is used and disclosed\n\nThe rules that govern how information flows and not merely\n    restrictions on acquiring personal information or data (Nissenbaum)\nBut our practical ability to manage the trade of personal\n    information needs to be fixed. How can we self-manage privacy?\n    notice: data processors should disclose what they are doing with\n        personal data\n    choice: people should be able to opt-out of uses of their data\n        that they dislike\nBut is self-management a feasible route?\n\nConfidentiality {#confidentiality}\n\nInformation exists in states between being completely private and\ncompletely public.\n\nPrivacy is not binary. Important to recognise that.\nBefore big data, individuals could more easily gauge the expected\n    uses of their personal data and weigh the benefits and the costs at\n    the time they provided their consent.\nFor companies, the potential for harm due to unintended\n    consequences, can quickly outweigh the value the big data innovation\n    is intended to provide. Not limited to harm to individuals, but also\n    institutions.\nBig data uses secondary information shared privately in confidence.\n    Can we trust this information to remain confidential? How can it be\n    regulated by law?\n\nTransparency {#transparency}\n\nThe power of big data comes in large part from secondary uses of data\nsets to produce new predictions and inferences. Institutions like data\nbrokers, often without our knowledge or consent, are collecting\nmassive amounts of data about us they can use and share in secondary\nways that we do not want or expect.\n\nWhy is there privacy for institutions but none for individuals? How\n    can this be better balanced? Those who collect, share, and use data\n    must be made more transparent and thus more accountable.\n\nChanges {#changes}\n\nChanges in law are essential but insufficient, usually slow. So how\ndo we fill this gap?\n\nChief Privacy Officers\nIn-house philosophers (eg. Damon Horowitz at Google)\nUsers (but requires institutions to be transparent)\nTechnologists can fill these gaps by rebutting “privacy is dead”\n    beliefs and moving to advance ethics of data, and creating new\n    business models, practices and technologies\nReview boards (IRBs) for consumer experiments\n\nIntellectual Property {#intellectual-property}\n\nCopyright {#copyright}\n\nCopyright is a form of protection provided to the authors of “original\nworks of authorship” including literary, dramatic, musical, artistic,\nand certain other intellectual works, both published and unpublished.\nThe 1976 Copyright Act generally gives the owner of copyright the\nexclusive right to reproduce the copyrighted work, to prepare\nderivative works, to distribute copies or phonorecords of the\ncopyrighted work, to perform the copyrighted work publicly, or to\ndisplay the copyrighted work publicly.\n\nThe copyright protects the form of expression rather than the subject\nmatter of the writing. For example, a description of a machine could\nbe copyrighted, but this would only prevent others from copying the\ndescription; it would not prevent others from writing a description of\ntheir own or from making and using the machine. Copyrights are\nregistered by the Copyright Office of the Library of Congress.\n\nTrademark {#trademark}\n\nA trademark is a word, name, symbol or device which is used in trade\nwith goods to indicate the source of the goods and to distinguish them\nfrom the goods of others. A servicemark is the same as a trademark\nexcept that it identifies and distinguishes the source of a service\nrather than a product. The terms “trademark” and “mark” are commonly\nused to refer to both trademarks and servicemarks.\n\nTrademark rights may be used to prevent others from using a\nconfusingly similar mark, but not to prevent others from making the\nsame goods or from selling the same goods or services under a clearly\ndifferent mark. Trademarks which are used in interstate or foreign\ncommerce may be registered with the Patent and Trademark Office.\n\nPatent {#patent}\n\nA patent for an invention is the grant of a property right to the\ninventor, issued by the Patent and Trademark Office. The term of a new\npatent is 20 years from the date on which the application for the\npatent was filed in the United States or, in special cases, from the\ndate an earlier related application was filed, subject to the payment\nof maintenance fees. US patent grants are effective only within the\nUS, US territories, and US possessions.\n\nThe right conferred by the patent grant is, in the language of the\nstatute and of the grant itself, “the right to exclude others from\nmaking, using, offering for sale, or selling” the invention in the\nUnited States or “importing” the invention into the United States.\nWhat is granted is not the right to make, use, offer for sale, sell or\nimport, but the right to exclude others from making, using, offering\nfor sale, selling or importing the invention.\n\nLabour Theory of Property {#labour-theory-of-property}\n\nLocke argues that when a person “mixes” his or her labor with the\nland, that person is entitled to the fruit of his or her labor. So if\na person tills and plants crops on a section of land that is not\nalready owned by another—an act which, Locke notes, requires\nconsiderable toil—that person has a right to claim ownership of the\ncrops.\n\nCriticisms {#criticisms}\n\nIntellectual Property are nonexclusionary in nature, and are thus\n    not scarce\nProperty right is a natural right\n\nUtilitarian Theory of Property {#utilitarian-theory-of-property}\n\nProperty rights are better understood as artificial rights or\nconventions devised by the state to achieve certain practical ends.\nAccording to utilitarian theory, granting property rights will\nmaximize the good for the greatest number of people in a given\nsociety. inventions. Incentives in the form of copyrights and patents\nwould motivate individuals to bring out their creative products and\nthat, as a result, American society in general would benefit.\n\nPersonality Theory {#personality-theory}\n\nAccording to the personality theory of property, the intellectual\nobject is an extension of the creator’s personality (i.e., the\nperson’s being, or soul). And it is because of this relationship\nbetween the intellectua object and the creator’s personality that\nadvocates of the personality theory believe that creative works\ndeserve legal protection.\n\nAppendix {#appendix}\n\nACM Code of Ethics {#acm-code-of-ethics}\n\nPrinciple 1: PUBLIC {#principle-1-public}\n\nSoftware engineers shall act consistently with the public interest. In\nparticular, software engineers shall, as appropriate:\n\n1.01. Accept full responsibility for their own work.\n\n1.02. Moderate the interests of the software engineer, the employer,\nthe client and the users with the public good.\n\n1.03. Approve software only if they have a well-founded belief that it\nis safe, meets specifications, passes appropriate tests, and does not\ndiminish quality of life, diminish privacy or harm the environment.\nThe ultimate effect of the work should be to the public good.\n\n1.04. Disclose to appropriate persons or authorities any actual or\npotential danger to the user, the public, or the environment, that\nthey reasonably believe to be associated with software or related\ndocuments.\n\n1.05. Cooperate in efforts to address matters of grave public concern\ncaused by software, its installation, maintenance, support or\ndocumentation.\n\n1.06. Be fair and avoid deception in all statements, particularly\npublic ones, concerning software or related documents, methods and\ntools.\n\n1.07. Consider issues of physical disabilities, allocation of\nresources, economic disadvantage and other factors that can diminish\naccess to the benefits of software.\n\n1.08. Be encouraged to volunteer professional skills to good causes\nand contribute to public education concerning the discipline.\n\nPrinciple 2: CLIENT AND EMPLOYER {#principle-2-client-and-employer}\n\nSoftware engineers shall act in a manner that is in the best interests\nof their client and employer, consistent with the public interest. In\nparticular, software engineers shall, as appropriate:\n\n2.01. Provide service in their areas of competence, being honest and\nforthright about any limitations of their experience and education.\n\n2.02. Not knowingly use software that is obtained or retained either\nillegally or unethically.\n\n2.03. Use the property of a client or employer only in ways properly\nauthorized, and with the client's or employer's knowledge and consent.\n\n2.04. Ensure that any document upon which they rely has been approved,\nwhen required, by someone authorized to approve it.\n\n2.05. Keep private any confidential information gained in their\nprofessional work, where such confidentiality is consistent with the\npublic interest and consistent with the law.\n\n2.06. Identify, document, collect evidence and report to the client or\nthe employer promptly if, in their opinion, a project is likely to\nfail, to prove too expensive, to violate intellectual property law, or\notherwise to be problematic.\n\n2.07. Identify, document, and report significant issues of social\nconcern, of which they are aware, in software or related documents, to\nthe employer or the client.\n\n2.08. Accept no outside work detrimental to the work they perform for\ntheir primary employer.\n\n2.09. Promote no interest adverse to their employer or client, unless\na higher ethical concern is being compromised; in that case, inform\nthe employer or another appropriate authority of the ethical concern.\n\nPrinciple 3: PRODUCT {#principle-3-product}\n\nSoftware engineers shall ensure that their products and related\nmodifications meet the highest professional standards possible. In\nparticular, software engineers shall, as appropriate:\n\n3.01. Strive for high quality, acceptable cost and a reasonable\nschedule, ensuring significant tradeoffs are clear to and accepted by\nthe employer and the client, and are available for consideration by\nthe user and the public.\n\n3.02. Ensure proper and achievable goals and objectives for any\nproject on which they work or propose.\n\n3.03. Identify, define and address ethical, economic, cultural, legal\nand environmental issues related to work projects.\n\n3.04. Ensure that they are qualified for any project on which they\nwork or propose to work by an appropriate combination of education and\ntraining, and experience.\n\n3.05. Ensure an appropriate method is used for any project on which\nthey work or propose to work.\n\n3.06. Work to follow professional standards, when available, that are\nmost appropriate for the task at hand, departing from these only when\nethically or technically justified.\n\n3.07. Strive to fully understand the specifications for software on\nwhich they work.\n\n3.08. Ensure that specifications for software on which they work have\nbeen well documented, satisfy the users’ requirements and have the\nappropriate approvals.\n\n3.09. Ensure realistic quantitative estimates of cost, scheduling,\npersonnel, quality and outcomes on any project on which they work or\npropose to work and provide an uncertainty assessment of these\nestimates.\n\n3.10. Ensure adequate testing, debugging, and review of software and\nrelated documents on which they work.\n\n3.11. Ensure adequate documentation, including significant problems\ndiscovered and solutions adopted, for any project on which they work.\n\n3.12. Work to develop software and related documents that respect the\nprivacy of those who will be affected by that software.\n\n3.13. Be careful to use only accurate data derived by ethical and\nlawful means, and use it only in ways properly authorized.\n\n3.14. Maintain the integrity of data, being sensitive to outdated or\nflawed occurrences.\n\n3.15 Treat all forms of software maintenance with the same\nprofessionalism as new development.\n\nPrinciple 4: JUDGMENT {#principle-4-judgment}\n\nSoftware engineers shall maintain integrity and independence in their\nprofessional judgment. In particular, software engineers shall, as\nappropriate:\n\n4.01. Temper all technical judgments by the need to support and\nmaintain human values.\n\n4.02 Only endorse documents either prepared under their supervision or\nwithin their areas of competence and with which they are in agreement.\n\n4.03. Maintain professional objectivity with respect to any software\nor related documents they are asked to evaluate.\n\n4.04. Not engage in deceptive financial practices such as bribery,\ndouble billing, or other improper financial practices.\n\n4.05. Disclose to all concerned parties those conflicts of interest\nthat cannot reasonably be avoided or escaped.\n\n4.06. Refuse to participate, as members or advisors, in a private,\ngovernmental or professional body concerned with software related\nissues, in which they, their employers or their clients have\nundisclosed potential conflicts of interest.\n\nPrinciple 5: MANAGEMENT {#principle-5-management}\n\nSoftware engineering managers and leaders shall subscribe to and\npromote an ethical approach to the management of software development\nand maintenance . In particular, those managing or leading software\nengineers shall, as appropriate:\n\n5.01 Ensure good management for any project on which they work,\nincluding effective procedures for promotion of quality and reduction\nof risk.\n\n5.02. Ensure that software engineers are informed of standards before\nbeing held to them.\n\n5.03. Ensure that software engineers know the employer's policies and\nprocedures for protecting passwords, files and information that is\nconfidential to the employer or confidential to others.\n\n5.04. Assign work only after taking into account appropriate\ncontributions of education and experience tempered with a desire to\nfurther that education and experience.\n\n5.05. Ensure realistic quantitative estimates of cost, scheduling,\npersonnel, quality and outcomes on any project on which they work or\npropose to work, and provide an uncertainty assessment of these\nestimates.\n\n5.06. Attract potential software engineers only by full and accurate\ndescription of the conditions of employment.\n\n5.07. Offer fair and just remuneration.\n\n5.08. Not unjustly prevent someone from taking a position for which\nthat person is suitably qualified.\n\n5.09. Ensure that there is a fair agreement concerning ownership of\nany software, processes, research, writing, or other intellectual\nproperty to which a software engineer has contributed.\n\n5.10. Provide for due process in hearing charges of violation of an\nemployer's policy or of this Code.\n\n5.11. Not ask a software engineer to do anything inconsistent with\nthis Code.\n\n5.12. Not punish anyone for expressing ethical concerns about a\nproject.\n\nPrinciple 6: PROFESSION {#principle-6-profession}\n\nSoftware engineers shall advance the integrity and reputation of the\nprofession consistent with the public interest. In particular,\nsoftware engineers shall, as appropriate:\n\n6.01. Help develop an organizational environment favorable to acting\nethically.\n\n6.02. Promote public knowledge of software engineering.\n\n6.03. Extend software engineering knowledge by appropriate\nparticipation in professional organizations, meetings and\npublications.\n\n6.04. Support, as members of a profession, other software engineers\nstriving to follow this Code.\n\n6.05. Not promote their own interest at the expense of the profession,\nclient or employer.\n\n6.06. Obey all laws governing their work, unless, in exceptional\ncircumstances, such compliance is inconsistent with the public\ninterest.\n\n6.07. Be accurate in stating the characteristics of software on which\nthey work, avoiding not only false claims but also claims that might\nreasonably be supposed to be speculative, vacuous, deceptive,\nmisleading, or doubtful.\n\n6.08. Take responsibility for detecting, correcting, and reporting\nerrors in software and associated documents on which they work.\n\n6.09. Ensure that clients, employers, and supervisors know of the\nsoftware engineer's commitment to this Code of ethics, and the\nsubsequent ramifications of such commitment.\n\n6.10. Avoid associations with businesses and organizations which are\nin conflict with this code.\n\n6.11. Recognize that violations of this Code are inconsistent with\nbeing a professional software engineer.\n\n6.12. Express concerns to the people involved when significant\nviolations of this Code are detected unless this is impossible,\ncounter-productive, or dangerous.\n\n6.13. Report significant violations of this Code to appropriate\nauthorities when it is clear that consultation with people involved in\nthese significant violations is impossible, counter-productive or\ndangerous.\n\nPrinciple 7: COLLEAGUES {#principle-7-colleagues}\n\nSoftware engineers shall be fair to and supportive of their\ncolleagues. In particular, software engineers shall, as appropriate:\n\n7.01. Encourage colleagues to adhere to this Code.\n\n7.02. Assist colleagues in professional development.\n\n7.03. Credit fully the work of others and refrain from taking undue\ncredit.\n\n7.04. Review the work of others in an objective, candid, and\nproperly-documented way.\n\n7.05. Give a fair hearing to the opinions, concerns, or complaints of\na colleague.\n\n7.06. Assist colleagues in being fully aware of current standard work\npractices including policies and procedures for protecting passwords,\nfiles and other confidential information, and security measures in\ngeneral.\n\n7.07. Not unfairly intervene in the career of any colleague; however,\nconcern for the employer, the client or public interest may compel\nsoftware engineers, in good faith, to question the competence of a\ncolleague.\n\n7.08. In situations outside of their own areas of competence, call\nupon the opinions of other professionals who have competence in that\narea.\n\nPrinciple 8: SELF {#principle-8-self}\n\nSoftware engineers shall participate in lifelong learning regarding\nthe practice of their profession and shall promote an ethical approach\nto the practice of the profession. In particular, software engineers\nshall continually endeavor to:\n\n8.01. Further their knowledge of developments in the analysis,\nspecification, design, development, maintenance and testing of\nsoftware and related documents, together with the management of the\ndevelopment process.\n\n8.02. Improve their ability to create safe, reliable, and useful\nquality software at reasonable cost and within a reasonable time.\n\n8.03. Improve their ability to produce accurate, informative, and\nwell-written documentation.\n\n8.04. Improve their understanding of the software and related\ndocuments on which they work and of the environment in which they will\nbe used.\n\n8.05. Improve their knowledge of relevant standards and the law\ngoverning the software and related documents on which they work.\n\n8.06 Improve their knowledge of this Code, its interpretation, and its\napplication to their work.\n\n8.07 Not give unfair treatment to anyone because of any irrelevant\nprejudices.\n\n8.08. Not influence others to undertake any action that involves a\nbreach of this Code.\n\n8.09. Recognize that personal violations of this Code are inconsistent\nwith being a professional software engineer.\n",
        "tags": []
    },
    {
        "uri": "/zettels/ising_models",
        "title": "Ising Models",
        "content": "\nAn Ising model is an array of spins (atoms that can take states \\\\(\\pm\n1\\\\)) that are magnetically coupled to each other. If one spin is, say\n$+1, is energetically favourable for its immediate neighbours to be in\nthe same state.\n\nLet the state \\\\(\\mathbb{x}\\\\) of an Ising model with \\\\(N\\\\) spins be a\nvector in which each component \\\\(x\\_n\\\\) takes values \\\\(+1\\\\) and \\\\(-1\\\\). If\ntwo spins \\\\(m\\\\) and \\\\(n\\\\) are neighbours, we say \\\\((m, n) \\in \\mathcal{N}\\\\).\nThe coupling between neighbouring spins is \\\\(J\\\\). We define \\\\(J\\_{mn} = J\\\\)\nif \\\\(m\\\\) and \\\\(n\\\\) are neighbours, and \\\\(J\\_{mn} = 0\\\\) otherwise. The energy\nof a state \\\\(\\mathbf{x}\\\\) is:\n\n\\begin{equation}\n  E(\\mathbb{x}| J, H) = - \\left[ \\frac{1}{2} \\sum\\_{m,n}\n    J\\{mn}x\\{m}x\\{n} + \\sum\\{n} H x\\_{n} \\right]\n\\end{equation}\n\nwhere \\\\(H\\\\) is the applied field. The probability that the state is\n\\\\(\\mathbb{x}\\\\) is:\n\n\\begin{equation}\nP(\\mathbb{x} | J, H) = \\frac{1}{Z(\\beta, J, H)} \\textrm{exp} \\left[ -\n  -\\beta E(\\mathbb{x}; J, H) \\right]\n\\end{equation}\n\nwhere \\\\(\\beta = \\frac{1}{k\\B T}\\\\), \\\\(k\\B\\\\) is the Boltzmann's constant,\nand:\n\n\\begin{equation}\n  Z(\\beta, J, H) = \\sum\\_{\\mathbb{x}} \\textrm{exp} \\left[ - \\beta\n    E(\\mathbb{x}; J,H) \\right]\n\\end{equation}\n\nThe Ising model is also an example of a Markov Random Field (MRF). We\ncreate a graph in the form of a 2D or 3D lattice, and connect\nneighbouring variables. We can define the following clique potential:\n\n\\begin{equation}\n  \\phi\\{st} (y\\s, y\\_t) = \\begin{pmatrix}\n    e^{w\\{st}} & e^{-w\\{st}} \\\\\\\\\\\\\n    e^{-w\\{st}} & e^{w\\{st}}\n  \\end{pmatrix}\n\\end{equation}\n\nHere \\\\(w\\_{st}\\\\) is the coupling strength between nodes \\\\(s\\\\) and \\\\(t\\\\). If\ntwo nodes are not connected, \\\\(w\\_{st} = 0\\\\). The weight matrix\n\\\\(\\mathbb{W}\\\\) is symmetric: \\\\(w\\{st} = w\\{ts}\\\\). All edges have the same\nstrength \\\\(J\\\\), where \\\\(w\\_{st} \\ne 0\\\\).\n\nThe Ising model is analogous to the Gaussian graphical models. First,\nassuming \\\\(y\\_t \\in \\\\{-1, +1\\\\}\\\\), we can write the unnormalized log\nprobability of an Ising model as follows:\n\n\\begin{equation}\n  \\log \\tilde{p}(\\mathbb{y}) = - \\sum\\{s \\sim t} y\\s w\\{st} y\\t +\n  \\sum\\{s}b\\s y\\_s = -\n  \\frac{1}{2}\\mathbb{y}^T \\mathbb{W} \\mathbb{y} + \\mathbb{b}^T \\mathbb{y}\n\\end{equation}\n\nwhere \\\\(\\theta = (\\mathbb{W}, \\mathbb{b})\\\\), and \\\\(\\mathbb{b}\\\\) is the\nbias term, corresponding to the local fields. If we define:\n\n\\begin{equation}\n  \\mu = - \\frac{1}{2}\\mathbb{\\Sigma}^{-1}\\mathbb{b},\n  \\mathbb{\\Sigma}^{-1} = - \\mathbb{W}, c = \\frac{1}{2}\\mathbb{\\mu}^T \\mathbb{\\Sigma}^{-1}\\mu\n\\end{equation}\n\nwe can rewrite this in a form that looks similar to a Gaussian:\n\n\\begin{equation}\n\\log \\tilde{p}(\\mathbb{y}) = - \\frac{1}{2}(\\mathbb{y} -\n\\mathbb{\\mu})^T \\Sigma ^{-1} (\\mathbb{y} - \\mathbb{\\mu}) + c\n\\end{equation}\n",
        "tags": [
            "machine-learning"
        ]
    },
    {
        "uri": "/zettels/java",
        "title": "Java",
        "content": "\ntags\n: §prog\\_lang\n\nHash Table {#hash-table}\n\nDefault java implementation:\n\nhashCode returns memory location of the object\nEvery object hashes to a different location\n\nLong {#long}\n\npublic int hashCode() {\n    return (int) (value ^ (value >> 32));\n}\n\nString {#string}\n\npublic int hashCode() {\n    int h = hash; // only caluclate hash once\n    if (h == 0 && count > 0) { // empty = 0\n        int off = offset;\n        char val[] = value;\n        int len = count;\n        for (int i = 0; i\n\nThe two functions come from two different interfaces that\nPriorityQueue implements:\n\nadd() comes from Collection.\noffer() comes from Queue.\n\nFor a capacity-constrained queue, the difference is that add() always\nreturns true and throws an exception if it can't add the element,\nwhereas offer() is allowed to return false if it can't add the\nelement.\n",
        "tags": []
    },
    {
        "uri": "/zettels/javascript",
        "title": "JavaScript",
        "content": "\nJavaScript is an interpreted programming language, originally designed\nfor the web.\n",
        "tags": []
    },
    {
        "uri": "/zettels/jensens_inequality",
        "title": "Jensen's Inequality",
        "content": "\ntags\n: §information\\_theory\n\nIf \\\\(f\\\\) is a convex function, and \\\\(x\\\\) is a random variable then:\n\n\\begin{equation}\n  E\\left[ f(x) \\right] \\ge f\\left( E[x] \\right)\n\\end{equation}\n",
        "tags": []
    },
    {
        "uri": "/zettels/kalman_filter",
        "title": "Kalman Filter",
        "content": "\ntags\n: Extended Kalman Filter, Gaussian Filter\n\nThe Kalman filter is a technique for belief computation in linear\nsystems. It implements belief computation over continuous states, and\nis not suitable for discrete or hybrid state spaces.\n\nIt uses the moments representation. At time \\\\(t\\\\), the belief is\nrepresented by mean \\\\(\\mu\\t\\\\) and covariance \\\\(\\Sigma\\t\\\\). It makes the\nfollowing assumptions:\n\nThe Markovian Assumption (Markovian Assumption)\nThe next state probability \\\\(p(x\\t | u\\t, x\\_{t-1})\\\\) is a linear\n    function in its arguments with added Gaussian noise:\n\n\\begin{equation}\n  x\\t = A\\t x\\{t-1} + B\\t u\\t + \\epsilon\\t\n\\end{equation}\n\n\\\\(x\\t\\\\) and \\\\(\\u\\t\\\\) are column vectors. This assumption defines the state\ntransition probability \\\\(p(x\\t | u\\t, x\\_{t-1})\\\\) by substituting the\nmean \\\\(A\\t x\\{t-1} + B\\t u\\t\\\\) and covariance \\\\(R\\_t\\\\) in the multi-variate\nnormal distribution formula.\n\nThe measurement probability \\\\(p(z\\t | x\\t)\\\\) is linear in its\n    arguments:\n\n\\begin{equation}\n  z\\t  = C\\t x\\t + \\delta\\t\n\\end{equation}\n\nfor some multivariate Gaussian noise \\\\(\\delta\\_t\\\\) with 0 mean and\nco-variance \\\\(Q\\_t\\\\).\n\nThe initial belief \\\\(\\text{bel}(x\\_0)\\\\) is normally distributed, with\n    initial belief \\\\(\\mu\\0\\\\) and covariance \\\\(\\Sigma\\0\\\\)\n\nThese 4 assumptions give rise to the Kalman Filter algorithm.\n\n\\begin{algorithm}\n  \\caption{Kalman Filter}\n  \\label{kalman\\_filter}\n  \\begin{algorithmic}[1]\n    \\Procedure{KalmanFilter}{$\\mu\\{t-1}, \\Sigma\\{t-1}, \\mu\\t, \\z\\t$}\n    \\State $\\overline{\\mu}\\t = A\\t \\mu\\{t-1} + B\\t \\mu\\_t$\n    \\State $\\overline{\\Sigma}\\t = A\\t \\Sigma\\{t-1} A\\t^T + R\\_t$\n    \\State ${K}\\t = \\overline{\\Sigma}\\t C\\t^T (C\\t \\overline{\\Sigma}\\t C\\t^T + Q\\_t)^{-1}$\n    \\State $\\mu\\t = \\overline{\\mu}\\t + K\\t(z\\t - C\\t\\overline{\\mu}\\t)$\n    \\State $\\Sigma\\t = (I - K\\t C\\t) \\overline{\\Sigma}\\t$\n    \\State \\Return $\\mu\\t, \\Sigma\\t$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\nCons {#cons}\n\nThe linearity assumptions are often unfulfilled in practice. For\nexample, circular trajectories cannot be described with linear state\ntransitions. To overcome this difficulty, people use the [Extended\nKalman Filter]({{}}).\n",
        "tags": []
    },
    {
        "uri": "/zettels/kl_divergence",
        "title": "Kl Divergence",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/laplace_method",
        "title": "Laplace's Method",
        "content": "\nSuppose we have an unnormalized probability density \\\\(P^\\star(x)\\\\),\nwhose normalizing constant:\n\n\\begin{equation}\n  Z\\_P \\equiv \\int P^\\star(x) dx\n\\end{equation}\n\nis of interest, and has a peak at point \\\\(x\\_0\\\\).\n\nWe perform a Taylor expansion of \\\\(\\ln P^\\star(x)\\\\) at this peak:\n\n\\begin{equation}\n  \\ln P^\\star(x) \\approxeq \\ln P^\\star(x\\0) - \\frac{c}{2}(x - x\\0)^2 + \\dots\n\\end{equation}\n\nwhere \\\\(c = - \\frac{\\partial^2}{\\partial x^2} \\ln P^\\star(x) \\text{\nwhere } {x = x\\_0}\\\\).\n\n\\\\(P^\\star(x)\\\\) can be approximated by an unnormalized Gaussian:\n\n\\begin{equation}\n  Q^\\star(x) \\equiv P^\\star(x\\0) \\textrm{exp}\\left[- \\frac{c}{2} (x-x\\0)^2\\right]\n\\end{equation}\n\nand the normalizing constant is approximated with:\n\n\\begin{equation}\n  Z\\Q \\equiv P^\\star(x\\0) \\sqrt{\\frac{2\\pi}{c}}\n\\end{equation}\n\nThis is easily generalizable to a K-dimensional space \\\\(\\mathbf{x}\\\\).\n",
        "tags": []
    },
    {
        "uri": "/zettels/large_batch_training",
        "title": "Large Batch Training",
        "content": "\ntags\n: §machine\\learning\\algorithms\n",
        "tags": []
    },
    {
        "uri": "/zettels/lars_optimizer",
        "title": "LARS Optimizer",
        "content": "\nLayer-wise Adaptive Rate Scaling (LARS) is a §nn\\_optimizer. The\ntechnique allows §large\\batch\\training without significant\ndecrease in accuracy (You et al., 2017). One\nof the secondary goals is §fast\\nn\\training.\n\nImplementations {#implementations}\n\npytorch-lars\n\nBibliography\nYou, Y., Gitman, I., & Ginsburg, B., Large batch training of convolutional networks, CoRR, (),  (2017).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/latex",
        "title": "LaTeX",
        "content": "\nUseful Tools {#useful-tools}\n\nMathpix\n: extracts LaTeX from PDFs\n",
        "tags": []
    },
    {
        "uri": "/zettels/leaky_integrate_and_fire",
        "title": "Leaky Integrate-And-Fire",
        "content": "\ntags\n: §spiking\\neural\\networks\n\nA Leaky Integrate-and-Fire neuron at layer \\\\(l\\\\) and index \\\\(i\\\\) can be\ndescribed in differential form as:\n\n\\begin{equation} \\label{eq:lif}\n  \\tau\\{\\mathrm{mem}} \\frac{\\mathrm{d} U\\{i}^{(l)}}{\\mathrm{d} t}=-\\left(U\\{i}^{(l)}-U\\{\\mathrm{rest}}\\right)+R I\\_{i}^{(l)}\n\\end{equation}\n\nwhere the terms denote:\n\n\\\\(U\\_{i}(t)\\\\)\n: membrane potential\n\n\\\\(U\\_{\\text{rest}}\\\\)\n: resting potential\n\n\\\\(\\tau\\_{\\text{mem}}\\\\)\n: membrane time constant\n\n\\\\(R\\\\)\n: input resistance\n\n\\\\(i\\_{i}(t)\\\\)\n: input current\n\n\\\\(U\\{i}\\\\) acts as a leaky integrator of the input current \\\\(I\\{i}\\\\).\nNeurons emit spikes when the membrane voltage reaches firing threshold\n\\\\(\\theta\\\\), and resets to resting potential \\\\(U\\_{\\text{\\rest}}\\\\).\n\nEquation eq:lif only describes the dynamics of a LIF neuron\nsub-threshold.\n",
        "tags": []
    },
    {
        "uri": "/zettels/learning",
        "title": "Learning",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/lecun_cake_analogy",
        "title": "LeCun's Cake Analogy",
        "content": "\nYann LeCun gave this cake analogy in NIPS 2016:\n\nIf we think of our brain as a cake, then the cake base is unsupervised\nlearning. The machine predicts any part of its input for any observed\npart (e.g. future frames in videos), all without the use of labelled\ndata. Supervised learning forms the icing on the cake, and\nreinforcement learning is the cherry on top.\n\nIn NIPS 2017, Pieter Abbeel reused this cake analogy to describe his\npaper §andrychowicz2017\\hindsight\\experience\\_replay: \"Intelligence is\na cake with many cherries\"\n",
        "tags": []
    },
    {
        "uri": "/zettels/leslie_lamport",
        "title": "Leslie Lamport",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/likelihood_field_model",
        "title": "Likelihood Field Model",
        "content": "\nKey Idea {#key-idea}\n\nProject an individual sensor measurement \\\\(z\\_t^k\\\\) into the global\ncoordinate frame of map \\\\(m\\\\). Discards max-range readings.\n\nAssumes three types of noise, similar to §range\\finder\\model:\n\nMeasurement noise: Gaussian\nFailures: point-mass distribution at \\\\(z\\_{\\text{max}}\\\\)\nRandom measurements: Uniform distribution \\\\(p\\_{\\text{rand}}\\\\)\n\nThe model is a mixture of these 3 densities:\n\n\\begin{equation}\n  z\\{\\mathrm{hit}} \\cdot p\\{\\mathrm{hit}}+z\\{\\mathrm{rand}} \\cdot p\\{\\mathrm{rand}}+z\\{\\mathrm{max}} \\cdot p\\{\\mathrm{max}}\n\\end{equation}\n\nIssues {#issues}\n\nDoes not explicitly model dynamic objects that cause short readings\nTreats sensors as being able to see through walls: ray casting\n    replaced by nearest neighbour function: incapable of determining\n    whether a path to a point is intercepted by an obstacle in the map\nDoes not account for map uncertainty\n\nThese issues can be addressed via extensions to the algorithm.\n\nRelated {#related}\n\n§range\\finder\\model\n§map\\_matching\n",
        "tags": []
    },
    {
        "uri": "/zettels/likelihood_principle",
        "title": "Likelihood Principle",
        "content": "\ntags\n: §machine\\_learning\n\nGiven a generative model for data \\\\(d\\\\) given parameters\n\\\\(\\mathrm{\\theta}\\\\), \\\\(P(d|\\mathrm{\\theta})\\\\), and having observed a\nparticular outcome \\\\(d\\_1\\\\), all inferences and predictions should depend\nonly on the function \\\\(P(d\\_1 | \\mathrm{\\theta})\\\\).\n",
        "tags": []
    },
    {
        "uri": "/zettels/linear_algebra",
        "title": "Linear Algebra",
        "content": "\nDefinitions {#definitions}\n\nA matrix of dimensions M-by-N looks like:\n\n\\begin{equation}\n  A = \\begin{bmatrix}\n    a\\{11} & a\\{12} & \\dots & a\\_{1N} \\\\\\\\\\\\\n    a\\{21} & a\\{22} & \\dots & a\\_{2N} \\\\\\\\\\\\\n    \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\\\\\\n    a\\{M1} & a\\{M2} & \\dots & a\\_{MN}\n    \\end{bmatrix}\n\\end{equation}\n\nIts transpose flips the dimensions. In the below example transposing a\nN-by-1 matrix makes it a 1-by-N matrix.\n\n\\begin{equation}\n  a^T= \\left[ a\\1, a\\2, \\dots, a\\_N \\right]\n\\end{equation}\n\nThe magnitude of a vector is given by \\\\(\\left| a \\right| = \\sqrt{a\\_1^2 +\na\\2^2 + \\dots + a\\n^2}\\\\).\n\nLinear Independence {#linear-independence}\n\nA set of vectors \\\\(\\mathbf{x}\\\\) is linearly dependent if there exist a set of\nscalars \\\\(\\mathbf{\\alpha}\\\\), not all zero, such that \\\\(\\sum\\{i=1}^m \\alpha\\i x\\_i = 0\\\\).\nIf the only way to satisfy the equation is to have \\\\(\\alpha\\_i = 0 \\forall i\\\\), then\nthe set of vectors \\\\(\\mathbf{x}\\\\) is linearly independent.\n\nSpan {#span}\n\nThe \"span\" of \\\\(\\hat{v}\\\\) and \\\\(\\hat{w}\\\\) is the set of all their linear\ncombinations:\n\n\\begin{equation}\n a \\hat{v} + b \\hat{w}\n\\end{equation}\n\nBasis {#basis}\n\nThe basis of a vector space is a set of linearly independent vectors\nthat span the full space.\n\nIf \\\\(\\mathbf{v}\\\\) is the basis set, then any N-by-1 vector \\\\(x\\\\) can be\nwritten as \\\\(x = \\sum^n\\{i=1}c\\i v\\i\\\\) for some scalar \\\\(c\\i\\\\).\n\nSpecial Matrices {#special-matrices}\n\ndiagonal matrix\n: a matrix with all off-diagonal entries equal to 0.\n\nidentity matrix\n: A diagonal matrix with diagonal entries equal to 1.\n\nsymmetric matrix\n: \\\\(A = A^T\\\\)\n\nskew symmetric matrix\n: \\\\(A = -A^T\\\\)\n\nMatrix properties {#matrix-properties}\n\n\\\\((AB)^-1 = B^-1 A^{-1}\\\\)\n\\\\((AB)^T = B^T A^T\\\\)\n\\\\((A^{-1})^T = (A^T)^{-1}\\\\)\n\\\\(A^{-1}A = I\\\\)\n\\\\((AB)C = A(BC)\\\\)\n\\\\(A(B+C) = AB + AC\\\\)\n\\\\((B+C)D = BD + CD\\\\)\n\nSolutions to linear systems {#solutions-to-linear-systems}\n\nGiven the matrix \\\\(A\\\\) and vector \\\\(b\\\\), the problem \\\\(Ax=b\\\\) has a solution\niff the vector \\\\(b\\\\) can be expressed as a linear combination of the\ncolumns of \\\\(A\\\\). If \\\\(A\\\\) is a square matrix and is invertible, then the\nsolution is \\\\(x = A^{-1}b\\\\).\n\nIf \\\\(A\\\\) is a rectangular matrix of size M-by-N, and if \\\\(A^T A\\\\) is\ninvertible (square and full rank), then the linear least squares\nsolution to \\\\(x\\\\) is given by \\\\(x = (A^T A)^{-1} A^T b\\\\).\n\nMatrices as Linear Transformations {#matrices-as-linear-transformations}\n\nA transformation is linear if it fulfills two properties:\n\nAll lines remain lines (they don't get curved)\nThe origin is fixed.\n\nUnder a linear transformation, grid lines remain parallel and evenly\nspaced. This property allows us to compute the transformed vector,\nonly by recording how the basis vectors are transformed.\n\n{{}}\n\nMatrix Multiplication as Composition {#matrix-multiplication-as-composition}\n\nOften, we want to describe the effects of multiple linear\ntransformations composed together, for example, a rotation, followed\nby a shear. The composition of linear transformations is also a linear\ntransformation, and can be described with a single matrix (see above).\n\n{{}}\n\n{{}}\n\nHence, we can think about matrix multiplication as computing where the\nfinal basis vectors land.\n\nDeterminant {#determinant}\n\nThe fact that linear transformations leave grid lines parallel and\nevenly spaced, means that the area of each unit square is scaled by the\nsame amount.\n\nThe determinant of a transformation is the amount of scaling of area\nof a unit square. If the determinant is negative, then the orientation\nof the resulting grid space is reversed.\n\n{{}}\n\nIn 3D space, the determinant is the volume of the parallelpiped.\n\n\\begin{equation}\ndet \\left( \\begin{bmatrix}\n  a & b & c \\\\\\\\\\\\\n  d & e & f \\\\\\\\\\\\\n  g & h & i \\\\\\\\\\\\\n\\end{bmatrix} \\right)  = a \\cdot det \\left( \\begin{bmatrix}\n  e & f \\\\\\\\\\\\\n  h & i\n\\end{bmatrix} \\right)\nb \\cdot det \\left( \\begin{bmatrix}\n  d & f \\\\\\\\\\\\\n  g & i\n\\end{bmatrix}  \\right)\nc \\cdot det \\left( \\begin{bmatrix}\n  d & e \\\\\\\\\\\\\n  g & h\n\\end{bmatrix} \\right)\n\\end{equation}\n\nMatrices for solving linear equations {#matrices-for-solving-linear-equations}\n\n{{}}\n\nSuppose we want to compute \\\\(\\vec{x}\\\\) such that \\\\(A\\vec{x} = \\vec{v}\\\\).\nThen we can compute the inverse of the matrix \\\\(A\\\\), which corresponds\nto the inverse transformation. For example if \\\\(A\\\\) were to rotate the\ngrid space clockwise 90 degrees, then the inverse of \\\\(A\\\\) would be to\nrotate the grid space anti-clockwise 90 degrees: \\\\(\\vec{x} = A^{^{-1}}\n\\vec{v}\\\\).\n\nSuppose the determinant of the transformation is 0. Then we know that\nit does not have an inverse. However, solutions can still exist.\n\nRank {#rank}\n\nThe rank is the number of dimensions of the output of the\ntransformation. It is easy to see that the maximum rank of the\ntransformation is the original dimensions of the matrix. Rank\ncorresponds to the maximal number of linearly independent columns of\n\\\\(A\\\\).\n\n\\\\(r(AB) \\le r(A)\\\\)\n\\\\(r(AB) \\le r(B)\\\\)\n\\\\(r(AB) \\le \\text{min}(r(A), r(B))\\\\)\n\nColumn Space {#column-space}\n\nThe column space of the matrix \\\\(A\\\\) is the set of all possible outputs of\n\\\\(A \\vec{v}\\\\). It is also the span of all the columns.\n\nDot Product {#dot-product}\n\nThe dot product \\\\(\\vec{w} \\cdot \\vec{v}\\\\) can be viewed as the\n\\\\((\\text{length of projected vector }\\vec{x}) \\cdot (\\text{length of\n}\\vec{x})\\\\).\n\nWe can think of \\\\(1 \\times 2\\\\) matrices as projection matrices, where the\nfirst column indicates where \\\\(\\hat{i}\\\\) lands, and the second column\nindicates where \\\\(\\hat{j}\\\\) lands. Suppose we have a vector \\\\(\\hat{i}\\\\),\nand we want to project it onto \\\\(\\hat{\\mu}\\\\). By symmetry, it's the same\nvalue as when \\\\(\\hat{\\mu}\\\\) is projected onto \\\\(\\hat{i}\\\\). However, this is\njust the \\\\(x\\\\) coordinate value of \\\\(\\hat{\\mu}\\\\).\n\n{{}}\n\nHence, \\\\(\\hat{i}\\\\) and \\\\(\\hat{j}\\\\) land at \\\\(\\mu\\x\\\\) and \\\\(\\mu\\y\\\\) respectively. We\ncan easily see the duality between matrix-vector product and dot\nproduct here.\n\n{{}}\n\n\\begin{equation}\n  a \\cdot b = |a||b| \\cos \\theta\n\\end{equation}\n\nsometimes, this is also notated as\n\n\\begin{equation}\n \\langle u, v \\rangle = \\sum\\{i=1}^{d}u\\i v\\_i = u^T v\n\\end{equation}\n\nCross Product {#cross-product}\n\nThe cross product of \\\\(\\vec{v}\\\\) and \\\\(\\vec{w}\\\\), denoted \\\\(\\vec{v} \\times\n\\vec{w}\\\\) is a vector. The vector has length equal to the area of a\nparallelogram obtained by duplicating and shifting the two vectors.\nThe sign of the cross product is determined using the right-hand rule.\nThis vector is perpendicular to the parallelogram.\n\n{{}}\n\n{{}}\n\nWe want to find the dual vector \\\\(\\hat{p}\\\\) that corresponds to the\ncross product.\n\n{{}}\n\nHow to translate a matrix {#how-to-translate-a-matrix}\n\nSuppose someone uses a different coordinate system (i.e. different\nbasis vectors), which we can represent with a matrix:\n\n\\begin{equation}\n  \\begin{bmatrix}\n    2 & -1 \\\\\\\\\\\\\n    1 & 1\n  \\end{bmatrix}\n\\end{equation}\n\nSuppose then that we want to apply a linear transformation to a vector\nin her coordinate system. In the case of a rotation 90 degrees\nanti-clockwise, it would be represented in a matrix as:\n\n\\begin{equation}\n  \\begin{bmatrix}\n    0 & -1 \\\\\\\\\\\\\n    1 & 0\n  \\end{bmatrix}\n\\end{equation}\n\nIn the \"default\" basis vector coordinate system. What does this\ntransformation look like in the new coordinate system? Given some\nvector \\\\(\\hat{v}\\\\) in the other language. First, we translate the\nvector into one in the default language:\n\n\\begin{equation}\n  \\begin{bmatrix}\n    2 & -1 \\\\\\\\\\\\\n    1 & 1\n  \\end{bmatrix}\n  \\hat{v}\n\\end{equation}\n\nThen, we apply the transformation to the vector in the default\nlanguage:\n\n\\begin{equation}\n  \\begin{bmatrix}\n    0 & -1 \\\\\\\\\\\\\n    1 & 0\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    2 & -1 \\\\\\\\\\\\\n    1 & 1\n  \\end{bmatrix}\n  \\hat{v}\n\\end{equation}\n\nThen, we apply to the inverse of the change in basis matrix, to return\nthe vector to the other language:\n\n\\begin{equation}\n  \\begin{bmatrix}\n    2 & -1 \\\\\\\\\\\\\n    1 & 1\n  \\end{bmatrix}^{-1}\n  \\begin{bmatrix}\n    0 & -1 \\\\\\\\\\\\\n    1 & 0\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    2 & -1 \\\\\\\\\\\\\n    1 & 1\n  \\end{bmatrix}\n  \\hat{v}\n\\end{equation}\n\nThis form \\\\(A^{{-1}}MA\\\\) is frequently encountered when dealing with\neigenvectors and eigenvalues.\n\nEigenvectors and eigenvalues {#eigenvectors-and-eigenvalues}\n\nConsider the span of a particular vector, that is, the set of vectors\nobtainable by applying a scaling constant to it. Some vectors remain\non their own span, even with linear transformations.\n\nThese vectors are called eigenvectors, and the value of the scaling\nconstant is called the eigenvalue. Mathematically, this is expressed\nas:\n\n\\begin{equation}\n  A \\hat{v} = \\lambda \\hat{v}\n\\end{equation}\n\nConsider a 3D rotation. If we can find an eigenvector for this 3D\ntransformation, then we have found the axis of rotation.\n\n\\begin{equation}\n  \\sum\\{i=1}^{n} \\lambda\\i = trace(A)\n\\end{equation}\n\n\\begin{equation}\n  \\prod\\{i=1}^{n}\\lambda\\i = \\lvert A \\rvert\n\\end{equation}\n\nEigenvectors corresponding to different eigenvalues are linearly\nindependent.\n\nEigenvectors of a real symmetric matrix are orthogonal and real.\n\nNull Space {#null-space}\n\nThe null space of a matrix \\\\(A\\\\) consists of all vectors \\\\(x\\\\) such that\n\\\\(Ax = 0\\\\). if \\\\(A\\\\) is rank-deficient, then there is a non-zero solution\nfor \\\\(x\\\\).\n\nSingular Value Decomposition {#singular-value-decomposition}\n\nGiven an input data matrix \\\\(A\\\\) consisting of \\\\(m\\\\) documents and \\\\(n\\\\)\nterms, we can decompose it into 3 matrices.\n\n\\begin{equation}\n  A\\{[m \\times n]} = U\\{[m \\times r]} \\Sigma\\{[r \\times r]} V\\{[n\n    \\times r]}^T\n\\end{equation}\n\n\\\\(U\\\\) are left singular vectors of size \\\\(m \\times r\\\\), which we can think of\nas \\\\(m\\\\) documents and \\\\(r\\\\) concepts. \\\\(\\Sigma\\\\) is a \\\\(r \\times r\\\\) diagonal matrix,\nrepresenting the strength of each concept, where \\\\(r\\\\) is the rank of\nthe matrix \\\\(A\\\\). \\\\(V\\\\) stores the right singular vectors, consisting of\n\\\\(n\\\\) terms and \\\\(r\\\\) concepts.\n\n{{}}\n\nIt is always possible to decompose a real matrix \\\\(A\\\\) into \\\\(A = U \\Sigma\nV^T\\\\), where:\n\n\\\\(U\\\\), \\\\(\\Sigma\\\\), and \\\\(V\\\\) are unique\n\\\\(U\\\\) and \\\\(V\\\\) are column orthonormal: \\\\(U^T U = I\\\\), \\\\(V^T V = I\\\\)\n\\\\(\\Sigma\\\\) is diagonal, and entries are positive, sorted in decreasing\n    order.\n\nThe entries on the diagonal of \\\\(\\Sigma\\\\) are known as the singular values,\nand they are the squareroots of the eigenvalues of both \\\\(AA^T\\\\) and \\\\(A^T\nA\\\\). The number of non-zero singular values equals to the rank of \\\\(A\\\\).\n\nSVD can be used to solve linear equations of the form \\\\(Ax = b\\\\), where\n\\\\(A\\\\) is known, and \\\\(b\\\\) is the zero-vector. We can do a SVD of \\\\(A\\\\) and\nwrite the solution for \\\\(x\\\\):\n\n\\begin{equation}\n  U \\Sigma V^T x = 0\n\\end{equation}\n\nIf A is full-rank, then \\\\(x\\\\) must be the zero-vector. Else, \\\\(x\\\\) has a\nnon-zero solution equal to the linear combination of the last few rows\nof \\\\(V^T\\\\) corresponding to zero singular values.\n\n{{}}\n\nReferences {#references}\n\nEssence of Linear Algebra\nLecture 47 - Singular Value Decomposition | Stanford University\nImmersive Linear Algebra\n",
        "tags": []
    },
    {
        "uri": "/zettels/links",
        "title": "General Links",
        "content": "\n12 Factor CLI Apps {#12-factor-cli-apps}\n\nInteresting best practices to follow. Especially about reading the [XDG\nSpec](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html).\n\nTech Yaks {#tech-yaks}\n\nNetflix's Culture {#netflix-s-culture}\n\nSomething to look back at:\n",
        "tags": []
    },
    {
        "uri": "/zettels/linux",
        "title": "Linux",
        "content": "\n127.0.1.1 in /etc/hosts is to allow some applications like Gnome to\n    resolve the hostname to an ip address with a canonical fully\n    qualified domain name - FQDN (Source)\nsudo needs to resolve hostnames because the sudoers file specifies\n    hostnames in the rules. Hence sudo requires both loopback and\n    \"real\". (Source)\npkexec lets authorized users execute programs as another user.\n",
        "tags": []
    },
    {
        "uri": "/zettels/lu_decomposition",
        "title": "LU Decomposition",
        "content": "\nThe key problem of Linear Algebra (§linear\\_algebra) is solving the\nequation \\\\(Ax=b\\\\). We know that with Gaussian elimination, we can\ndecompose \\\\(A = LU\\\\), where \\\\(L\\\\) is a lower-triangular matrix and \\\\(U\\\\) is\nan upper triangular matrix. An example speaks a thousand words:\n\n\\begin{equation}\n  A x=\\left[\\begin{array}{ccc}{2} & {1} & {1} \\\\ {4} & {-6} & {0} \\\\ {-2} & {7} & {2}\\end{array}\\right]\\left[\\begin{array}{l}{u} \\\\ {v} \\\\ {w}\\end{array}\\right]=\\left[\\begin{array}{c}{5} \\\\ {-2} \\\\ {9}\\end{array}\\right]=b\n\\end{equation}\n\nWe have that:\n\n\\begin{equation}\n  U x=\\left[\\begin{array}{ccc}{2} & {1} & {1} \\\\ {0} & {-8} & {-2} \\\\ {0} & {0} & {1}\\end{array}\\right]\\left[\\begin{array}{l}{u} \\\\ {v} \\\\ {w}\\end{array}\\right]=\\left[\\begin{array}{c}{5} \\\\ {-12} \\\\ {2}\\end{array}\\right]=c\n\\end{equation}\n\nAnd the process from getting from A to U is \\\\(GFEA = U\\\\):\n\n\\begin{equation}\n  G F E=\\left[\\begin{array}{ccc}\n                {1} & {} & {} \\\\\\\\\\\\\n                {} & {1} & {} \\\\\\\\\\\\\n                {} & {1}& {1}\n              \\end{array}\\right]\n            \\left[\\begin{array}{ccc}\n                    {1} & {} & {} \\\\\\\\\\\\\n                    {} & {1} & {} \\\\\\\\\\\\\n                    {1} & {} & {1}\n                  \\end{array}\\right]\n                \\left[\\begin{array}{ccc}\n                        {1} & {} & {1} \\\\\\\\\\\\\n                        {-2} & {1} & {} \\\\\\\\\\\\\n                        {} & {} & {1}\\end{array}\\right]\n                    =\\left[\\begin{array}{ccc}\n                             {1} & {} & {} \\\\\\\\\\\\\n                             {-2} & {1} & {} \\\\\\\\\\\\\n                             {-1} & {1} & {1}\n                           \\end{array}\\right]\n\\end{equation}\n\nWe have that \\\\(E^{-1} F^{-1} G^{-1} U= LU = A\\\\):\n\n\\begin{equation}\n  E^{-1} F^{-1} G^{-1}=\\left[\n    \\begin{array}{ccc}\n      {1} & {} & {}\\\\\\\\\\\\\n      {2} & {1} & {} \\\\\\\\\\\\\n      {} & {} & {1}\n    \\end{array}\\right]\n  \\left[\\begin{array}{ccc}\n          {1} & {}  & {} \\\\ {} & {1} & {} \\\\ {-1} & {} & {1}\n        \\end{array}\\right]\n      \\left[\\begin{array}{ccc}\n              {1} & {} & {} \\\\\\\\\\\\\n              {} & {1} & {} \\\\\\\\\\\\\n              {} & {-1} & {1}\\end{array}\\right]=\n          \\left[\\begin{array}{ccc}{1} & {} & {} \\\\ {2} & {1} & {} \\\\ {-1} & {-1} & {1}\\end{array}\\right]=L\n\\end{equation}\n\nThe \\\\(A = LU\\\\) decomposition is exactly the matrix that solves \\\\(Ax=b\\\\).\nEach triangular system requires \\\\(\\frac{n^2}{2}\\\\) steps each, compared\nto the \\\\(O(n^3)\\\\) system of factoring \\\\(A\\\\).\n\nThe triangular factorization can be written \\\\(A = LDU\\\\) where \\\\(L\\\\) and\n\\\\(U\\\\) have 1s on the diagonal, and \\\\(D\\\\) is the diagonal matrix of pivots:\n\n\\begin{equation}\n  A=\\left[\\begin{array}{ll}{1} & {2} \\\\ {3} & {4}\\end{array}\\right]=\\left[\\begin{array}{ll}{1} & {1} \\\\ {3} & {1}\\end{array}\\right]\\left[\\begin{array}{rr}{1} & {2} \\\\ {} & {-2}\\end{array}\\right]=\\left[\\begin{array}{ll}{1} \\\\ {3} & {1}\\end{array}\\right]\\left[\\begin{array}{ll}{1} & {} \\\\ {} & {-2}\\end{array}\\right]\\left[\\begin{array}{ll}{1} & {2} \\\\ {} & {1}\\end{array}\\right]=L D U\n\\end{equation}\n\nThis factorization is uniquely determined by \\\\(A\\\\).\n",
        "tags": []
    },
    {
        "uri": "/zettels/machine_learning_algorithms",
        "title": "Machine Learning Algorithms",
        "content": "\ntags\n: §machine\\_learning\n",
        "tags": []
    },
    {
        "uri": "/zettels/machine_learning",
        "title": "Machine Learning",
        "content": "\nWhen do we need machine learning? {#when-do-we-need-machine-learning}\n\nTwo aspects of a given problem may call for the use of programs that\nlearn and improve on the basis of their \"experience\":\n\nThe problem's complexity: some tasks that require elaborate\n    introspection that cannot be well-defined in programs, such as\n    driving, are ill-suited for coding by hand. Tasks that are beyond\n    human capabilities such as the analysis of large datasets also fall\n    in this category.\nAdaptivity: Programmed tools are rigid, while machine learning\n    tools allows for adaptation to the environment they interact with.\n\nThe Learning Problem {#the-learning-problem}\n\nWhat is Learning? {#what-is-learning}\n\nAn agent is said to be learning if it improves its performance P on\ntask T based on experience/observations/data E. T must be fixed, P\nmust be measurable, E must exist. See Learning Agents.\n\nInductive Bias\n\n    The incorporation of prior knowledge biases the learning mechanism.\n    This is also called inductive bias. The incorporation of prior\n    knowledge is inevitable for the success of learning algorithms (the\n    no-free-lunch theorem). The stronger the prior knowledge that one\n    starts the learning process with, the easier it is to learn from\n    further examples.\n\nTypes of Learning {#types-of-learning}\n\nActive vs Passive Learning\n\n    An active learner interacts with the environment at training time,\n    (e.g. by posing queries or performing experiments), while a passive\n    learner only observes the information provided by the environment.\n\nOnline vs Batch Learning\n\n    In online learning, the hypothesis has to be updated each time a new\n    label is received.\n\nReinforcement Learning\n\n    Each action in a state has an associated cost and a probability\n    distribution of the next state.\n\n    Goal is to learn a policy (mapping from state to action) that\n    minimizes the sum of expected current and future costs.\n\nSupervised Learning\n\n    Since learning involves an interaction between the learner and the\n    environment, one can divide tasks according to the nature of that\n    interaction.\n\n    Supervised learning describes a scenario in which the training\n    examples contain significant information that is missing in the unseen\n    \"test examples\". In unsupervised learning, there is no distinction\n    between the training and test data. The goal is to come up with some\n    summary, or compressed version of that data.\n\n    Measure of success\n\n        A loss function helps measure our success. Given a set \\\\(H\\\\) of\n        hypothesis of models, and a domain \\\\(Z\\\\), let \\\\(l\\\\) be a function from \\\\(H\n          \\times Z\\\\) to non-negative real numbers $l: \\\\(H \\times Z \\rightarrow\n          \\mathbb{R}\\_{+}\\\\).\n\n        The risk function is the expected loss of the hypothesis,\n\n        \\begin{equation\\*}\n          L\\D(h) = E\\{z \\sim D}[l(h,z)]\n        \\end{equation\\*}\n\n        We are interested in finding a hypothesis \\\\(h\\\\) that has a small risk,\n        or expected loss, typically using Empirical Risk Minimization.\n\n    Assumptions Made ⚠\n\n        One common assumption is that the data in the data generation\n            process is independently and identically distributed (IID),\n            according to the distribution \\\\(D\\\\).\n\n        Q: Given a large enough training set, do you expect the long term test\n        error to be similar to the training error?\n\n        If IID, then yes\n        If not, there is likely dependencies, but under certain conditions,\n            yes.\n            If sampling mixes well, it will not take long for D' to look\n                like a steady set distribution.\n        If dependencies are exploited, there is a possibility of attaining\n            lower training and test error.\n\nIs Learning Feasible? {#is-learning-feasible}\n\nThe target function \\\\(f\\\\) that we want to learn is unknown. The\nperformance of a hypothesis on the training set \\\\(D\\\\) tells us nothing\nabout the performance on the data outside of \\\\(D\\\\).\n\nAs long as \\\\(f\\\\) is unknown, knowing \\\\(D\\\\) cannot exclude any patterns of\n\\\\(f\\\\) outside of \\\\(D\\\\), and the predictions of \\\\(g\\\\) would be meaningless.\n\nProbabilistic View\n\n    If we accept a probabilistic answer, that is \\\\(D\\\\) tells us something\n    likely about \\\\(f\\\\) outside of \\\\(D\\\\), then learning is feasible, only with\n    a small price.\n\n    Learning a hypothesis \\\\(g\\\\) approximates the target function \\\\(f\\\\) well,\n    i.e. \\\\(E\\_{out}(g) \\approx 0\\\\). However, probabilistic analysis via\n    Hoeffding's Inequality gives \\\\(E\\{out}(g) \\approx E\\{in}(g)\\\\).\n    Therefore, we still need to ensure \\\\(E\\_{in}(g) \\approx 0\\\\).\n\nTraining vs Testing {#training-vs-testing}\n\nGeneralisation Error {#generalisation-error}\n\nWe can define generalisation error as the discrepancy between \\\\(E\\_in\\\\)\nand \\\\(E\\_out\\\\). The Hoeffding Inequality characterises the generalization\nerror with a probabilistic bound:\n\n\\begin{align}\nP[|E\\{in}(g) - E\\{out}(g)| > \\epsilon] \\le 2Me^{-2\\epsilon^2N}\n\\end{align}\n\nPick a tolerance level \\\\(\\delta\\\\), and assert with probability\n\\\\(1-\\delta\\\\) that\n\n\\begin{align}\n  E\\{out}(g) \\le E\\{in}(g) + \\sqrt{\\frac{1}{2N}\\ln \\frac{2M}{\\delta}}\n\\end{align}\n\nNotice the error bound depends on \\\\(M\\\\), the size of the hypothesis\nset \\\\(H\\\\). Most learning models have infinite \\\\(H\\\\), including the simple\nperceptron. Hence, to study generalisation in such models, we need to\nderive a counterpart that deals with infinite \\\\(H\\\\).\n\nNotice that the \\\\(M\\\\) factor was obtained by taking the disjunction of\nevents. Let \\\\(B\\m\\\\) be the bad event that \\\\(|E\\{in}(h\\m) - E\\{out}(h\\_m)|\n> \\epsilon\\\\). Notice that these bad events are often strongly\noverlapping, and the disjunction of these events form a much smaller\narea.\n\nThe mathematical theory of generalisation hinges on this observation.\nUpon accounting for the overlaps of different hypotheses, we will be\nable to replace the number of hypotheses \\\\(M\\\\) with an effective finite\nnumber, even while \\\\(M\\\\) is infinite.\n\nGrowth Function {#growth-function}\n\nThe growth function is the quantity that will formalize the\neffective number of hypotheses.\n\nEach \\\\(h \\in H\\\\) generates a dichotomy which is \\\\(h\\\\) is \\\\(-1\\\\) or \\\\(h\\\\) i-\n\\\\(+1\\\\). We then formally define dichotomies as follows:\n\n\\begin{align}\nH(x\\1, \\dots, x\\n) = \\left\\\\{ h(x\\1), h(x\\2), \\dots, h(x\\_n) | h \\in H \\right\\\\}\n\\end{align}\n\nConcept Learning {#concept-learning}\n\nA concept is a boolean-valued function over a set of input instances\n(each comprising input attributes). Concept learning is a form of\nsupervised learning. Infer an unknown boolean-valued function from\ntraining-examples.\n\nHypothesis {#hypothesis}\n\nThere is a trade-off between expressive power and smaller\nhypothesis space. Large hypothesis spaces are bad, because search is\ngoing to take a long time, and also requires more data. Humans exploit\nstructure in the hypothesis space to guide search and learn faster.\n\nA hypothesis \\\\(h\\\\) is consistent with a set of training examples \\\\(D\\\\) iff\n\\\\(h(x) = c(x)\\\\) for all \\\\( \\in D\\\\).\n\nInductive Learning {#inductive-learning}\n\nAny hypothesis found to approximate the target function well over a\nsufficient large set of training examples will also approximate the\ntarget function well over other unobserved examples.\n\nConcept Learning is Search {#concept-learning-is-search}\n\nThe goal is to search for a hypothesis \\\\(h \\in H\\\\) that is consistent\nwith \\\\(D\\\\).\n\nExploit Structure in Concept Learning {#exploit-structure-in-concept-learning}\n\n\\\\(h\\j\\\\) is more general than or equal to \\\\(h\\k\\\\) (denoted \\\\(h\\j \\ge\\{g}\nh\\k\\\\)) iff any input instance \\\\(x\\\\) that satisfies \\\\(h\\j\\\\) also satisfies\n\\\\(h\\_k\\\\).\n\nThis is relation is a partial order.\n\nFind-S Algorithm {#find-s-algorithm}\n\nIntuition: Start with the most specific hypothesis \\\\(h\\\\). Whenever it\nwrongly classifies a positive training example, we \"minimally\"\ngeneralize it to satisfy its input instance.\n\nLimitations\n\n    Can't tell whether Find-S has learnt the target concept\n    Can't tell when training examples are inconsistent\n    Picks a maximally specific \\\\(h\\\\)\n    Depending on \\\\(H\\\\), there may be several solutions\n\nVersion Space {#version-space}\n\n\\begin{equation\\*}\n  VS\\_{H,D} = {h \\in H | h \\text{ is consistent with }D}\n\\end{equation\\*}\n\nIf \\\\(c \\in H\\\\), then D can reduce \\\\(VS\\_{H,D}\\\\) to \\\\({c}\\\\).\nIf D is insufficient, then \\\\(VS\\{H,D}\\\\) represents the uncertainty_\n    of what the target concept is\n\\\\(VS\\_{H,D}\\\\) contains all consistent hypotheses, including maximally\n    specific hypotheses\n\nThe general boundary G of \\\\(VS\\_{H,D}\\\\) is the set of maximally general\nmembers of \\\\(H\\\\) consistent with \\\\(D\\\\).\n\nThe specific boundary S of \\\\(VS\\_{H,D}\\\\) is the set of maximally general\nmembers of \\\\(H\\\\) consistent with \\\\(D\\\\).\n\n\\begin{equation\\*}\n  VS\\{H,D} = {h \\in H | \\exists s \\in S \\exists g \\in G g \\ge\\g h\n    \\ge\\_g s }\n\\end{equation\\*}\n\nList-Then-Eliminate Algorithm {#list-then-eliminate-algorithm}\n\nIterate through all hypotheses in \\\\(H\\\\), and eliminate any hypothesis\nfound inconsistent with any training example. This algorithm is often\nprohibitively expensive.\n\nCandidate-Elimination Algorithm {#candidate-elimination-algorithm}\n\nStart with most general and specific hypotheses. Each training example\n\"minimally\" generalizes S and specializes G to remove inconsistent\nhypotheses from version space.\n\nDecision Tree Learning {#decision-tree-learning}\n\nDecision Tree Learning is a method of learning which approximates\ndiscrete-valued functions that is robust to noisy data, and is capable\nof learning disjunctive expressions\n\nIt is most appropriate when:\n\ninstances are represented as attribute pairs\nthe target function has discrete output values\nDisjunctive descriptions may be required\nThe training data may contain errors\nThe training data may contain missing attribute values\n\nID3 algorithm {#id3-algorithm}\n\nID3 learns decision trees by constructing them top down. Each instance\nattribute is evaluated using a statistical test to determine how well\nit alone classifies the examples. The best attribute is selected and\nused as the test at the root node of the tree.\n\nWhich is the best attribute?\n\n    A statistical property called information gain measures how well a\n    given attribute separates the training examples according to their\n    target classification.\n\n    Information gain is the expected reduction in entropy caused by\n    partitioning the examples according to this attribute:\n\n    \\begin{align}\n      Gain(S,A) = Entropy(S) - \\sum\\{v\\in Values(A)}\\frac{|S\\v|}{|S|}Entropy(S\\_v)\n    \\end{align}\n\n    For example:\n\n    \\begin{align}\n      Values(Wind) &= Weak, Strong \\\\\\\\\\\\\n      S &= [9+, 5-] \\\\\\\\\\\\\n      S\\_{Weak} &\\leftarrow [6+, 2-] \\\\\\\\\\\\\n      S\\_{Strong} &\\leftarrow [3+, 3-] \\\\\\\\\\\\\n      Gain(S, Wind) &= Entropy(S) - \\frac{8}{14}Entropy(S\\_{Weak}) -\n                      \\frac{6}{14}Entropy(S\\_{Strong}) \\\\\\\\\\\\\n                   &=0.048\n    \\end{align}\n\nHypothesis Space Search\n\n    ID3 can be characterised as searching a space of hypotheses for one\n    that fits the training examples. The hypothesis space searched is the\n    set of possible decision trees. ID3 performs a simple-to-complex,\n    hill-climbing search. The evaluation measure that guides the search is\n    the information gain measure.\n\n    Because ID3's hypothesis space of all decision trees is a complete\n    space of finite discrete-valued functions, it avoids the risk that the\n    hypothesis space might not contain the target function.\n\n    ID3 maintains only a single hypothesis as it searches through the\n    space of decision trees. ID3 loses the capabilities that follow from\n    explicitly representing all consistent hypothesis.\n\n    ID3 in its pure form performs no backtracking in its search, and can\n    result in locally but not globally optimal target functions.\n\n    ID3 uses all training examples at each step to make statistically\n    based decisions, unlike other algorithms that make decisions incrementally.\n\nInductive bias\n\n    The inductive bias of decision tree learning is that shorter trees are\n    preferred over larger trees (Occam's razor). Trees that place high\n    information gain attributes close to the root are preferred over those\n    that do not. ID3 can be viewed as a greedy heuristic search for the\n    shortest tree without conducting the entire breadth-first search\n    through the hypothesis space.\n\n    Notice that ID3 searches a complete hypothesis space incompletely, and\n    candidate-elimination searches an incomplete hypothesis space\n    completely. The inductive bias of ID3 follows from its search strategy\n    (preference bias), while that of candidate elimination follows from\n    the definition of its search space. (restriction bias).\n\nWhy Prefer Shorter Hypotheses?\n\n    fewer shorter hypothesis than larger ones, means it's less likely\n        to over-generalise\n\nDensity Estimation {#density-estimation}\n\nDensity Estimation refers to the problem of modeling the probability\ndistribution \\\\(p(x)\\\\) of a random variable \\\\(x\\\\), given a finite set \\\\(x\\_1,\nx\\2, \\dots, x\\n\\\\) of observations.\n\nWe first look at parametric distributions, which are governed by a\nsmall number of adaptive parameters. In a frequentist treatment, we\nchoose specific values for the parameters optimizing some criterion,\nsuch as the likelihood function. In a Bayesian treatment, we\nintroduce prior distributions and then use Bayes' theorem to compute\nthe corresponding posterior distribution given the observed data.\n\nAn important role is played by conjugate priors, which yield\nposterior distributions of the same functional form.\n\nThe maximum likelihood setting for parameters can give severely\nover-fitted results for small data sets. To develop a Bayesian\ntreatment to this problem, we consider a form of prior distribution\nwith similar form as the maximum likelihood function. this property is\ncalled conjugacy. For a binomial distribution, we can choose the\nbeta distribution as the prior.\n\nUnsupervised Learning {#unsupervised-learning}\n\nIn unsupervised learning, given a training set \\\\(S = \\left(x\\_1, \\dots,\n x\\_m\\right)\\\\), without a labeled output, one must construct a \"good\"\nmodel/description of the data.\n\nExample use cases include:\n\nclustering\ndimension reduction to ind essential parts of the data and reduce\n    noise (e.g. PCA)\nminimises description length of data\n\nK-means Clustering {#k-means-clustering}\n\nInput: \\\\(\\\\{x^{(1), x^{(2)}, x^{(3)}, \\dots, x^{(m)}}\\\\}\\\\).\n\nRandomly initialize cluster centroids.\nFor all points, compute which cluster centroid is the closest.\nFor each cluster centroid, move centroids to the average points\n    belonging to the cluster.\nRepeat until convergence.\n\nK-means is guaranteed to converge. To show this, we define a\ndistortion function:\n\n\\begin{equation}\n  J(c, \\mu) = \\sum\\{i=1}^m || x^{(i)} - \\mu\\{c^{(i)}}||^2\n\\end{equation}\n\nK means is coordinate ascent on J. Since \\\\(J\\\\) always decreases, the\nalgorithm converges.\n\nGaussian Mixture Model {#gaussian-mixture-model}\n\nBy Bayes' Theorem:\n\n\\begin{equation}\nP(X^{(i)}, Z^{(i)}) = P(X^{(i)} | Z^{(i)})P(Z^{(i)})\n\\end{equation}\n\n\\begin{equation}\nZ^{(i)} \\sim \\text{multinomial}(\\phi)\n\\end{equation}\n\n\\begin{equation}\nX^{(i)} | Z^{(j)} \\sim \\mathcal{N}(\\mu\\j, \\Sigma\\j)\n\\end{equation}\n\nRefile {#refile}\n\nData Compression {#data-compression}\n\nIn lossy compression, we seek to trade off code length with\nreconstruction error.\n\nIn vector quantization, we seek a small set of vectors \\\\({z\\_i}\\\\) to\ndescribe a large dataset of vectors \\\\({x\\_i}\\\\), such that we can\nrepresent each \\\\(x-i\\\\) with its closest approximation in \\\\({z\\_i}\\\\) with\nsmall error. (Clustering problem)\n\nIn transform coding, we transform the data, usually using a linear\ntranformation. The data in the transformed domain is quantized,\nusually discarding the small coefficients, corresponding to removing\nsome of the dimensions.\n\nGenerative Learning Algorithms {#generative-learning-algorithms}\n\nDiscriminative algorithms model \\\\(p(y | x)\\\\) directly from the training\nset.\n\nGenerative algorithms model \\\\(p(y | x)\\\\) and \\\\(p(y)\\\\). Then \\\\(argmax\\_y\np(y|x) = argmax\\y \\frac{p(x|y)p(y)}{p(x)} = argmax\\y p(x|y)p(y)\\\\).\n\nMultivariate Normal Distribution\n\n    A multivariate normal distribution is parameterized by a mean vector\n    \\\\(\\mu \\in R^n\\\\) and a covariance matrix \\\\(\\Sigma \\in R^{n \\times n}\\\\), where \\\\(\\Sigma \\ge\n    0\\\\) is symmetric and positive semi-definite.\n\nTODO  Gaussian Discriminant Analysis\n\n    In Gaussian Discriminant Analysis, p(x | y) is distributed to a\n    multivariate normal distribution.\n\n    \\begin{align}\n      y &\\sim Bernoulli(\\phi) \\\\\\\\\\\\\n      x|y = 0 &\\sim N(\\mu\\_0, \\Sigma) \\\\\\\\\\\\\n      x|y = 1 &\\sim N(\\mu\\_1, \\Sigma)\n    \\end{align}\n\n    We can write out the distributions:\n\n    \\begin{align}\n      p(y) &= \\phi^y (1 - \\phi)^{1-y} \\\\\\\\\\\\\n      p(x | y = 0) &= \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{n/2}} exp \\left( - \\frac{1}{2} (x - \\mu\\0)^T \\Sigma^{-1}(x - \\mu\\0) \\right) \\\\\\\\\\\\\n      p(x | y = 1) &= \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{n/2}} exp \\left( - \\frac{1}{2} (x - \\mu\\1)^T \\Sigma^{-1}(x - \\mu\\1) \\right)\n    \\end{align}\n\n    Then, the log-likelihood of the data is:\n\n    \\begin{align}\n      l(\\phi, \\mu\\0, \\mu\\1, \\Sigma) &= \\log \\prod\\{i=1}^m p(x^{(i)}, y^{(i)}; \\mu\\0, \\mu\\_1, \\Sigma) \\\\\\\\\\\\\n      &= \\log \\prod\\{i=1}^m p(x^{(i) }| y^{(i)}; \\mu\\0, \\mu\\_1, \\Sigma)p(y^{(i)}; \\phi)\n    \\end{align}\n\n    We maximize \\\\(l\\\\) with respect to the parameters.\n\nThe Natural Language Decathlon: Multitask Learning as Question Answering: Richard Socher {#the-natural-language-decathlon-multitask-learning-as-question-answering-richard-socher}\n\npawper\n\nJoint work with Bryan McCann, Nitish Keskar and Caiming Xiong\n\nLimits of Single-task Learning {#limits-of-single-task-learning}\n\nWe can hill climb to local optima if \\\\(|dataset| > 100 \\times C\\\\)\nFor more general model, we need continuous learning in a single model\n\nFor pre-training in NLP, we're still stuck at the word vector level.\nThis compared to vision, where most of the model can be pre-trained,\nonly retraining the final few layers.\n\nWhy has weight & model sharing not happened so much in NLP? {#why-has-weight-and-model-sharing-not-happened-so-much-in-nlp}\n\nNLP requires many types of reasoning: logical, linguistic etc.\nRequires short and long-term memory\nNLP has been divided into intermediate and separate tasks to make\n    progress (Benchmark chasing in each community)\nCan a single unsupervised task solve it all? No, language clearly\n    requires supervision in nature.\n\nMotivation for Single Multitask model {#motivation-for-single-multitask-model}\n\nStep towards AGI\nImportant building block for:\n    Sharing weights\n    Transfer learning\n    Zero-shot learning\n    Domain adaptation\nEasier deployment in production\nLowering the bar for anybody to solve their NLP task\n\nEnd2end model vs parsing as intermediate step (e.g. running POS tagger\nfirst).\n\nThe 3 equivalent supertasks of NLP {#the-3-equivalent-supertasks-of-nlp}\n\nAny NLP task can be mapped to these 3 super tasks:\n\nLanguage Modeling\nQuestion Answering\nDialogue\n\nMultitask learning as QA {#multitask-learning-as-qa}\n\nQuestion Answering\nMachine Translation\nSummarization\nNLI\nSentiment Classification\nSemantic Role Labeling\nRelation Extraction\n\nMeta supervised learning: {x, y} to {x, t, y}\n\nDesigning a model for decaNLP {#designing-a-model-for-decanlp}\n\nNo task-specific modules or parameters because task ID assumed to be unavailable\n\n{{}}\n\nStart with a context\nAsk a question\nGenerate answer one at a time by\n    Pointing to context\n    Pointing to question\n    Choosing a word\n\nLearnings {#learnings}\n\nTransformer Layers yield benefits in single-task and multitask\n    setting\nQA and SRL have strong connections\nPointing to the question is essential, despite the task being just\n    classification for some subtasks\nMulitasking helps a lot with zero-shot tasks\n\n(Latest version of the paper coming out soon -- ICLR 2018)\n\nTraining Strategies {#training-strategies}\n\nFully Joint\nCurriculum learning doesn't work\nAnti-curriculum training works instead\n    Start with a really hard task\n\nStructuring Data Science Projects {#structuring-data-science-projects}\n\nCookiecutter Data Science provides a decent project structure, and\nuses the ubiquitous build tool Make to build data projects. (DrivenData, 2019)\n\n├── LICENSE\n├── Makefile            requirements.txt`\n│\n├── setup.py           (Dan Frank, 2016) still primarily uses Jupyter notebooks, but\nhas 2 main points. First, they strip the results from the Jupyter\nnotebooks before committing. Second, they ensure that the notebooks\ncan be reproduced on the work laptops and on their cloud infrastructure.\n\nBibliography\nDrivenData,  (2019). Home - cookiecutter data science. Retrieved from https://drivendata.github.io/cookiecutter-data-science/. Online; accessed 06 January 2019. ↩\n\nFrank, D. (2016). Reproducible research: stripe's approach to data science. Retrieved from https://stripe.com/blog/reproducible-research. Online; accessed 06 January 2019. ↩\n",
        "tags": [
            "machine-learning"
        ]
    },
    {
        "uri": "/zettels/machine_teaching",
        "title": "Machine Teaching",
        "content": "\ntags\n: §machine\\_learning\n\nDefinitions (Simard et al., 2017) {#definitions}\n\nmachine learning research\n: machine learning research aims at\n    making the learner better by improving ML algorithms\n\nmachine teaching research\n: machine teaching research aims at\n    making the teacher more productive at building machine learning\n    models\n\nProblem Formulation {#problem-formulation}\n\nMachine learning takes a given training dataset \\\\(D\\\\) and learns a model\n\\\\(\\hat{\\theta}\\\\). The learner can take various formrs, including version\nspace learners, Bayesian learners, deep neural networks, or cognitive\nmodels.\n\nIn contrast, in machine teaching the target model \\\\(\\theta^\\*\\\\) is given,\nand the teacher finds a teaching set \\\\(D\\\\) -- not necessarily i.i.d. -\nsuch that a machine learner trained on \\\\(D\\\\) will approximately learn\n\\\\(\\theta^\\*\\\\).\n\nIn machine teaching, we wish to solve the following optimization\nproblem:\n\n\\begin{align}\n  \\begin{matrix}\n    \\textrm{min}\\_{D, \\hat{\\theta}} & \\textrm{TeachingRisk}(\\hat{\\theta}) +\n                             \\eta \\textrm{TeachingCost}(D) \\\\\\\\\\\\\n                             \\textrm{s.t.} & \\hat{\\theta} = \\textrm{MachineLearning}(D).\n  \\end{matrix}\n\\end{align}\n\nWhere \\\\(\\textrm{TeachingRisk(\\hat{\\theta})}\\\\) is a generic function for\nhow unsatisfied the teacher is. The target model \\\\(\\theta^\\*\\\\) is folded\ninto the teaching risk function. The teaching cost function is also\ngeneralized beyond the number of teaching items. For example,\ndifferent teaching items may have different cognitive burdens for a\nhuman student to absorb. (Zhu et al., 2018)\n\nThere are other formulations of machine teaching that place different\nconstraints on the teaching. For example, one may want to minimize the\nteaching cost, while constraining the teaching risk, or instead choose\nto optimize the teaching risk given constraints on the teaching cost.\n\nWhy bother if \\\\(\\theta^\\*\\\\) is known? {#why-bother-if--theta--is-known}\n\nThere are applications where the teacher needs to convey the target\nmodel \\\\(\\theta^\\*\\\\) to a learner via training data. For example:\n\nIn education problems, the teacher may know \\\\(\\theta^\\*\\\\) but is unable\n    to telepathize the model to the students. If the teacher possesses a\n    good cognitive model on how students learn from samples, they can\n    use machine teaching to optimize the choice of learning examples.\nIn training-set poisoning, an attacker manipulates the behaviour of\n    a machine learning system by maliciously modifying the training\n    data. An attacker knowing the algorithm may send specially designed\n    training examples to manipulate the learning algorithm.\n\nFaster Teaching Via POMDP Teaching (Rafferty et al., 2015) {#faster-teaching-via-pomdp-teaching}\n\nThe authors formulate teaching as a POMDP, and use a\ndecision-theoretic approach to planning teaching. Assuming knowledge\nabout the student's learning model, the teacher is able to find\noptimal teaching actions.\n\n{{}}\n\nA POMDP is specified as a tuple:\n\n\\begin{equation}\n\\langle S, A, Z, p(s'|s, a), p(z|s,a), r(s,a), \\gamma \\rangle\n\\end{equation}\n\nS\n: set of states\n\nA\n: set of actions\n\nZ\n: set of observations\n\n\\\\(p(s' | s, a)\\\\)\n: transition model\n\n\\\\(p(z|s, a)\\\\)\n: Probability of observing z\n\n\\\\(r(s,a)\\\\)\n: Reward/cost model\n\n\\\\(\\gamma\\\\)\n: Discount factor\n\nPOMDP planning seeks to choose actions that minimize\n\\\\(\\sum\\{t=0}^\\infty \\gamma^t r(s\\t, a\\_t)\\\\).\n\nThe learner model specifies the space \\\\(S\\\\) of possible knowledge\nstates, and transition model \\\\(p(s'|s ,a)\\\\) for how knowledge changes.\n\nSimple learner models for concept learning can be specified. For\nexample, in the memoryless model, if an action is consistent with the\ncurrent concept, then the state stays the same. Else, the learner\ntransitions to a state that is consistent with the action, with\nprobability proportional to the prior probability of the concept:\n\n\\begin{equation}\n  p(s\\{t+1} = c\\i | s\\t = c\\j , a\\_t) = \\begin{cases}\n    p\\0(c\\i) & \\textrm{ if $c\\i$ is consistent with $a\\t$} \\\\\\\\\\\\\n    0 & \\textrm{otherwise}\n  \\end{cases}\n\\end{equation}\n\nExperiments showed that POMDP planning leads to faster teaching.\n\nLimitations {#limitations}\n\nPOMDP relies on having models of learning. The models that we\n    currently have do not fully model human learning. Human learners\n    can also learn better if they are aware they are being taught. The\n    models are not accurate enough to know when to decide to terminate\n    teaching. **Are we able to learn to teach without explicitly\n    assuming a student model?**\n\nBeyond time to teach, it is difficult to incorporate other factors\n    such as motivation. The learners may have their own reward\n    function, and a joint optimization of the student and teacher\n    rewards is computationally more difficult.\n\nPOMDP can be computationally intractable, requiring the use of\n    techniques such as MCTS and forward search, sampling only possible\n    actions taken.\n    Suppose a task is modelled with a discrete state space. A task\n        with 1000 states will result in a belief state of 1000\n        dimensions. To overcome this \"curse of dimensionality\",\n        point-based POMDP algorithms like HSVI2 and SARSOP use\n        probabilistic sampling.\n\n    One can also factor out the fully observable state components to\n        reduce the dimensionality of the belief space into \\\\(S = X \\times\n              Y\\\\), where \\\\(X\\\\) is the space of all possible values fully\n        observable variables, and \\\\(Y\\\\) is the space of partially\n        observable variables.  (Yanzhu Du et al., 2010) Since state variable \\\\(x\\\\)\n        is fully observable, we only need to maintain belief \\\\(b\\_Y\\\\) for\n        the state variables in \\\\(Y\\\\).\n\nMachine Teaching For Inverse Reinforcement Learning (Brown \\& Niekum, 2018) {#machine-teaching-for-inverse-reinforcement-learning}\n\nOptimal Teaching for IRL gives:\n\nInsights into the intrinsic difficulty of teaching certain\n    sequential decision-making tasks\nProvides a lower bound on the number of samples needed by the\n    active IRL algorithm\nOptimal teaching can be used to design algorithms that better\n    leverage highly informative demonstrations which do not follow the\n    i.i.d assumptions made by many IRL applications\n\nMachine Teaching Problem for IRL {#machine-teaching-problem-for-irl}\n\nGiven an MDP, \\\\(M\\\\), and the teacher's reward function, \\\\(R^\\* =\n\\mathbf{w}^{\\*^T} \\phi (s)\\\\), find the set of demonstrations, \\\\(D\\\\), that\nminimizes the following optimization problem:\n\n\\begin{equation}\n  \\textrm{min}\\_{D} \\textrm{TeachingCost}(D) \\textrm{ s.t. }\n  \\textrm{Loss}(\\mathbf{w^\\*}, \\hat{\\mathbf{w}}) \\le \\epsilon,\n  \\hat{\\mathbf{w}} = IRL(D)\n\\end{equation}\n\nOptimizing this is hard, since there are a large number of candidate\nsets of demonstrations, and the IRL problem needs to be solved for\neach candidate set. The paper proposes a greedy set-cover\napproximation algorithm that requires solving only a single\npolicy-evaluation problem, using the Behavioural Equivalence Class\n(BEC) of the teacher's policy.\n\nTeaching Inverse Reinforcement Learners via Features and Demonstrations (Haug et al., 2018) {#teaching-inverse-reinforcement-learners-via-features-and-demonstrations}\n\nIt is difficult to specify a reward function that captures all\nimportant aspects. In these situations, learning from demonstrations\ntransforms the need of specifying this reward function to the task of\nproviding examples of optimal behaviour.\n\nThe paper considers the following setting:\n\nThe true reward function is a linear combination of features known\n    to the teacher\nThe learner also assumes the reward function is a linear combination\n    of features, different from the important ones (e.g. observing only\n    a subset)\nThe teaching risk is proposed to bound the performance gap of the\n    teacher and learner as a function of the learner's worldview\n\nTeaching risk is defined as:\n\n\\begin{equation}\n  \\rho\\left(A^{L} ; \\mathbf{w}^{\\*}\\right) :=\\max \\_{v \\in\n    \\operatorname{ker} A^{L},\\\\|v\\\\| \\leq 1}\\left\\langle\\mathbf{w}^{\\*},\n    v\\right\\rangle\n\\end{equation}\n\nWhere \\\\(A^L\\\\) is the learner's worldview. Geometrically it is the cosine\nof the angle between ker \\\\(A^L\\\\) and \\\\(\\mathbf{w}^\\*\\\\).\n\nLimiting the set of teachable features, choosing features that allow\nfor minimizing teaching risk experimentally shows better performance\nthan randomly choosing features.\n\nLearner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints (Tschiatschek et al., 2019) {#learner-aware-teaching-inverse-reinforcement-learning-with-preferences-and-constraints}\n\nThis paper considers the setting where the learner has preferences.\nThis captures:\n\nbehavioural bias\nmismatched worldviews\nphysical constraints\n\nLearner-aware teaching shows significant performance improvements\n\nMath of the paper is beyond me right now.\n\nInteractive Teaching Algorithms for Inverse Reinforcement Learning (Kamalaruban et al., 2019) {#interactive-teaching-algorithms-for-inverse-reinforcement-learning}\n\nConsiders the setting where the learner is assisted by a teacher. Two\nsub-settings are considered:\n\nWhere the teacher can fully observe the student's current policy,\n    and understands the student's dynamics (for theoretical bounds)\nWhere the teacher only has a noisy estimate of the learner's\n    current policy, and does not understand the student's dynamics\n\nThe environment is modelled as a MDP, where the learner does not have\naccess to the reward furncion R. The teaching objective is to achieve\na high-performing policy through learning from teacher demonstrations.\n\nThe learner is asssumed to use the MCE-IRL algorithm. Theoretical\nanalysis of the omniscient teacher shows that only \\\\(O(\\log\n\\frac{1}{\\epsilon})\\\\) demonstrations are required to achieve the\nteaching objective, an exponential improvement compared to selecting\ndemonstrations at random.\n\nIn the black box setting, the strategy considered picks the most\ninformative demonstration. This is evaluated experimentally. The black\nbox teacher is shown to learn faster than the agnostic teacher, in\nboth the linear and non-linear reward setting. In the non-linear\nreward setting, both learners are unable to learn a good policy, but\nin the black box teaching setting progress is made much quicker.\n\nWould be interesting to work through the proofs.\n\nBayesian Teaching {#bayesian-teaching}\n\nBayesian teaching aims to induce a target model in the learner by\npresenting teaching sets of data. This involves two sides of\ninference:\n\nTeacher's inference: over the space of possible teaching sets\nLearner's inference: over the space of possible target models\n\nBayesian Teaching as Model Explanation {#bayesian-teaching-as-model-explanation}\n\nThe intuition is that subsets of training data that lead a model to\nthe same (or approximately similar) inference as the model trained on\nall the data should be useful for understanding the fitted model.\n(Ravi Sojitra, 2018)\n\nBelow is an example of using Bayesian teaching, limited to a teaching\nset of dimension 2, to understand an MNIST model.\n\n{{}}\n\nOne can inspect the best and worst teaching sets to understand what\nthe model finds to be the best and worst representations for a\nparticular number.\n\nHence, Bayesian teaching is also useful in telling us which examples\nare most valuable: better suited to induce the desired target model.\n\nLearning To Interactively Learn and Assist (Woodward et al., 2019) {#learning-to-interactively-learn-and-assist}\n\nRewards and demonstrations are often defined and collected before\ntraining begins, when the human is most uncertain about what\ninformation would help the agent.\n\nKey idea: use interactive learning in contrast to rewards or\ndemonstrative learning to enable an agent to learn from another agent\nwho knows the current task.\n\nInteractive learning\n\nRobot Teaching and the Sim2Real gap {#robot-teaching-and-the-sim2real-gap}\n\nObtaining real-world training data can be expensive, and many RL\nalgorithms are sample-inefficient. Hence, many models are trained in a\nsimulated environment, and the \"sim2real\" gap causes these models to\nperform poorly on real-world tasks.\n(Lilian Weng, 2019)\n\nThere are several approaches to closing the sim2real gap:\n\nSystem Identification\n    System identification involves building a mathematical model for\n        a physical system. This requires careful calibration, which can\n        be expensive.\nDomain Adaptation\n    This refers to a set of transfer learning techniques that update\n        the data distribution in the simulated environment to match that\n        of the real world. Many of these are build on adversarial loss or GAN.\nDomain Randomization\n    A variety of simulated environments with randomized properties\n        are created, and to allow for training a robust model that works\n        across all these environments.\n\nBoth DA and DR are unsupervised. While DA requires a large amount of\nreal data samples to capture the distribution, DR requires little to no\nreal data.\n\nDomain Randomization {#domain-randomization}\n\nDefinitions\n\n    source domain\n    : The environment we have full access to (the\n        simulator). This is where training happens.\n\n    target domain\n    : The environment we want to transfer our model to\n        (the real world)\n\n    randomization parameters\n    : A set of parameters in the source\n        domain, which we can sample \\\\(\\xi\\\\)\n\nGoal\n\n    During policy training, episodes are collected from the source domain\n    with randomization applied. The policy learns to generalize across\n    all the environments. The policy parameter \\\\(\\theta\\\\) is trained to\n    maximize the expected reward \\\\(R(\\cdot)\\\\) average across a distribution\n    of configurations:\n\n    \\begin{equation}\n    \\theta^{\\*}=\\arg \\max \\{\\theta} \\mathbb{E}\\{\\xi \\sim \\Xi}\\left[\\mathbb{E}\\{\\pi\\{\\theta}, \\tau \\sim e\\_{\\xi}}[R(\\tau)]\\right]\n    \\end{equation}\n\n    where \\\\(\\tau\\_{\\xi}\\\\) is a trajectory collected in the source domain\n    randomized with \\\\(\\xi\\\\). **Discrepancies between the source and target\n    domains are modelled as variability in the source domain**.\n\n    In uniform domain randomization, each randomization parameter\n    \\\\(\\xi\\{i}\\\\) is bounded by an interval \\\\(\\xi\\{i}\n    \\in\\left[\\xi\\{i}^{\\mathrm{low}}, \\xi\\{i}^{\\mathrm{high}}\\right], i=1,\n    \\ldots, N\\\\), and each parameter is uniformly sampled within the range.\n\n    TODO  read ,\n\nTODO  Domain Randomization as Optimization (read )\n\n    One can view learning of randomization parameters as a bilevel\n    optimization.\n\n    Assume we have access to the real environment \\\\(e\\_{\\textrm{real}}\\\\) and\n    the randomization configuration is sampled from a distribution\n    parameterized by \\\\(\\phi\\\\), \\\\(\\xi \\sim P\\_{\\phi}(\\xi)\\\\), we would like to\n    learn a distribution on which policy \\\\(\\pi\\_\\theta\\\\) is trained on can\n    achieve maximal performance in \\\\(e\\_{\\textrm{real}}\\\\):\n\n    \\begin{equation}\n    \\begin{array}{c}{\\phi^{\\*}=\\arg \\min \\_{\\phi}\n      \\mathcal{L}\\left(\\pi\\{\\theta^{\\prime}(\\phi)} ; e\\{\\text { real\n      }}\\right)} \\\\ {\\text { where } \\theta^{\\*}(\\phi)=\\arg \\min\n      \\{\\theta} \\mathbb{E}\\{\\xi \\sim\n      P\\{\\phi}(\\xi)}\\left[\\mathcal{L}\\left(\\pi\\{\\theta} ;\n      e\\_{\\xi}\\right)\\right]}\\end{array}\n    \\end{equation}\n\n    where \\\\(\\mathcal{L}(\\pi ; e)\\\\) is the loss function of policy \\\\(\\pi\\\\)\n    evaluated in the environment \\\\(e\\\\).\n\nGuided Domain Randomization\n\n    Vanilla Domain Randomization assumes to access to the real data, and\n    randomization configuration is sampled as broadly and uniformly as\n    possible in sim, hoping that the real environment is covered under\n    this broad distribution.\n\n    Idea: guide domain randomization to use configurations that are \"more\n    realistic\". This avoids training models in unrealistic environments.\n\nTODO  read\n\nInvariant Risk Minimization (Arjovsky et al., 2019) {#invariant-risk-minimization}\n\nKey idea: To learn invariances across environments, find a data\nrepresentation such that the optimal classifier on top of that\nrepresentation matches for all environments.\n\nConsider a cow/camel classifier. If we train on labeled images where\nmost pictures of cows are taken on green pastures, and pictures of\ncamels in desserts, the classifier may learn to classify green\nlandscapes as cows, and beige landscapes as camels.\n\nTo solve this problem, we need to identify which properties of the\ntraining data are spurious correlations (e.g. background), and which\nare actual phenomenon of interest (animal shape). Spurious\ncorrelations are expected not to hold in unseen data.\n\nThe goal is to learn correlations invariant across training\nenvironments.\n\n{#}\n\nBibliography\nSimard, P. Y., Amershi, S., Chickering, D. M., Pelton, A. E., Ghorashi, S., Meek, C., Ramos, G., …, Machine teaching: a new paradigm for building machine learning systems, CoRR, (),  (2017).  ↩\n\nZhu, X., Singla, A., Zilles, S., & Rafferty, A. N., An overview of machine teaching, CoRR, (),  (2018).  ↩\n\nRafferty, A. N., Brunskill, E., Griffiths, T. L., & Shafto, P., Faster teaching via pomdp planning, Cognitive Science, 40(6), 1290–1332 (2015).  http://dx.doi.org/10.1111/cogs.12290 ↩\n\nDu, Y., Hsu, D., Kurniawati, H., Lee, W. S., Ong, S. C. W., & Png, S. W., A pomdp approach to robot motion planning under uncertainty, In ,  (pp. ) (2010). : . ↩\n\nBrown, D. S., & Niekum, S., Machine teaching for inverse reinforcement learning: algorithms and applications, CoRR, (),  (2018).  ↩\n\nHaug, L., Tschiatschek, S., & Singla, A., Teaching inverse reinforcement learners via features and demonstrations, CoRR, (),  (2018).  ↩\n\nTschiatschek, S., Ghosh, A., Haug, L., Devidze, R., & Singla, A., Learner-aware teaching: inverse reinforcement learning with preferences and constraints, CoRR, (),  (2019).  ↩\n\nKamalaruban, P., Devidze, R., Cevher, V., & Singla, A., Interactive teaching algorithms for inverse reinforcement learning, CoRR, (),  (2019).  ↩\n\nSojitra, R. (2018). Bayesian teaching as model explanation: an mnist example. Retrieved from https://ravisoji.com/2018/03/04/bayesian-teaching-as-explanation.html. Online; accessed 19 May 2019. ↩\n\nWoodward, M., Finn, C., & Hausman, K., Learning to interactively learn and assist, CoRR, (),  (2019).  ↩\n\nWeng, L. (2019). Domain randomization for sim2real transfer. Retrieved from https://lilianweng.github.io/lil-log/2019/05/05/domain-randomization.html. Online; accessed 28 June 2019. ↩\n\nArjovsky, M., Bottou, L\\'eon, Gulrajani, I., & Lopez-Paz, D., Invariant Risk Minimization, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/map_matching",
        "title": "Map Matching",
        "content": "\nMap matching compiles small numbers of consecutive scans into\nlocal maps \\\\(m\\_{\\mathrm{local}}\\\\). Maps are then compared to the global\nmap \\\\(m\\\\), such that the more similar \\\\(m\\\\) and \\\\(m\\_{\\mathrm{local}}\\\\), the\nlarger \\\\(p(m\\\\mathrm{local} | x\\t, m)\\\\). Cells in the local map are\ntransformed to the coordinates of the global map, and the map\ncorrelation function is computed.\n\nMap matching is easy to compute, but does not yield smooth\nprobabilities in the pose parameter \\\\(x\\_t\\\\). One solution is to convolve\nthe map with a Gaussian smoothness kernel before running map matching.\n\nMap matching considers the free-space in the scoring of 2 maps,\ncompared to the §likelihood\\field\\model which uses only the\nend-points.\n\nIssues {#issues}\n\nNo plausible physical explanation\nResult of map matching may incorporate areas beyond actual\n    measurement range.\n\nRelated {#related}\n\n§likelihood\\field\\model\n",
        "tags": []
    },
    {
        "uri": "/zettels/markov_chains",
        "title": "Markov Chains",
        "content": "\ntags\n: §statistics, §stochastic\\_processes\n\nIntroduction {#introduction}\n\nConsider a process that has a value in ecah time period. Let \\\\(X\\_n\\\\)\ndenote its value in time period \\\\(n\\\\), and suppose we want to make a\nprobability model for the sequence of successive values $X\\0, X\\1,\n&hellip;, $. The simplest model would probably assume that the \\\\(X\\_n\\\\) are\nindependent random variables, but often such an assumption is clearly\nunjustified. However, it may be reasonable to assume that the\nconditional distribution of \\\\(X\\_{n+1}\\\\) given all the past observations,\nonly depends on \\\\(X\\_n\\\\). Such an assumption defines a Markov chain, a\ntype of stochastic process.\n\nLet \\\\(\\left\\\\{ X\\_n, n = 0,1,2,\\dots \\right\\\\}\\\\) be a stochastic process\nthat takes on a finite or countable number of possible values. Unless\notherwise mentioned, this set of possible values of the process will\nbe denoted by the set of nonnegative integers \\\\(\\left\\\\{ 0,1,2, \\dots\n\\right\\\\}\\\\). If \\\\(X\\_n = i\\\\), then the process is said to be in state \\\\(i\\\\)\nat time \\\\(n\\\\). We suppose that whenever the process is in state \\\\(i\\\\),\nthere is a fixed probabibility \\\\(P\\_{ij}\\\\) that it will be next be in\nstate \\\\(j\\\\). That is, we suppose that:\n\n\\begin{equation}\n  P\\left\\\\{ X\\{n+1} = j | X\\n = i, X\\{n-1} = i\\{n-1}, \\dots \\right\\\\} = P\\_{ij}\n\\end{equation}\n\nfor all states \\\\(i\\0, i\\1, \\dots, i\\_{n-1}, i, j\\\\) and all \\\\(n \\ge 0\\\\). The\nvalue \\\\(P\\_{ij}\\\\) represents the probability that the proces will, when\nin state \\\\(i\\\\), next make a transition into state \\\\(j\\\\).\n\nChapman-Kolmogorov Equations {#chapman-kolmogorov-equations}\n\nHaving defined the one-step transition probabilities \\\\(P\\_{ij}\\\\), we can\nnow define the \\\\(n\\\\) -step transition probaiblities \\\\(P\\_{ij}^n\\\\) to be the\nprobability that the process in state \\\\(i\\\\) will be in state \\\\(j\\\\) after\n\\\\(n\\\\) additional transitions. That is:\n\n\\begin{equation}\n  P\\{ij}^n = P\\left\\\\{ X\\{n+k} = j | X\\_k = i \\right\\\\}\n\\end{equation}\n\nThe Chapman-Kolmogorov equations provide a method for computing these\nn-step transition probabilities. These equations are:\n\n\\begin{equation}\n  P\\{ij}^{n+m} = \\sum\\{k=0}^{\\infty} P\\{ij}^n P\\{kj}^m \\text{ for all\n  } n,m \\ge 0, \\text{ all } i,j\n\\end{equation}\n\nand are most easily understood by noting that \\\\(P\\{ik}^n P\\{kj}^m\\\\)\nrepresents the probability that starting in \\\\(i\\\\) the process will go to\nstate \\\\(j\\\\) in \\\\(n+m\\\\) transitions through a path which takes it into\nstate \\\\(k\\\\) at the nth transition. Summing over all intermediate states\nyields the total probability that the process will be in state \\\\(j\\\\)\nafter \\\\(n+m\\\\) transitions.\n\nIf we let \\\\(\\mathbf{P}^{(n)}\\\\) denote the matrix of $n$-step transition\nprobabilities \\\\(P\\_{ij}^n\\\\) then the Chapman-Kolmogorov equation asserts\nthat:\n\n\\begin{equation}\n  \\mathbf{P}^{(n+m)} = \\mathbf{P}^n \\cdot \\mathbf{P}^m\n\\end{equation}\n\nThe $n$-step transition matrix can be obtained by multiplying the\nmatrix \\\\(\\mathbf{P}\\\\) by itself \\\\(n\\\\) times.\n\nClassification of States {#classification-of-states}\n\nState \\\\(j\\\\) is said to be accessible from state \\\\(i\\\\) if \\\\(P^{n}\\_{ij} > 0\\\\)\nfor some \\\\(n \\ge 0\\\\). Two states \\\\(i\\\\) and \\\\(j\\\\) that are accessible to each\nother are said to communicate, and we write \\\\(i \\leftrightarrow j\\\\).\n\nAny state communicates with itself, since by definition \\\\(P\\_{ii}^0 =\n1\\\\).\n\nThe relation of communication satisfies the following 3 properties:\n\nState \\\\(i\\\\) communicates with state \\\\(i\\\\), for all \\\\(i \\ge 0\\\\)\nIf state \\\\(i\\\\) communicates with state \\\\(j\\\\), then state \\\\(j\\\\)\n    communicates with state \\\\(i\\\\).\nIf state \\\\(i\\\\) communicates with state \\\\(j\\\\), and state \\\\(j\\\\)\n    communicates with state \\\\(k\\\\), then state \\\\(i\\\\) communicates with state\n    \\\\(k\\\\).\n\nTwo states that communicate are said to be in the same class. The\nconcept of communication divides the state space into several separate\nclasses. The Markov chain is said to be irreducible if there is only 1\nclass, that is all states communicate with each other.\n\nLimiting Probabilities {#limiting-probabilities}\n\nSuppose we have a \\\\(\\mathbf{P}\\\\) matrix such as:\n\n\\begin{equation}\n  \\mathbf{P}^{(4)} = \\left[ \\begin{matrix}\n    0.5749 & 0.5241 \\\\\\\\\\\\\n    0.5668 & 0.4332\n  \\end{matrix} \\right]\n\\end{equation}\n\nWe can compute \\\\(\\mathbf{P}^{(8)}\\\\):\n\n\\begin{equation}\n  \\mathbf{P}^{(8)} = \\left[ \\begin{matrix}\n    0.572 & 0.428 \\\\\\\\\\\\\n    0.570 & 0.430\n  \\end{matrix} \\right]\n\\end{equation}\n\nNotice that \\\\(\\mathbf{P}^{(8)}\\\\) is almost identical to\n\\\\(\\mathbf{P}^{(4)}\\\\), and each of the rows of \\\\(\\mathbf{P}^{(8)}\\\\) has\nalmost identical entries. It seems that \\\\(P^n\\_{ij}\\\\) is converging to\nsome value that is the same for all \\\\(i\\\\). There seems to exist a\nlimiting probability that the process will be in state \\\\(j\\\\) after a\nlarge number of transitions, and this value is independent of the\ninitial state.\n\nState \\\\(i\\\\) is said to have period \\\\(d\\\\) if \\\\(P^n\\_{ii} = 0\\\\) whenever \\\\(n\\\\) is\nnot divisible by \\\\(d\\\\), and \\\\(d\\\\) is the largest integer with this\nproperty. For instance, it may be possible for the process to enter\nstate \\\\(i\\\\) only at times \\\\(2, 4, 6, \\dots, 8\\\\), in which case state \\\\(i\\\\)\nhas period. A state with period 1 is said to be aperiodic.\n\nIn a finite-state Markov chain, all recurrent states are positive\nrecurrent. Positive recurrent, aperiodic states are called ergodic.\n\nFor an irreducible ergodic Markov chain \\\\(\\lim\\_{n \\rightarrow \\infty}\nP^n\\_{ij}\\\\) exists and is independent of \\\\(i\\\\). Furthermore, letting\n\n\\begin{equation}\n  \\pi\\j = \\lim\\{n \\rightarrow \\infty} P^n\\_{ij}, j \\ge 0\n\\end{equation}\n\nthen \\\\(\\pi\\_j\\\\) is the unique nonnegative solution of:\n\n\\begin{equation}\n  \\pi\\j =\\sum\\{i=0}^{\\infty} \\pi\\i P\\{ij}, j \\ge 0, \\sum\\_{i =\n    0}^{\\infty} \\pi\\_{j} = = 1\n\\end{equation}\n\nWe can obtain an expression for \\\\(P(X\\_{n+1} = j)\\\\) by conditioning on\nthe state at time \\\\(n\\\\):\n\n\\begin{align}\n  P(X\\{n+1} = j)  &= \\sum\\{i=0}^{\\infty} P(X\\{n+1} = j | X\\n = i)\n                    P(X\\_n = i) \\\\\\\\\\\\\n                  &= \\sum\\{i=0}^{\\infty} P\\{ij} P(X\\_n = i)\n\\end{align}\n\nThese long run proportions \\\\(\\pi\\_j, j \\ge 0\\\\) are often called\nstationary probabilities. The reason being that if the initial state\nis chosen according to probabilities \\\\(\\pi\\_j, j \\ge 0\\\\), then the\nprobability of being in state \\\\(j\\\\) at any time \\\\(n\\\\) is also equal to\n\\\\(\\pi\\_j\\\\).\n",
        "tags": []
    },
    {
        "uri": "/zettels/markov_decision_process",
        "title": "Markov Decision Process",
        "content": "\nA sequential decision problem for a fully observable, stochastic\nenvironment with a Markovian transition model and additive rewards is\ncalled a Markov decision process. It consists of a set of states, with\ninitial state \\\\(s\\_0\\\\), a set \\\\(ACTIONS(s)\\\\) of actions in each state, a\ntransition model \\\\(P(s'|s, a)\\\\), and a reward function \\\\(R(s)\\\\).\n\nA policy, denoted \\\\(\\pi\\\\), specifies what the agent should do in any state\n\\\\(s\\\\). This action is denoted by \\\\(\\pi(s)\\\\). The optimal policy \\\\(\\pi^\\*\\\\) yields the\nhighest expected utility.\n\nThe careful balancing of risk and reward is a characteristic of MDPs\nthat does not arise in deterministic search problems.\n\nOptimality in Markov Decision Processes {#optimality-in-markov-decision-processes}\n\nFinite Horizon {#finite-horizon}\n\n\\begin{equation}\n  E\\left( \\sum\\{t=0}^{h} r\\t \\right)\n\\end{equation}\n\nInfinite Horizon {#infinite-horizon}\n\n\\begin{equation}\n  E\\left( \\sum\\{t=0}^{\\infty} \\gamma^t r\\t \\right)\n\\end{equation}\n\nAverage-reward {#average-reward}\n\n\\begin{equation}\n\\lim\\{h \\rightarrow \\infty} E\\left( \\sum\\{t=0}^{h} \\frac{1}{h} r\\_t \\right)\n\\end{equation}\n\nLearning Performance (Kaelbling {\\it et al.}, 1996) {#learning-performance}\n\nAsymptotic convergence:\n\n\\begin{equation}\n\\pi\\_n \\rightarrow \\pi^\\* \\text { as } n \\rightarrow \\infty\n\\end{equation}\n\nPAC:\n\n\\begin{equation}\n  P(N\\_{errors} > F(\\cdot, \\epsilon, \\delta)) \\le \\delta\n\\end{equation}\n\nDoes not give any guarantee about the policy while it is learning\n\nRegret (e.g. bound \\\\(B\\\\) on total regret):\n\n\\begin{equation}\n  \\mathrm{max} \\sum\\{t=0}^{T} r\\{tj} - r\\_t (Dann {\\it et al.}, 2017)\n\nBibliography\nKaelbling, L. P., Littman, M. L., & Moore, A. W., Reinforcement learning: a survey, Journal of artificial intelligence research, 4(), 237–285 (1996).  ↩\n\nDann, C., Lattimore, T., & Brunskill, E., Unifying pac and regret: uniform pac bounds for episodic reinforcement learning, In , Advances in Neural Information Processing Systems (pp. 5713–5723) (2017). : . ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/markov_localization",
        "title": "Markov Localization",
        "content": "\nMarkov Localization {#markov-localization}\n\nA direct extension of the §bayes\\_filter, but using the map \\\\(m\\\\) of the\nenvironment:\n\n\\begin{algorithm}\n  \\caption{Markov Localization}\n  \\label{markov\\_localization}\n  \\begin{algorithmic}[1]\n    \\Procedure{Markov Localization}{$\\text{bel}(x\\{t-1}), u\\t, z\\_t, m$}\n    \\ForAll{$x\\_t$}\n    \\State $\\overline{\\text{bel}}(t) = \\int p(x\\t | u\\t, x\\_{t-1}, m)\n    \\text{bel}(x\\_{t-1}) dx$\n    \\State $\\text{bel}(t) = \\eta p(z\\t | x\\t, m)\\overline{\\text{bel}}(t) (x\\_t)$\n    \\EndFor\n    \\State \\Return $bel(x\\_t)$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\nThe initial belief reflects initial knowledge of the robot pose, and\ncan be instantiated differently:\n\nIf the initial pose is known, \\\\(\\mathrm{bel}(x\\_0)\\\\) is a point-mass\ndistribution such that:\n\n\\begin{equation}\n  \\operatorname{bel}\\left(x\\{0}\\right)=\\left\\\\{\\begin{array}{ll}{1} & {\\text { if } x\\{0}=\\bar{x}\\_{0}} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n\\end{equation}\n\nHowever, point-mass distributions are discrete and do not have a\ndensity, so in most scenarios, a narrow Gaussian centered around\n\\\\(\\overline{x}\\_0\\\\) is used instead.\n\nIf the initial pose is unknown, \\\\(\\mathrm{bel}(x\\_0)\\\\) is initialized\nwith a uniform distribution over the space of all legal poses in the map.\n",
        "tags": []
    },
    {
        "uri": "/zettels/markovian_assumption",
        "title": "Markovian Assumption",
        "content": "\nKey idea: the past and future data are independent, conditioned on the\n present.\n\nThe Markovian assumption can be violated in many ways:\n\nUnmodelled dynamics in the environment\nInaccuracies in probabilistic models\nApproximation errors when using approximate representations of\n    belief functions\n",
        "tags": []
    },
    {
        "uri": "/zettels/matplotlib",
        "title": "Matplotlib",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/mc_methods",
        "title": "Monte Carlo Methods",
        "content": "\ntags\n: Machine Learning Algorithms, Probabilistic Graph Models\n\nMonte Carlo methods make use of random numbers to solve the following\nproblems:\n\nGenerating samples \\\\(\\left\\\\{x^{( r)}\\right\\\\}^{R}\\_{r=1}\\\\) from a given\n    probability distribution \\\\(P(x)\\\\).\nEstimate expectation of functions under this distribution:\n\n\\begin{equation}\n  \\Phi = \\langle \\phi(x) \\rangle = \\int d^N P(x) \\phi(x)\n\\end{equation}\n\nThis probability distribution is called the target density. The\ntarget density is often the posterior of a model's parameters, given\nobserved data.\n\nIf we solve the first problem of sampling, then these samples can be\nused to solve the second problem via the Monte Carlo estimator:\n\n\\begin{equation}\n  \\hat{\\phi} = \\frac{1}{R}\\sum\\_{r} \\phi(\\mathbf{x}^{( r)})\n\\end{equation}\n\nIf the samples are generated from \\\\(P(x)\\\\), then the expectation of\n\\\\(\\hat{\\phi}\\\\) is the same as the expectation of \\\\(\\phi\\\\). The variance of\n\\\\(\\hat{\\phi}\\\\) decreases as \\\\(\\sigma^2/R\\\\), where \\\\(\\sigma^2\\\\) is the\nvariance of \\\\(\\phi\\\\). This is so important that it is restated here:\n\n> The accuracy of the Monte Carlo estimate is dependent only on the\n> variance of \\\\(\\phi\\\\), and not on the dimensionality of the space sampled.\n\nWhy is sampling hard? {#why-is-sampling-hard}\n\nSuppose we can evaluate \\\\(P(x)\\\\) up to a multiplicative constant \\\\(Z\\\\):\n$P^*(x) = \\\\(P(x) Z\\\\). To generate samples from \\\\(P(x)\\\\), we need to\nknow the normalizing constant \\\\(Z\\\\). Even if we knew \\\\(Z\\\\), there is no\nobvious way to sample without enumerating most or all of the possible\nstates.\n",
        "tags": []
    },
    {
        "uri": "/zettels/mcts",
        "title": "Monte Carlo Tree Search",
        "content": "\nMonte Carlo Tree Search {#monte-carlo-tree-search}\n\nWe are still far from making anything that even resembles a strong AI.\nWhat makes MCTS different from Minimax?\n\nMinimax can take an impractical amount of time to do a full search of\nthe game tree, especially games with high branching factor. Some games\nare highly open-ended, with game trees that are highly complex. This\nmakes it difficult to write an evaluation function for each state.\nMCTS is a technique that will give good results for games, and is\ndomain-independent.\n\nUCB1 constructs statistical confidence intervals:\n\n\\begin{equation}\n  \\bar{x\\i} \\pm \\sqrt{\\frac{2 \\ln n}{n\\i}}\n\\end{equation}\n\nwhere:\n\n\\\\(\\bar{x\\_i}\\\\) is the mean payout for action \\\\(i\\\\)\n\\\\(n\\_i\\\\) is the number of simulations of action \\\\(i\\\\)\n\\\\(n\\\\) is the total number of plays\n\nThe strategy is to pick the action with the highest upper bound each time.\n\nHow could an AI possibly \"plan\" ahead when there are so many potential\nmoves and counter moves in Go?\n\nMCTS builds a statistics tree (detailing value of nodes) that\npartially maps onto the entire tree. Statistics tree guides the AI.\n\nMCTS constructs the statistics tree at the starting point.\n\nSelection\n: All child nodes have now been visited at least once.\n    Now AI can select the best child node.\n    based on how good the statistics are\n    how much the child node has been \"ignored\"\n\nExpansion\n: Add a new node that the AI will investigate\n\nSimulation\n: starting from position represented by left child node,\n    make random moves repeatedly until the game is won or lost\n\nUpdate\n: Depending on win or loss, update left child node in stats\n\nThe parent nodes inherit statistics from child nodes.\n\nThe node with the highest number of simulations will be chosen as the\nnext move.\n\nThe first phase, selection, lasts until the statistics necessary to\ntreat each position reached as a multi-armed bandit problem is\ncollected.\n\nThe second phase, expansion, occurs when the algorithm can longer be\napplied. An unvisited child is randomly chosen, and a new record node\nis added to the tree of statistics.\n\nAfter expansion, the remainder of the playout is in phase 3,\nsimulation. This is done as a typical monte carlo simulation.\n\nWhen the playout reaches the end, the update phase takes place. All of\nthe positions visited during this playout have their play count and\ntheir win count incremented.\n\nSome great references for productionized implementations of MCTS\ninclude:\n\nminigo/strategies.py\nminigo/mcts.py\n\nReferences {#references}\n\nIntroduction to Monte Carlo Tree Search - Jeff Bradberry\nA Deep Dive into Monte Carlo Tree Search\n",
        "tags": []
    },
    {
        "uri": "/zettels/meta_learning",
        "title": "Meta Learning",
        "content": "\ntags\n: Reinforcement Learning ⭐\n\nLearning to learn: learn an update rule from related tasks\n\nFor example, tasks are related through a low-dimensional embedding.\n\n{{}}\n\nModel-Agnostic Meta Learning (MAML) {#model-agnostic-meta-learning--maml}\n\nBased on 2nd-order gradient descent:\n\n2-stage gradient-based approach on batches of tasks \\\\(\\mathcal{T}\\\\):\n\nInner loop:\n\n\\begin{equation}\n\\theta\\i' = \\theta - \\alpha \\nabla\\\\theta L\\{\\mathcal{T}}(f\\\\theta)\n\\end{equation}\n\nOuter Loop:\n\n\\begin{equation}\n  \\theta=\\theta-\\beta \\nabla\\{\\theta} \\sum\\{\\mathcal{T}\\{i} \\sim p(\\mathcal{T})} \\mathcal{L}\\{\\mathcal{J}\\{i}}\\left(f\\{\\theta\\_{i}^{\\prime}}\\right)\n\\end{equation}\n\nResources {#resources}\n\nICML 2019 Meta-learning Tutorial\n",
        "tags": []
    },
    {
        "uri": "/zettels/metropolis_hastings",
        "title": "Metropolis-Hastings Method",
        "content": "\nIn Importance Sampling and Rejection Sampling, the proposal\ndistribution \\\\(q(x)\\\\) needs to be similar to \\\\(p(x)\\\\). The\nMetropolis-Hastings method uses a proposal density \\\\(q(x;x^(t))\\\\) that\nis dependent on the current state \\\\(x^(t)\\\\). A simple distribution such\nas a Gaussian centered on \\\\(x^(t)\\\\) can be used.\n\nMethod {#method}\n\nEvaluate \\\\(p^\\star(x)\\\\) for any \\\\(x\\\\).\nA tentative new state \\\\(x'\\\\) is generated from the proposal density \\\\(q(x';x^{(t)})\\\\).\nCompute \\\\(a = \\frac{p^\\star(x')}{p^\\star(x^{(t)})} \\frac{q(x^{(t)};x')}{q(x';x^{(t)})}\\\\)\nIf \\\\(a \\ge 1\\\\), accept new state and set \\\\(x^{(t+1)}= x'\\\\), else set\n    \\\\(x^{(t+1)} = x^{(t)}\\\\)\n\nPros and Cons {#pros-and-cons}\n\nWill still give answers in high-dimensional settings\nLengthy simulations may be needed for convergence, because of the\n    quadratic dependence on the lengthscale-ratio. A random walk is\n    extremely slow, and should try to be suppressed.\n\nSuppressing Random Walks {#suppressing-random-walks}\n\nHamiltonian Monte-Carlo methods make use of gradient information to\nreduce random-walk behaviour.\n",
        "tags": []
    },
    {
        "uri": "/zettels/ml_papers",
        "title": "Machine Learning Papers",
        "content": "\ntags\n: Papers\n",
        "tags": []
    },
    {
        "uri": "/zettels/model_based_rl",
        "title": "Model-Based Reinforcement Learning",
        "content": "\nIn model-free Reinforcement Learning ⭐, we assumed that \\\\(p(s\\_{t+1} |\ns\\t, a\\t)\\\\) is unknown, and no attempt is made to learn it. However, we\noften know the dynamics. For example, in games such as chess, and\neasily modeled systems, like car navigation.\n\nModel-based Reinforcement learning is about learning system dynamics,\nand using the learnt system to make decisions.\n\nIn the stochastic open-loop case,\n\n\\begin{equation}\n  p\\{\\theta}(s\\1, \\dots, s\\T | a\\1, \\dots, a\\T) = p(s\\1)\n  \\prod\\{t=1}^{T} p(s\\{t+1} | s\\t, a\\t)\n\\end{equation}\n\nand we'd want to choose:\n\n\\begin{equation}\n  a\\1, \\dots, a\\T = \\mathrm{argmax}\\{a\\1, \\dots, a\\_T} E \\left[\n    \\sum\\{t} r(s\\t, a\\t) | a\\1, \\dots, a\\_T \\right]\n\\end{equation}\n\nIn a closed-loop setting, the process by which the agent performs\nactions receives feedback. Planning in the open-loop case can go\nextremely awry.\n\nIn the stochastic closed-loop case,\n\n\\begin{equation}\n  p(s\\1, a\\1, \\dots, s\\T, a\\T) = p(s\\1) \\prod\\{t=1}^{T} \\pi(a\\t | s\\t)\n  p(s\\{t+1} | s\\t, a\\_t)\n\\end{equation}\n\nwhere \\\\(\\pi = \\mathrm{argmax}\\{\\pi}E\\{\\tau \\sim p(\\tau)} \\left[ \\sum\\_t\nr(s\\t, a\\t)\\right]\\\\)\n\nSimple Model-based RL {#simple-model-based-rl}\n\nThe most basic algorithm does this:\n\nRun base policy to obtain tuples \\\\((s\\{t+1}, s\\t, a\\_t)\\\\)\nUse supervised learning on the tuples to learn the dynamics \\\\(f(s\\_t,\n       a\\t)\\\\), minimizing loss \\\\(\\sum\\i |f(s\\t, a\\t) - s\\_{t+1}|^2\\\\)\nUse the learnt model for control\n\nProblem: distributional shift. We learn from some base distribution\n\\\\(\\pi\\{0}\\\\), but control via \\\\(\\pi\\{f}\\\\). This can be addressed by\nre-sampling \\\\((s, a, s')\\\\) to \\\\(D\\\\) after step 3.\n\nPerformance gap in model-based RL {#performance-gap-in-model-based-rl}\n\nModel-based RL tends to plateau in performance much earlier than\nmodel-free RL. This is because it needs to use a function approximator\nthat:\n\nDoes not overfit a low amount of samples\nBut also is expressive enough\n\nAnd this turns out to be hard. Below, the model is overfitted, and the\nplanner might want to exploit going to this non-existent peak, and\nresult in nonsensical behaviour.\n\nWe can use uncertainty estimation to detect where the models may be\nwrong, for example by using §gaussian\\_processes.\n\nFor planning under uncertainty, one can use the expected value,\noptimistic value, or pessimistic value, depending on application.\n\nHow can we have uncertainty-aware models? {#how-can-we-have-uncertainty-aware-models}\n\nUse output entropy: high entropy means model is uncertain.\n    But model might overfit and be confident, but wrong!\nEstimate model uncertainty: the model is certain about the data,\n    but we are not certain about the model.\n    estimate \\\\(\\log p(\\theta | D)\\\\)\nOr use Bayesian NN, Bootstrap ensembles\n\nProblems with model-based RL {#problems-with-model-based-rl}\n\nHigh dimensionality\nRedundancy\n\nPartial observability {#partial-observability}\n\nLatent space models: separately learn \\\\(p(o\\t | s\\t)\\\\) (observation\nmodel; high dimensional, but not dynamic) and \\\\(p(s\\{t+1} | s\\t,\na\\t)\\\\) (dynamics model; low dimensional, but dynamic), and \\\\(p(r\\t |\ns\\t, a\\t)\\\\) (reward model).\n",
        "tags": []
    },
    {
        "uri": "/zettels/model_compression",
        "title": "Model Compression",
        "content": "\ntags\n: §machine\\learning\\algorithms\n",
        "tags": []
    },
    {
        "uri": "/zettels/motion_model_with_maps",
        "title": "Motion Model With Maps",
        "content": "\ntags\n: Velocity Motion Model, Odometry Motion Model\n\nOften, we are given map \\\\(m\\\\) of the environment, giving us further\ninformation about the robot pose \\\\(x\\_t\\\\). In general,\n\n\\begin{equation}\n  p\\left(x\\{t} | u\\{t}, x\\{t-1}\\right) \\neq p\\left(x\\{t} | u\\{t}, x\\{t-1}, m\\right)\n\\end{equation}\n\nAnd the map-based motion model should give better results. Computing\nthis motion model in closed form is difficult. An approximation via\nfactorization works well where the distance \\\\(x\\{t-1}\\\\) and \\\\(x\\t\\\\) is\nsmall.\n\n\\begin{equation}\n  p\\left(x\\{t} | u\\{t}, x\\{t-1}, m\\right)=\\eta p\\left(x\\{t} | u\\{t}, x\\{t-1}\\right) p\\left(x\\_{t} | m\\right)\n\\end{equation}\n",
        "tags": []
    },
    {
        "uri": "/zettels/multivariable_calculus",
        "title": "Multi-variable Calculus",
        "content": "\nStudy of generalizations of the fundamental theorem of calculus to\nhigher dimensions:\n\nFundamental theorem of calculus for line integrals\nGreen's theorem\nStoke's theorem\n",
        "tags": []
    },
    {
        "uri": "/zettels/nat_eliason",
        "title": "Nat Eliason",
        "content": "\nActive user of Roam Research\n\nsocials\n: website, twitter\n",
        "tags": []
    },
    {
        "uri": "/zettels/negotiation",
        "title": "Negotiation",
        "content": "\ntags\n: §soft\\_skills\n\nA good negotiator prepares, going in, to be ready for possible\n    surprises; a great negotiator aims to use their skills to reveal\n    the surprise they are certain to find\n\nDon't commit assumptions; instead view them as hypotheses and use\n    the negotiation to test them rigorously\n\nPeople who view negotiation as a battle of arguments become\n    overwhelmed by the voices in their head. Negotiation is not an act\n    of battle; it's a process of discovery. The goal is to uncover as\n    much information as possible.\n\nTo quiet the voices in your head, make your sole and\n    all-encompassing focus the other person and what they have to say.\n\nSlow. It. Down. If we're in too much of a hurry, people feel as if\n    they're not being heard.\n\nPut a smile on your face. When people are in a positive frame of\n    mind, they are more likely to collaborate and problem-solve.\n\nUse voice inflections appropriately:\n    The late-night DJ voice: Inflect your voice downward, keeping it\n        calm and slow. This creates an aura of authority and trustworthiness.\n    The positive/playful voice: This should be the default voice.\n        Maintain a light and encouraging attitude.\n    The assertive voice: Used rarely.\n\nMirrors: Repeat the last three words of what someone has just said.\n    This insinuates similarity, which facilitates bonding. It\n    encourages the other side to empathise and bond with you.\n\nTactical Empathy: By acknowledging negative feelings and the other\n    person's situation, you immediately convey that you are listening.\n\nFocus on clearing the barriers to agreement. Do an accusation\n    audit: get barriers and negative influences into the open\n\nPause. After you label a barrier or mirror a statement, the\n    counterpart will fill the silence.\n\nLabel your counterpart's fears to diffuse their power.\n\nList the worst things that the other party could say about you and\n    say them before the other person can.\n\nRemember you're dealing with a person who wants to be appreciated\n    and understood. Use labels to reinforce and and encourage positive\n    perceptions and dynamics.\n\nBreak the habit of getting people to say \"yes\". Being pushed for\n    yes makes people defensive.\n\n\"No\" is not a failure. \"No\" often means \"wait\", or \"I'm not\n    comfortable with that\". It is the start of a negotiation, not the end.\n\n\"Yes\" is the final goal of a negotiation, but don't aim for it at\n    the start.\n\nSaying \"No\" makes the speaker feel safe, secure and in control.\n    Your counterpart defines their space and gains the confidence and\n    the comfort to listen to you. \"Is now a bad time to talk?\" is\n    always better than \"Do you have a few minutes to talk?\"\n\nSometimes the only way to get your counterpart to listen and\n    engage with you is by forcing them into a \"No\". This means\n    intentionally mislabeling one of their emotions or desires or\n    asking a ridiculous question.\n\nNegotiate in their word. Don't beat them with logic. Ask them\n    questions that open paths to your goals.\n\nIf a potential business partner is ignoring you, contact with a\n    clear and concise \"No\"-oriented question.\n\nCreating unconditional positive regard opens the door to changing\n    thoughts and behaviours. Humans have an innate urge toward\n    socially constructive behaviour. The more a person feels\n    understood, the more likely that urge for constructive behaviour\n    will take hold.\n\n\"That's right\" is better than \"yes\". Reaching \"that's right\" in a\n    negotiation creates breakthroughs.\n\nUse a summary to trigger a \"that's right\". The building blocks of\n    a good summary are a label combined with paraphrasing. Identify,\n    re-articulate, and emotionally affirm the world according to your\n    counterpart.\n\nAll negotiations are defined by a network of underlying desires\n    and needs.\n\nMeeting halfway often leads to bad deals for both sides.\n\nApproaching deadlines entice people to rush the negotiating\n    process and do impulsive things that are against their best\n    interests.\n\nThe word \"fair\" is an emotional term people usually exploit to put\n    the other side on the negative and gain concessions. Don't get\n    suckered into a concession, always ask why.\n\nYou can bend your counterpart's reality by anchoring his starting\n    point. Before you make an offer, emotionally anchor them by saying\n    how bad it will be. Set an extreme anchor to make your \"real\"\n    offer seem reasonable, or use a range to seem less aggressive.\n\nPeople will take more risks to avoid a loss than to realize a\n    gain. Make sure your counterpart sees that there is something to\n    lose with inaction.\n\nThe listener is the one in control of the conversation.\n\nDon't try to force your opponent to admit that you are right.\n    Aggressive confrontation is the enemy of constructive negotiation.\n\nAvoid questions that can be answered with \"yes\" or tiny pieces of\n    information. These inspire the human need for reciprocity; you\n    will be expected to give something back.\n\nAsk calibrated questios that start with the words \"how\" or \"what\".\n    Give your counterpart the illusion of control, inspire them to\n    speak at length, revealing important information.\n\nDon't ask questions that start with \"why\" unless you want your\n    counterpart to defend a goal that serves you.\n\nCalibrate your questions to point your counterpart toward solving\n    your problem. This will encourage them to expend their energy on\n    devising a solution.\n\nAsking calibrated \"how\" questions keep your counterpart engaged\n    but off-balance. Answering the questions give your counterpart the\n    illusion of control.\n\nUse \"how\" questions to shape the negotiating environment. Get your\n    counterpart searching for solutions to your problem.\n\nDon't just pay attention to the people you're negotiating with\n    directly; always identify the motivations of the player's behind\n    the scenes.\n\nFollow the 7-38-55 percent rule by paying close attention to tone\n    of voice and body language. Incongruence between words and\n    non-verbal signs will show when your counterpart is lying or\n    uncomfortable with a deal.\n\nIs the \"Yes\" real or counterfeit? Test with the rule of three: use\n    calibrated questions, summaries and labels to get your counterpart\n    to reaffirm their agreement at least three times.\n\nA person's use of pronouns offers deep insights into their\n    relative authority. If you're hearing a lot of \"my\", \"I\", the real\n    power to decide probably lies elsewhere. Picking up a lot of \"we\",\n    \"they\", it's more likely you're dealing with a savvy decision\n    maker with their options open.\n\nUse your own name to make yourself a real person to the other\n    side, and even get your own personal discount.\n\nIdentify your counterpart's negotiating style: accomodator,\n    analyst, or assertive\n\nReferences {#references}\n\nNever Split the Difference\n",
        "tags": []
    },
    {
        "uri": "/zettels/networking",
        "title": "Computer Networking",
        "content": "\ntags\n: §operating\\_systems\n\nIntroduction {#introduction}\n\nWhat is the Internet? {#what-is-the-internet}\n\nThe internet is a computer network that interconnects hundreds of\nmillions of computing devices throughout the world.\n\nEnd systems are connected together by a network of communication links\nand packet switches. The common packet switches are routers and\nlink-layer switches. The sequence of communication links and packet\nswitches traversed by a packet from the sending end system to the\nreceiving end system is known as the route, or path through a network.\n\nEnd systems access the internet through _Internet Service Providers\n(ISPs)_. ISPs provide a variety of types of network access to the end\nsystems.\n\nEnd systems, packet switches and other pieces of the Internet run\nprotocols that control the sending and receiving of information within\nthe internet. The Internet's principal protocols are the Transmission\nControl Protocol (TCP) and the Internet Protocol (IP), known\ncollectively as TCP/IP.\n\nWhat is a protocol? {#what-is-a-protocol}\n\n> A protocol defines the format and the order of messages exchanged\n> between two or more communicating entities, as well as the actions\n> taken on the transmission and/or receipt of a message or other event.\n\nThe Network Edge {#the-network-edge}\n\nThe computers and other devices that are connected to the Internet are\noften called end systems. This is because they sit at the edge of the\nInternet. Each system are also referred to as hosts because they host\napplication programs such as the web browser, or a server. Hosts are\nsometimes further divided into 2 categories: clients and servers.\n\nAccess networks include the digital subscriber line (DSL), cable, and\nfiber to the home (FTTH). Ethernet and WiFi access networks are now\ncommon across enterprise, university campuses as well as in homes.\nWide-area wireless access such as 4G and LTE provide wireless access\nto the Internet by sending and receiving packets through a base\nstation.\n\nPhysical Mediums {#physical-mediums}\n\nFor reach transmitter-receiver pair, bits are interchanged. These bits\nare sent by electromagnetic waves, or optical pulses across a physical\nmedium.\n\nThe least expensive and most commonly used transmission medium is the\ntwisted-pair copper wire. It consists of 2 insulated copper wires,\ntwisted togethter to reduce electrical interference from similar pairs\nclose by.\n\n{{}}\n\nCoaxial cables consist of 2 copper conductors, but the two conductors\nare concentric rather than parallel. These achieve high data\ntransmission rates, and can be used as a guided shared medium. They\nare common in cable television systems.\n\n{{}}\n\nThe Network Core {#the-network-core}\n\nPacket Switching\n\n    In a network application, end systems exchange messages with each\n    other. Messages can contain anything the application designer wants.\n    To send a message from a source end system to a destination end\n    system, the source breaks long messages into smaller chunks of data\n    known as packets. Each packet travels through communication links and\n    packet switches. Packets are transmitted over each communication link\n    at a rate equal to the full transmission rate of the link. So if a\n    source end system or a packet switch is sending a packet of \\\\(L\\\\) bits\n    over a link with transmission rate \\\\(R\\\\) bits/sec, then the time to\n    transmit the packet is \\\\(\\frac{L}{R}\\\\) seconds.\n\n    Most packet switches use store-and-forward transmission at the inputs\n    to the links. This means that the packet switch must receive the\n    entire packet before it can begin to transmit the first bit of the\n    packet onto the outbound link.\n\n    Each packet switch has multiple links attached to it. For each\n    attached link, the packet switch has an output buffer, which stores\n    packets that the router is about to send into that link. If an\n    arriving packet needs to be transmitted onto a link but finds the link\n    busy with the transmission of another packet, the arriving packet must\n    wait in the output buffer. This results in output buffer _queuing\n    delays_. Packet loss will occur -- either the arriving packet or one of\n    the already-queued packets will be dropped.\n\n    How does a router determine which link it should forward the packet\n    onto? In the Internet, each end system is assigned an IP address. The\n    source end system includes the destination IP address in the packet's\n    header. Each router has a forwarding table that maps destination\n    addresses to its outbound links.\n\nCircuit Switching\n\n    In circuit-switched networks, the resources needed along apath\n    (buffers, link transmission rate) to provide for communication between\n    end-systems are reserved for the duration of the communication session\n    between the end systems. Traditional telephone networks are examples\n    of such circuit-switched networks.\n\n    A circuit in a link is implemented with either _frequency-division\n    multiplexing (FDM)_ or time-division multiplexing (TDM). With FDM, the\n    frequency spectrum of a link is divided up among the connections\n    established across the link. For a TDM link, time is divided into\n    frames of fixed duration, and each frame is divided into a fixed\n    number of time slots.\n\n    Packet switching is offers better sharing of transmission capacity\n    than circuit switching, and is simpler and more efficient. However,\n    circuit switching can be more suitable for real-time services.\n\nA Network of Networks {#a-network-of-networks}\n\nA PoP is a group of one or more routers in the provider's network\nwhere customer ISPs can connect into a provider ISP. For a customer\nnetwork to connect to a provider's PoP, it can lease a high-speed link\nfrom a third-party telecommunications provider to directly connect one\nof its routers to a router at the PoP. Any ISP may choose to\nmulti-home, that is, to connect to two or more provider ISPs.\n\n{{}}\n\nDelays in Packet-Switched Networks {#delays-in-packet-switched-networks}\n\nProcessing delay\n    Time needed to check bit-level errors in packet\nQueuing delay\n    Time spent waiting to be transmitted in the link\nTransmission delay\n    Equal to \\\\(L/R\\\\). Transmission delays are typically on the order\n        of microseconds to milliseconds in practice.\nPropagation delay\n    The bit propagates at the propagation speed of the link,\n        depending on the physical medium. This speed is roughly the\n        speed of light.\n\nPacket loss can occur when it arrives to find a full queue. The router\nwill drop the packet.\n\nGiven these delays, we can compute the end-to-end delay.\n\n\\begin{equation}\n  d\\{\\text{e2e}} = N(d\\{\\text{proc}} + d\\{\\text{trans}} + d\\{\\text{prop}})\n\\end{equation}\n\nThis does not account for the average queuing delay of the node.\n\nThroughput {#throughput}\n\nThe instantaneous throughput at any instant of time is the rate (in\nbits/sec) at which a host is receiving the file.\n\nProtocol Layers and Their Service Models {#protocol-layers-and-their-service-models}\n\nThe Internet Protocol stack consists of 5 layers: the physical link,\nnetwork, transport, and application layers. The OSI reference model\nconsists of 7 layers.\n\n{{}}\n\nApplication layer\n: network applications and application layer\n    protocols reside here. These protocol include HTTP, SMUT and FTP.\n    The packet of information at this layer is a message.\n\nTransport Layer\n: in the Internet there are 2 transport protocols:\n    TCP and UDP, each with their own use-case. Each transport-layer\n    packet is called a segment.\n\nNetwork Layer\n: responsible for moving packets known as datagrams\n    from one host to another. It has many routing protocols.\n\nLink Layer\n: The network layer relies on this layer to deliver the\n    datagram to the next node along the route. These\n    services depend on the specific link-layer protocol\n    employed for the link. For example, cable access\n    networks may use the DOCSIS protocol. Link layer\n    protocols include Ethernet and WiFi. Link-layer\n    packets are referred to as frames.\n\nPhysical Layer\n: responsible of moving individual bits across\n    physical mediums.\n\nApplication Layer {#application-layer}\n\nNetworking applications have application-layer protocols that define\nthe format and order of the messages exchanged between processes, as\nwell as define the actions taken on the transmission or receipt of a\nmessage.\n\nExample of application-layer protocols include:\n\nHTTP (HyperText Transfer Protocol [RFC 2616]), which defines how\n    messages are passed between browser and web-server\nSMTP (Simple mail Transfer Protocol [RFC 821]), a protocol for mail\n    exchange\n\nClient and Servers {#client-and-servers}\n\nA network application protocol typically has 2 parts, a client side\nand a server side. The host that initiates the session is often\nlabeled the client. A host can act as both a client and server at the\nsame time. As a concrete example, a mail server host runs the client\nside of SMTP (for sending email), and the server side of SMTP (for\nreceiving email).\n\nSockets {#sockets}\n\nApplications communicate by sending messages over a socket. A\nprocess's socket can be thought of as the process's door: it sends\nmessages into, and receives messages from the network through this\nsocket. It is the interface between the application layer and\ntransport layer within a host.\n\n{{}}\n\nAddressing Processes {#addressing-processes}\n\nIn order for a process on one host to send a message to a process on\nanother host, the sending process must identify the receiving process.\nTo identify the receiving process, one must specify these 2 pieces of\ninformation:\n\nThe name or address of the host machine\nAn identifier that specifies the identity of the receiving process\n    on the destination host\n\nIn Internet applications, the destination host is specified by its IP\naddress. The IP address is a 32-bit quantity that uniquely identifies\nthe interface that connects to the internet. These need to be globally\nunique. A receive-side port number serves the purpose of identifying\nthe correct process on the system.\n\nThe user agent is an interface between the user and the network\napplication. For example, user agents for browsing the Web include\nFirefox and Chrome.\n\nTransmission Control Protocol (TCP) {#transmission-control-protocol--tcp}\n\nThe Internet makes available 2 transport protocols to applications,\nnamely UDP and TCP. When a developer creates a new application for the\nInternet, they must choose between the two protocols. Each protocol\noffers a different service model.\n\nTCP includes a connection-oriented service and a reliable data\ntransfer service.\n\nTCP has the client and server exchange transport-layer control\ninformation with each other before the application-level messages\nbegin to flow. This hand-shaking procedure alerts the client and\nserver, and a TCP connection is said to exist between the sockets of\nthe 2 processes. When the application is done with sending messages,\nit must tear down the connection.\n\nThe communicating processes can rely on TCP to deliver all data sent\nwithout error, and in proper order. TCP also includes a\ncongestion-control mechanism. The mechanism throttles a process when\nthe network is congested, attempting to limit each TCP connection to\nits fair share of network bandwidth. This control mechanism benefits\nthe Internet, rather than the direct benefit of the communicating\nprocesses.\n\nTCP does not provide:\n\nA guaranteed minimum transmission rate\nAny delay guarantees\n\nUser Datagram Protocol (UDP) {#user-datagram-protocol--udp}\n\nUDP is connectionless, so there is no handshaking before the 2\nprocesses start to communicate. It provides an unreliable data\ntransfer service. Hence, it provides no guarantee that a message will\never reach the receiving socket. Messages that do arrive may arrive\nout-of-order.\n\nOn the other hand, UDP does not include a congestion-control\nmechanism, so a sending process can pump data into a UDP socket at any\nrate.\n\nThis protocol is largely used by real-time applications.\n\nHTTP {#http}\n\nHTTP is implemented in 2 programs: a client program and a server\nprogram. Clients and servers talk to each other by exchanging HTTP\nmessages. HTTP defines the structure of these messages.\n\nHTTP use TCP as their underlying transport protocol. The HTTP client\nfirst initiates a TCP connection with the server. Once the connection\nis established, the browser and server processes access TCP through\ntheir socket interfaces. The client sends HTTP request messages\nthrough the socket interface, and receives HTTP response messages from\nits socket interface.\n\nHTTP can use both nonpersistent and persistent connections. The use of\npersistent connections is the default mode for HTTP/1.1.\n\nWith non-persistent connections, each TCP connection is closed after\nthe server sends the object. T he response time for a new HTTP request\nis 2 round-trip times plus the transmission time at the server of the\nHTML file.\n\nWith persistent connections, the server leaves the TCP connection open\nafter sending a response. Subsequent requests and responses between\nthe same client and server can be sent over the same connection.\n\nHere's an exmple of the HTTP Request message:\n\nGET /somedir/page.html HTTP/1.1\nHost: www.someschool.edu\nConnection: close\nUser-agent: Mozilla/4.0\nAccept-language: fr\n(extra carriage return, line feed)\n\nThe general form of a request message looks like this:\n\n{{}}\n\nThe response message looks like this:\n\nHTTP/1.1 200 OK\nConnection: close\nDate: Thu, 06 Aug 1998 12:00:15 GMT\nServer: Apache/1.3.0 (Unix)\nLast-Modified: Mon, 22 Jun 1998 09:23:24 GMT\nContent-Length: 6821\nContent-Type: text/html\n\n(data data ...)\n\n{{}}\n\nUser-server Interaction: Cookies {#user-server-interaction-cookies}\n\nThe HTTP server is stateless. This simplifies server design, and\npermits engineers to develop high-performance web servers that can\nhandle thousands of simultaneous TCP connections. For a website to\nidentify users, HTTP uses cookies. Cookies, defined in [RFC 6265],\nallows sites to keep track of users.\n\nCookie consists of 4 components:\n\nA cookie header line in the HTTP response message\nA cookie header line in the HTTP request message\nA cookie file kept on the user's end system, and is managed by the\n    user's web browser\nA back-end database at the Web site\n\n{{}}\n\nWeb Caching {#web-caching}\n\nA web cache -- also called a proxy server -- may satisfy HTTP requests\non behalf of an origin Web server.\n\nDNS {#dns}\n\nPeople prefer the more mnemonic hostname identifier (e.g.\nwww.google.com), whil emrouters prefer fixed-length, hierarchically\nstructured IP addresses.\n\nIn order to reconcile these different preferences, we need a directory\nservice that translates hostnames to IP addresses. This is the main\ntask of the Internet's Domain Name System (DNS).\n\nThe DNS is (1) a distributed database implemented in a hierarchy of\nname servers and (2) an application-layer protocol that allows hosts\nand name servers to communicate in order to provide the translation\nservice. The DNS protocol runs over UDP and uses port 53.\n\nDNS is commonly employed by other application-layer protocols, such as\nHTTP, to translate user-supplied host names to IP addresses.\n\nNo one name server has all of the mappings of all of the hosts in the\ninternet. DNS uses a large number of name servers organized in a\nhierarchical fashion and distributed around the world.\n\nLocal name servers\n: Each ISP -- such as a university -- has a\n    local name server. when a host issues a DNS query message, the\n    message is first sent to the host's local name server.\n\nRoot name servers\n: There are a dozen or so root name servers,\n    situated primarily in North America. When a local name server is\n    unable to respond to a DNS query, it acts as a DNS client and\n    makes a DNS query to a root name server.\n\nAuthoritative name servers\n: The root name server may not know the\n    IP address of a particular host. Instead the root name server\n    knows the IP address of the authoritative name server that has\n    the desired mapping. A name server is authoritative for a host if\n    it always has a DNS record that translates the host's hostname to\n    that host's IP address. When an authoritative name server is\n    queried by a root server, the authoritative name server responds\n    with a reply containing the desired mapping.\n\nTransport Layer {#transport-layer}\n\nThe transport layer resides between the application and network\nlayers. A transport layer protocol provides for logical communication\nbetween application processes running on different hosts. Although the\ncommunicating processes are not physically connected to each other,\nfrom the application's viewpoint, they are physically connected.\nApplication processes use the logical communication provided by the\ntransport layer to send messages to each other, free from the worries\nof the physical infrastructure used to carry these messages.\n\nTransport-layer protocols are implemented on end-systems but not in\nnetwork routers. Network routers only act on the network-layer fields\nof the layer 3 PDUs.\n\nOn the sending side, the transport layer converts the messages it\nreceives from a sending application process into 4-PDUs\n(transport-layer protocol data units). This is done by (possibly)\nbreaking the application messages into smaller chunks and adding a\ntransport-layer header to each chunk. The transport layer then passes\nthese 4-PDUs to the network layer, which are then translated into\n3-PDUs.\n\nOn the receiving side, the transport layer removes the transport\nheader from the 4-PDUs, reassembles the message, and passes it to the\nreceiving application process.\n\nAll transport-layer protocols provide an application\nmultiplexing/demultiplexing service. A transport protocol can possibly\nprovide other services to invoking applications, including reliable\ndata transfer, bandwidth guarantees and delay guarantees.\n\nRelationship between the transport layer and network layer {#relationship-between-the-transport-layer-and-network-layer}\n\nThe transport layer lies just above the network layer. The\ntransport-layer protocol provides logical communication between\nprocesses running on different hosts, while the network-layer protocol\nprovides logical communication between hosts.\n\nThe transport-layer protocols live in the ned-systems. Within an\nend-system, a transport protocol moves messages from application\nprocesses to the network edge and vice versa, but it doesn't have any\nsay about how the messages are moved within the network core. A\ncomputer network may make available multiple transport protocols, with\neach protocol offering a different service model to applications.\n\nThe services that a transport protocol can provide are often\nconstrained by the service model of the underlying network-layer\nprotocol. For example, if the network-layer protocol cannot provide\nbandwidth or delay guarantees, then the transport-layer protocol on\ntop of it cannot as well.\n\nOverview of Transport layer in the Internet {#overview-of-transport-layer-in-the-internet}\n\nthe Internet, and more generally a TCP/IP network, makes available two\ndistinct transport-layer protocols to the application layer: UDP (User\nDatagram Protocol) and TCP (Transmission Control Protocol).\n\nUDP provides an unreliable, connectionless service to the invoking\napplication. TCP provides a reliable, connection-oriented service to\nthe invoking application.\n\nThe Internet's network-layer protocol is called IP, which stands for\nInternet Protocol. The IP service model is a best-effort delivery\nservice. This means that IP makes its \"best effort\" to deliver\nsegments between communicating hosts, but makes no guarantees.\n\nThe fundamental responsibility of TCP and UDP is to extend IP's\ndelivery service between 2 end systems to a delivery service between\ntwo processes running on the end system. Extending host-to-host\ndelivery to process-to-process delivery is called application\nmultiplexing and demultiplexing. UDP and TCP also provide\nerror-detection fields in their headers. These are the only 2 services\nUDP provides.\n\nTCP offers additional services. First, it provides reliable data\ntransfer. Using flow control, sequence numbering, acknowledgments,\nand timers, TCP ensures that data is delivered correctly and in order.\nTCP also uses congestion control, which is a service provided to the\nInternet as a whole rather than a service provided to the invoking\napplication. This is done by regulating the rate at which the\nsending-side TCPs can send traffic into the network.\n\nMultiplexing and Demultiplexing Applications {#multiplexing-and-demultiplexing-applications}\n\nThe job of delivering the data in a transport-layer segment to the\ncorrect application process is called demultiplexing. The job of\ngathering data at the source host from different application\nprocesses, enveloping the data with header information to create\nsegments and passing to the network layer, is called multiplexing.\n\nThis is performed by TCP and UDP by including two special fields in\nthe segment headers: the source port-number field and the **destination\nport-number field**.\n\nThe port numbers ranging from 0 to 1023 are called **well-known port\nnumbers**, and are restricted, which means that they are reserved for\nuse by well-known application protocols such as HTTP and FTP: HTTP\nuses port 80, and FTP uses port 21.\n\nTo identify the appropriate host, the transport segment also contains\nthe source and destination IP addresses.\n\nConnectionless Transport: UDP {#connectionless-transport-udp}\n\nUDP, defined in RFC 768, does as little as a transport protocol can\ndo. Aside from multiplexing/demultiplexing and some light error\nchecking, it adds nothing to IP.\n\nUDP takes messages from the application processes, attaches source and\ndestination port fields for the multiplexing/demultiplexing service,\nadds two other small fields, and passes the resulting segment to the\nnetwork layer. The network layer encapsulates the segment into an IP\ndatagram and then makes a best-effort attempt to deliver the segment's\ndata to the correct application process.\n\nNote that with UDP there is no handshaking between sending and\nreceiving transport-layer entities before sending a segment. Hence,\nUDP is said to be connectionless.\n\nDNS is an example of an application-layer protocol that uses UDP.\n\nThere are many applications more suited to UDP for the following\nreasons:\n\nNo connection establishment\n: TCP uses a three-way handshake before\n    it starts to transfer data. UDP does not introduce any delay to\n    establish a connection.\n\nNo connection state\n: TCP maintains connection state in the end\n    systems. This connection state includes receive and send buffers,\n    congestion control parameters, and sequence and acknowledgment\n    number parameters. Hence, a server devoted to a particular\n    application can typically support many more active clients over UDP.\n\nSmall packet header overhead\n: The TCP segment has 20 bytes of\n    header overhead per segment, while UDP only has 8 bytes of overhead.\n\nUnregulated send rate\n: TCP has a congestion control mechanism\n    that throttles the sender when one or more links between sender\n    and receiver become excessively congested. This throttling can\n    have severe impact on real-time applications. The speed at which\n    UDP sends data is only constrained by the rate at which the\n    application generates data, the capabilities of the source, and\n    the access bandwidth to the Internet.\n\nTCP cannot be employed with mulitcast, multicast applications run\nover UDP. It is possible to have reliable data transfer using UDP, by\nbuilding this into the application itself.\n\nUDP Segment Structure\n\n    {{}}\n\n    The UDP segment structure is defined in RFC 768. The UDP header has\n    only four fields, each consisting of 2 bytes. The port numbers allow\n    the destination host to pass the application data to the correct\n    process running on the destination end system. The length field\n    specifies the number of bytes in the UDP segment (header plus data).\n    An explicit length value is needed since the size of the data field\n    may differ between UDP segments.\n\n    The checksum provides for error detection. It determines whether bits\n    within the UDP segment have been altered as it moved from source to\n    destination. UDP at the sender side performs the 1s complement of the\n    sum of all the 16-bit words in the segment, with any overflow\n    encountered during the sum being wrapped around. This result is put\n    in the checksum field of the UDP segment.\n\n    For example, suppose we have 3 16-bit words:\n\n        0110011001100000\n    0101010101010101\n    1000111100001100\n\n    The sum of the first two words is:\n\n        0110011001100000\n    0101010101010101\n    ----------------\n    1011101110110101\n\n    Adding the third word gives:\n\n        1011101110110101\n    1000111100001100\n    ----------------\n    0100101011000010\n\n    Note this last addition had overflow, which is wrapped around. Thus\n    the 1s complement of the sum 0100101011000010 is 1011010100111101.\n\n    UDP provides a checksum because there is no guarantee that all the\n    links between source and destination provide error checking. One of\n    the links may use a link-layer protocol that does not provide error\n    checking. Even if segments are correctly transferred across a link,\n    it's possible that bit errors could be introduced when a segment is\n    stored in a router's memory. Hence, UDP must provide error-detection\n    at the transport layer, on an end-end basis. Because IP is supposed to\n    run over just about any layer-2 protocol, it is useful for the\n    transport layer to provide error checking as a safety measure. UDP\n    provides nothing for error recovery. Some implementations of UDP\n    simply discard the damaged segment; others pass the damaged segment to\n    the application with a warning.\n\nPrinciples of Reliable Data Transfer {#principles-of-reliable-data-transfer}\n\nThe service abstraction provided to the upper-layer entities is that\nof a reliable channel through which data can be transferred. With a\nreliable channel, no transferred data bits are corrupted or lost, and\nall are delivered in the order in which they are sent. This is the\nservice model that TCP offers to the Internet applications that invoke\nit.\n\nIt is the responsibility of a reliable data transfer protocol to\nimplement this service abstraction. This task is made more difficult\nby the fact that the layers beneath it may be unreliable. Here, we\ndevelop increasingly complex models for the sender and receiver sides\nof a reliable data transfer protocol.\n\nNetwork Layer {#network-layer}\n\nThe network layer implements host-to-host communication service.\nUnlike the transport and application layer, there is a piece of the\nnetwork layer in each and every host and router in the network.\n\nForwarding and Routing {#forwarding-and-routing}\n\nForwarding\n: Forwarding involves the transfer of a packet from an\n    incoming link to an outgoing link within a single router.\n\nRouting\n: Routing involves all of a routers, whose collective\n    interactions via routing protocols determine the paths\n    that packets take on their trips from source to destination.\n\nEvery router has a forwarding table. A router forwards a packet by\nexamining the value of a field in the arriving packet's header. and\nthen using this header value to index into the router's forwarding\ntable.\n\n{{}}\n\nLink-layer switches base their forwarding decision on values in the\nfields of the link layer frame; switches are thus referred to as\nlink-layer devices. Routers base their forwarding decision on the\nvalue in the network layer field, and are thus network-layer devices.\nRouters require services at both layer 2 and 3.\n\nrouters along the chosen path from source to destination require\nhandshaking with each other in order to set up state before\nnetwork-layer data packets within a given source-to-destination can\nbegin to flow. This process is referred to as connection setup.\n\nServices network protocols may provide {#services-network-protocols-may-provide}\n\nThere are many potential services a network protocol may provide, and\nthese include:\n\nguaranteed delivery\nguaranteed delivery with bounded delay\nin-order packet delivery\nguaranteed minimal bandwidth\nguaranteed maximum jitter\nsecurity services\n\nThe Internet's network layer protocol provides a single service,\nknown as the best-effort service.\n\n{{}}\n\nIt may seem like the Internet network-layer protocol provides no\nservice at all. ATM networks provide service models with more services\nthan the Internet IP protocol.\n\nVirtual Circuit and Datagram Networks {#virtual-circuit-and-datagram-networks}\n\nComputer networks that provide only a connection service at the\nnetwork layer are called virtual-circuit networks. Computer networks\nthat provide only a connectionless service an the network layer are\ncalled datagram networks.\n\nThe implementations of connection-oriented services in the transport\nlayer and connection service in the network layer are fundamentally\ndifferent: the network-layer connection service is implemented in the\nrouters in the network core, as well as in the end systems.\n\nVC networks\n\n    A VC consists of:\n\n    a path between the source and destination hosts\n    VC numbers, one number for each link along the path\n    entries in the forwarding table in each router along the path\n\n    A packet belonging to a virtual circuit will carry a VC number in its\n    header. Because a virtual circuit may have a different VC number on\n    each link, each intervening router must replace the VC number of each\n    traversing packet with a new VC number. This VC number is obtained\n    from the forwarding table.\n\n    In a VC network, the network's routers must maintain connection staet\n    information for the ongoing connections. Specifically, each time a new\n    connection is established across a router, a new connection entry must\n    be added to the router's forwarding table, and each time a connection\n    is released, an entry must be removed from the table.\n\n    There are 3 phases in a virtual circuit:\n\n    VC Setup: The sending transport layer contacts teh network layer,\n        specifies the receiver address and waits for the network to set up\n        the VC. The network layer determines the path between sender and\n        receiver, that is, the series of links and routers through which\n        all packets of the VC will travel. The network layer also\n        determines the VC number for each link along the path. Finally, the\n        network layer adds an entry in the forwarding table in each router\n        along the path. The network layer may also reserve resources (e.g.\n        bandwidth) along the path of the VC during the setup.\n    Data transfer: the packet can begin to flow along the VC\n    VC Teardown: The sender (or receiver) informs the VC of its desire\n        to terminate the VC. The network layer typically informs the end\n        system on the other side of the network, and update the forwarding\n        tables in each of the packet routers on the path from source to\n        destination that the VC no longer exists.\n\n    The messages that are passed between routers to set up the VC are\n    known as signalling messages, and the protocols to exchange these\n    messages are referred to as signaling protocols.\n\nDatagram Networks\n\n    In a datagram network, each time  an end system wants to send a\n    packet, it stamps the packet with the address of the destination end\n    system and then pops the packet into the network. There is no VC\n    setup, and routers do not maintain any state information.\n\n    As a packet is transmitted from source to destination, it passes\n    through a series of routers. Each of these routers use the packet's\n    destination address to forward the packet.\n\n    Routers typically use the longest prefix matching rule, which matches\n    the packet's IP address to a prefix entry in the forwarding table to\n    choose the link interface to forward the packet.\n\nThe Internet Protocol (IP) {#the-internet-protocol--ip}\n\n{{}}\n\nDatagram {#datagram}\n\nThe network-layer packet is referred to as a datagram. The format of a\ndatagram is as follows:\n\n{{}}\n\nThe key fields include:\n\nVersion number\n: the IP protocol version of the datagram\n\nHeader length\n: An IPv4 datgram may have variable header length,\n    but most IP datagrams do not contain options.\n\nType of service\n: the specific level of service to be provided is a\n    policy issue determined by the router's administrator. Services\n    include high throughput, and low delay.\n\nDatagram length\n: the total length of the IP datagram (header plus\n    data), measured in bytes\n\nIdentifier flags, framentation offset\n: these concern IP\n    fragmentation. IPv6 disallows fragmentation in the routers.\n\nTTL\n: the time to live field ensure that datagrams do not circulate\n    forever in the network.\n\nProtocol\n: This fieldl is used only when an IP datagram reaches its\n    final destination. The value indicates the specific\n    transport-layer protocol (e.g. TCP) to which the data\n    portion of the IP datagram should be passed.\n\nHeader checksum\n: Aids a router in detecting bit errors in a\n    received IP datagram. Routers typically discard erroneous\n    datagrams.\n\nSource and destination IP addresses\n: When a source creates a\n    datagram, it inserts its IP address into the source IP address,\n    and inserts the address of the destination into the destination\n    IP address\n\nOptions\n: These allow extensions to the IP header.\n\nData\n: The payload to transfer\n\nIP Datagram Fragmentation {#ip-datagram-fragmentation}\n\nNot all link-layer protocols can carry network-layer packets of the\nsame size. For example, Ethernet frames can carry up to 1500 bytes of\ndata, whereas frames for some wide-area links can carry no more than\n576 bytes. The maximum amount of data that a link-layer frame can\ncarry is called the maximum transmission unit (MTU). Because each IP\ndatagram is encapsulated within the link-layer frame for transport\nfrom one router to the next, the MTU of the link-layer protocol places\na hard limit on the length of an IP datagram.\n\nTo resolve this issue, an IP datagram needs to be able to split itself\ninto two or more smaller IP datagrams. These smaller datagrams are\ncalled fragments. Fragments need to be reassembled before reaching the\ntransport layer at the destination.\n\nTo perform the reassembly task, IPv4 put identification, flag and\nfragmentation offset fields in the datgram header.\n\n{{}}\n\nIPv4 addressing {#ipv4-addressing}\n\nA host typically only has 1 link into the network. A router has\nmultiple interfaces, one for each of its links. IP requires each host\nand router interface to have its own IP address. Thus, an IP address\nis technically associated with an interface, rather than with the host\nor router containing that interface.\n\nEach IP address is 32 bits long, thus there are a total of \\\\(2^32\\\\)\npossible IP addresses. These addresses are typically written in\ndotted-decimal notation (e.g. 193.32.216.9).\n\nA subnet is also called an IP network, and refers to the network\ninterconnecting several interfaces via one router interface. IP\naddressing assigns an address to this subnet: 223.1.1.0/24, where the\n/24 notation, sometimes known as the subnet mask, indicates that the\nleftmost 24-bits of the 32-bit quantity define the subnet address.\n\nTo determine the subnets in the system:\n\ndetach each interface from its host or router, creating islands of\n    isolated networks, with interfaces terminating the endpoints of the\n    isolated networks.\neach of these isolated networks is a subnet.\n\nThe Internet's address alignment strategy is known as Classless\nInterdomain Routing (CIDR). An organization is typically assigned a\nblock of contiguous addresses, that is, a range of addresses with a\ncommon prefix.\n\nObtaining a block of addresses\n\n    The ISP itself may be allocated a block of IP addresses, for example\n    200.23.16.0/20. The ISP can in turn divide its address block into\n    equal-sized contiguous address blocks, and give these blocks to\n    organizations. Internet Corporation for Assigned Names and Numbers\n    (ICANN) is the global authority on managing IP addresses, and is also\n    responsible for the DNS root servers.\n\nObtaining a host address: The dynamic host configuration protocol\n\n    Once an organization has obtained a block of addresses, it can assign\n    individual IP addresses to the host and router interfaces in its\n    organization. Host addresses can be configured via the Dynamic Host\n    Configuration Protocol (DHCP). It allows a host to obtain (be\n    allocated) an IP address automatically. A host may be assigned a\n    temporary IP address that will be different each time the host\n    connects to the network.\n\n    For a newly arriving host, the DHCP protocol is a 4-step process:\n\n    DHCP server discovery: A DHCP discover message is sent using a UDP\n        packet to port 67. The DHCP client creates an IP datagram\n        containing its DHCP discover message along with the broadcast IP\n        address of 255.255.255.255, and a \"this host\" source IP address of\n        0.0.0.0. The DHCP client passes the IP datagram to the link layer,\n        which broadcasts this rame to all nodes attached to the subnet.\n    DHCP server offer(s): A DHCP offer message is broadcast to all\n        nodes on the subnet, using the IP broadcast address of\n        255.255.255.255. The client may be able to choose from among\n        several offers, if there are several DHCP servers present on the\n        subnet. The DHCP offer message contains the transaction ID of the\n        received discover message, the proposed IP address, and an IP\n        address lease time.\n    DHCP request: the client will choose among the DHCP offers, and\n        respond to the selected offer with a DHCP request message, echoing\n        back its configuration parameters\n    DHCP ACK: The server responds to the DHCP request message with a\n        HCP ACK message, confirming the requested parameters.\n\n    {{}}\n\nNAT\n\n    To address allocation of IP addresses in small networks, the network\n    translation protocol (NAT) has foundn increasingly widespread use.\n    There are address spaces reserved for private networks, or a realm of\n    private addresses. These are:\n\n    10.0.0.0/8\n    172.16.0.0/12\n    192.168.0.0/16\n\n    These addresses can be used without coordination with IANA or an\n    Internet registry. These IP addresses only have meaning within the\n    private network.\n\n    {{}}\n\nRouting\n\n    Recall that the Internet is a \"network-of-networks\": A hierarchy of\n    Autonomous Systems (AS), e.g. ISPs each own routers and links. Due to\n    the size and decentralized administration of the internet, routing on\n    the Internet is done hierarchically.\n\n    There are 2 forms of routing:\n\n    Intra-AS routing\n    : intra-as routing finds a good patht between two\n        routers within an AS. The 2 commonly used protocols are RIP and OSPF.\n\n    Inter-AS routing\n    : inter-as routing handles the interfaces between\n        ASs. The de facto protocol for this is BGP.\n\n    In intra-AS routing, there is a single administrator, so no policy\n    decisions are needed. The routing policies here have a large focus on\n    performance.\n\n    In inter-AS routing, admins will often want control over how traffic\n    is routed, or who routes through its net. In this situation, policy\n    may be prioritized over performance.\n\n    Routing can be viewed as a least-cost path problem between two\n    vertices (routers) in a graph (network of routers).\n\n    Routing algorithms are classified as follows:\n\n    Link state algorithms. In this scenario, all routers have complete\n    knowledge of the network topology, and the link costs. Routers\n    periodically broadcast link costs to each other. Djikstra's algorithm\n    is often used to compute the least-cost path locally.\n\n    Distance vector algorithms. In this scenario, routers know\n     physically-connected neighbours, and link costs to neighbours.\n     Routers exchange \"local views\" and update their own \"local views\". An\n     iterative process of computation is taken:\n\n    Swap local view with direct neighbours.\n    Update own's local view.\n    Repeat 1-2 until no more change to local view.\n\n    The Bellman-Ford equation is used to find the least-cost path:\n\n    \\begin{equation}\n      d\\x(y) = \\textrm{min}\\v(c(x,v) + d\\_v(y))\n    \\end{equation}\n\n    To find the least cost path, \\\\(x\\\\) needs to know the cost from each\n    of its direct neighbour to \\\\(y\\\\). Each neighbour \\\\(v\\\\) sends its distance\n    vector \\\\((y, k)\\\\) to \\\\(x\\\\), telling \\\\(x\\\\) that the cost from \\\\(v\\\\) to \\\\(y\\\\) is\n    \\\\(k\\\\).\n\n    In the Distance Vector algorithm, every router sends its distance\n    vectors to its directly connected neighbours. When router $x4 finds\n    out that $y4 is advertising a path to \\\\(z\\\\) than \\\\(x\\\\) currently knows:\n\n    \\\\(x\\\\) will update its distance vector to \\\\(z\\\\) accordingly\n    \\\\(x\\\\) will note down that all packets for \\\\(z\\\\) should be sent to \\\\(y\\\\).\n        This information will be used to create the forwarding table of \\\\(x\\\\).\n\n    After every router has exchanged several rounds of updates, every\n    router would be aware of the least-cost paths.\n\n    the Routing Information Protocol (RIP) implements the distance vector\n    algorithm. It uses hop count as the cost metric (insensitive to\n    network congestion). In RIP, entries in the routing table are\n    aggregated subnet masks (routing to destination subnet). Routing\n    tables are exchanged every 30 seconds over UDP port 520.\n",
        "tags": []
    },
    {
        "uri": "/zettels/neuroscience_experimental_evidence",
        "title": "Neuroscience Experimental Evidence",
        "content": "\ntags\n: §spiking\\neural\\networks, §neuroscience\n\nEligibility Traces {#eligibility-traces}\n\nNeurons in the brain maintain traces of preceding activity on the\nmolecular level, for example in the form of calcium ions or\nactivated CaMKII enzymes. That is, they maintain a fading memory of\nevents where the presynaptic neuron fired before the postsynaptic\nneuron, which induces synaptic plasticity, if followed by a top-down\nlearning signal.\n\nTop-down learning signals {#top-down-learning-signals}\n\nTop-down learning signals such as dopaminergistic signals, are\nabundant, and inform local population of neurons about the behavioural\nresults.\n",
        "tags": []
    },
    {
        "uri": "/zettels/neuroscience_rl",
        "title": "Neuroscience and Reinforcement Learning",
        "content": "\ntags\n: Reinforcement Learning ⭐, Neuroscience ⭐\n\nReinforcement Learning ⭐ is similar to how humans learn:\n\nLearning by trial-and-error\nReward is often delayed\n\nTD errors (Temporal Difference Learning) model the activity of\ndopamine neurons (Schultz et al., 1997)\n\nBibliography\nSchultz, W., Dayan, P., & Montague, P. R., A neural substrate of prediction and reward, Science, 275(5306), 1593–1599 (1997).  http://dx.doi.org/10.1126/science.275.5306.1593 ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/neuroscience",
        "title": "Neuroscience ⭐",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/nix",
        "title": "Nix/NixOS",
        "content": "\nNixOS is a Linux operating System built upon the Nix package manager. Nix\nrefers to both the package manager, and the name of the [programming\nlanguage]({{}}) it is written in.\n\nNix Software {#nix-software}\n\ntarget/lorri provides great direnv (Unix) /nix-shell integration\nnmattia/niv provides dependency management for Nix projects\n\nLinks {#links}\n\nnix-community/awesome-nix\nNixOS Weekly Newsletter\nNix Pills\nNix Shorts\n\nArticles {#articles}\n\nI was Wrong about Nix - Christine Dodrill\n",
        "tags": []
    },
    {
        "uri": "/zettels/nlp",
        "title": "Natural Language Processing",
        "content": "\ntags\n: §machine\\learning, §deep\\learning\n\nHow to Represent Words {#how-to-represent-words}\n\nHow do we represent words as input to any of our models? There are an\nestimated 13 million tokens for the English language, some of them\nhave similar meanings.\n\nThe simplest representation is one-hot vector: we can represent\nevery word as a \\\\(\\mathbb{R}^{|V|x}\\\\) vector with all 0s and one 1 at\nthe index of that word in the sorted english language.\n\nTo reduce the size of this space, we can use SVD-based methods. For\ninstance, accumulating co-occurrence count into a matrix X, and\nperform SVD on X to get a \\\\(USV^T\\\\) decomposition. We can then use the\nrows of U as the word embeddings for all words in the dictionary.\n\nWindow-based Co-occurence Matrix {#window-based-co-occurence-matrix}\n\nCount the number of times each word appears inside a window of a\nparticular size around the word of interest. We calculate this count\nfor all words in the corpus.\n\nIssues with SVD-based methods {#issues-with-svd-based-methods}\n\nDimensions of the matrix can change very often (new words added to corpus)\nMatrix is extremely sparse as most words don't co-occur\nMatrix is very high-dimensional\nQuadratic cost to perform SVD\n\nSome solutions include:\n\nIgnore function words (blacklist)\nApply a ramp window (distance between words)\nUse Pearson correlation and set negative counts to 0 instead of raw count.\n\nIteration-based Methods (e.g. word2vec) {#iteration-based-methods--e-dot-g-dot-word2vec}\n\nDesign a model whose parameters are the word vectors, and train the\nmodel on a certain objective. Discard the model, and use the vectors\nlearnt.\n\nEfficient tree structure to compute probabiltiies for all the vocabulary\n\nLanguage Models\n\n    The model will need to assign a probability to a sequence of tokens.\n    We want a high probability for a sentence that makes sense, and a low\n    probability for a sentence that doesn't.\n\n    Unigrams completely ignore this, and a nonsensical sentence might\n    actually be assigned a high probability. Bigrams are a naive way of\n    evaluating a whole sentence.\n\n    Continuous bag-of-words (CBOW)\n\n        Predicts a center word from the surrounding context in terms of word\n        vectors.\n\n        For each word, we want to learn 2 vectors:\n\n        v: (input vector) when the word is in the context\n        u: (output vector) when the word is in the center\n\n        known parameters\n            input: sentence represented by one-hot vectors\n            output: one hot vector of the known center word\n\n        Create 2 matrices \\\\(\\mathbb{V} \\in \\mathbb{R}^{n \\times |V|}\\\\) and\n        \\\\(\\mathbb{U} \\in \\mathbb{R}^{|V| \\times n}\\\\), where n is the arbitrary\n        size which defines the size of the embedding space.\n\n        \\\\(V\\\\) is the input word matrix such that the i-th column of \\\\(V\\\\) is the\n        $n$-dimensional embedded vector for word \\\\(w\\_i\\\\) when it is an input to\n        this model. \\\\(U\\\\) is the output word matrix.\n\n        Generate one-hot word vectors for the input context of size m.\n        Get our embedded word vectors for the context: (\\\\(v\\_{c-m} =\n                 Vx^{c-m}\\\\), ...)\n        Average these vectors to get \\\\(\\hat{v} = \\frac{\\sum v}{2m} \\in \\mathbb{R^n}\\\\)\n        Generate a score vector \\\\(z = U\\hat{v} \\in \\mathbb{R}^{|V|}\\\\) As the\n            dot product of similar vectors is higher, it will push simila words\n            close to each other in order to achieve a high score.\n        Turn the scores into probabilities \\\\(\\hat{y} = softmax(z) \\in \\mathbb{R}^{|V|}\\\\)\n\n        Minimise loss function (cross entropy), and use stochastic gradient\n        descent to update all relevant word vectors.\n\n    Skipgram\n\n        Predicts the distribution of context words from a center word. This is\n        very similar to the CBOW approach, wind the input and output vectors\n        reversed. Here, a naive Bayes assumption is invoked: that given the\n        center word, all output words are completely independent.\n\n        Input vector: one-hot vectors corresponding to the vocabulary\n\n        The neural network is consists of one hidden layer of \\\\(n\\\\) units, where\n        \\\\(n\\\\) is the desired dimension of the word vector. The output layer is a\n        softmax regression layer, outputting a value between 0 and 1. We want\n        the value for the context words to be high, and the non-context words\n        to be low.\n\n        After training, the output layer is discarded, and the weights at the\n        hidden layer are the word vectors we want.\n\nTraining Methods\n\n    In practice, negative sampling works better for frequently occurring\n    words and lower-dimensional vectors, and hierachical softmax works\n    better for infrequent words.\n\n    Negative Sampling\n\n        Take \\\\(k\\\\) negative samples, and minimise the probability of the two\n        words co-occurring while also maximising the probability of the two\n        words in the same window co-occur.\n\n    Hierarchical Softmax\n\n        Hierarchical Softmax uses a binary tree to represent all words in the\n        vocabulary. Each leaf of the tree is a word, and there is a unique\n        path from root to leaf. The probability of a word \\\\(w\\\\) given a vector\n        \\\\(w\\i\\\\), \\\\(P(w|w\\i)\\\\), is equal to the probability of a random walk\n        starting from the root and ending in the leaf node corresponding to\n        \\\\(w\\\\).\n\nGlobal Vectors for Word Representation (GloVe) {#global-vectors-for-word-representation--glove}\n\nCount-based methods of generating word embeddings rely on global\nstatistical information, and do poorly on tasks such as word analogy,\nindicating a sub-optimal vector space structure.\n\nword2vec presents a window-based method of generating word-embeddings\nby making predictions in context-based windows, demonstrating the\ncapacity to capture complex linguistic patterns beyond word\nsimilarity.\n\nGloVe consists of a weighted least-squares model that combines the\nbenefits of the word2vec skip-gram model when it comes to word analogy\ntasks, but also trains on global word-word co-occurrence counts, and\nproduces a word vector space with meaningful sub-structure.\n\nThe appropriate starting point for word-vector learning should be with\nratios of co-occurrence probabilities rather than the probabilities\nthemselves. Since vector spaces are inherently linear structures, the\nmost natural way to encode the information present in a ratio in the\nword vector space is with vector differences.\n\nThe training objective of GloVe is to learn word vectors such that\ntheir dot product equals the logarithm of the words’ probability of\nco-occurrence. Owing to the fact that the logarithm of a ratio equals\nthe difference of logarithms, this objective associates (the logarithm\nof) ratios of co-occurrence probabilities with vector differences in\nthe word vector space.\n\nCo-occurrence Matrix\n\n    Let \\\\(X\\\\) denote the word-word co-occurrence matrix, where \\\\(X\\_{ij}\\\\)\n    indicate the number of times word \\\\(j\\\\) occur in the context of word\n    \\\\(i\\\\). Let \\\\(X\\i = \\sum\\k X\\_{ik}\\\\) be the number of times any word \\\\(k\\\\)\n    appears in the context of word \\\\(i\\\\). Let \\\\(P\\{ij} = P(w\\j|w\\_i) =\n    \\frac{X\\{ij}}{X\\i}\\\\) be the probability of j appearing in the context\n    of word \\\\(i\\\\).\n\nTopic Modeling {#topic-modeling}\n\nTopics are distributions over keywords\nDocuments are distributions over topics\n\nTopic summaries are NOT perfect. UTOPIAN allows user interactions for\nimproving them.\n\nTopic Lens\nUTOPIAN\n\nDimension Reduction {#dimension-reduction}\n\nMultidimensional Scaling {#multidimensional-scaling}\n\nTries to preserve given pairwise distances in a low-dimensional space.\n\nWord Vector Evaluation {#word-vector-evaluation}\n\nIntrinsic {#intrinsic}\n\nEvaluation on a specific/intermediate subtask\nFast to compute\nHelps understand the system\nNot clear if really helpful unless correlation to real task is established\n\nExtrinsic {#extrinsic}\n\nEvaluation on a real task\nCan take a long time to compute accuracy\nUnclear if the subsystem is the problem or its interaction or other subsystems\nIf replacing exactly one subsystem with another improves accuracy ➡ Winning!\n\nVisualizing Word Embeddings {#visualizing-word-embeddings}\n\nNeural Networks {#neural-networks}\n\nMost data are not linearly separable, hence the need for non-linear\nclassifiers. Neural networks are a family of classifiers with\nnon-linear decision boundaries.\n\nA neuron {#a-neuron}\n\nA neuron is a generic computational unit that takes \\\\(n\\\\) inputs and\nproduces a single output. The sigmoid unit takes a $n$-dimensional\nvector $x4 and produces a scalar activation (output) \\\\(a\\\\). This neuron\nis also associated with an $n$-dimensional weight vector, \\\\(w\\\\), and a\nbias scalar, \\\\(b\\\\). for example, the output of this neuron is then:\n\n\\begin{equation\\*}\na = \\frac{1}{1 + exp(-(w^Tx + b))}\n\\end{equation\\*}\n\nThis is also frequently formulated as:\n\n\\begin{equation\\*}\na = \\frac{1}{1 + exp(-([w^T\\text{ }b] \\cdot [x\\text{ }1]))}\n\\end{equation\\*}\n\nA single layer of neurons {#a-single-layer-of-neurons}\n\nThis idea can be extended to multiple neurons by considering the case\nwhere the input \\\\(x\\\\) is fed as input to multiple such neurons.\n\n\\begin{align\\*}\n  \\sigma\\left( z \\right) =\n  \\begin{bmatrix}\n    \\frac{1}{1 + exp(z\\_1)} \\\\\\\\\\\\\n    \\vdots \\\\\\\\\\\\\n    \\frac{1}{1 + exp(z\\_m)}\n  \\end{bmatrix}\n\\end{align\\*}\n\n\\begin{align\\*}\n  b =\n  \\begin{bmatrix}\n    b\\_1 \\\\\\\\\\\\\n    \\vdots \\\\\\\\\\\\\n    b\\_m\n  \\end{bmatrix}\n  \\in \\mathbb{R}^m\n\\end{align\\*}\n\n\\begin{align\\*}\n  W =\n  \\begin{bmatrix}\n    && w^{(1)T} && - \\\\\\\\\\\\\n    && \\dots && \\\\\\\\\\\\\n    && w^{(m)T} && -\n  \\end{bmatrix}\n  \\in \\mathbb{R}^{m\\times n}\n\\end{align\\*}\n\n\\begin{equation\\*}\nz = Wx + b\n\\end{equation\\*}\n\nThe activations of the sigmoid function can be written as:\n\n\\begin{align\\*}\n  \\begin{bmatrix}\n    a^{(1)} \\\\\\\\\\\\\n    \\vdots \\\\\\\\\\\\\n    a^{(m)}\n  \\end{bmatrix}\n  = \\sigma\\left( Wx + b \\right)\n\\end{align\\*}\n\nActivations indicate the presence of some weighted combination of\nfeatures. We can use these combinations to perform classification\ntasks.\n\nFeed-forward computation {#feed-forward-computation}\n\nNon-linear decisions cannot be classified in by feeding inputs\ndirectly into a softmax function. Instead, we use another matrix \\\\(U\n\\in \\mathbb{R}^{m\\times 1}\\\\) to generate an unnormalized score for a\nclassification task from the activations.\n\n\\begin{equation\\*}\ns = U^Ta = U^Tf\\left( Wx + b \\right)\n\\end{equation\\*}\n\nwhere f is the activation function.\n\nMaximum Margin Objective Function {#maximum-margin-objective-function}\n\nNeural networks also need an optimisation objective. the maximum\nmargin objective ensures that the score computed for \"true\" labeled\ndata points is higher than the score computed for \"false\" labeled data\npoints, i.e. \\\\(\\text{minimize} J = max(s\\c - s, 0)\\\\), where \\\\(s\\c\\\\) is the\ncorrupt false window. This optimization can be achieved through\nbackpropagation and gradient descent.\n\nGradient Checks {#gradient-checks}\n\nOne can numerically approximate these gradients, allowing us to\nprecisely estimate the derivative with respect to any parameter,\nserving as a sanity check on the correctness of our analytic\nderivatives.\n\n\\begin{equation\\*}\n  f'(\\theta) = \\frac{J(\\theta^{i+}) - J(\\theta^{i-})}{2\\epsilon}\n\\end{equation\\*}\n\nDependency Parsing {#dependency-parsing}\n\nDependency Grammar and Dependency Structure {#dependency-grammar-and-dependency-structure}\n\nParse trees in NLP are used to analyse the syntatic structure of\nsentences.\n\nConsistency Grammar uses phrase structure grammar to organize words\ninto nested constituents.\n\nDependency structure of sentences shows which words depend on (modify\nor are arguments of) other words. These binary assymetric relations\nbetween words are called dependencies and are depicted as arrows\ngoing from the head to the dependent. Usually these dependencies\nform a tree structure.\n\nDependency Parsing {#dependency-parsing}\n\nDependency parsing is the task of analysing the syntactic dependency\nstructure of a given input sentence S. The output of a dependency\nparser is a dependency tree where the words of the input sentence are\nconnected by typed dependency relations.\n\nFormally, the dependency parsing problem asks to create a mapping from\nthe input sentence with words \\\\(S = w\\0w\\1\\dots w\\n\\\\) (where \\\\(w\\0\\\\) is\nthe ROOT) to its dependency graph \\\\(G\\\\).\n\nThe two subproblems are:\n\nLearning\n: Given a training set \\\\(D\\\\) of sentences annotated with\n    dependency graphs, induce a parsing model \\\\(M\\\\) that can\n    be used to parse new sentences.\n\nParsing\n: Given a parsing model \\\\(M\\\\), and a sentence \\\\(S\\\\), derive the\n    optimal dependency graph \\\\(D\\\\) for \\\\(S\\\\) according to \\\\(M\\\\).\n\nTransition-based Parsing {#transition-based-parsing}\n\nThis relies on a state machine which defines the possible transitions\nto create the mapping from the input sentence to the dependency tree.\nThe learning problem involves predicting the next transition in the\nstate machine. The parsing problem constructs the optimal sequence of\ntransitions for the input sentence, given the previously induced model.\n\nGreedy Deterministic Transition-based Parsing\n\n    The transition system is a state machine, which consists of states\n    and transitions between those states. The model induces a sequence\n    of transitions from some initial state to one of several terminal\n    states. For any sentence \\\\(S = w\\0w\\1\\dots w\\_n\\\\) a state can be\n    described with a triple \\\\(c = \\left(\\sigma, \\beta, A)\\\\):\n\n    a stack \\\\(\\sigma\\\\) of words \\\\(w\\_i\\\\) from S,\n    a buffer \\\\(\\beta\\\\) of words \\\\(w\\_i\\\\) from S,\n    a set of dependency arcs \\\\(A\\\\) of the form \\\\(\\left(w\\i, r, w\\j\\right)\\\\)\n\n    For each sentence, there is an initial state, and a terminal state.\n\n    There are three types of transitions:\n\n    shift\n    : Remove the first word in the buffer, and push it on top of\n        the stack\n\n    left-arc\n    : add a dependency arc \\\\((w\\j, r, w\\i)\\\\) to the arc set A,\n        where \\\\(w\\_i\\\\) is the word second to the top of the stack,\n        and \\\\(w\\_j\\\\) is the word at the top of the stack. Remove\n        \\\\(w\\_i\\\\) from the stack.\n\n    right-arc\n    : add a dependency arc \\\\((w\\i, r, w\\j)\\\\) to the arc set A,\n        where \\\\(w\\_i\\\\) is the word second to the top of the stack,\n        and \\\\(w\\_j\\\\) is the word at the top of the stack. Remove\n        \\\\(w\\_i\\\\) from the stack.\n\nNeural Dependency Parsing\n\n    Feature Selection\n\n        \\\\(S\\_{word}\\\\)\n        : vector representation of the words in 4s$\n\n        \\\\(S\\_{tag}\\\\)\n        : Part-of-Speech (POS) tags, comprising of a small\n            discrete set \\\\(P = {NN, NP, \\dots}\\\\)\n\n        \\\\(S\\_{label}\\\\)\n        : Arc-labels, comprising of a small discrete set,\n            describing the dependency relation.\n\n        For each feature type, we will have a corresponding embedding matrix,\n        mapping from the feature's one-hot encoding, to a d-dimensional dense\n        vector representation.\n\n        The full embedding for \\\\(S\\_{word}\\\\) is \\\\(E^w \\in \\mathbb{R}^{d\\times\n        N\\w}\\\\) where \\\\(N\\w\\\\) is the vocabulary size. The POS and label embedding\n        matrices are \\\\(E^t \\in \\mathbb{R}^{d\\times N\\_t}\\\\) and \\\\(E^l \\in\n        \\mathbb{R}^{d\\times N\\l}\\\\) where \\\\(N\\t\\\\) and \\\\(N\\_l\\\\) are the number of\n        distinct POS tags and arc labels respectively.\n\n    The network\n\n        The network contains an input layer \\\\([x^w, x^t, x\\_l]\\\\), a hidden layer,\n        and a final softmax layer with a cross-entropy loss function.\n\n        We can define a single weight matrix in the hidden layer, to operate\n        on a concatenation of \\\\([x^w, x^t, x^l]\\\\), or we can use three weight\n        matrices \\\\([W^w\\1, W^t\\1, W^l\\_1]\\\\), one for each input type. We then\n        apply a non-linear function and use one more affine (fully-connected)\n        layer \\\\([W\\_2]\\\\) so that there are an equivalent number of softmax\n        probabilities to the number of possible transitions (the output\n        dimension).\n\n        {{}}\n\nBasic Visualization Techniques of Text Data {#basic-visualization-techniques-of-text-data}\n\nWord Cloud {#word-cloud}\n\n{{}}\n\nIn word cloud, it is difficult to determine optimal placing of words.\nIn addition, word clouds do not show relation between words.\n\nWord Tree {#word-tree}\n\nThemeRiver {#themeriver}\n\n{{}}\n\nTime-series representation: view which keywords occur more frequently\nover time. It is a type of visualization known as a stacked linegraph.\n\nTIARA Visualization {#tiara-visualization}\n\n{{}}\n\nPhrase Nets {#phrase-nets}\n\n{{}}\n\nPhrasenets are useful for exploring how words are linked in a text and\nlike word clouds and word trees can be informative for early data\nanalysis.\n\nFor more... {#for-more-dot-dot-dot}\n\nDeep Visualization Techniques {#deep-visualization-techniques}\n\n DeepEyes\n\nDimensionality Reduction (aka Manifold Learning) {#dimensionality-reduction--aka-manifold-learning}\n\nLinear tSNE optimization for the Web\n",
        "tags": []
    },
    {
        "uri": "/zettels/nn_optimizer",
        "title": "Neural Network Optimizer",
        "content": "\ntags\n: §machine\\learning\\algorithms\n",
        "tags": []
    },
    {
        "uri": "/zettels/nonparametric_filter",
        "title": "Non-parametric Filters",
        "content": "\nNon-parametric filters do not make strong assumptions on the posterior\ndensity, making them well suited for complex, multi-modal beliefs. The\nrepresentational power comes at the cost of computational complexity.\n\nSome non-parametric techniques allow adaptivity. For example, particle\nfilters can adapt the number of particles based on the available\ncompute resources.\n\nRelated {#related}\n\n§bayes\\_filter\n§particle\\_filter\n§histogram\\_filter\n",
        "tags": []
    },
    {
        "uri": "/zettels/normalizing_flows",
        "title": "Normalizing Flows",
        "content": "\ntags\n: §machine\\learning\\algorithms\n\nNormalizing flows provide a way of constructing probability\ndistributions over continuous random variables. In flow-based\nmodelling, we would like to express a D-dimensional vector\n\\\\(\\mathbf{x}\\\\) as a transformation \\\\(T\\\\) of a real vector \\\\(\\mathbf{u}\\\\)\nsampled from \\\\(p\\_u(\\mathbf{u})\\\\):\n\n\\begin{equation}\n  \\mathbf{x}=T(\\mathbf{u}) \\quad \\text { where } \\quad \\mathbf{u} \\sim p\\_{\\mathrm{u}}(\\mathbf{u})\n\\end{equation}\n\nThe transformation \\\\(T\\\\) must be invertible and both \\\\(T\\\\) and \\\\(T^{-1}\\\\)\nmust be differentiable. These transformations are known as\ndiffeomorphisms, and require that \\\\(\\mathbf{u}\\\\) must be\nD-dimensional. Under these conditions, the density of \\\\(x\\\\) is\nwell-defined, and can be obtained via the change-of-variables theorem:\n\n\\begin{equation}\n  p\\{\\mathbf{x}}(\\mathbf{x})=p\\{\\mathbf{u}}(\\mathbf{u})\\left|\\operatorname{det} J\\_{T}(\\mathbf{u})\\right|^{-1} \\quad \\text { where } \\quad \\mathbf{u}=T^{-1}(\\mathbf{x})\n\\end{equation}\n\nThe density can also be equivalently written in terms of the Jacobian\nof \\\\(T^{-1}\\\\):\n\n\\begin{equation}\n  p\\{\\mathbf{x}}(\\mathbf{x})=p\\{\\mathbf{u}}\\left(T^{-1}(\\mathbf{x})\\right)\\left|\\operatorname{det} J\\_{T-1}(\\mathbf{x})\\right|\n\\end{equation}\n\nIn practice, a flow-based model is constructed by implementing \\\\(T\\\\) or\n\\\\(T^{-1}\\\\) with a neural network, and \\\\(p\\_{\\mathrm{u}}(\\mathbf{u})\\\\) as a\nsimple density such as a multivariate normal.\n\nOne can think of transformations \\\\(T\\\\) as expanding and contracting the\nspace \\\\(R^{D}\\\\) in order to mold the density\n\\\\(p\\{\\mathrm{u}}(\\mathbf{u})\\\\) into \\\\(p\\{\\mathrm{x}}(\\mathbf{x})\\\\).\n\nThese invertible, differentiable transformations are composable.\nComplex transformations can be constructed by composing simple\ntransformations.\n\n\\begin{aligned}\n  \\left(T\\{2} \\circ T\\{1}\\right)^{-1} &=T\\{1}^{-1} \\circ T\\{2}^{-1} \\\\ \\operatorname{det} J\\{T\\{2} \\circ T\\{1}}(\\mathbf{u}) &=\\operatorname{det} J\\{T\\{2}}\\left(T\\{1}(\\mathbf{u})\\right) \\cdot \\operatorname{det} J\\{T\\{1}}(\\mathbf{u})\n\\end{aligned}\n\nFlow-based models provide 2 operations, with differing computational complexity:\n\nSampling from the model, requiring ability to sample from\n    \\\\(p\\_{\\mathrm{u}}(\\mathbf{u})\\\\) and computing the forward\n    transformation \\\\(T\\\\).\nEvaluating the model's density, requiring computing the inverse\n    transformation \\\\(T^{-1}\\\\) and its Jacobian determinant.\n\nFlow-based models can represent any distribution \\\\(p\\_x(\\mathbf{x})\\\\),\nunder reasonable conditions on \\\\(p\\_x(\\mathbf{x})\\\\).\n\nFlow-based models for modeling and inference {#flow-based-models-for-modeling-and-inference}\n\nFitting a flow-based model \\\\(p\\_x(\\mathbf{x}; \\theta)\\\\) to a target\ndistribution \\\\(p\\_{x^\\star}(\\mathbf{x})\\\\) can be done by minimizing some\ndivergence or discrepancy between them. The minimization is performed\nwith respect to the model's parameters, \\\\(\\theta = \\\\{\\phi, \\psi\\\\}\\\\),\nwhere \\\\(\\phi\\\\) are parameters of \\\\(T\\\\) and \\\\(\\psi\\\\) are parameters of\n\\\\(p\\_{\\mathrm{u}}(\\mathbf{u})\\\\).\n\nFor example, one can use the forward KL divergence:\n\n\\begin{aligned} \\mathcal{L}(\\boldsymbol{\\theta}) &=D\\{\\mathrm{KL}}\\left[p\\{\\mathbf{x}}^{\\}(\\mathbf{x}) \\\\| p\\{\\mathbf{x}}(\\mathbf{x} ; \\boldsymbol{\\theta})\\right] \\\\ &=-\\mathbb{E}\\{p\\{\\mathbf{x}}^{\\}(\\mathbf{x})}\\left[\\log p\\{\\mathbf{x}}(\\mathbf{x} ; \\boldsymbol{\\theta})\\right]+\\text { const. } \\\\ &=-\\mathbb{E}\\{p\\{\\mathbf{x}}^{\\*}(\\mathbf{x})}\\left[\\log p\\{\\mathbf{u}}\\left(T^{-1}(\\mathbf{x} ; \\boldsymbol{\\phi}) ; \\boldsymbol{\\psi}\\right)+\\log \\left|\\operatorname{det} J\\{T^{-1}}(\\mathbf{x} ; \\boldsymbol{\\phi})\\right|\\right]+\\text { const. } \\end{aligned}\n\nThe forward KL divergence is well-suited where we have samples from\nthe target distribution, but cannot necessarily evaluate the target\ndensity \\\\(p\\_{x^\\star}(\\mathbf{x})\\\\). We can estimate this expectation by\nMonte Carlo using samples from \\\\(p\\_{x^\\star}(\\mathbf{x})\\\\).\n\nFitting the model requires computing \\\\(T^{-1}\\\\), its Jacobian\ndeterminant and the density \\\\(p\\_u(\\mathbf{u}; \\psi)\\\\), and\ndifferentiating through all three. We do not need to compute \\\\(T\\\\) or\nsample from \\\\(p\\_u(\\mathbf{u}, \\psi)\\\\), although these operations will be\nneeded if we want to sample from the model after fitting.\n\nThe reverse KL-divergence is suitable when we have the ability to\nevaluate the target density \\\\(p\\_{x^\\star}(\\mathbf{x})\\\\), but are not\nnecessarily able to sample from it.\n\nThere is some duality between the forward and reverse-mode\nKL-divergence in flow-based models. Fitting the model to the target\nvia reverse KL-divergence is equivalent to fitting\n\\\\(p\\_{u^\\star}(\\mathbf{u}; \\phi)\\\\) to the base via forward KL-divergence.\n\nAlternative divergences include f-divergences, which use density\nratios, or integral probability metrics (IPM) that uses differences.\n\nComputational Complexities {#computational-complexities}\n\nIncreasing the \"depth\" (number of composed sub-flows) of the\ntransformation results in only \\\\(O(K)\\\\) growth in computation\ncomplexity, where \\\\(K\\\\) is the depth of the flow.\n\nOne crucial operation is the computation of the Jacobian determinant.\nIn automatic-differentiation frameworks, this has computational cost\nof \\\\(O(D^3)\\\\), where \\\\(D\\\\) is the number of inputs and outputs of a neural\nnetwork. For practical applications we choose neural network designs\nthat reduce the cost to \\\\(O(D)\\\\).\n\nExamples of such efficient sub-flow transformations include:\n\nautoregressive flows\nlinear flows\nresidual flows\n\nPractical Considerations {#practical-considerations}\n\nComposing a large number of flows bring their own challenges.\n\nNormalization {#normalization}\n\nAs with deep neural networks, normalizing the intermediate\nrepresentation is crucial for stable gradients throughout the flow.\nModels such as Glow employ variants of batch normalization. Batch\nnormalization can be implemented as a composition of 2 affine\ntransformations. The first has scale and translation parameters set\nby the batch statistics, and the second has free parameters \\\\(\\alpha\\\\)\n(scale) and \\\\(\\beta\\\\) (translation):\n\n\\begin{equation}\n  \\mathrm{BN}(\\mathrm{z})=\\alpha \\odot \\frac{\\mathrm{z}-\\hat{\\mu}}{\\sqrt{\\hat{\\sigma}^{2}+\\epsilon}}+\\beta, \\quad \\mathrm{BN}^{-1}\\left(\\mathrm{z}^{\\prime}\\right)=\\hat{\\mu}+\\frac{\\mathrm{z}^{\\prime}-\\beta}{\\alpha} \\odot \\sqrt{\\hat{\\sigma}^{2}+\\epsilon}\n\\end{equation}\n\nGlow uses a variant called activation normalization, which is\npreferable when training with small mini-batches since batch norm's\nstatistics become noisy and can destabilize training.\n\nMulti-scale architectures {#multi-scale-architectures}\n\nBecause \\\\(\\mathbf{x}\\\\) and \\\\(\\mathbf{u}\\\\) must have the same\ndimensionality, and \\\\(T\\_k\\\\) must preserve this dimensionality, the\ntransformations can be extremely expensive. To combat this issue, one\ncan clamp sub-dimensions of the intermediate flow \\\\(z\\_k\\\\) such that no\nadditional transformation is applied. Doing so allows us to apply\nsteps to a subset of dimensions, which is less costly.\n\nThis kind of optimization is natural when dealing with granular data\ntypes such as pixels.\n\nContinuous Flows {#continuous-flows}\n\nWe can construct flows in continuous time by parameterizing the flow's\ninfinitesimal dynamics, and then integrating to find the corresponding\ntransformation. The flow is defined by an ordinary differential\nequation (ODE) that describes the flow's evolution in time.\n\nResources {#resources}\n\nNormalizing Flows for Probabilistic Modeling and Inference (Papamakarios et al., 2019)\n\nBibliography\nPapamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., & Lakshminarayanan, B., Normalizing flows for probabilistic modeling and inference, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/notetaking",
        "title": "Note-taking",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/numpy",
        "title": "Numpy",
        "content": "\ntags\n: §data\\_science\n\nResources {#resources}\n\nFrom Python to Numpy\n",
        "tags": []
    },
    {
        "uri": "/zettels/ocaml",
        "title": "OCaml",
        "content": "\ntags\n: §prog\\_lang\n\nPattern Matching {#pattern-matching}\n\nPatterns cannot be used to expressed arbitrary conditions. this is a\ndesign choice, that allows better support for patterns in the\ncompiler, as well as greater efficiency in match statements.\n\nOCaml is often able to generate machine code that jumps directly to a\nmatched case based on an efficient set of runtime checks.\n\nThis is a more general phenomena: pattern matching is very efficient,\nand pattern matching code is usually a win over what you might write\nby hand.\n\nError Handling {#error-handling}\n\n> The use of options to encode errors underlines the fact that it’s not\n> clear whether a particular outcome, like not finding something on a\n> list, is an error or is just another valid outcome. This depends on\n> the larger context of your program, and thus is not something that a\n> general-purpose library can know in advance. One of the advantages of\n> error-aware return types is that they work well in both situations.\n\nOcaml ppx {#ocaml-ppx}\n\nPPX provides a new API for syntactic extensions in OCaml.\n\nSome common ppx's:\n\njanestreet/ppx\\_let\njanestreet/ppx\\_expect\njanestreet/ppx\\sexp\\conv\n\nWriting JS with Ocaml {#writing-js-with-ocaml}\n\nCurrently there are 2 options:\n\njs\\of\\ocaml\nBucklescript\n\nAccording to Yaron Minsky, both options are good:\n\n> We’ve made extensive use of js\\of\\ocaml for internal apps at Jane Street. I can’t give a detailed comparison with Bucklescript, but I can tell you what I know of the tradeoffs.\n>\n> First, js\\of\\ocaml runs pretty fast, but I’ve heard tell that Bucklescript is faster. js\\of\\ocaml now supports separate compilation of Javascript, so subsequent recompilations are quite zippy, in my experience. That said, the initial compilation takes material time. Dune does separate compilation for js\\of\\ocaml by default, and does a single, more compact javascript executable when run in production mode. Anyway, we haven’t found performance of the compiler to be an obstacle with js\\of\\ocaml.\n>\n> js\\of\\ocaml is highly compatible with OCaml’s semantics. Advanced libraries like Async and Incremental that make fairly aggressive use of OCaml’s memory model work under js\\of\\ocaml without modification, which is great. I believe you have to be a bit more careful when compiling with Bucklescript. (See incr\\_dom 31 for an interesting application of Incremental to the browser.)\n>\n> js\\of\\ocaml is highly compatible in a another way: it is essentially always fully up to date with the latest OCaml. That’s because it’s easier to maintain, by virtue of operating only on OCaml bytecode. Bucklescript is a more fullsome set of patches to the compiler, and so it typically lags a few versions behind. That alone is for us a sufficiently compelling reason to stick to js\\of\\ocaml.\n>\n> Bucklescript seems to have a more active web-dev community, associated with the Reason community. I think this is mostly because Bucklescript generates easy to read javascript output. I don’t care much about readable JavaScript output (especially in a world with sourcemaps), but the community surely has value. For example, the OCaml React bindings are currently Bucklescript-only (though @jordwalke has suggested that porting to js\\of\\ocaml would be totally doable.)\n\nReferences {#references}\n\nReal World OCaml\n",
        "tags": []
    },
    {
        "uri": "/zettels/occams_razor",
        "title": "Occam's Razor",
        "content": "\ntags\n: §machine\\learning, §information\\theory\n\nOccam's razor is the principle that states a preference for simpler\nmodels. This is not just a philosophical choice: Bayesian\nprobabilistic inference automatically embodies this principle,\nquantitatively. To see this, we evaluate 2 alternative theories\n\\\\(\\mathcal{H}\\1\\\\) and \\\\(\\mathcal{H}\\2\\\\), in light of observing data\n\\\\(\\mathcal{D}\\\\).\n\n\\begin{equation}\n  \\frac{P(\\mathcal{H}\\1|\\mathcal{D})}{P(\\mathcal{H}\\2|\\mathcal{D})} =\n  \\frac{P(\\mathcal{H}\\1)}{P(\\mathcal{H}\\2)} \\frac{P(\\mathcal{D}|\\mathcal{H}\\1)}{P(\\mathcal{D}|\\mathcal{H}\\2)}\n\\end{equation}\n\nThe ratio \\\\(\\frac{P(\\mathcal{H}\\1)}{P(\\mathcal{H}\\2)}\\\\) denotes how much\nour initial beliefs favoured \\\\(\\mathcal{H}\\1\\\\) over \\\\(\\mathcal{H}\\2\\\\). The\nsecond ratio expresses how well relatively the observed data\n\\\\(\\mathcal{D}\\\\) were predicted by the 2 hypotheses.\n\nSimple models make precise computations, while complex models spread\ntheir predictive probabilities more thinly over their larger\nhypothesis space. In the case where the data are compatible with both\ntheories, the simpler \\\\(\\mathcal{H}\\_1\\\\) would turn out to be more\nprobable than the more complex \\\\(\\mathcal{H}\\_2\\\\). Hence the second term\nautomatically embodies the Occam's razor.\n\nGelman on the Occam Factor {#gelman-on-the-occam-factor}\n\nSource: David MacKay and Occam’s Razor « Statistical Modeling, Causal Inference, and ...\n\nGelman is not fond of Mackay's above argument about Bayesian inference\nembodying Occam razor. His argument seems to be about wanting to keep\nmore complex models:\n\n> once I’ve set up a model I’d like to keep all of\n> it, maybe shrinking some parts toward zero but not getting rid of\n> coefficients entirely.\n\nI still don't see a contradiction with Mackay's proposed argument.\nMaybe I'm missing something...\n\nRelated {#related}\n\n§two\\levels\\of\\_inference\n",
        "tags": []
    },
    {
        "uri": "/zettels/occupancy_grid_mapping",
        "title": "Occupancy Grid Mapping",
        "content": "\nIn §robot\\_localization, it is assumed that the robot is given a map in\nadvance. This is sometimes not the case.\n\nAcquiring maps with mobile robots is a challenging task, because:\n\nThe hypothesis space of all possible maps is huge (infinite). Under\n    discrete approximations like grid approximations, this space\n    remains computationally intractable for many Bayesian approaches.\nLearning maps is a chicken-and-egg problem, hence it is often\n    referred to as the simultaneous localization and mapping (SLAM)\n    problem.\n\nFactors in hardness of mapping {#factors-in-hardness-of-mapping}\n\nsize\n: the larger the environment, the more difficult\n\nnoise in perception\n: noise-free sensors lead to simple mapping solutions\n\nperceptual ambiguity\n: the more frequently different places look\n    alike, the harder to establish correspondence in different points in time\n\ncycles\n: cycles in the environment are particularly difficult to\n    map. Cycles make robots return via different paths, resulting in\n    large amounts of accumulated odometric errors.\n\nOccupancy Grid Mapping {#occupancy-grid-mapping}\n\n**We assume some oracle informs us of the exact robot path during\nmapping.**\n\nOccupancy grid maps address the problem of generating consistent maps\nfrom noisy and uncertain measurement data, under the assumption that\nthe robot pose is known. This technique is not useful in generating\nmaps for path-planning and navigating, since no robot odometry is\nperfect.\n\nThe gold standard of any occupancy grid mapping algorithm is to\ncalculate the posterior over maps given the data:\n\n\\begin{equation}\n  p( m | z\\{1:t},x\\{1:t})\n\\end{equation}\n\nThe controls \\\\(u\\_{1:t}\\\\) play no role, since the robot pose is known.\nThe most common of occupancy maps are 2-D floorplan maps: a 2-D slice\nof the 3D world. These techniques generalize to 3D at significant\ncomputational expense.\n\nLet \\\\(m\\_{i}\\\\) done the grid cell with index \\\\(i\\\\). An occupancy grid map\npartitions the space into finitely many grid cells:\n\n\\begin{equation}\n  m=\\sum\\{i} \\mathbf{m}\\{i}\n\\end{equation}\n\nEach \\\\(m\\_i\\\\) is attached a binary occupancy value, which specifies\nwhether the cell is occupied.\n\nWith this decomposition, the problem remains computationally\nintractable. The standard approach further breaks down the problem,\ninstead estimating:\n\n\\begin{equation}\n  p\\left(\\mathbf{m}\\{i} | z\\{1: t}, x\\_{1: t}\\right)\n\\end{equation}\n\nAssuming conditional independence between grid cells, the posterior\nover maps is approximated as:\n\n\\begin{equation}\n  p\\left(m | z\\{1: t}, x\\{1: t}\\right)=\\prod\\i p\\left(\\mathbf{m}\\{i} | z\\{1: t}, x\\{1: t}\\right)\n\\end{equation}\n\nOccupancy is often represented in its log-odds form:\n\n\\begin{equation}\n  l\\{t, i}=\\log \\frac{p\\left(\\mathbf{m}\\{i} | z\\{1: t}, x\\{1: t}\\right)}{1-p\\left(\\mathbf{m}\\{i} | z\\{1: t}, x\\_{1: t}\\right)}\n\\end{equation}\n\nThe algorithm is as follows:\n\n\\begin{algorithm}\n  \\caption{Occupancy Grid Mapping}\n  \\label{occupancy\\grid\\mapping}\n  \\begin{algorithmic}[1]\n    \\Procedure{Occupancy Grid Mapping}{$\\\\{l\\{t-1, i}\\\\}, x\\t, z\\_t$}\n    \\ForAll{cells $\\mathbf{m}\\_i$}\n    \\If {$\\mathbf{m}\\i$ in perceptual field of $z\\t$}\n    \\State $l\\{t,i} = l\\{t-1,i} + \\mathrm{inverse sensor\n      model}(\\mathbf{m}\\i, x\\t,z\\t) - l\\0$\n    \\Else\n    \\State $l\\{t,i} = l\\{t-1,i}$\n    \\EndIf\n    \\State \\Return $l\\_{t,i}$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\nOccupancy grid mapping requires an inverse sensor model. One can\nalso utilize the space claimed by the robot itself during mapping, by\nreturning a large negative number for all grid cells occupied by the\nrobot when at \\\\(x\\_t\\\\).\n\nMulti-sensor integration {#multi-sensor-integration}\n\nDifferent sensors have different characteristics, and are sensitive to\ndifferent kinds of obstacles. There are 2 basic approaches to fusing\ndata from multiple sensors:\n\nExecute the algorithm with different sensor modalities, replacing\n    the inverse sensor model accordingly.\n    This causes the result of Bayes filtering to be ill-defined.\nBuild separate maps for different sensor types, and integrate them\n    using the most conservative estimate.\n\n2 is the more appropriate approach.\n\nInverting the Measurement Model {#inverting-the-measurement-model}\n\nThe occupancy grid mapping algorithm requires a marginalized inverse\nmeasurement model: \\\\(p\\left(\\mathbf{m}\\_{i} | x, z\\right)\\\\). It is\ntermed \"inverse\" because it reasons from effects to causes.\n\nWe can do so by using Bayes rule, assuming \\\\(p(m|x) = p(m)\\\\):\n\n\\begin{aligned}\n  p(m | x, z) &=\\frac{p(z | x, m) p(m | x)}{p(z | x)} \\\\ &=\\eta p(z |\n  x, m) p(m)\n\\end{aligned}\n\nHowever, the occupancy grid mapping algorithm approximates the\nposterior over maps by its marginals, so the inverse model for the ith\ngrid cell is obtained via:\n\n\\begin{equation}\n  p\\left(\\mathbf{m}\\{i} | x, z\\right)=\\eta \\int\\{m: m(i)=\\mathbf{m}\\_{i}} p(z | x, m) p(m) d m\n\\end{equation}\n\nWhich is impossible to compute given the large space of all maps. It\nis often approximated via sampling. One simple way is to generate\nrandom triplets of pose, measurements and map occupancy values for any\ngrid cell \\\\(m\\_i\\\\). From which we can learn a simple network to predict\noccupancy values.\n\nMaintaining Dependencies in Grid Cells {#maintaining-dependencies-in-grid-cells}\n\nThe standard algorithm decomposes the grid, making the assumption that\nthe occupancy value in each grid cell is conditionally independent\nfrom the others. This is as strong assumption.\n\nHowever, the full map posterior is not computable, due to the large\nnumber of possible maps defined over a grid. However, despite this, it\nis still maximizable (MAP). This leads to maps that are more\nconsistent with the data, but requires full availability of data.\nAnother downside is that the MAP map does not capture the residual\nuncertainty in the map.\n",
        "tags": []
    },
    {
        "uri": "/zettels/odometry_motion_model",
        "title": "Odometry Motion Model",
        "content": "\nThe odometry model uses the relative information of the robot's\ninternal odometry. While both the §velocity\\motion\\model and the\nodometry motion model suffer from drift and slippage, the velocity\nmotion model additionally suffers from the mismatch between the actual\nmotion controller and the mathematical model. Odometry information are\nonly available in retrospect, making the model unusable for planning.\n",
        "tags": []
    },
    {
        "uri": "/zettels/operating_systems",
        "title": "Operating Systems",
        "content": "\nOperating System Key Concepts {#operating-system-key-concepts}\n\nMotivation {#motivation}\n\nAbstraction\n    Presents common high level functionality to users\n    Efficient and Portable\nResource Allocator\nProgram Control\n    Monitor execution of program, and manage resource access privileges\n\nAlarm signal {#alarm-signal}\n\nThe alarm signal causes the operating system to suspend whatever it is\ndoing, save its registers on the stack, and start running a special\nsignal-handling procedure.\n\nIdentification {#identification}\n\nEach person authorized to use the OS is assigned a UID (User\nIdentification). Each process started has the UID of the person who\nstarted it. The child process inherits the UID from the parent. Users\ncan also be members of groups, each with a GID (Group Identification).\n\nFile systems {#file-systems}\n\nBefore a file can be read or written, it must be opened, at which time\nthe permissions are checked. If access is permitted, the system\nreturns a small integer called the file descriptor to use in\nsubsequent operations.\n\nSpecial files are provided in order to make I/O devices look like\nfiles. That way, they can be read and written using the same system\ncalls as are used for reading and writing files. block special files\nare used to model devices that consist of a collection of randomly\naddressable blocks, such as disks. A program can open a block special\nfile, and access a particular block to read it. **character special\nfiles** are used to model printers, modems and other devices that\naccept or output a character stream.\n\nPipe {#pipe}\n\nA sort of pseudofile that can be used to connect two processes. If\nprocess A and B wish to talk using a pipe, they must set it up in\nadvance.\n\n{{}}\n\nProcess segments {#process-segments}\n\n{{}}\n\nFiles in UNIX {#files-in-unix}\n\nEvery file in UNIX has a unique number, its i-number, that identifies\nit. The i-number is an index into a table of i-nodes, one per file,\ntelling who owns the file, where its disk-blocks are and so one. A\ndirectory is a file containing a set of (i-number, ASCII name) pairs.\n\nHypervisors {#hypervisors}\n\nIn practice, the real distinction between a type 1 hypervisor and a\ntype 2 hypervisor is that a type 2 makes uses of a host operating\nsystem and its file system to create processes, store files, and so\non. A type 1 hypervisor has no underlying sup- port and must perform\nall these functions itself.\n\nProcess {#process}\n\nA program in execution. Associated with each process is a **address\nspace**. Address space is a list of memory locations from 0 to maximum\nin which the program can write to.\n\nInformation about all the processes are stored in the operating system\ntable called the process table, which is an array structure, one for\neach process currently in existence.\n\nA suspended process consists of its address space, and its entry in\nthe process table, which contains the contents of the registers and\nother items required to resume the process later on.\n\nProcess Creation {#process-creation}\n\nOS needs to load its code and any static data (e.g. initialized\nvariables) into memory, into the address space of the process. Modern\nOSes load these lazily, via the machinery of paging and swapping.\n\nRun-time stack\n    Used for local variables, function parameters and return\n        addresses\nHeap\n    Dynamically allocated data, programs request this space by\n        calling malloc() and free it explicitly using free()\nI/O related tasks\n    3 open file descriptors: stdin, stdout and stderr\n\nFunction Call {#function-call}\n\nControl Flow issues {#control-flow-issues}\n\nNeed to jump to the function body\nNeed to resume when the function call is done\nMinimally, need to store the PC of the caller\n\nData Storage issues {#data-storage-issues}\n\nNeed to pass parameters to the function\nNeed to capture the return result\nMay have local variable declarations\n\nStack Memory {#stack-memory}\n\nDefine new region of memory, called stack memory, for function\ninvocations. A new hardware register, the stack pointer, stores\nthe current memory address of the top of the stack.\n\nWhen the stack grows, the stack pointer decreases. The stack grows\nfrom bottom up. This is true for most architectures.\n\nStack Frame {#stack-frame}\n\nThe stack memory stores a bunch of stack frames, one stack frame for\neach function invocation. The stack frame stores:\n\nLocal variables\nParameters\nReturn PC\nSaved Registers\nSaved Stack Pointer\nFrame Pointer\n\nFunction Call Convention (FCC) {#function-call-convention--fcc}\n\nThere are different ways to setup stack frames. An example scheme is\ndescribed below.\n\nCaller passes parameters with registers and/or stack\nCaller saves return PC on stack\nTransfer Control from Caller to Callee\nCallee save registers used by callee. Save old SP and FP\nCallee allocates space for local variables on stack\nCallee updates stack pointer to top of stack\n\nTeardown:\n\nCallee: Restore saved registers, FP, SP\nTransfer control from callee to caller using saved PC\nCaller: Continues execution in caller\n\nFrame Pointer {#frame-pointer}\n\nStack Pointer is hard to use as it can change. Frame pointer points to\na fixed location in a stack frame, and other items are accessed as\noffsets from the frame pointer.\n\nDynamic Memory Allocation {#dynamic-memory-allocation}\n\nHigh Level Languages allow dynamic allocation of memory space, e.g.\nC's malloc. These memory blocks have different behaviours. First,\nthey are only allocated at runtime, and hence cannot be placed in the\ndata region. Next, there is no definition deallocation timing, and\nhence cannot be placed in the stack region.\n\nHence, a new region is needed, called the heap. Heap memory is a lot\ntrickier to manage. Variable size, and allocation/deallocation timing\nis not known before hand.\n\nProcess Identification {#process-identification}\n\nTo distinguish processes from each other, a process ID (PID) is\nassigned to each process.\n\n5-state Process Model {#5-state-process-model}\n\nNew\n: The process creation is started, but has not been\n    allocated the required resources.\n\nReady\n: Process is ready to run, but the OS has not chosen to run\n    it yet.\n\nRunning\n: A process is running if it is executing instructions on\n    the processor.\n\nBlocked\n: Process has performed some kind of operation that\n    makes it not ready to run until another event has\n    taken place, e.g. being blocked by I/O.\n\nTerminated\n: Process is finished, may require OS cleanup.\n\n{{}}\n\nData structures required {#data-structures-required}\n\nProcess Table\n: keeps track of all processes\n\nPCB\n: contains the entire execution context for a process\n\n{{}}\n\nMechanism: Limited Direct Execution {#mechanism-limited-direct-execution}\n\nThe OS must virtualize the CPU in an efficient manner, while retaining\ncontrol over the system. To do so, both hardware and operating systems\nsupport will be required. The OS will often use a judicious bit of\nhardware support in order to accomplish its work effectively.\n\nAccess Control {#access-control}\n\nIn user mode, applications do not have full access to hardware\nresources. The OS runs in kernel mode, which has access to the full\nresources of the machine.\n\nCode can request access to system resource by calling the trap call,\nwhich raises the privilege level to kernel mode. Once finished, the OS\ncalls the return-from-trap instruction, which returns the calling\nuser program, while reducing the privilege level back to user mode.\n\nDuring bootup, the machine is started in kernel mode. The OS sets up a\ntrap table, and informs the hardware of the location of specialised\ntrap handlers, which is the code to run when certain exceptional\nevents occur. One such example is the hard-disk interrupt.\n\nGeneral System Call Mechanism {#general-system-call-mechanism}\n\nUser program invokes the library call, using the normal function\n    call mechanism\nLibrary call places the system call number in a designated location\nLibrary call executes a special instruction to switch from user\n    mode to kernel mode (known as TRAP)\nNow in kernel mode, the appropriate system call handler is determined:\n    Using the system call number as index\n    This step is usually handled by a dispatcher\nSystem call handler is executed\nControl is returned to the library call, and switches from\n    kernel mode to user mode\nLibrary call return to user program, via normal function\n    return mechanism\n\nSwitching between processes {#switching-between-processes}\n\nCooperative Approach {#cooperative-approach}\n\nProcesses transfer control of the CPU to the OS by making system\ncalls. The OS regains control of the CPU by waiting for a system call\nor an illegal operation of some kind to take place.\n\nNon-cooperative Approach {#non-cooperative-approach}\n\nThe question is: what ca the OS do to ensure that a rogue process\ndoes not take over the machine?\n\nThe answer is: timer interrupt. A timer device is programmed to\nraise an interrupt at a fixed interval. Each time the interrupt is\nraised, a pre-configured interrupt handler in the OS runs.\n\nAt this time, the OS will decide whether to continue running the\nprocess, or switch to a different one. This is the role of the\nscheduler.\n\nIf the decision is to switch processes, then the OS executes a\nlow-level piece of code which is referred to as the _context\nswitch_. The OS saves a few register values for the current\nexecuting process. This includes:\n\nProgram Counter (PC)\nStack Pointer (Pointing to the new context)\n\nException and Interrupts {#exception-and-interrupts}\n\nExecuting a machine level instruction can lead to exceptions, for\nexample arithmetic errors.\n\nExceptions are synchronous, and occur due to program execution. An\nexception handle is executed automatically.\n\nExternal events can interrupt the execution of a program. These are\nusually hardware related: timer, keyboard events etc.\n\nWhen an exception or an interrupt handler executes, control is\ntransferred to a handler routine automatically.\n\nA handler does the following:\n\nSave Register/CPU state\nPerform Register/CPU\nRestore Register/CPU\nReturn from interrupt\n\nScheduling {#scheduling}\n\nAssumptions made:\n\nEach job runs for the same amount of time\nAll jobs arrive at the same time\nAll jobs only use the CPU (i.e. they perform no I/O)\nThe run-time of each job is known\n\nScheduling Metrics {#scheduling-metrics}\n\nTurn-around time\n\nFirst Come First Served (FCFS) {#first-come-first-served--fcfs}\n\nExample:\n\nA, B and C arrived at time T=0.\nA runs first, followed by B, then C\n\nAverage Turnaround time:\n(10 + 20 + 30)/3 = 20\n\nPros {#pros}\n\nEasy to implement\n\nCons {#cons}\n\nConvoy effect: a number of relatively-short potential consumers\nof a resource get queued behind a heavyweight resource consumer.\n\nE.g. A takes 100 TU, B and C 10\nAverage turnaround time: (100 + 110 + 120)/3\nif instead, B and C were scheduled before A, it would be (10 + 20+\n    120)/3\n\nShortest Job First (SJF) {#shortest-job-first--sjf}\n\nSchedule the job that takes the shortest TU.\n\nPros {#pros}\n\nOptimizes for Turnaround time\n\nCons {#cons}\n\nRelies on unrealistic assumptions. For example, if A takes 100TU, and\nB and C takes 10 TU, but B and C arrive only shortly after A, then A\nwill still get queued, and the turnaround time will be high (convoy\nproblem again)\n\nShortest Time-to-Completion First (SRT) {#shortest-time-to-completion-first--srt}\n\nAny time a new job enters the system, it determines the job that has\nthe least time left, and schedules that one first.\n\nPros {#pros}\n\nGood turnaround time\n\nCons {#cons}\n\nBad for response time and interactivity.\n\nRound Robin {#round-robin}\n\nInstead of running jobs to completion, RR runs a job for a _time\nslice, also sometimes called a scheduling quantum_. After the time\nslice, the next job in the run queue is scheduled. The length of the\ntime slice must be a multiple of the length of the timer-interrupt\nperiod.\n\nThe shorter the time slice, the better the performance of RR under the\nresponse-time metric. However, if the time slice is too short, there\nwill be a lot of overhead, and the cost of context switching will\ndominate the overall performance.\n\nIncorporating I/O {#incorporating-i-o}\n\nBy treating each CPU burst as a job, the scheduler makes sure\nprocesses that are \"interactive\" get run frequently.\n\nMulti-level Feedback Queue (MLFQ) {#multi-level-feedback-queue--mlfq}\n\nOptimise turnaround time.\nMake the system responsive to interactive users, minimise _response\n    time_.\n\nHow to schedule without perfect knowledge? (Knowing the length of the\njob). Many jobs have phases of behaviour, and are thus predictable.\n\nMLFQ has a number of distinct queues, each assigned a different\npriority level. At any given time, a job that is ready to run is on\na single queue.\n\nRule 1: If Priority(A) > Priority(B), A runs\nRule 2: If Priority(A) = Priority(B), A and B run in RR\n\nNote that job priority changes over time.\n\nFirst try at MLFQ:\n\nRule 3: When a job enters the system, it is placed at the highest\n    priority (the top most queue)\nRule 4a: If a job uses up an entire time slice while running, its\n    priority is reduced (it moves down one queue)\nRule 4b: If a job gives up the CPU before the time slice is up, it\n    stays at the same priority level.\n\nProblems:\n\nstarvation: if there are \"too many\" interactive jobs in the\n    system, they will combine to consume all CPU time, and\n    long-running jobs will never receive any CPU time.\nGaming the scheduler: One can stop using the CPU right before the\n    time slice ends, then it will maintain at top priority.\n\nAttempt 2:\n\nRule 5: After some time period S, move all the jobs in the system to\n    the topmost queue\n\nThis solves two problems:\n\nProcesses are guaranteed not to starve: by sitting in the top\n    queue, a job will share the CPU with other high-priority jobs in a\n    round-robin fashion, and will eventually receive service\nIf a CPU-bound job has become interactive, the scheduler treats it\n    properly once it has received the priority boost\n\nAttempt 3:\nInstead of forgetting how much of a time slice a process used at a\ngiven level, the scheduler should keep track, once a process has used\nits allotment, it is demoted to the next priority queue.\n\nRule 4: Once a job uses up its time allotment at a given level\n    (regardless of how many times it has given up the CPU), its priority\n    is reduced\n\nTuning MLFQ {#tuning-mlfq}\n\nVarying time-slice length across different queues. Shorter time\n    slices are comprised of interactive jobs, and quickly alternating\n    between them makes sense\nThe low-priority queues are CPU bound, and longer time slices work well.\n\nLottery Scheduling {#lottery-scheduling}\n\nTickets are used to represent the share of a resource that a process\nshould receive. Lottery scheduling achieves probabilistic fair sharing\nof the CPU resources.\n\nConcurrency {#concurrency}\n\nProcesses take a single physical CPU and turn it into multiple virtual\nCPUs, enabling the illusion of multiple programs running at the same\ntime.\n\nNow, we will examine the abstraction for running a single process:\nthat of a thread.\n\nThread {#thread}\n\nThe state of a single thread is similar to that of a process. It has a\nprogram counter (PC) that tracks where the program is fetching\ninstructions from. Each thread has its own private set of registers it\nuses for computation. If 2 threads are running on a single processor,\nswitching from a running one (T1) to running the other (T2) requires a\ncontext switch. Thread Control Blocks (TCBs) store the state of\neach thread of a process. Unlike the context switch for processes, the\naddress space for threads remain the same.\n\n{{}}\n\nExample Thread creation {#example-thread-creation}\n\n#include\n#include\n#include\n\nvoid mythread(void arg) {\n  printf(\"%s\\n\", (char *) arg);\n  return NULL;\n}\n\nint main (int argc, char* argv[]) {\n  pthread_t p1, p2;\n  br int rc;\n  printf(\"main: begin\\n\");\n  rc = pthread_create(&p1, NULL, mythread, \"A\"); assert(rc == 0);\n  rc = pthread_create(&p2, NULL, mythread, \"B\"); assert(rc == 0);\n  //join waits for the threads to finish\n  rc = pthread_join(p1, NULL); assert (rc == 0);\n  rc = pthread_join(p2, NULL); assert (rc == 0);\n  printf(\"main: end\");\n  return 0;\n}\n\nIssues with Uncontrolled Scheduling {#issues-with-uncontrolled-scheduling}\n\nRace Condition {#race-condition}\n\nContext switches that occur at untimely points in the execution can\nresult in the wrong result. Because multiple threads executing this\ncode can result in a race condition, we call this code a _critical\nsection. What's required for this code to run properly is mutual\nexclusion_. This property guarantees that if one thread is executing\nwithin the critical section, others will be prevented from doing so.\n\nKey Terms {#key-terms}\n\nCritical Section\n: piece of code that accesses a shared resource,\n    usually a variable or data structure\n\nRace Condition\n: A situation which arises if multiple threads of\n    execution enter the critical section at roughly\n    the same time; both attempt to update the shared\n    data structure at the same time, leading to\n    surprising and sometimes undesirable outcomes\n\nIndeterminate Program\n: Consists of one or more race conditions;\n    the output is non deterministic, something typically expected of\n    computer programs\n\nMutual Exclusion\n: threads use mutual exclusion primitives to\n    avoid the problems that concurrency yields, such as race conditions\n\nThe wish for atomicity {#the-wish-for-atomicity}\n\nWhat if we had a super-instruction like this:\n\nmemory-add 0x8044a1c, $0x1\n\nAssume this instruction adds a value to a memory location, and the\nhardware guarantees that it executes atomically. This would be easy if\nthe instruction set contained only 1 instruction. However, in the\ngeneral case this is not possible.\n\nInstead, we ask the hardware for a few useful instructions upon which\nwe can build a general set of what is called _synchronisation\nprimitives_.\n\nThread API {#thread-api}\n\n#include\n\nint pthreadcreate (pthreadt * thread,\n                    const pthreadattrt* attr,\n                    void * (start_routine) (void )\n                    void * arg);\n\nthread is a pointer to the structure of type pthread_t, used to\n    interact with the thread\nattr is used to specify attributes this thread might have,\n    including setting the stack size, and scheduling priority of the\n    thread. We can usually pass NULL in.\nstart_routine is the function this thread should start running in\narg is the argument start_routine requires.\n\nint pthreadjoin(pthreadt trhead, void ** value_ptr);\n\npthread_join waits for the thread's completion.\n\nLocks API {#locks-api}\n\nint pthreadmutexlock(pthreadmutext *mutex);\nint pthreadmutexunlock(pthreadmutext *mutex);\n\n// Usage\n//sets the lock to default values, making the lock usable\npthreadmutext lock = PTHREADMUTEXINITIALIZER;\n\n// dynamic way to do it is to make a call:\nint rc = pthreadmutexinit(&lock, NULL);\nassert (rc == 0); //always check success!\n\npthreadmutexlock(&lock);\n// Critical section\nx = x + 1;\npthreadmutexunlock(&lock);\n\nint pthreadmutextrylock(pthreadmutext *mutex);\nint pthreadmutextimedlock(pthreadmutext *mutex,\n                            struct timespec *abs_timeout);\n\nThese two calls are used in lock acquisition. trylock returns\nfailure if the lock is already held, and timedlock returns after a\ntimeout or after acquiring the lock, whichever happens first.\n\nCondition Variables {#condition-variables}\n\nint pthreadcondwait(pthreadcondt cond, pthreadmutext mutex);\nint pthreadcondsignal(pthreadcondt *cond);\n\ncondition variables are useful when some kind of signalling must\n take place between threads.\n\npthreadmutext lock = PTHREADMUTEXINITIALIZER;\npthreadcondt init = PTHREADCONDINITIALIZER;\n\nint rc = pthreadmutexlock(&lock); assert(rc == 0);\nwhile (initialized == 0) {\n  int rc = pthreadcondwait(&init, &lock);\n  assert (rc == 0);\n}\npthreadmutexunlock(&lock);\n\n//Some other thread\npthreadmutexlock(&lock);\ninitialized = 1;\npthreadcondsignal(&init);\npthreadmutexunlock(&lock);\n\nProperties of Correct CS Implementation {#properties-of-correct-cs-implementation}\n\nMutual Exclusion\n: If process P1 is executing in critical section,\n    all other processes are prevented from entering the critical section\n\nProgress\n: If no process is in a critical section, one of the\n    waiting processes should be granted access\n\nBounded Wait\n: After process p1 request to enter critical section,\n    there exists an upperbound of number of times other\n    processes can enter the critical section before p1\n\nIndependence\n: Process not executing in critical section should\n    never block other process\n\nLocks {#locks}\n\nCalling the routine lock() tries to acquire the lock; if no other\nthread holds the lock, the thread will acquire the lock and enter the\ncritical section; this thread is sometimes said to be the owner of\nthe lock.Once the owner of the lock calls unlock(), the lock in\nnow available again. If no othre threads are waiting for the lock\n(i.e. no other thread has called lock() and is stuck), the state of\nthe lock is simply changed to free, if thee are waiting threads, one\nof them will acquire the lock.\n\nPthread Locks {#pthread-locks}\n\nThe name that the POSIX library uses for a lock is a mutex, as it is\nused to provide mutual exclusion between threads. Different locks\ncan be initialized to protect different critical sections.\n\nEvaluating locks {#evaluating-locks}\n\nmutual exclusion\n: does the lock work, preventing multiple threads\n    from entering a critical section?\n\nfairness\n: Does each thread contending for the lock get a fair shot?\n\nperformance\n: Are the time overheads added by using the lock significant?\n\nApproach 1: Controlling Interrupts {#approach-1-controlling-interrupts}\n\nUsing a special hardware instruction, turn off all interrupts during\ncritical section:\n\nvoid lock() {\n  DisableInterrupts();\n}\n\nvoid unlock() {\n  EnableInterrupts();\n}\n\nPros\n\n    Simplicity\n\nCons\n\n    Requires calling thread to perform a privileged operation\n    Doesn't work on multiprocessor systems\n\nApproach 2: Test and Set {#approach-2-test-and-set}\n\nHardware support for atomicity was created. This is known as the\ntest-and-set instruction, or atomic exchange.\n\nThe idea is to use a variable to indicate whether some thread has\npossession of a lock. Calling lock() then tests and sets that variable.\n\nHowever, this presents several issues:\n\nNo Mutex!\nThe thread waiting to acquire a lock is endlessly checking for the\n    value of flag, a technique known as spin-waiting, which wastes\n    time waiting for another thread to release a lock.\n\nWith hardware support for test-and-set, we achieve mutex, and have a\nspin lock! To work correctly on a single processor, it requires a\npreemptive scheduler, one that will interrupt a thread via  atimer, in\norder to run a different thread, from time to time.\n\nEvaluating the spin lock:\n\n    correctness\n    : YES\n\n    fairness\n    : NO, a thread may spin forever under contention\n\n    performance\n    : NO, high performance overheads\n\n    Other hardware primitives one can use to write locks:\n\n    LoadLinked and StoreConditional\n    Fetch-And-Add (ticket lock)\n\nTwo Phase Locks {#two-phase-locks}\n\nA two-phase lock realises that spinning can be useful, particularly if\nthe lock is about to be released. In the first-phase, the lock spins\nfor a while, hoping that it can acquire a lock. However, if the lock\nis not acquired during the first phase, the second phase is entered,\nwhere the caller is put to sleep, and only woken up when the lock\nbecomes free later.\n\nSemaphores vs Spinlocks {#semaphores-vs-spinlocks}\n\nSpinlocks are more efficient if locking only for a short period of\ntime: the CPU cycles used during busy waiting could take a much\nshorter time than making context switches.\n\nClassical Synchronization Problems {#classical-synchronization-problems}\n\nProducer/Consumer {#producer-consumer}\n\nProducers and Consumers share a bounded buffer K\n\nBlocking Version contains 3 semaphores:\n\nBinary semaphore (initialized to 1) [mutex]\n!Full (initialized to 4) [!Full]\n!Empty (initialized to 0) [!Empty]\n\n//Producer\nwhile (TRUE) {\n  Produce Item;\n  wait(notFull);\n  wait(mutex);\n  buffer[in] = item;\n  in = (in + 1) % K;\n  count++;\n  signal(mutex);\n  signal(notEmpty);\n }\n\n// Consumer\nwhile (TRUE) {\n  wait(notEmpty);\n  wait(mutex);\n  item = buffer[out];\n  out = (out + 1) % K;\n  count--;\n  signal(mutex);\n  signal(notFull);\n  Consume Item;\n}\n\nReaders/Writers {#readers-writers}\n\nProcesses share a data structure D\n\nReader: Retrieves information from D\nWriter: Modifies information from D\nWriter must have exclusive access\nReaders can read with other readers\n\nwhile (true) {\n  wait(roomEmpty);\n  //Modifies data;\n  signal(roomEmpty);\n}\n\nwhile (true) {\n  wait(mutex);\n  nReader++;\n  if (nReader == 1) {\n    wait(roomEmpty);\n  }\n  signal(mutex);\n\n  // Reads data;\n  wait(mutex);\n  nReader --;\n  if (nReader == 0) {\n    signal(roomEmpty);\n  }\n  signal(mutex);\n}\n\nDining Philosophers {#dining-philosophers}\n\nPhilosophers sitting in a circle, requiring resource from both left\nand right side.\n\nTanenbaum Solution {#tanenbaum-solution}\n\n#define N 5\n#define LEFT i\n#define RIGHT ((i+1)%N)\n#define THINKING 1\n#define HUNGRY 1\n#define EATING 2\n\nint state[N];\n\nvoid philosopher(int i) {\n  while(true) {\n    think();\n    hungry();\n    takeChpStcks(i);\n    eat();\n    putChpStcks(i);\n  }\n}\nvoid takeChpStcks(i) {\n  wait(mutex);\n  state[i] = HUNGRY;\n  safeToEat(i);\n  signal(mutex);\n  wait(s[i]);\n}\n\nvoid safeToEat(i) {\n  if (state[i] == HUNGRY &&\n      state[LEFT] !=EATING &&\n      state[RIGHT] != EATING) {\n    state[i] = EATING;\n    signal(s[i]);\n  }\n}\n\nvoid putChpStcks(i) {\n  wait(mutex);\n  state[i] = THINKING;\n  safeToEat(LEFT);\n  safeToEat(RIGHT);\n  signal(mutex);\n}\n\nAddress Spaces {#address-spaces}\n\nWhile saving and storing register-level state is relatively fast,\nsaving the entire contents of memory to disk is non-performant. We'd\nrather leave processes in memory while switching between them,\nallowing the OS to implement time sharing efficiently.\n\nThe OS will need to create an easy to use abstraction of physical\nmemory. This abstraction is called the address space.\n\nGoals of Virtual Memory {#goals-of-virtual-memory}\n\ntransparency\n: invisible to the running program, and the program\n    behaves as if it has its own private virtual memory\n\nefficiency\n: time and space efficient, via hardware support such\n    as TLBs.\n\nprotection\n: protects processes from one another, delivering the\n    property of isolation among processes.\n\nTypes of Memory {#types-of-memory}\n\nstack memory\n: allocations and de-allocations are handled\n    implicitly by the compiler\n\nheap memory\n: allocations and de-allocations are explicitly\n    handled by programmer\n\nAddress translation {#address-translation}\n\nThe hardware transforms each memory access, changing the virtual\naddress provided by the instruction to a physical address where the\ndesired information is actually located.\n\n1950's base and bounds technique {#1950-s-base-and-bounds-technique}\n\n2 hardware registers in each CPU: the base register, and the _bounds\nregister_. This pair allows the placement of address space anywhere in\nphysical memory, and do so while ensuring that the process can only\naccess its own address space.\n\nThe OS decides where in physical memory it should be loaded, and sets\nthe base register to that value.\n\nAny memory reference generated by the process is translated:\n\n\\\\( physical address = virtual address + base \\\\)\n\nBecause this translation is done at runtime, this technique is also\nknown as dynamic relocation.\n\nThe OS must take action when a process is created, searching a data\n    structure to find room for the new address space and then mark it used.\nThe OS must take action when a process is terminated, reclaiming\n    all of the memory is use.\nOS must take action when a context switch occurs. This is because\n    there is only one base and bounds register, and their values\n    differ for each running program.\nAccess to the base and bounds register is privileged, and special\n    hardware instructions are required.\n\nDynamic relocation is inefficient. If the space inside the allocated\nunit is not all used, it is wasted, and this is called **internal\nfragmentation**.\n\nSegmentation {#segmentation}\n\nA segment is a contiguous portion of the address space of a particular\nlength. We have 3 logically-different segments: code, stack and heap.\nThe OS can place each of these segments in different parts of physical\nmemory, and avoid filling physical memory with unused virtual address\nspace.\n\nTo support segmentation of this form, the Memory Management Unit (MMU)\nhas, instead of one, 3 base and bounds register pairs\n\nWhen an illegal address is accessed, the hardware detects that the\naddress is out of bounds, and the OS terminates the offending process\nwith a segmentation fault.\n\nTo figure out which segment to use, bits in the virtual address are\noften reserved. This is the virtual address 4200 in binary form:\n\n{{}}\n\nThe hardware would need to know how the segments grow to translate\nvirtual address differently. In the case of the stack, a negative\noffset is added to the base to calculate the correct physical address.\n\nSharing {#sharing}\n\nTo save memory, it useful to share certain memory segments, like\nthe code segment.\n\nTo support sharing in a safe way, protection bits are added per\nsegment, indicating whether or not a program can read or write to a\nsegment.\n\n{{}}\n\nOS support {#os-support}\n\nPhysical memory quickly becomes full of little holes of free space,\nmaking it difficult to allocate to new segments, or grow existing\nones. This is external fragmentation.\n\nOne solution is to compact physical memory by rearranging the\nexisting segments. However, compaction is expensive as copying\nsegments is memory intensive and require processor time.\n\nOne approach is to use a free-list management algorithm that tries to\nkeep large extents of memory for allocation. Algorithms include:\n\nBest-fit\nWorst-fit\nFirst-fit\nBuddy System\n\nFree Space Management {#free-space-management}\n\nA free list contains a set of elements that describe the free space\nstill remaining in the heap. To track the size of allocated regions,\nthe header contains the size of the allocated region.\n\ntypedef struct _headert {\n  int size;\n  int magic;\n}\n\nGrowing the Heap {#growing-the-heap}\n\nTraditional allocators start of with a small heap. When more memory is\nrequired, the sbrk system call is made to request for more memory.\nThe OS finds free physical pages, maps them to the address space of\nthe requesting process, and returns the value of the end of the new heap.\n\nFree Space Allocation Strategies {#free-space-allocation-strategies}\n\nBest fit {#best-fit}\n\nSearch the free list and find chunks of free memory that are as big\n    or bigger than the requested size\nReturn the smallest chunk\n\nNaive implementations are slow because of the exhaustive search\nrequired to find the correct free block.\n\nWorst fit {#worst-fit}\n\nFind the largest chunk\nReturn the requested size, keeping the remaining chunk on the free\n    list\n\nExhaustive search is also required to determine the largest chunk, and\nstudies have shown this still leads to excess fragmentation.\n\nNext Fit {#next-fit}\n\nKeep an extra pointer to the location within the list where one was\n    looking last\nSpread searches for free space throughout the list more uniformly\n\nPerformance similar to first fit.\n\nSegregated Lists {#segregated-lists}\n\nIf a particular application has one (or a few) popular-size requests\nthat it makes, keep a separate list just to manage objects of that\nsize; all other requests are forwarded to a more general memory\nallocator.\n\nWhen the kernel boots up, it allocates a number of object caches for\nkernel objects that are likely to be requested frequently.\n\nBuddy Allocation {#buddy-allocation}\n\nFree memory is conceptually thought of as one big space of size\n\\\\(2^N\\\\).\n\nWhen a request is made, search for free space recursively divides free\nspace into 2 until a block big enough to accommodate the result is\nfound.\n\nHere is an example of a 64KB free space getting divided in the search\nfor a 7KB block:\n\n{{}}\n\nThis scheme can suffer from internal fragmentation. Upon freeing a\nblock, the allocator checks its buddy and sees if the block is still\nfree. If so, it can coalesce the 2 blocks.\n\nPaging {#paging}\n\nInstead of splitting the address space into logical segments, we\nsplit it into fixed-sized units called a page.\n\nWith paging, physical memory is also split into a number of pages, and\nthese pages are called page frames.\n\nTo record where each virtual page of the address place is placed in\nphysical memory, the OS keeps a per-process data structure known as\na page table. The page table stores address translations for each\nof the virtual pages of the address space.\n\nAddress Translation {#address-translation}\n\nTo translate a virtual address, the address is split into 2\ncomponents:\n\n{{}}\n\nvirtual page number (VPN)\noffset within the page\n\nSuppose the page size is 16 bytes, in a 64-byte address space. We need\nto be able to select 4 pages, and the first 2 bits of the address can\ndo just that, giving us a 2 bit VPN.\n\nInside Page Tables {#inside-page-tables}\n\nThe simplest form of a page table is the linear page table, which is\nan array. The OS indexes the array by the VPN and looks up the\npage-table entry (PTE) at that index in order to find the desired\nPhysical Frame Number (PFN).\n\nWithin each PTE, a number of different bits are stored.\n\nvalid bit\n: indicates whether a particular transaction is valid.\n    This is crucial for supporting a sparse address space.\n\nprotection bit\n: indicating whether a page can be read from,\n    written to, or executed from.\n\npresent bit\n: indicates whether this page is in physical memory or\n    in disk (swapped out).\n\ndirty bit\n: indicates whether a page has been modified since it was\n    brought into memory.\n\nreference bit\n: track whether a page has been accessed, and can be\n    useful in deciding which pages to swap out (e.g.\n    swapping out the least popular pages).\n\n{{}}\n\nHow Paging Works {#how-paging-works}\n\nExamine the following instruction: movl 21, %eax. We assume the\nhardware performs the translation.\n\nThe system fetches the proper page table entry from the process's\n    page table. To know the location of the page table, the system\n    looks at the page-table base register, containing the address of\n    the page table.\nThe system translates the virtual address (21) to the correct\n    physical address:\n\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\nPTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE))\n\nIn this example, VPN_MASK would be set to 0x30, or 110000 in binary,\nto extract the VPN bits. Once the PFN is obtained from the page table,\nit is left-shifted, and OR'd with the offset to form the final address.\n\nNotice each memory reference requires performing one extra memory\nreference, which is very costly.\n\nSpeeding up Paging with TLBs {#speeding-up-paging-with-tlbs}\n\nPaging alone shows high performance overheads. This is because the\nmapping information is generally stored in physical memory, and\nrequires an extra memory lookup for each virtual memory address.\n\nTo speed up address translation, a hardware feature called\ntranlation-lookaside buffer, or TLB, is added.\n\nA TLB is a part of the chip's memory management unit (MMU), and is a\nhardware cache of popular virtual-to-physical address translations.\n\nTLB Basic Algorithm {#tlb-basic-algorithm}\n\nWe first look up the virtual address in the TLB. If it exists in the\nTLB, then it is a TLB Hit, and the PFN is extracted from the TLB.\n\nIf the CPU does not find the translation in the TLB,  this is a\nTLB miss. The hardware accesses the page table to find the\ntranslation, and updates the TLB.\n\nTLB performance gains benefit from:\n\nSpatial locality: e.g. elements in an array are packed into the\n    same page, so only the first access is a TLB miss.\nTemporal locality: elements accessed recently are more likely to be\n    accessed again.\n\nManaging TLB Misses {#managing-tlb-misses}\n\nOld architectures had hardware -managed TLBs, where the hardware new\nexactly where the page tables were located (via registers), the page\ntable's exact format: the hardware would \"walk\" the page table and\nfind the correct page-table entry.\n\nNewer architectures have a software-managed TLB. On a TLB miss, the\nhardware raises an exception, which pauses the instruction stream,\nraises the privilege level to kernel mode, and jumps to a **trap\nhandler**.\n\nThe primary advantage of software-managed TLBs is flexibility. The\nOS can use any data structure to implement the page table, without\nhardware change. The hardware also doesn't do much on a TLB miss: it\nsimply raises an exception, and the OS TLB miss handler will handle\nthe rest.\n\nHandling Context Switches {#handling-context-switches}\n\nThe VPN-PFN mapping for different processes are different, and the TLB\nwill need to account for context switching.\n\nOne approach is to flush the TLB on context switches. However, each\ntime a process runs or resumes, it will incur TLB misses.\n\nTo reduce this overhead, some systems provide an **address space\nidentifier (ASID)* in the TLB. the ASID* acts as a process\nidentifier, like the PID but with fewer bits.\n\n{{}}\n\nTLB Replacement Policies {#tlb-replacement-policies}\n\nLeast Recently Used (LRU)\n    takes advantage of locality\nRandom\n    no weird corner cases that have pessimal behaviour\n\nMaking Page Tables Smaller {#making-page-tables-smaller}\n\nSimple array-based page tables are too big, taking up too much memory\non physical systems.\n\nBigger Pages {#bigger-pages}\n\nUsing larger pages will lead to a reduction in the number of page\nentries required, and reduce the size of the page table. However, this\nleads to a lot of wastage in each page (internal fragmentation).\n\nPaging and Segments {#paging-and-segments}\n\nInstead of having a single page table for the entire address space of\nthe process, have on per logical segment.\n\nThe virtual address now looks like this:\n\n{{}}\n\nThe downsides for segmentation apply: segmentation is not flexible,\nand if the heap is rarely used, for example, then there is wastage.\n\nMulti-level Page Tables {#multi-level-page-tables}\n\n{{}}\n\nA new data structure called the page directory is introduced. the\npage directory consists of a number of page directory entries (PDE).\nA PDE has minimally a valid bit and a page frame number.\n\nThe page directory only allocates space proportional to the amount of\naddress space being used, and is generally compact and supports\nsparse address spaces.\n\nWith the page directory, the added level of indirection allows us to\nplace pages anywhere in physical memory. However, on a TLB miss, two\nloads from memory will be required to get the right translation\ninformation from the page table. Hence, there is a time-space\ntradeoff.\n\n{{}}\n\nTo extend this idea to multi-level page tables, we can partition the\nVPN even further:\n\n{{}}\n\nInverted Page Tables {#inverted-page-tables}\n\nInstead of having one page table per process, we can have a single\npage table for each physical physical page of the system.\n\nFinding the correct entry is expensive via a linear scan, and hash\ntables are often built over the base structure to speed lookups.\n\nBeyond Physical Memory {#beyond-physical-memory}\n\nSometimes the address space of a running process does not fit into\nmemory. In that case, we require an additional level in the memory\nhierachy, some place to stash portions of the address space that is\ncurrently not in use. We call this the swap space.\n\nMechanisms {#mechanisms}\n\nSwap Space {#swap-space}\n\nSwap space is some space on disk reserved for moving pages back and\nforth. The OS will need to memorise the disk address of a given page.\n\nThe size of the swap space determines the number of pages it can\nstash.\n\nThe Present Bit {#the-present-bit}\n\nWhen a TLB miss occurs, the OS will have to look at the swap space\nbefore to find if the page is present in physical memory.\n\nWhen the hardware looks in the PTE, it may find that the page is not\npresent in physical memory. We have to add one new piece of\ninformation in the PTE, called the present bit. If the present bit\nis set to one, then the page is present in physical memory and\neverything proceeds as above. If it set to 0, the page is on disk\nsomewhere. Accessing a page not in physical memory is commonly\nreferred to as a page fault.\n\nUpon a page fault, the OS will run the page-fault handler, and\nfetch the page from disk and load it into memory. The OS will then\nupdate the page table to mark the page as present, and update the PFN\nfield of the PTN to record the in-memory location of the newly fetched\npage.\n\nPage-replacement Policy {#page-replacement-policy}\n\nWhen memory is full, upon a page fault, the OS will have to decide\nwhich page in the memory to kick out. This replacement policy is known\nas the page-replacement policy.\n\nBelow is the page-fault control algorithm:\n\nPFN = findFreePhysicalPage();\nif (PFN == -1) {                  /* can't find free page */\n  PFN = EvictPage();              /* Evict some old page */\n }\nDiskRead(PTE.diskAddr, PFN);    /* sleep, waiting for IO */\nPTE.present = true;\nPTE.PFN = PFN;\nRetryInstruction();\n\nWhen Replacements Really Occur {#when-replacements-really-occur}\n\nThe OS actively keeps a small portion of memory free. To do this, the\nOS maintains a high watermark (HW) and a low watermark (LW). When\nthe OS notices that there are fewer than LW pages available, a\nbackground thread runs, freeing memory. This is sometimes called a\nswap daemon or a page daemon.\n\nMany systems cluster a number of pages and write them at once to the\nswap partition.\n\nWith the addition of a page daemon, the control flow algorithm can\nbe changed. The thread trying to read a page would simply wait for\nthe page daemon free up enough memory if the LW threshold is hit.\n\nPolicies {#policies}\n\nThe main memory can be viewed as a cache mechanism. Knowing the number\nof cache hits and cache misses allows us to calculate the **average\nmemory access time (AMAT)** of a program. An optimal page-replacement\npolicy will reduce the number of cache misses overall. The approach\nwould be to replace the page that will be accessed furthest in the\nfuture.\n\nThe first few accesses when the cache begins in an empty state is a\ncold-start miss.\n\nFIFO {#fifo}\n\nIn FIFO replacement, pages are placed in a queue when they enter the\nsystem, and when a replacement occurs, the page on the tail of the\nqueue is evicted.\n\nBelady's anomaly\n\n    One would normally expect the cache hit rate to increase when the\n    cache gets larger. However, with FIFO, it gets worse, and this\n    phenomenon is called Belady's anomaly. LRU has the stack property:\n    for algorithms with this property, a cache of size N+1 naturally\n    includes the contents of a cache of size N. FIFO and Random (among\n    others) do not obey this stack property.\n\nInteresting Links {#interesting-links}\n\nWhat is a CPU to do when it has nothing to do? {#what-is-a-cpu-to-do-when-it-has-nothing-to-do}\n\nWhen a CPU is idle (has no task scheduled), the CPU would have to run\nuseless instructions until it is needed for real work. The kernel can\nput them into low-power idle states, but entry and exit of these idle\nstates are not free. The kernel needs to predict the length of the\nidle state, and decide whether or not to put the CPU into a low-power\nstate. This is the job of the idle loop.\n\nIn this loop, the CPU scheduler notices that a CPU is idle because it\nhas no work for the CPU to do. The scheduler then calls the governor,\nwhich does its best to predict the appropriate idle state to enter.\nThere are currently two governors in the kernel, called \"menu\" and\n\"ladder\".\n\nThe tick is a timer run by the CPU scheduler for time-sharing the CPU:\ndifferent jobs run for different number of ticks, and take turns in\nexecution. This tick is not needed during idling, and running it\nprevents the CPU from entering deep idle states.\n\nIn Linux kernel TODO How new-lines affect the Linux kernel performance {#how-new-lines-affect-the-linux-kernel-performance}\n",
        "tags": []
    },
    {
        "uri": "/zettels/optimal_control",
        "title": "Optimal Control and Planning",
        "content": "\nHow can we make decisions if we know the dynamics of the environment?\n\nStochastic optimization {#stochastic-optimization}\n\nStochastic optimization for open-loop planning:\n\nWe wish to choose \\\\(a\\1, \\dots a\\T = \\mathrm{argmax}\\{a\\1, \\dots a\\_T}\nJ(a\\1, \\dots, a\\T)\\\\) for some objective \\\\(J\\\\).\n\nGuess and Check {#guess-and-check}\n\nAn extremely simple method, that's parallelizable:\n\npick \\\\(A\\1, \\dots A\\N\\\\) from some distribution\nchoose \\\\(A\\i\\\\) based on \\\\(\\mathrm{argmax} J(A\\i)\\\\).\n\nCross-entropy Method (CEM) {#cross-entropy-method--cem}\n\npick \\\\(A\\1, \\dots A\\N\\\\) from some initial distribution \\\\(p(A)\\\\)\nEvaluate \\\\(J(A\\1), \\dots J(A\\N)\\\\)\npick the elites \\\\(A\\{i1}, \\dots A\\{im}\\\\) with the highest value\nfit distribution $P(A) to the elites\n\nWith continuous inputs, a multi-variate normal distribution is a\ncommon choice for \\\\(p(A)\\\\). In the discrete Case, Monte-Carlo tree\nsearch (§mcts) is typically used.\n\nUsing Derivatives {#using-derivatives}\n\nDifferentiable Dynamic Programming (DDP)\nLQR\n",
        "tags": []
    },
    {
        "uri": "/zettels/optimization",
        "title": "Optimization",
        "content": "\nWhat is Convex Optimization? {#what-is-convex-optimization}\n\nConvex optimization is a special class of mathematical optimization\nproblems, which includes least-squares and linear programming\nproblems.\n\nThere are many advantages to recognizing or formulating a problem as a\nconvex optimization problem. First, the problem can be solved reliably\nand efficiently, using interior-point methods or other special methods\nfor convex optimization. There are also theoretical or conceptual\nadvantages of formulating a problem as a convex optimization problem.\n\nMathematical Optimization {#mathematical-optimization}\n\nAn optimization problem has the form:\n\n\\begin{align} \\label{dfn:optimization}\n  &\\text{minimize} &f\\_0(x) \\\\\\\\\\\\\n  &\\text{subject to} &f\\i(x) \\le b\\i, i = 1, \\dots, m\n\\end{align}\n\nHere the vector \\\\(x = (x\\1, \\dots, x\\n)\\\\) is the optimization variable\nof the problem, the function \\\\(f\\_0 : \\mathbb{R^n} \\rightarrow\n\\mathbb{R}\\\\) is the objective function, \\\\(f\\_i \\mathbb{R^n} \\rightarrow\n\\mathbb{R}\\\\) are the (inequality) constraint functions, and the\nconstants \\\\(b\\1, \\dots, b\\m\\\\) are the limits, or bounds, for the\nconstraints.\n\nWe consider families or classes of optimization problems,\ncharacterized by particular forms of the objective and constraint\nfunctions. The optimization problem is a linear program if the\nobjective and constraint functions \\\\(f\\0, \\dots, f\\m\\\\) are linear.\n",
        "tags": []
    },
    {
        "uri": "/zettels/options_framework",
        "title": "Options Framework",
        "content": "\nAn option is defined as a tuple containing:\n\nAn initiation function (precondition)\nAn internal policy (behaviour)\nA termination function (post-condition)\n\nThis helps put learning and planning algorithms at the same level of\nabstraction. (Stolle \\& Precup, 2002)\n\nModels vs Actions {#models-vs-actions}\n\nmodels of actions consist of immediate reward and transition\n    probability to next state\nmodels of options consist of reward until termination, and\n    (discounted) transition to termination state\n\nThey look a lot like value functions, and can use the TD error to\ntrain the model §td\\_learning.\n\nRelated {#related}\n\n§generalized\\value\\functions\n\nBibliography\nStolle, M., & Precup, D., Learning options in reinforcement learning, In , International Symposium on abstraction, reformulation, and approximation (pp. 212–223) (2002). : . ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/org-mode",
        "title": "Org-Mode",
        "content": "\nCan add headline with numbers with (org-num-mode)\nC-c C-x - runs org-timer-item, which is useful for taking notes.\n",
        "tags": []
    },
    {
        "uri": "/zettels/org-roam",
        "title": "Org-Roam",
        "content": "\nOrg-roam is a package I built trying to emulate Roam Research in\nEmacs and Org-Mode.\n\nMedia {#media}\n\nIntroducing Org Roam - Jethro Kuan\nOrg-roam - Jack Baty's weblog\nBSAG » Further adventures in org-mode\n",
        "tags": []
    },
    {
        "uri": "/zettels/pac_learning",
        "title": "PAC Learning",
        "content": "\ntags\n: §machine\\_learning\n\nERM for finite hypothesis classes {#erm-for-finite-hypothesis-classes}\n\nWe note that Empirical Risk Minimization can easily overfit\nto the training data. To correct for this, we introduce inductive\nbias, restricting the hypothesis space \\\\(\\mathcal{H}\\\\).\n\nThe simplest type of restriction is to impose an upper bound on the\nsize of a class. Here, we show that if a hypothesis class is finite,\nthen ERM will not overfit given a sufficiently large sample size.\n\nLet \\\\(h\\_S\\\\) denote the result of applying ERM to \\\\(S\\\\):\n\n\\begin{equation}\n  h\\S \\in \\textrm{argmin}\\{h \\in \\mathcal{H}} L\\_S(h)\n\\end{equation}\n\nWe make 2 assumptions. First, the realizability assumption, implying\nthat every ERM hypothesis we have that \\\\(L\\S(h\\S) = 0\\\\). However, we are\nmore interested in the true risk \\\\(L\\{(D,f)}(h\\S)\\\\) rather than the\nempirical risk.\n\nThe Realizability Assumption: There exists \\\\(h^\\* \\in \\mathcal{H}\\\\) such\nthat \\\\(L\\_{(D,f)}(h^\\*)= 0\\\\). That is, with probability 1 over random\nsamples \\\\(S\\\\), where the instances are sampled according to \\\\(D\\\\), and\nlabelled according to \\\\(f\\\\), we have \\\\(L\\_S(h^\\*) = 0\\\\).\n\nAny guarantee on the error with respect to the underlying distribution\n\\\\(D\\\\), must depend on the relationship between \\\\(D\\\\) and \\\\(S\\\\). Here, we\nmake the second assumption that the training examples are drawn i.i.d.\n\nSince \\\\(L\\{(D,f)}(h\\S)\\\\) depends on the training set, which is drawn via\na random process, it is also a random variable.\n\nWe introduce 2 parameters:\n\nthe probability of getting a non-representative sample, denoted by\n    \\\\(\\delta\\\\). We denote \\\\((1 - \\delta)\\\\) the confidence parameter of our prediction.\nWe denote \\\\(\\epsilon\\\\) as the accuracy parameter of the prediction.\n    The event that \\\\(L\\{(D,f)}(h\\S) > \\epsilon\\\\) is a failure of the\n    learner, while \\\\(L\\{(D,f)}(h\\S) \\le \\epsilon\\\\) is the event where the\n    predictor is approximately correct.\n\nWe are interested in upper bounding the probability to sample m-tuple\nof instances that will lead to failure of the learner. Formally, let\n\\\\(S\\x = \\left(x\\1, \\dots, x\\_m \\right)\\\\) be the instances of the training\nset. We would like to upper-bound:\n\n\\begin{equation}\n  D^M(\\left\\\\{ S\\x ; L\\{(D,f)}(h\\_S) > \\epsilon \\right\\\\})\n\\end{equation}\n\nLet \\\\(H\\_B\\\\) be the set of bad hypotheses, that is,\n\n\\begin{equation}\n  \\mathcal{H}\\B = \\left\\\\{ h \\in \\mathcal{H} : L\\{(D,f)}(h)> \\epsilon \\right\\\\}\n\\end{equation}\n\nIn addition, let:\n\n\\begin{equation}\nM = \\left\\\\{ S\\x: \\exists h \\in \\mathcal{H}\\B, L\\_S(h) = 0 \\right\\\\}\n\\end{equation}\n\nbe the set of misleading samples. For every \\\\(S\\_x \\in M\\\\), there is a\nbad hypothesis, \\\\(h \\in \\mathcal{H}\\_B\\\\) that looks like a good\nhypothesis in \\\\(S\\_x\\\\).\n\nSince the realizability assumption implies \\\\(L\\S(h\\S) = 0\\\\), then the\nevent \\\\(L\\{(D,f)}(h\\S) > \\epsilon\\\\) will only happen if our sample is\nin the set of misleading examples \\\\(M\\\\).\n\nThen:\n\n\\begin{equation}\n  D^m(\\left\\\\{ S\\x : L\\{(D,f)}(h\\_S) > \\epsilon \\right\\\\}) \\le D^m(M)\n  =D^m(\\cup\\{h \\in \\mathcal{H}\\B} {S\\x: L\\S(h) = 0})\n\\end{equation}\n\nBecause the training samples are i.i.d.:\n\n\\begin{align}\n  D^m(\\left\\\\{ S\\x: L\\S(h) = 0\\right\\\\}) &= D^m(\\left\\\\{ S\\_x: \\forall i,\n                                         h(x\\i) = f(x\\i) \\right\\\\}) \\\\\\\\\\\\\n  &=  \\prod\\{i=1}^{m}D(\\left\\\\{ x\\i: h(x\\i) = f(x\\i) \\right\\\\})\n\\end{align}\n\nfor each individual sampling of an element of the training set, we\nhave:\n\n\\begin{equation}\n  D(\\left\\\\{ x\\i: h(x\\i) = y\\i \\right\\\\}) = 1 - L\\{(D,f)}(h) \\le 1- \\epsilon\n\\end{equation}\n\nUsing the inequality \\\\(1 - \\epsilon \\le e^{-\\epsilon}\\\\), we obtain that:\n\n\\begin{equation}\n  D^m(\\left\\\\{ S\\x: L\\S(h) = 0 \\right\\\\}) \\le (1 - \\epsilon)^m \\le\n  e^{-\\epsilon m}\n\\end{equation}\n\nApplying the union bound, we get:\n\n\\begin{equation}\n  D^m(\\left\\\\{ S\\x: L\\{(D,f)}(h\\S) > \\epsilon \\right\\\\}) \\le \\left| \\mathcal{H}\\B \\right|(1 - \\epsilon)^m \\le\n  \\left| \\mathcal{H}\\_B \\right| e^{-\\epsilon m}\n\\end{equation}\n\nWith this result, we can show that where \\\\(m \\ge\n\\frac{\\log(|\\mathcal{H}|/\\delta)}{\\epsilon}\\\\), the error \\\\(L\\{(D,f)(h\\S)\n\\le \\epsilon}\\\\) for every ERM hypothesis \\\\(h\\_S\\\\).\n\nFormulation {#formulation}\n\nA hypothesis class \\\\(\\mathcal{H}\\\\) is PAC learnable if there exist a\nfunction \\\\(m\\_{\\mathcal{H}} : (0,1)^2 \\rightarrow \\mathbb{N}\\\\) and a\nlearning algorithm with the following property: For every \\\\(\\epsilon,\n\\delta \\in (0,1)\\\\), for every distribution \\\\(\\mathcal{D}\\\\) over\n\\\\(\\mathcal{X}\\\\), and with respect to \\\\(\\mathcal{H}, \\mathcal{D},\n\\mathcal{f}\\\\), then when running the learning algorithm on \\\\(m \\ge\nm\\_{\\mathcal{H}}(\\epsilon, \\delta)\\\\) i.i.d. examples generated by\n\\\\(\\mathcal{D}\\\\), and labeled by \\\\(f\\\\), the algorithm returns a hypothesis\n\\\\(h\\\\) such that, with probability of at least \\\\(1 - \\delta\\\\) (over the\nchoice of examples), \\\\(L\\_{(D,f)}(h) \\le \\epsilon\\\\).\n\nThe accuracy parameter \\\\(\\epsilon\\\\) determines how far the output\nclassifier can be from the optimal one, and the confidence parameter\n\\\\(\\delta\\\\) indicates how likely the classifier is to meet the accuracy\nrequirement.\n\nUnder the data access model, these approximations are inevitable: for\nexample, the training set is randomly generated, and there is a chance\nthe training samples will be non-informative.\n\nSample Complexity {#sample-complexity}\n\nThe function \\\\(m\\_{\\mathcal{H}}: (0,1)^2 \\rightarrow \\mathbb{N}\\\\)\ndetermines the sample complexity of learning \\\\(\\mathcal{H}\\\\): that is,\nhow many examples are required to guarantee a probably approximately\ncorrect solution.\n\nWe have shown previously that every finite hypothesis class is PAC\nlearnable with sample complexity:\n\n\\begin{equation}\n  m\\_{\\mathcal{H}} (\\epsilon, \\delta) \\le \\lceil \\frac{\\log(|\\mathcal{H}|/\\delta)}{\\epsilon} \\rceil\n\\end{equation}\n\nThere are infinite hypothesis classes that are PAC learnable, and the\ncombinatorial measure called the VC dimension is what deterimines its\nPAC learnability.\n\nGeneralizing the Learning Model {#generalizing-the-learning-model}\n\nRemoving the Realizability Assumption {#removing-the-realizability-assumption}\n\nWe have initially required that the learning algorithm succeeds on a\npair of data distribution \\\\(D\\\\) and labeling function \\\\(f\\\\) provided that\nthe realizability assumption is met. For practical learning tasks,\nthis assumption is too strong.\n\nThe realizability assumption requires that there exists \\\\(h^\\* \\in\n\\mathcal{H}\\\\) such that \\\\(\\mathbb{P}\\_{x \\sim D} [h^\\*(x) = f(x)] = 1\\\\). We\nreplace the\n\nLearning Problems beyond binary classification {#learning-problems-beyond-binary-classification}\n\nMany learning tasks take a different form, such as regression tasks,\nwhere a real-world value is predicted. Formally, let \\\\(\\mathcal{D}\\\\) be\na probability distribution over \\\\(\\mathcal{X} \\times \\mathcal{Y}\\\\),\nwhere \\\\(\\mathcal{X}\\\\) is our domain set, and \\\\(\\mathcal{Y}\\\\) is a set of\nlabels. \\\\(\\mathcal{D}\\\\) is a joint distribution over domain points and\nlabels. We can think of it as being composed of two parts: a\ndistribution \\\\(D\\_x\\\\) over unlabeled domain points, and a conditional\nprobability over labels for each domain point, \\\\(D(x,y) | x\\\\).\n\nFor a probability distribution \\\\(D\\\\), one can measure how likely \\\\(h\\\\) is\nto make an error when labeled points are randomly drawn according to\nD. We redefine the true error to be:\n\n\\begin{equation}\n  L\\_D(h) = D(\\left\\\\{ (x,y): h(x) \\ne y \\right\\\\})\n\\end{equation}\n\nWe would like to find a predictor \\\\(h\\\\) for which the error is\nminimized. However, the learner does not know \\\\(D\\\\), but instead has\naccess to its training data, \\\\(S\\\\).\n\nThe Bayes predictor {#the-bayes-predictor}\n\nGiven any probability distribution \\\\(D\\\\) over \\\\(X\\\\), the best label\npredicting function \\\\(X\\\\) to \\\\(\\left\\\\{ 0,1 \\right\\\\}\\\\) will be:\n\n\\begin{equation}\nf\\_D(x) = \\begin{cases}\n  1 & \\textrm{if } P[y=1|x] \\ge \\frac{1}{2} \\\\\\\\\\\\\n  0 & \\textrm{otherwise }\n\\end{cases}\n\\end{equation}\n\nThis is the Bayes optimal predictor, and no other classifier has a\nlower error. It can be shown that no algorithm can be guaranteed to\nfind a predictor that is as good as the Bayes optimal predictor.\n\nNow, we require that the learning algorithm will find a predictor\nwhose error is not much larger than the best possible error of a\npredictor in some given benchmark hypothesis class. The strength of\nsuch a requirement depends on the choice of that hypothesis class.\nThat is, the algorithm returns a hypothesis \\\\(h\\\\) such that with\nprobability \\\\(1 - \\delta\\\\):\n\n\\begin{equation}\nL\\D(h) \\le \\textrm{min}{h'\\in \\mathcal{H}} L\\D(h') + \\epsilon\n\\end{equation}\n\nLearning Via Uniform Convergence {#learning-via-uniform-convergence}\n\nGiven a hypothesis class, \\\\(\\mathcal{H}\\\\) The ERM learning paradigm\nworks as follows: upon receiving a training sample, \\\\(S\\\\), the learner\nevaluates the risk of each \\\\(h\\\\) in \\\\(H\\\\), on the given sample, and\noutputs a member of \\\\(H\\\\) that minimizes this risk. Hence, all we need\nis that uniformly over all hypotheses in the hypothesis class, the\nempirical risk will be close to the true risk. We formalize it as\nfollows:\n\nA training set \\\\(S\\\\) is called a $&epsilon;$-representative sample\n(w.r.t. domain \\\\(Z\\\\), hypothesis class \\\\(\\mathcal{H}\\\\), loss function \\\\(l\\\\),\nand distribution \\\\(D\\\\)) if\n\n\\begin{equation}\n  \\forall h \\in \\mathcal{H}, | L\\S(h) - L\\D(h)| \\le \\epsilon\n\\end{equation}\n\nWhenever a sample is (&epsilon;/2)-representative, the ERM learning\nrule is guaranteed to return a good hypothesis.\n\nThis lemma implies that to ensure that the ERM rule is an agnostic PAC\nlearner, it suffices to show that with probability of at least \\\\(1 -\n\\delta\\\\) over the random choice of a training set, it will be an\n$&epsilon;$-representative set. This is referred to as the _uniform\nconvergence_ property.\n\nA hypothesis class \\\\(H\\\\) has the uniform convergence property (w.r.t.\ndomain \\\\(Z\\\\), and loss function \\\\(l\\\\)) if there exists a function\n\\\\(m\\_{\\mathcal{H}}^{Vc} : (0,1)^2 \\rightarrow \\mathbb{N}\\\\) such that for\nevery \\\\(\\epsilon, \\delta \\in (0,1)\\\\) and for every probability\ndistribution \\\\(\\mathcal{D}\\\\) over \\\\(Z\\\\), if \\\\(S\\\\) is a sample of \\\\(m \\ge\nm\\_{\\mathcal{H}}^{VC}(\\epsilon, \\delta)\\\\) examples drawn i.i.d according\nto \\\\(\\mathcal{D}\\\\), then with probability of at least \\\\(1 - \\delta\\\\), \\\\(S\\\\)\nis $&epsilon;$-representative.\n\nWe also need the measure concentration inequality by Hoeffding, which\nquantifies the gap between empirical averages andn their expected\nvalue:\n\nLet \\\\(\\theta\\1, \\dots, \\theta\\m\\\\) be a sequence of i.i.d. random\nvariables, and assume that for all \\\\(i\\\\), \\\\(E[\\theta\\_i] = \\mu\\\\) and P[a\n&le; &theta;\\_i &le; b] = 1$. Then for any \\\\(\\epsilon > 0\\\\):\n\n\\begin{equation}\n  P\\left[ \\left| \\frac{1}{m}\\sum\\{i=1}^{m}\\theta\\i - \\mu \\right| >\n    \\epsilon \\right] \\le 2 \\textrm{exp} \\left( \\frac{-2m\\epsilon^2}{(b-a)^2} \\right)\n\\end{equation}\n\nClasses of functions for which the uniform convergence property holds\nare also called Glivenko-Cantelli classes. The fundamental theorem of\nlearning theory states that in binary classification problems, uniform\nconvergence is not only a sufficient condition for learnability, but\nis also a necessary condition. This is not the case for more general\nlearning problems.\n",
        "tags": []
    },
    {
        "uri": "/zettels/papers",
        "title": "Papers",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/para_method",
        "title": "PARA Method",
        "content": "\ntags\n: §productivity\n\nPARA is a method of organising digital information, designed by Tiago\nForte. Tiago Forte also proposed the method of §progressive\\_summarization.\n\nPARA stands for:\n\nproject\n: a series of tasks linked to a goal. E.g. app mockups,\n    project plans, blog post etc.\n\nareas\n: a sphere of activity with a standard to be maintained over\n    time. E.g. Health, Finances, Travel, Hobbies\n\nresources\n: a topic or theme of ongoing interest. E.g. habit\n    formation, project management, interior design\n\narchive\n: inactive items from the above 3 categories\n\nProjects vs Areas {#projects-vs-areas}\n\nProjects always have a goal to be achieved, typically with some hard\ndeadline. The end of a project is signalled by the occurrence of some\ndiscrete event.\n\nAreas are ongoing standards, that may have no end date or eventual\noutcome. Projects always fall into areas.\n\nWhy distinguishing projects and areas is important {#why-distinguishing-projects-and-areas-is-important}\n\nThis helps someone understand the true extent of their commitments.\nCommitments are towards projects -- responsibilities with tangible\noutcomes. Knowing how projects connect to your long-term goals (areas)\nis important.\n\nContrasting with §zettelkasten {#contrasting-with-zettelkasten--zettelkasten-dot-md}\n\nThis seems to be in contrast with other techniques like §zettelkasten,\nwhich propose no such organization, instead using links between\ndifferent information.\n\nMy personal thoughts:\n\nPARA looks like it would form a good hierarchical overview of tasks\n    and information. Zettelkasten does not provide this.\nZettelkasten still remains a useful tool for low-level,\n    inter-project information-linking.\n\nResources {#resources}\n\nThe PARA Method: A Universal System for Organizing Digital Information | Praxis\n",
        "tags": []
    },
    {
        "uri": "/zettels/particle_filter",
        "title": "Particle Filter",
        "content": "\nParticle filters approximate the posterior by a finite number of\nparameters. The posterior \\\\(\\text{bel}(x\\_t)\\\\) is represented by a set of\nrandom state samples drawn from this posterior. This representation\ncan represent a much broader space of distributions, but is\napproximate.\n\nThe samples of a posterior distribution are called particles, denoted\nby:\n\n\\begin{equation}\n  X\\t := x\\t^{[1]}, x\\_t^{[2]}, \\dots\n\\end{equation}\n\nEach particle \\\\(x\\_t^{[m]}\\\\) is a concrete instantiation of the state at\ntime \\\\(t\\\\): a hypothesis as to what the true world state may be at time\n\\\\(t\\\\). \\\\(M\\\\) denotes the number of particles in the particle set \\\\(X\\_t\\\\).\nThe number of particles is often large, and sometimes a function of\n\\\\(t\\\\) or other quantities related to the belief.\n\nKey Idea {#key-idea}\n\nThe likelihood for a state hypothesis \\\\(x\\_t\\\\) to be included in the\nparticle set \\\\(X\\_t\\\\) should be proportional to its Bayes filter\nposterior \\\\(\\text{bel}(x\\_t)\\\\):\n\n\\begin{equation}\n  x\\t^{[m]} \\sim p(x\\t : z\\{1:t}, u\\{1:t})\n\\end{equation}\n\nHence, the denser the subregion of the state space populated by\nsamples, the more likely it is that the true state falls into this\nregion. This property holds asymptotically for \\\\(m \\rightarrow \\infty\\\\).\n\nAlgorithm {#algorithm}\n\nThe algorithm first constructs a temporary particle set \\\\(\\overline{X}\\\\)\nwhich is reminiscent to the belief \\\\(\\overline{\\text{bel}}(x\\_t)\\\\). It\ndoes this by systematically processing each particle \\\\(x\\_{t-1}^{[m]}\\\\)\nin the input particle set \\\\(X\\_{t-1}\\\\).\n\n\\begin{algorithm}\n  \\caption{Particle Filter}\n  \\label{particle\\_filter}\n  \\begin{algorithmic}[1]\n    \\Procedure{ParticleFilter}{$X\\{t-1}, u\\t, z\\_t$}\n    \\State $\\overline{X}\\t = X\\t = \\phi$\n    \\For {$m = 1 \\text{ to } M$}\n    \\State sample $x\\t^{[m]} \\sim p(x\\t | u\\t, x\\{t-1}^{[m]})$\n    \\State $w\\t^{[m]} = p(z\\t | x\\_t^{[m]})$\n    \\State $\\overline{X}\\t = \\overline{X}\\t + \\langle x\\t^{[m]} , w\\t^{[m]} \\rangle$\n    \\EndFor\n    \\For {$m = 1 \\text{ to } M$}\n    \\State draw $i$ with probability $\\proportional w\\_t^{[i]}$\n    \\State add $x\\t^{[i]} to X\\t$\n    \\EndFor\n    \\State \\Return $X\\_t$\n    \\EndProcedure\n  \\end{algorithmic}\n\\end{algorithm}\n\n\\\\(w\\t^{[m]}\\\\) is the importance factor for the particle \\\\(x\\t^{[m]}\\\\): the\nprobability of measurement \\\\(z\\t\\\\) under the particle \\\\(x\\t^{[m]}\\\\).\n\nThe second for-loop implements importance re-sampling. The algorithm\ndraws with replacement \\\\(M\\\\) particles from \\\\(\\overline{X}\\_t\\\\). Whereas\n\\\\(\\overline{X}\\_t\\\\) is distributed according to\n\\\\(\\overline{\\text{bel}}(x\\_t)\\\\), the resampling causes them to be\ndistributed according to the posterior \\\\(\\text{bel}(x\\t) = \\eta p(z\\t |\nx\\t^{m]})\\overline{\\text{bel}}(x\\t)\\\\). (see [§importance\\_sampling)\n\nProperties {#properties}\n\nThere are four complimentary sources of approximation error:\n\nThere are finitely many particles. Non-normalized values \\\\(w\\_t^{m}\\\\)\n    are drawn from an M-dimensional space, but after normalization they\n    reside in a space of dimension \\\\(M-1\\\\). The effect of loss of\n    dimensionality becomes less pronounced with larger \\\\(M\\\\).\nThe resampling process induces a loss of diversity in the particle\n    population, manifesting as an approximation error. This is the\n    variance of the estimator. This is countered with several variance\n    reduction techniques:\n    Reducing the frequency of resampling\n    low variance sampling\nDivergence of proposal and target distribution. Particles are\n    generated from a proposal distribution that does not consider the\n    measurement. If at one extreme, the sensors of the robot are highly\n    inaccurate, but its motion is accurate, the target distribution\n    will be similar to the proposal distribution and the particle\n    filter will be efficient. However, the opposite configuration can\n    cause the distributions to diverge substantially.\nParticle deprivation problem: in high dimensional spaces, there may\n    be no particles in the vicinity to the correct state. That is,\n    the number of particles might be too small to cover all relevant\n    regions of high likelihood.\n",
        "tags": []
    },
    {
        "uri": "/zettels/pdf_crop",
        "title": "PDF Cropping",
        "content": "\ntags\n: §pdf\\_tools\n\nCropping PDFs can be done through pdfcrop. pdfcrop only takes a\nsingle argument, use a bash function to batch crop:\n\npdfconstcrop() {\n    pdfcrop --bbox \"$(\n        pdfcrop --verbose \"$@\" |\n        grep '^%%HiResBoundingBox: ' |\n        cut -d' ' -f2- |\n        datamash -t' ' min 1 min 2 max 3 max 4\n    )\" \"$@\"\n}\n\npdfcrop_all() {\n    for FILE in *.pdf; do\n        pdfconstcrop --margins '20 20 20 20' \"${FILE}\"\n    done\n}\n\nThis script ensures that the cropped pages are of the same size.\nRequires datamash.\n",
        "tags": []
    },
    {
        "uri": "/zettels/pdf_nup",
        "title": "PDF Nup",
        "content": "\ntags\n: §pdf\\_tools\n\nThis puts several pages of a PDF together. 2x1 is a particularly\ncommon combination.\n\nalias p2x1=\"pdfnup --nup 2x1 --landscape --suffix '2x1' --batch \"\n",
        "tags": []
    },
    {
        "uri": "/zettels/pdf_tools",
        "title": "Pdf Tools",
        "content": "\ntags\n: §linux\n",
        "tags": []
    },
    {
        "uri": "/zettels/pgm",
        "title": "Probabilistic Graph Models",
        "content": "\ntags\n: §machine\\_learning\n\nMotivation {#motivation}\n\nMost tasks require a person or an automated system to reason: to take\nthe available information and reach conclusions, both about what might\nbe true in the world and about how to act. Probabilistic graphical\nmodels represent a general framework that can be used to allow a\ncomputer system to reason.\n\nUsing this approach of declarative representation, we construct,\nwithin the computer, a model of the system about which we would like\nto reason. This model encodes our knowledge of how the system works in\na computer-readable form. This representation can then be manipulated\nby various algorithms that can answer questions based on the model.\n\nThe key property of a declarative representation is the separation of\nknowledge and reasoning. The representation has its own clear\nsemantics, separate from the algorithms that one can apply to it.\nThus, we can develop a general suite of algorithms that apply any\nmodel within a broad class, whether in the domain of medical diagnosis\nor speech recognition. Conversely, we can improve our model for a\nspecific application domain without having to modify our reasoning\nalgorithms constantly.\n\nUncertainty is a fundamental quantity in many real-world situations.\nTo obtain meaningful conclusions, we need to reason not just about\nwhat is possible, but what is probable.\n\nThe calculus of probability theory provides us with a formal framework\nfor considering multiple outcomes and their likelihood. This\nframeworks allows us to consider options that are unlikely, yet not\nimpossible, without reducing our conclusions to content-free lists of\nevery possibility.\n\nOne finds that probabilistic models are very liberating. Where in a\nmore rigid formalism, we might find it necessary to enumerate every\npossibility, here we can often sweep a multitude of annoying\nexceptions and special cases under the probabilistic rug, by\nintroducing outcomes that roughly correspond to \"something unusual\nhappens\". This type of approximation is often inevitable, as we can\nonly rarely provide a deterministic specification of the behaviour of\na complex system. Probabilistic models allow us to make this explicit,\nand therefore provide a model that is more faithful to reality.\n\nGraphical Representation {#graphical-representation}\n\nProbabilistic graphical models use a graph-based representation as the\nbasis for compactly encoding a complex distribution over a\nhigh-dimensional space. In this graphical representation, the nodes\ncorrespond to the variables in our domain, and the edges correspond to\ndirect probabilistic interactions between them.\n\n{{}}\n\nBayesian Networks {#bayesian-networks}\n\nThe goal is to represent a joint distribution \\\\(P\\\\) over some set of\nrandom variables \\\\(\\left\\\\{X\\1, \\dots, X\\n\\right\\\\}\\\\). Even in the\nsimplest case where these numbers are binary-valued, a joint\ndistribution requires the specification of \\\\(2^n-1\\\\) numbers. For all\nbut the smallest \\\\(n\\\\), the joint distribution in unmanageable from\nevery perspective. Computationally, it is expensive to manipulate and\ngenerally too large to store in memory. Cognitively, it is impossible\nto acquire so many numbers from a human expect, moreover, the numbers\nare very small and do not correspond to events that people can\nreasonably contemplate. Statistically, if we want to learn the\ndistribution from data, we would need ridiculously large amounts of\ndata to estimate many parameters robustly. These problems have been\nthe main barrier to the adoption of probabilistic methods for expert\nsystems until the development of the methodologies presented below.\n\nThe compact representations explored are based on two key ideas: the\nrepresentation of independence properties of the distribution, and the\nuse of an alternative parameterization that allows us to exploit these\nfine-grained independencies.\n\nBayesian networks build on the same intuitions as the naive Bayes\nmodel by exploiting conditional independence properties of the\ndistribution in order to allow a compact and natural representation.\nThe core of the Bayesian network representation is the directed graph\n\\\\(G = (V,E)\\\\) together with a random variable \\\\(x\\_i\\\\) for each node \\\\(i \\in\nV\\\\), one conditional probability distribution (CPD) p(x\\i | xA\\i)\nper node, specifying the probability of \\\\(x\\_i\\\\) conditioned on its\nparent's values.\n\n{{}}\n\nThis graph \\\\(G\\\\) can be viewed in two very different ways:\n\nas a data structure that provides the skeleton for representing a\n    joint distribution compactly in a factorized way;\nas a compact representation for a set of conditional independence\n    assumptions about a distribution.\n\nThese two views, are in a strong sense, equivalent.\n\nBayesian Network Semantics {#bayesian-network-semantics}\n\nWe can formally define the Bayesian network structure as follows:\n\nLet \\\\(\\mathrm{Pa}\\{X\\i}^G\\\\) denote the parents of \\\\(X\\_i\\\\) in G, and\n\\\\(\\mathrm{NonDescendants}\\{X\\i}\\\\) denote the variables in the graph that\nare not descendants of \\\\(X\\_i\\\\). Then \\\\(G\\\\) encodes the following set of\nindependence assumptions, called the local independencies, denoted by\n\\\\(I\\_l(G)\\\\):\n\n\\begin{equation}\n  \\text{ For each variable } X\\i: \\left( X\\i \\perp \\mathrm{NonDescendants}\\{X\\i}\n         | \\mathrm{Pa}\\{X\\i}^G \\right)\n\\end{equation}\n\nIn other words, the local independencies state that each node \\\\(X\\_i\\\\) is\nconditionally independent of its nondescendants given its parents.\n\nThe formal semantics of a Bayesian network graph is as a set of\nindependence assertions. On the other hand, our representation was a\ngraph annotated with conditional probability distributions (CPDs).\nHere, we show that these two definitions are, in fact equivalent. A\ndistribution \\\\(P\\\\) satisfies the local independencies associated with a\ngraph \\\\(G\\\\) if and only if \\\\(P\\\\) is representable as a set of CPDs\nassociated with the graph \\\\(G\\\\). We begin by formalizing the basic\nconcepts.\n\nI-Maps\n\n    Let \\\\(P\\\\) be a distribution over \\\\(\\mathcal{X}\\\\). We define\n    \\\\(\\mathcal{I}(P)\\\\) to be the set of independence assertions of the form\n    \\\\((X \\perp Y | Z)\\\\) that hold in \\\\(P\\\\).\n\n    Now, we need to show that \\\\(\\mathcal{I}\\_l(G) \\subseteq \\mathcal{I}(P)\\\\).\n\n    Let \\\\(K\\\\) be any graph object associated with a set of independencies\n    \\\\(\\mathcal{I}(K)\\\\). We say that \\\\(K\\\\) is an I-map for a set of\n    independencies \\\\(\\mathcal{I}\\\\) if \\\\(\\mathcal{I}(K) \\subseteq\n    \\mathcal{I}\\\\).\n\n    Now, we can say we need to show that \\\\(G\\\\) is an I-map for \\\\(P\\\\).\n\n    For \\\\(G\\\\) to be an I-map for \\\\(P\\\\), it is necessary that \\\\(G\\\\) does not\n    mislead us regarding independencies of \\\\(P\\\\): any independence that \\\\(G\\\\)\n    asserts must also hold in \\\\(P\\\\). Conversely, \\\\(P\\\\) may have additional\n    independencies not reflected in \\\\(G\\\\).\n\nI-Map to Factorization\n\n    A BN structure \\\\(G\\\\) encodes a set of conditional independence\n    assumptions; every distribution for which \\\\(G\\\\) is an I-map must\n    satisfy these assumptions.\n\n    Consider the joint distribution \\\\(P(I, D, G, L, S)\\\\); from the chain\n    rule for probability, we can decompose the distribution in the\n    following way:\n\n    \\begin{equation}\n      P(I, D, G, L, S) = P(I) P(D|I) P(G|I, D) P(L | I, D, G) P(S | I, D,\n      G, L)\n    \\end{equation}\n\n    This decomposition requires no assumptions. We may however be able to\n    apply our conditional independence assumptions induced from the BN.\n\n    We say that a distribution \\\\(P\\\\) over the same space factorizes\n    according to a BN graph \\\\(G\\\\) if \\\\(P\\\\) can be expressed as a product:\n\n    \\begin{equation}\n      P(X\\1, \\dots, X\\n) = \\prod\\{i=1}^{n} P(X\\i | \\mathrm{Pa}\\{X\\i}^G).\n    \\end{equation}\n\n    This equation is called the chain rule for BNs. The individual factors\n    \\\\(P(X\\i | \\mathrm{Pa}\\{X\\_i}^G)\\\\) are called conditional probability\n    distributions (CPDs) or local probabilistic models.\n\n    We can now show that if \\\\(G\\\\) is a BN structure over a set of random\n    variables \\\\(X\\\\), and \\\\(P\\\\) be a joint distribution over the same space,\n    then if \\\\(G\\\\) is an I-map for \\\\(P\\\\), \\\\(P\\\\) factorizes according to \\\\(G\\\\).\n\n    Proof:\n\n    Assume, without loss of generality, that \\\\(X\\1, \\dots, X\\n\\\\) is a\n    topological ordering of the variables in \\\\(X\\\\) relative to \\\\(G\\\\). First,\n    we use the chain rule for probabilities:\n\n    \\begin{equation}\n      P(X\\1, \\dots, X\\n) = \\prod\\{i=1}^{n}P(X\\i | X\\1, \\dots, X\\{i-1}).\n    \\end{equation}\n\n    Now consider one of the factors \\\\(P(X\\i|X\\1, \\dots, X\\_{i-1})\\\\). As \\\\(G\\\\)\n    is an I-map for \\\\(P\\\\), we have \\\\((X\\i \\perp \\mathrm{ND}\\{X\\_i} |\n    \\mathrm{Pa}\\{X\\i}^G) \\in I(P)\\\\). By assumption, all of \\\\(X\\_i\\\\)'s parents\n    are in the set \\\\(X\\1, \\dots, X\\{i-1}\\\\). Furthermore, none of \\\\(X\\_i\\\\)'s\n    descendants can possibly be in the set. Hence\n\n    \\begin{equation}\n      \\left\\\\{ X\\1, \\dots, X\\{i-1} \\right\\\\} = \\mathrm{Pa}\\{X\\i} \\in \\mathbf{Z}\n    \\end{equation}\n\n    where \\\\(\\mathbf{Z} \\in \\mathrm{ND}\\{X\\i}\\\\). Form the local\n    independencies for \\\\(X\\_i\\\\) and from the position property it follows\n    that \\\\(X\\i \\perp \\mathbf{Z} | \\mathrm{Pa}\\{X\\i}\\\\). Hence \\\\(P(X\\i| X\\_1,\n    \\dots X\\{i-1}) = P(X\\i | \\mathrm{Pa}\\{X\\i})\\\\).\n\n    Applying this transformation to all of the factors in the chain rule\n    decomposition gives the desired result.\n\nFactorization to I-map\n\n    This is simple to prove, by manipulation of probabilities.\n\nIndependencies in Graphs {#independencies-in-graphs}\n\nDependencies and independencies are crucial for understanding the\nbehaviour of a distribution. Independency properties are also\nimportant for answering queries: they can be exploited to reduce\nsubstantially the computational cost of inference. Therefore, it is\nimportant that our representations make these properties clearly\nvisible both to a user and to algorithms that manipulate the BN data\nstructure.\n\nThe immediate question that arises is whether there exist independence\nproperties that we can read off directly from \\\\(G\\\\).\n\nD-separation\n\n    We want to be able to guarantee that an independence \\\\((\\mathbf{X}\n    \\perp \\mathbf{Y} | \\mathbf{Z})\\\\), holds in a distribution associated\n    with a BN structure \\\\(G\\\\). It helps to consider its converse: \"Can we\n    imagine a case where it does not?\"\n\n    Direct Connection\n\n    If \\\\(X \\rightarrow Y\\\\), then we can construct a distribution such that\n    \\\\(X\\\\) and \\\\(Y\\\\) are correlated regardless of any evidence about of the\n    other variables in the network. (e.g. \\\\(Val(X) = Val(Y)\\\\))\n\n    Indirect Connection\n\n    Consider a 3-node network where \\\\(X\\\\) and \\\\(Z\\\\) are not directly connected\n    but through \\\\(Y\\\\). There are four possible 2-edge trails:\n\n    {{}}\n\n    We say that Q, W are d-separated when variables \\\\(O\\\\) are observed if\n    they are not connected by an active path. An undirected path in the\n    Bayesian network \\\\(G\\\\) is called active given observed variables \\\\(O\\\\)\n    if for every triple of variables \\\\(X, Y, Z\\\\) on the path, one of the\n    following holds:\n\n    Casual trail\n    : \\\\(X \\leftarrow Y \\leftarrow Z, Y \\not\\in O\\\\) active\n        iff \\\\(Y\\\\) is not observed\n\n    Evidential trail\n    : \\\\(X \\rightarrow Y \\rightarrow Z, Y \\not\\in O\\\\)\n        active iff \\\\(Y\\\\) is not observed\n\n    Common cause\n    : \\\\(X \\leftarrow Y \\rightarrow Z, Y \\not\\in O\\\\) active\n        iff \\\\(Y\\\\) is not observed\n\n    Common effect\n    : \\\\(X \\rightarrow Y \\leftarrow Z, Y \\text{ or any\n                           descendants} \\in O\\\\) active iff either \\\\(Y\\\\) or one of\n        \\\\(Y\\\\)'s descendants is observed\n\n    Consider the general trail \\\\(X\\1 \\rightleftharpoons X\\2\n    \\rightleftharpoons \\dots \\rightleftharpoons X\\_n\\\\). Let \\\\(\\mathbf{Z}\\\\) be a\n    subset of observed variables. Then the trail is active given\n    \\\\(\\mathbf{Z}\\\\) if:\n\n    Whenever we have a v-structure \\\\(X\\{i-1} \\rightarrow X\\i \\leftarrow\n          X\\{i+1}\\\\), then \\\\(X\\i\\\\) or one of its descendants are in \\\\(\\mathbf{Z}\\\\);\n    no other node along the trail is in \\\\(\\mathbf{Z}\\\\).\n\n    Let \\\\(\\mathbf{X}, \\mathbf{Y}, \\mathbf{Z}\\\\) be three sets of nodes in\n    \\\\(\\mathcal{G}\\\\). We say that \\\\(\\mathbf{X}\\\\) and \\\\(\\mathbf{Y}\\\\) are\n    d-separated given \\\\(\\mathbf{Z}\\\\), denoted\n    \\\\(\\mathrm{d-sep}\\_{\\mathcal{G}}(\\mathbf{X}; \\mathbf{Y} | \\mathbf{Z})\\\\),\n    if there is no active trail between any node \\\\(X \\in \\mathbf{X}\\\\), and\n    \\\\(Y \\in \\mathbf{Y}\\\\) given \\\\(\\mathbf{Z}\\\\). We use\n    \\\\(\\mathcal{I}(\\mathcal{G})\\\\) to denote this set of independencies that\n    correspond to d-separation:\n\n    \\begin{equation}\n      \\mathcal{I}(\\mathcal{G}) = \\left\\\\{ (\\mathbf{X} \\perp \\mathbf{Y} |\n        \\mathbf{Z}) : \\mathrm{d-sep}\\_{\\mathcal{G}}(\\mathbf{X}; \\mathbf{Y} | \\mathbf{Z}) \\right\\\\}\n    \\end{equation}\n\n    This set is also called the set of global Markov independencies. These\n    independencies are precisely those that are guaranteed to hold for\n    every distribution over \\\\(G\\\\).\n\n    A nice tutorial on d-separation can be found here.\n    (UCLSA, nil)\n\nMarkov Blanket\n\n    Consider a joint distribution \\\\(p(X\\1, \\dots, x\\D)\\\\) represented by a\n    directed graph having \\\\(D\\\\) nodes. Consider the conditional distribution\n    of a particular node with variables \\\\(x\\_i\\\\) conditioned on all the\n    remaining variables \\\\(x\\_{j \\ne i}\\\\). We have:\n\n    \\begin{equation}\n      p(x\\i | x\\{\\\\{j \\ne i\\\\}}) = \\frac{p(x\\1, \\dots, x\\D)}{\\int p(x\\_1,\n        \\dots, x\\D) dx\\i} = \\frac{\\prod\\{k}p(x\\k | \\textrm{pa}\\k)}{\\prod\\k\n      p(x\\k | \\textrm{pa}\\k)dx\\_i}\n    \\end{equation}\n\n    We observe that any factor \\\\(p(x\\k | \\textrm{pa}\\k)\\\\) that does not have\n    any functional dependence on \\\\(x\\_i\\\\) can be taken outside the integral,\n    and will therefore cancel between the numerator and the denominator.\n    The only factors that will remain are the conditional distribution\n    \\\\(p(x\\i | \\textrm{pa}\\i)\\\\) for the node \\\\(x\\_i\\\\) itself, and conditional\n    distributions for any nodes \\\\(x\\k\\\\) such that node \\\\(x\\i\\\\) is in teh\n    conditioning set of \\\\(p(x\\k | \\textrm{pa}\\k)\\\\), in other words for which\n    \\\\(x\\i\\\\) is a parent of \\\\(x\\k\\\\). The conditional \\\\(p(x\\i | \\textrm{pa}\\i)\\\\)\n    will depend on the parents of node \\\\(x\\_i\\\\), and the conditionals\n    \\\\(p(x\\k | \\textrm{pa}\\k)\\\\) will depend on nthe children of \\\\(x\\_i\\\\), as\n    well as the co-parents: variables corresponding to parents of node\n    \\\\(x\\k\\\\) other than \\\\(x\\i\\\\). This set of nodes is called the Markov Blanket.\n\n    {{}}\n\nSoundness and Completeness {#soundness-and-completeness}\n\nSoundness\n: If a distribution \\\\(P\\\\) factorizes according to \\\\(G\\\\), then\n    \\\\(\\mathcal{I}(G) \\subseteq \\mathcal{I}(P)\\\\).\n\nCompleteness\n: If we have 2 variables \\\\(X\\\\) and \\\\(Y\\\\) that are\n    independent given \\\\(\\mathbf{Z}\\\\), then \\\\(X\\\\) and \\\\(Y\\\\) are\n    d-separated. We find that this is ill-defined,\n    because it does not specify the distribution in\n    which \\\\(X\\\\) and \\\\(Y\\\\) are independent.\n\nFaithful\n: A distribution \\\\(P\\\\) is faithful to \\\\(G\\\\) if, whenever \\\\((X\n                  \\perp Y | \\mathbf{Z}) \\in I(P)\\\\), then\n    \\\\(\\mathrm{d-sep}\\_{G}(X;Y|\\mathbf{Z})\\\\). Any independence\n    in \\\\(P\\\\) is reflected in the d-separation properties of\n    the graph.\n\nThe notion of faithfulness is the converse of our notion of soundness.\nHowever, it can be shown that this desirable property of faithfulness\nis false.\n\nWe can, however, adopt a weaker but useful definition of completeness:\n\nIf \\\\((X \\perp Y | \\mathbf{Z})\\\\) in all distributions \\\\(P\\\\) that factorize\nover \\\\(G\\\\), then \\\\(\\mathrm{d-sep}\\_G(X;Y|\\mathbf{Z})\\\\).\n\nUsing this definition, we can show that If \\\\(X\\\\) and \\\\(Y\\\\) are not\nd-separated given \\\\(\\mathbf{Z}\\\\) in \\\\(G\\\\), then \\\\(X\\\\) and \\\\(Y\\\\) are dependent\ngiven \\\\(Z\\\\) in some distribution \\\\(P\\\\) that factorizes over \\\\(G\\\\).\n\nThis completeness result tells us that our definition of \\\\(I(G)\\\\) is\nthe maximal one: for any independence assertion that is not a\nconsequence of d-separation in \\\\(g\\\\), we can always find a\ncounterexample distribution \\\\(P\\\\) that factorizes over \\\\(G\\\\).\n\nIn fact, for almost all distributions \\\\(P\\\\) that factorize over \\\\(G\\\\),\nthat is for all distributions except for a set of measure zero in the\nspace of CPD parameterizations, we have \\\\(I(P) = I(G)\\\\).\n\nAn algorithm for d-separation {#an-algorithm-for-d-separation}\n\nThere is a linear-time (in the size of the graph) algorithm for\ndetermining the set of d-separations. The algorithm has 2 phases:\n\nTraverse the graph bottom up, from the leaves to the roots, marking\n    all nodes that are in \\\\(\\mathbf{Z}\\\\) or that have descendants in\n    \\\\(\\mathbf{Z}\\\\). These nodes will serve to enable v-structures.\nTraverse breadth-first from \\\\(X\\\\) to \\\\(Y\\\\), stopping the traversal\n    along a trail when we get to a blocked node.\n\nA node is blocked if:\n\nit is the \"middle\" node in a v-structure and unmarked in phase I, or\nIt is not a middle node and is in \\\\(\\mathbf{Z}\\\\)\n\nIf the BFS gets us from \\\\(X\\\\) to \\\\(Y\\\\), then there is an active trail\nbetween them.\n\n{{}}\n\nI-equivalence {#i-equivalence}\n\nThe notion of \\\\(I(G)\\\\) specifies a set of conditional independence\nassertions that are associated with a graph. This allows us to\nabstract away of details of the graph structure, viewing it purely as\na specification of independence properties.\n\nOne important implication of this perspective is the observation that\nvery different BN structures can actually be equivalent, in that they\nencode the same set of conditional independence assumptions.\n\nThis brings us to the notion of I-equivalence:\n\nTwo graphs \\\\(K\\1\\\\) and \\\\(K\\2\\\\) over \\\\(X\\\\) are I-equivalent if \\\\(I(K\\_1) =\nI(K\\_2)\\\\). The set of all graphs over \\\\(X\\\\) are partitioned into mutually\nexclusive and exhaustive I-equivalence classes.\n\nThis notion implies that any distribution \\\\(P\\\\) that can be factorized\nover one of these graphs can be factorized over the other.\nFurthermore, there is no intrinsic property of \\\\(P\\\\) that would allow us\nto associate it with one graph rather than an equivalent one. This\nobservation has important implications with respect to our ability to\ndetermine the directionality of influence.\n\nFrom Distributions to Graphs {#from-distributions-to-graphs}\n\nGiven a distribution \\\\(P\\\\), to what extent can we construct a graph \\\\(G\\\\)\nwhose independencies are a reasonable surrogate for the independencies\nin \\\\(P\\\\)? We will never actually take a fully-specified distribution \\\\(p\\\\)\nand construct a graph \\\\(G\\\\) for it, as this is way too large. However,\nanswering this question is an important contextual exercise, that h\nhelps in understanding the process of constructing a BN that\nrepresents our model of the world.\n\nMinimal I-maps\n\n    One approach to finding a graph that represents a distribution \\\\(p\\\\) is\n    simply to take any graph that is an I-map for \\\\(P\\\\). However, a complete\n    graph is an I-map for any distribution, but it does not reveal any\n    independencies in the distribution. This intuition leads us to the\n    definition of a minimal I-map:\n\n    A graph \\\\(K\\\\) is a minimal I-map for a set of independencies \\\\(i\\\\) if it\n    is an I-map for \\\\(I\\\\), and if the removal of even a single edge from \\\\(K\\\\)\n    renders it not an I-map.\n\n    To obtain a minimal I-map we simply follow a natural algorithm that\n    arises through the factorization theorem. Note that the minimal I-map\n    is not necessarily unique in this construction.\n\n    {{}}\n\n    Minimal I-maps fail to capture all the independencies that hold in the\n    distribution. Hence, that \\\\(G\\\\) is a minimal I-map for \\\\(P\\\\) is far from a\n    guarantee that \\\\(G\\\\) captures the independence structure in \\\\(P\\\\).\n\nPerfect Maps\n\n    A graph \\\\(K\\\\) is a P-map for a distribution \\\\(P\\\\), for a set of\n    independencies \\\\(I\\\\) if we have that \\\\(I(K) = I\\\\). We say that \\\\(K\\\\) is a\n    perfect map for \\\\(P\\\\) if \\\\(I(K) = I(P)\\\\).\n\n    Unfortunately, not every distribution has a perfect map. There exists\n    an algorithm for finding the DAG representing the P-map for a\n    distribution of a P-map if it exists, but is quite involved. See\n    (Koller et al., 2009).\n\nUndirected Graphical Models {#undirected-graphical-models}\n\n(The bulk of the material is from Murphy's book\n(Murphy, 2014))\n\nFor some domains, being forced to choose a direction for the edges, as\nrequired by a DGM is awkward. For example, if we're modelling an\nimage, we might suppose that the neighbouring pixels are correlated.\nWe may form a DAG model with a 2d lattice topology as such:\n\n{{}}\n\nHowever, representing the conditional probabilities in this way is\nrather unnatural: the Markov blanket of node \\\\(X\\_8\\\\) includes its\nnon-neighbours. Instead, we may want to use a UGM, or Markov Random\nField (MRF).\n\n{{}}\n\nConditional Independence Properties of UGMs {#conditional-independence-properties-of-ugms}\n\nUGMs define CI relationships via simple graph separation as follows:\n\nglobal Markov property\n: \\\\(A \\perp B | \\mathbf{C}\\\\) if there is no\n    path between A and B in the graph upon removing all nodes in \\\\(\\mathbf{C}\\\\).\n\nlocal Markov property\n: \\\\(A \\perp V \\setminus \\\\{\\textrm{mb}(A),\n         A\\\\} | \\textrm{mb}(A)\\\\)\n\npairwise Markov property\n: \\\\(A \\perp B | V \\setminus \\left{ A,\n         B\\right}\\\\)\n\nThe global Markov property implies the local and pairwise Markov\nproperties. If \\\\(p(x) > 0\\\\) for all \\\\(x\\\\), then the pairwise Markov\nproperty implies the global Markov property. This result allows us to\nuse pairwise CI statements to construct a graph from which global\nstatements can be extracted.\n\nRepresentation Power {#representation-power}\n\nDGMs and UGMs can perfectly represent different set of distributions.\nThe set of distributions that are perfectly represented by both DGMs\nand UGMs are termed chordal.\n\n{{}}\n\nIn general, CI properties in UGMs are monotonic, in the following\nsense: if \\\\(A \\perp B | C\\\\), then \\\\(A \\perp B | C \\cup D\\\\). In DGMs, CI\nproperties can be non-monotonic, since conditioning on extra variables\ncan eliminate conditional independencies due to explaining away.\n\nIf all the variables are collapsed in each maximal clique to make\n\"mega-variables\", the resulting graph will be a tree if the\ndistribution is chordal.\n\nThe Undirected alternative to d-separation {#the-undirected-alternative-to-d-separation}\n\nIt is tempting tot simply convert the DGM to a UGM by dropping the\norientation of the edges, but this is incorrect because a v-structure\nhas different CI properties than the undirected chain. To avoid such\nincorrect CI statemnets, we can add edges between the \"unmarreid\"\nparents A and C, and then drop the arrows from the edges, forming in\na connected undirected graph. This process is called moralization.\n\nMoralization loses some CI information, and therefore we cannot used a\nmoralized UGM to determine CI properties of the DGM.\n\nParameterization of MRFs {#parameterization-of-mrfs}\n\nSince there is no topological ordering in an unordered graph, \\\\(p(y)\\\\)\ncannot be represented with the chain rule. Instead, potential\nfunctions or factors are associated with each maximal clique in the\ngraph The join distribution is defined to be proportional to the\nproduct of clique potentials. The Hammersley-Clifford theorem shows\nthat any positive distribution whose CI properties can be represented\nby a UGM can be represented in this way.\n\nA positive distribution \\\\(p(y) > 0\\\\) satisfies the CI properties of an\nundirected graph \\\\(G\\\\) iff p can be represented as a product of\nfactors, one per maximal clique, i.e.,\n\n\\begin{equation}\n  p(\\mathbf{y}|\\mathbf{\\theta}) = \\frac{1}{Z(\\mathbf{\\theta})}\n  \\prod\\{c\\in \\mathcal{C}} \\Phi\\c(\\mathbf{y}\\c | \\mathbf{\\theta}\\c)\n\\end{equation}\n\nwere \\\\(C\\\\) is the set of all the (maximal) cliques of \\\\(G\\\\), and\n\\\\(Z(\\mathbf{\\theta})\\\\) is the partition function given by\n\n\\begin{equation}\n  Z(\\mathbf{\\theta}) = \\sum\\{x} \\prod\\{c \\in \\mathcal{C}} \\Phi\\c(\\mathbf{y}\\c|\\mathbf{\\theta}\\_c)\n\\end{equation}\n\nConnection between statistical physics {#connection-between-statistical-physics}\n\nThere is a model known as the Gibbs distribution, which can be written\nas follows:\n\n\\begin{equation}\n  p(\\mathbf{y} | \\mathbf{\\theta}) = \\frac{1}{Z(\\mathbf{\\theta})}\n  \\mathrm{exp} \\left( - \\sum\\{c} E(\\mathbf{y}\\c | \\mathbf{\\theta}\\_c) \\right)\n\\end{equation}\n\nwhere \\\\(E(\\mathbf{y}\\_c)\\\\) is the energy associated with the variables in\nclique \\\\(c\\\\). We can convert this to a UGM by defining:\n\n\\begin{equation}\n  \\Phi\\c(\\mathbf{y}\\c | \\mathbf{\\theta}\\c) = \\mathrm{exp}\\left( - E(\\mathbf{y}\\c | \\mathbf{\\theta}\\_c) \\right)\n\\end{equation}\n\nHere we see that high probability states correspond to low energy\nconfigurations. We are also free to restrict the parameterization to\nthe edges of the graph. A rather convenient formulation is the\npairwise MRF.\n\nRepresenting Potential Functions {#representing-potential-functions}\n\nIf the variables are discrete, we can represent the potential or\nenergy functions as tables of (non-negative) numbers, as with CPTs.\nHowever, the potentials are not probabilities, but rather a\nrepresentation of relative \"compatibility\" or \"happiness\" between the\ndifferent assignments.\n\nA more general approach is to define the log potentials as a linear\nfunction of the parameters:\n\n\\begin{equation}\n  \\log \\psi\\c (\\mathbf{y}\\c) = \\phi\\c (\\mathbf{y}\\c)^T \\mathbf{\\theta}\\_c\n\\end{equation}\n\nwhere \\\\(\\phi\\c (\\mathbf{x}\\c\\_)\\\\) a feature vector derived from the\nvalues of the variables \\\\(mathbf{y}\\_c\\\\). The resultant log probability\nhas the form:\n\n\\begin{equation}\n  \\log p(\\mathbf{y} | \\mathbf{\\theta}) = \\sum\\_{c}\n  \\phi\\c(\\mathbf{y}\\c)^T \\mathbf{\\theta}\\_c - Z(\\mathbf{\\theta})\n\\end{equation}\n\nThis is also known as the maximum entropy or log linear model.\n\nSeveral popular probability models, such as the Ising model, Potts model and\nHopfield networks, can be conveniently expressed as UGMs.\n\nParameter Estimation in UGMs {#parameter-estimation-in-ugms}\n\nConsider an MRF in log-linear form:\n\n\\begin{equation}\n  p(\\mathbf{y} | \\mathbf{\\theta}) = \\frac{1}{Z(\\mathbf{\\theta})}\n  \\mathrm{exp} \\left( \\sum\\{c}\\mathbf{\\theta\\c}^T \\phi\\_c(\\mathbf{y})\\right)\n\\end{equation}\n\nwhere \\\\(c\\\\) indexes the cliques. The scaled log-likelihood is given by:\n\n\\begin{equation}\n  l(\\mathbf{\\theta}) =\n  \\frac{1}{N}\\sum\\{i}\\log(\\mathbf{y}\\i|\\mathbf{\\theta}) =\n  \\frac{1}{N}\\sum\\_{i}\\left[\n    \\sum\\{c}\\mathbf{\\theta}\\c^T\\phi\\c(\\mathbf{y}\\i) - \\log Z(\\mathbf{\\theta}) \\right]\n\\end{equation}\n\nSince MRFs are in the exponential family, we know that this function\nis convex in \\\\(\\mathbf{\\theta}\\\\), and has a unique global maximum which\nwe can find using gradient-based optimizers.\n\nThe derivative for the weights of a particular clique is given by:\n\n\\begin{equation}\n  \\frac{\\partial l}{\\partial \\mathbf{\\theta}\\_c} =\n  \\frac{1}{N}\\sum\\{i}\\left[ \\phi\\c(\\mathbf{y}\\_i) -\n    \\frac{\\partial}{\\partial \\mathbf{\\theta}\\_c} \\log Z(\\mathbf{\\theta}) \\right]\n\\end{equation}\n\nThe derivative of the log partition function wrt to\n\\\\(\\mathbf{\\theta\\_c}\\\\) is just the expectation of the cth feature under\nthe model, and hence the gradient of the log-likelihood is:\n\n\\begin{equation}\n  \\frac{\\partial l}{\\partial \\mathbf{\\theta}\\_c} =\n  \\frac{1}{N}\\sum\\{i}\\left[ \\phi\\c(\\mathbf{y}\\_i) -\n    \\mathcal{E}[\\phi\\_c(\\mathbf{y})]\\right]\n\\end{equation}\n\nIn the first term, we fix \\\\(\\mathbf{y}\\\\) to its observed values; this is\nsometimes called the clamped term. In the second term \\\\(\\mathbf{y}\\\\) is\nfree; this is sometimes called the unclamped term. Computing the\nunclamped term requires inference in the model, and must be done once\nper gradient step, making it much slower than DGM training.\n\nApproximate methods for computing the MLEs of MRFs {#approximate-methods-for-computing-the-mles-of-mrfs}\n\nWhen fitting a UGM there is (in general) no closed form solution for\nthe ML or the MAP estimate of the parameters, so we need to use\ngradient-based optimizers. This gradient requires inference. In models\nwhere inference is intractable, learning is also intractable. This\nmotivates computationally faster alternatives to ML/MAP estimation,\nsuch as pseudo likelihood, and stochastic maximum likelihood.\n\n{{}}\n\n{{}}\n\nConditional Random Fields (CRFs) {#conditional-random-fields--crfs}\n\nA CRF is a version of an MRF where all the clique potentials are\nconditioned on input features:\n\n\\begin{equation}\n  p(\\mathbf{y} | \\mathbf{x}, \\mathbf{w}) = \\frac{1}{Z(\\mathbf{x},\n    \\mathbf{w})} \\prod\\{c} \\psi\\c(\\mathbf{y}\\_c | \\mathbf{x}, \\mathbf{w})\n\\end{equation}\n\nIt can be thought of as a structured output extension of logistic\nregression. A log-linear representation of the potentials is often\nassumed.\n\nThe advantage of a CRF over a MRF is analogous to the advantage of a\ndiscriminative classifier over a generative classifier, where we don't\nneed to \"waste resources\" modeling things that we always observe, but\ninstead model the distribution of labels given the data.\n\nIn the CRF, we can also make the potentials of the model be\ndata-dependent. For example, we can make the latent labels in an NLP\nproblem depend on global properties of the sentence.\n\nHowever, CRF requires labeled training data, and are slower to train.\n\nBibliography\nUCLSA,  (nil). Causality - discussion. Retrieved from http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html. Online; accessed 11 February 2019. ↩\n\nKoller, D., Friedman, N., & Bach, F., Probabilistic graphical models: principles and techniques (2009), : MIT press. ↩\n\nMurphy, K. P., Machine learning: a probabilistic perspective. 2012, Cit{\\'e} en, (), 117 (2014).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/podcasts",
        "title": "Podcasts",
        "content": "\nPodcasts {#podcasts}\n\nPlanet Money\nThe Indie Hackers Podcast\nThe Knowledge Project\nOmega Tau\nNo Such Thing As A Fish\nHardcore History\n",
        "tags": []
    },
    {
        "uri": "/zettels/point_estimation_bayesian",
        "title": "Point Estimation in Bayesian Statistics",
        "content": "\ntags\n: §bayesian\\_statistics\n\nSuppose we have a posterior distribution over parameters \\\\(\\theta\\\\).\nSometimes, we would like to obtain a point estimate \\\\(\\hat{\\theta}\\\\) of\n\\\\(\\theta\\\\).\n\nTo do so, we may select a summary feature of \\\\(p(\\theta | y)\\\\) such as\nits mean, median or mode.\n\nFor symmetric posterior densities, the mean and median are equal. If\nthe posterior is also unimodal, then all 3 coincide.\n\nFor asymmetric posteriors, the median is often preferred. The mode\nonly considers the value corresponding to the maximum value of the\ndensity, while the mean may give too much weight to extreme outliers.\n",
        "tags": []
    },
    {
        "uri": "/zettels/policy_gradients",
        "title": "Policy Gradients",
        "content": "\ntags\n: Machine Learning Algorithms, Reinforcement Learning ⭐\n\nKey Idea {#key-idea}\n\nThe objective is:\n\n\\begin{equation}\n  \\theta^{\\star}=\\arg \\max \\{\\theta} E\\{\\tau \\sim p\\{\\theta}(\\tau)}\\left[\\sum\\{t} r\\left(\\mathbf{s}\\{t}, \\mathbf{a}\\{t}\\right)\\right]\n\\end{equation}\n\nTo evaluate the objective, we need to estimate this expectation, often\nthrough sampling by generating multiple samples from the distribution:\n\n\\begin{equation}\n  J(\\theta)=E\\{\\tau \\sim p\\{\\theta}(\\tau)}\\left[\\sum\\{t} r\\left(\\mathbf{s}\\{t}, \\mathbf{a}\\{t}\\right)\\right] \\approx \\frac{1}{N} \\sum\\{i} \\sum\\{t} r\\left(\\mathbf{s}\\{i, t}, \\mathbf{a}\\_{i, t}\\right)\n\\end{equation}\n\nRecall that:\n\n\\begin{equation}\n  \\nabla\\{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum\\{i=1}^{N} \\underbrace{\\nabla\\{\\theta} \\log \\pi\\{\\theta}\\left(\\tau\\{i}\\right)}\\{\\sum\\{t=1}^{T} \\nabla\\{\\theta} \\log \\{\\theta} \\pi\\{\\theta}\\left(\\mathbf{a}\\{i, t} | \\mathbf{s}\\{i, t}\\right)}r\\left(\\tau\\_{i}\\right)\n\\end{equation}\n\nThis makes the good stuff more likely, and bad stuff less likely, but\nscaled by the rewards.\n\nComparison to Maximum Likelihood {#comparison-to-maximum-likelihood}\n\npolicy gradient\n: \\\\(\\nabla\\_{\\theta} J(\\theta) \\approx \\frac{1}{N}\n      \\sum\\{i=1}^{N}\\left(\\sum\\{t=1}^{T} \\nabla\\_{\\theta} \\log\n      \\pi\\{\\theta}\\left(\\mathbf{a}\\{i, t} | \\mathbf{s}\\_{i,\n      t}\\right)\\right)\\left(\\sum\\{t=1}^{T} r\\left(\\mathbf{s}\\{i, t},\n      \\mathbf{a}\\_{i, t}\\right)\\right)\\\\)\n\nmaximum likelihood\n: \\\\(\\nabla\\{\\theta} J\\{\\mathrm{ML}}(\\theta) \\approx \\frac{1}{N} \\sum\\{i=1}^{N}\\left(\\sum\\{t=1}^{T} \\nabla\\{\\theta} \\log \\pi\\{\\theta}\\left(\\mathbf{a}\\{i, t} | \\mathbf{s}\\{i, t}\\right)\\right)\\\\)\n\nPartial Observability {#partial-observability}\n\nThe policy gradient method does not assume that the system follows the\nMarkovian Assumption! The algorithm only requires the ability to\ngenerate samples, and a function approximator for\n\\\\(\\pi\\{\\theta}(a\\t |o\\_t)\\\\).\n\nIssues {#issues}\n\nPolicy gradients have high variance: the gradient is noisy, easily\n    affected by a constant change in rewards\n\nProperties of Policy gradients {#properties-of-policy-gradients}\n\nOn-policy\n\nThe objective is an expectation under trajectories sampled under that\npolicy. This can be tweaked into an off-policy method using\nImportance Sampling.\n\n{{}}\n\n\\begin{aligned} \\nabla\\{\\theta^{\\prime}} J\\left(\\theta^{\\prime}\\right) &=E\\{\\tau \\sim \\pi\\{\\theta}(\\tau)}\\left[\\frac{\\pi\\{\\theta^{\\prime}}(\\tau)}{\\pi\\{\\theta}(\\tau)} \\nabla\\{\\theta^{\\prime}} \\log \\pi\\{\\theta^{\\prime}}(\\tau) r(\\tau)\\right] \\quad \\text { when } \\theta \\neq \\theta^{\\prime} \\\\ &=E\\{\\tau \\sim \\pi\\{\\theta}(\\tau)}\\left[\\left(\\prod\\{t=1}^{T} \\frac{\\pi\\{\\theta^{\\prime}}\\left(\\mathbf{a}\\{t} | \\mathbf{s}\\{t}\\right)}{\\pi\\{\\theta}\\left(\\mathbf{a}\\{t} | \\mathbf{s}\\{t}\\right)}\\right)\\left(\\sum\\{t=1}^{T} \\nabla\\{\\theta^{\\prime}} \\log \\pi\\{\\theta^{\\prime}}\\left(\\mathbf{a}\\{t} | \\mathbf{s}\\{t}\\right)\\right)\\left(\\sum\\{t=1}^{T} r\\left(\\mathbf{s}\\{t}, \\mathbf{a}\\{t}\\right)\\right)\\right] \\end{aligned}\n\nProblem: with large T the first term becomes extremely big or small.\n\nVariance Reduction {#variance-reduction}\n\nCausality {#causality}\n\nThe policy at time \\\\(t'\\\\) cannot affect the reward at time \\\\(t\\\\) when \\\\(t }}\n\nThis is just expected reward, but weighted by gradient magnitudes.\n\nPolicy Gradient in practice {#policy-gradient-in-practice}\n\nGradients have high variance\nConsider using much larger batches\nTweaking learning rates might be important\nAdaptive learning rates are fine, there are some policy-gradient\n    oriented learning rate adjustment methods\n\nREINFORCE {#reinforce}\n\nFor each episode,\n    generate \\\\(\\tau = s\\0, a\\0, r\\1, \\dots, s\\{t-1},\n              a\\{t-1}, r\\t\\\\) by following \\\\(\\pi\\_{\\theta}(a |s)\\\\)\n    For each step \\\\(i = 0, \\dots, t-1\\\\):\n        \\\\(R\\i = \\sum\\{k=i}^{t} \\gamma^{t-k} r\\_k\\\\) (Unbiased estimate of\n            remaining episode return under \\\\(\\pi\\_{\\theta}\\\\) starting from \\\\(i\\\\))\n        \\\\(\\hat{A\\i} = R\\i - b\\\\) (Advantage function: subtract base line \\\\(b\\\\) to lower variance)\n            Advantage function tells you how relatively good this\n                action is\n        $&theta; = \\\\(\\theta + \\alpha \\nabla\\\\theta \\log \\pi\\{\\theta}\n                     (a| s\\i) \\hat{A}\\i\\\\)\n\nObjective: \\\\(J(\\theta) = \\sum\\{\\tau} P\\{\\theta}(\\tau)R(\\tau)\\\\)\n\n\\begin{align}\n  \\nabla\\\\theta J(\\theta) &=  \\nabla\\\\theta \\sum\\{\\tau} P\\\\theta(\\tau)\n                            R(\\tau) \\\\\\\\\\\\\n                          &= \\sum\\{\\tau} \\nabla\\\\theta P\\_\\theta(\\tau)R(\\tau)\n\\end{align}\n\nActor critics use learned estimate (e.g. $\\hat{A}(s, a) = \\hat{Q}(s,\na) - \\hat{V}(s).)\n\nPolicy Gradients an Policy Iteration {#policy-gradients-an-policy-iteration}\n\nPolicy gradients involves estimating \\\\(\\hat{A}{s,a}\\\\), and using it to\nimprove the policy, much like policy iteration which evaluates\n\\\\(A(s,a)\\\\) and use it to create a better, deterministic policy.\n\n\\begin{align}\n  J(\\theta') - J(\\theta)  &= J(\\theta') - E\\{s\\0 \\sim p(s\\_1)}\\left[\n                            V^{\\pi\\\\theta}(s\\0) \\right] \\\\\\\\\\\\\n                          &=J(\\theta') - E\\{\\tau \\sim p\\{\\theta'}(\\tau)}\\left[\n                            V^{\\pi\\\\theta}(s\\0) \\right] \\\\\\\\\\\\\n                          &= J(\\theta') - E\\_{\\tau \\sim\n                            p\\_{\\theta'}(\\tau)} \\left[\n                            \\sum\\_{t=0}^{\\infty} \\gamma^t\n                            V^{\\pi\\\\theta} (s\\t) - \\sum\\_{t=1}^{\\infty} \\gamma^t\n                            V^{\\pi\\\\theta} (s\\t)\\right] \\\\\\\\\\\\\n                          &= J(\\theta') + E\\_{\\tau \\sim\n                            p\\_{\\theta'}(\\tau)} \\left[\n                            \\sum\\_{t=0}^{\\infty} \\gamma^t (\\gamma\n                            V^{\\pi\\\\theta}(s\\{t+1}) -\n                            V^{\\pi\\\\theta})(s\\t) \\right] \\\\\\\\\\\\\n                          &= E\\{\\tau \\sim \\p\\{\\theta'}(\\tau)} \\left[\n                            \\sum\\{t=0}^{\\infty} \\gamma^t r(s\\t, a\\_t)\n                            \\right] + E\\_{\\tau \\sim\n                            p\\_{\\theta'}(\\tau)} \\left[\n                            \\sum\\_{t=0}^{\\infty} \\gamma^t (\\gamma\n                            V^{\\pi\\\\theta}(s\\{t+1}) -\n                            V^{\\pi\\\\theta})(s\\t) \\right] \\\\\\\\\\\\\n                          &= E\\{\\tau \\sim p\\{\\theta'}(\\tau)} \\left[\n    \\sum\\t \\gamma^t A^{\\pi\\\\theta} (s\\t, a,\\t) \\right]\n\\end{align}\n\nWe have an expectation under \\\\(\\theta'\\\\), but samples under \\\\(\\theta\\\\). We\nuse marginals representation, and Importance Sampling to remove the\nexpectation under \\\\(\\pi\\_\\theta'\\\\), but can we ignore the other\ndistribution mismatch?\n\n{{}}\n\nWe can bound the distribution change from \\\\(p\\{\\theta}(s\\t)\\\\) to\n\\\\(p\\{\\theta'}(s\\t)\\\\). (See Trust Region Policy Optimization paper)\n\nWe can measure the distribution mismatch with KL divergence.\n\nThen, we can enforce the constraint of a small KL divergence by using\na loss function with  the Lagrange Multiplier:\n\n\\begin{equation}\n  L(\\theta', \\lambda) = \\sum\\{t} E\\{s\\t \\sim p\\\\theta(s\\_t)} \\left[\n    E\\{a\\t \\sim \\pi\\{theta}(a\\t|s\\_t)} \\left[\n      \\frac{p\\{theta'}(a\\t|s\\t)}{p\\\\theta(a\\t|s\\t)} \\gamma^t\n      A^{\\pi\\\\theta} (s\\t, a\\_t) \\right] \\right] - \\lambda \\left(\n    D\\{KL}(\\pi\\{\\theta'}(a\\t|s\\t) || \\pi\\\\theta (a\\t|s\\_t)) - \\epsilon \\right)\n\\end{equation}\n\nMaximize \\\\(L'\\\\) wrt to \\\\(\\theta'\\\\)\n\\\\(\\lambda  \\leftarrow \\lambda + \\alpha (D\\_{KL} - \\epsilon)\\\\)\n\nIntuition: raise \\\\(\\lambda\\\\) if constraint violated too much, else lower\nit.\n\nAlternatively, optimize within some region, and use a Taylor expansion\nto approximate the function within that region.\n\nNatural Gradients {#natural-gradients}\n\nResources {#resources}\n\nDeep Reinforcement Learning Through Policy Optimization - NIPS 2016 Tutorial\nCS285 Fa19 9/16/19 - YouTube\nCS285 Fa19 9/30/19 - YouTube\n",
        "tags": []
    },
    {
        "uri": "/zettels/pomdp",
        "title": "Partially Observable Markov Decision Processes (POMDPs)",
        "content": "\nPartially Observable MDPs (POMDPs) {#partially-observable-mdps--pomdps}\n\nThe assumption of full observability, accompanied with the Markov\nassumption for the transition model means that the optimal policy\ndepends only on the current state. When the environment is partially\nobservable, the agent does not know which state it is in. The agent\nthen cannot execute \\\\(\\pi(s)\\\\). The utility of a state \\\\(s\\\\) and the optimal\naction in \\\\(s\\\\) does not only depend on \\\\(s\\\\), but also how much the agent\nknows when it is in \\\\(s\\\\).\n\nIn addition to the elements of the MDP -- the transition model\n\\\\(P(s'|s, a)\\\\), actions \\\\(A(s)\\\\), and reward function \\\\(R(s)\\\\), it also has\na sensor model \\\\(P(e|s)\\\\). The sensor model specifies the probability of\nperceiving evidence \\\\(e\\\\) in state \\\\(s\\\\). For example, a sensor might\nmeasure the number of adjacent walls. A noisy sensor might return the\nwrong value with some probability.\n\nBelief states are the set of actual states the agent might be in. In\nPOMDPs, these belief states are probability distributions over all\npossible states. The agent can calculate its current belief state as\nthe conditional probability distribution over the actual states given\nthe sequences of percepts and actions so far.\n\nIf the previous belief state is \\\\(b(s)\\\\), and the agent performs some\naction \\\\(a\\\\) and perceives evidence \\\\(e\\\\), then the new belief state is\ngiven by:\n\n\\begin{equation}\nb'(s') = \\alpha P(e | s') \\sum\\_s P(s' | s, a) b(s)\n\\end{equation}\n\nwhere \\\\(\\alpha\\\\) is a normalizing constant that makes the belief state sum\nto 1.\n\n**The optimal action in a POMDP depends only on the agent's belief\nstate*. The optimal policy can be described by a mapping \\\\(\\pi^\\ (b)\\\\) from\nbelief states to actions.\n\nThe decision cycle of a POMDP can be broken down into 3 steps:\n\nGiven the current belief state \\\\(b\\\\), execute the action \\\\(a = \\pi^\\* (b)\\\\).\nReceive percept \\\\(e\\\\).\nUpdate the belief state to \\\\(b'\\\\) and repeat.\n\nIf we knew the action and the subsequent percept, then the update to\nthe belief state would be a deterministic one, following the update\nequation. The subsequent percept is not yet known, so the agent will\narrive in one of several possible belief states. The probability of\nperceiving \\\\(e\\\\), given that \\\\(a\\\\) is was the action taken from belief\nstate \\\\(b\\\\), is given by:\n\n\\begin{align}\n  P(e | a,b) &= \\sum\\_{s'} P(e | a, s', b) P(s' | a, b) \\\\\\\\\\\\\n             &= \\sum\\_{s'} P(e|s')P(s'|a, b) \\\\\\\\\\\\\n             &= \\sum\\{s'} P(e | s')\\sum\\{s'}P(s'|s, a)b(s)\n\\end{align}\n\n\\begin{align}\n  P(b' | a, b) &= \\sum\\_eP(b' | e, a, b) P(e | a, b) \\\\\\\\\\\\\n               &= \\sum\\eP(b' | e, a, b)\\sum\\{s'} P(e|s')\\sum\\_{s'} P(s'\n                 | s, a)b(s)\n\\end{align}\n\nWhere \\\\(P(b' | e, a, b) = 1\\\\) if \\\\(b' = FORWARD(b,a,e)\\\\) and \\\\(0\\\\)\notherwise.\n\nBecause POMDPs have continuous state space, new algorithms for\ncomputing or approximating the optimal policies for MDPs do not apply here.\n\nValue iteration for POMDPs {#value-iteration-for-pomdps}\n\nThe value iteration algorithm for the MDP computed one utility value\nfor each state. With infinitely many belief states, we need to be more\ncreative.\n\nConsider conditional plans, and how the expected utility of executing\na fixed conditional plan varies with the initial belief state.\n\nLet the utility of executing a fixed conditional plan \\\\(p\\\\) starting\n    in physical state \\\\(s\\\\) be \\\\(\\alpha\\_p(s)\\\\). Then the expected utility of\n    executing \\\\(p\\\\) in belief state \\\\(b\\\\) is \\\\(\\sum\\s b(s) \\alpha\\p (s)\\\\) Hence the\n    expected utility of a fixed conditional plan varies linearly with\n    \\\\(b\\\\).\nAt any given belief state \\\\(b\\\\), the optimal policy will choose to\n    execute the conditional plan with the highest expected utility, and\n    the expected utility is just the utility of that conditional plan:\n\n\\begin{equation}\n  U(b) = U^{\\pi^\\*}(b) = max\\{p} b \\cdot \\alpha\\p\n\\end{equation}\n\nIf the optimal policy \\\\(\\pi^\\*\\\\) chooses to execute \\\\(p\\\\) starting at \\\\(b\\\\),\nthen it is reasonable to expect that it might choose to execute \\\\(p\\\\) in\nbelief states that are close to \\\\(b\\\\).\n\nFrom these 2 observations, we see that the utility function \\\\(U(b)\\\\) on\nbelief states, being the maximum of a collection of hyperplanes, will\nbe piecewise linear and convex.\n\nLet \\\\(p\\\\) be a depth-d conditional plan whose initial action is \\\\(a\\\\) and\nwhose depth-d-1 subplan for percept \\\\(e\\\\) is \\\\(p.e\\\\), then\n\n\\begin{equation}\n  \\alpha\\p{s} = R(s) + \\gamma \\left( \\sum\\{s'} P(s' | s,a)\\sum\\e P(e|s')\\alpha\\{p.e}(s') \\right)\n\\end{equation}\n\nThis recursion gives rise to a value iteration algorithm:\n\nfunction POMDP-VALUE-ITERATION returns a utility function\n  inputs: pomdp\n          e, the maximum error allowed for utility\n  locals: U, U' sets of plans p\n\n  U' <- set containing the empty plan ], with \\alpha_[ = R(s)\n  repeat\n    U <- U'\n    U' <- set of all plans computed with above equation\n    U' <- REMOVE-DOMINATED-PLANS(U')\n  until MAX-DIFFERENCE(U, U') < e(1-\\gamma) / \\gamma\n  return U\n\nThe algorithm's complexity is dominated by the number of plans\ngenerated: given \\\\(|A|\\\\) possible actions and \\\\(|E|\\\\) possible\nobservations, there are \\\\(|A|^{O(|E|^{d-1})}\\\\) distinct depth-d plans,\nmaking the algorithm hopelessly inefficient for larger problems.\n",
        "tags": []
    },
    {
        "uri": "/zettels/portfolio_composition",
        "title": "Portfolio Composition",
        "content": "\nGeneral recommendations {#general-recommendations}\n\nInvest in ETFs, not single stocks  (§investing\\in\\etfs)\nInvest \"110 minus your age\" into stocks, and the rest into bonds\nFor the stock component, half should be in local-stock ETF and half\n    in global stock ETF\n    Global stock ETF is inherently more volatile, affected by\n        currency\nThe bond component should be entirely local\n\nGood ETFs {#good-etfs}\n\nSPDR Straits Times Index ETF: ES3\n    big, liquid, 0.3% expense ratio\nMBH, Nikko Asset Management SGD Investment-Grade Corporate Bond ETF\nIWDA, the iShares Core MSCI World ETF\n    invests in every developed market in the world, 0.2% expense ratio\n",
        "tags": []
    },
    {
        "uri": "/zettels/presentations",
        "title": "Presentations",
        "content": "\ntags\n: §writing\n\nCreating Good Presentations {#creating-good-presentations}\n\nAdjust to the frequency of your audience so that the message\n    resonates deeply. The audience will then display self-organizing\n    behaviour.\n\nHas a clear beginning, middle and end\n\nHas identifiable  inherent structure\n\nThe first plot point is an incident that captures the audience's\n    intrigue and interest\n\nThe audience is the hero in your story. The aim is to get the\n    audience to overcome the reluctance to change, and enter the world\n    you are presenting.\n\n{{}}\n\nFamiliar content is comforting, oppositional content is stimulating,\ncombined these produce forward movement.\n",
        "tags": []
    },
    {
        "uri": "/zettels/probability_theory",
        "title": "Probability Theory",
        "content": "\ntags\n: §statistics\n\nSetup {#setup}\n\nSuppose that we are about to perform an experiment whose outcome is\nnot predictable in advance. The set of all possible outcomes of an\nexperiment is known as the sample space \\\\(S\\\\).\n\nFor example, if the experiment consists of flipping a coin, then:\n\n\\begin{equation}\n  S = (H, T)\n\\end{equation}\n\nAny subset \\\\(E\\\\) of the sample space \\\\(S\\\\) is known as an event. Some\nexamples of event include: \\\\(E = (H)\\\\) where \\\\(E\\\\) is the event that a\nhead appears on the flip of the coin.\n\nWe can define unions and intersections between 2 or more events. The\nunion of two events \\\\(E \\cup F\\\\) is a new event to consist of all\noutcomes that are either in \\\\(E\\\\) or \\\\(F\\\\).\n\nProbabilities Defined on Events {#probabilities-defined-on-events}\n\nConsider an experiment whose sample space is \\\\(S\\\\). For each event \\\\(E\\\\)\nof the sample space \\\\(S\\\\), we assume that a number \\\\(P(E)\\\\) is defined and\nsatisfies 3 conditions:\n\n\\\\(0 \\le P(E) \\le 1\\\\)\n\\\\(P(S) = 1\\\\)\nFor any sequence of events \\\\(E\\1, E\\2, \\dots\\\\) that are mutually\n    exclusive:\n\n\\begin{equation}\n  P( \\mathop{\\cup}\\{n=1}^{\\infty} E\\n) = \\sum\\{n=1}^{\\infty} P(E\\n)\n\\end{equation}\n\nConditional Probabilities {#conditional-probabilities}\n\nConditional probabilities are a powerful and useful concept. First, we\nare often interested in calculating probabilities and expectations\nwhen some partial information is available. Second, in calculating a\ndesired probability or expectation, it is often extremely useful to\nfirst \"condition\" on some appropriate random variable.\n\nWe denote \\\\(P(E|F)\\\\) the conditional probability that \\\\(E\\\\) occurs given\nthat \\\\(F\\\\) has occurred. This is valid for all events \\\\(E\\\\) and \\\\(F\\\\) that\nsatisfy the 3 conditions above.\n\nRecall that for any 2 events \\\\(E\\\\) and \\\\(F\\\\), the conditional probability\nof \\\\(E\\\\) given \\\\(F\\\\) is defined, as long as \\\\(P(F) > 0\\\\), by:\n\n\\begin{equation}\n  P(E|F) = \\frac{P(EF)}{P(F)}\n\\end{equation}\n\nIf \\\\(X\\\\) and \\\\(Y\\\\) are discrete random variables, it is natural to define\nthe conditional probability mass function of \\\\(X\\\\) given that \\\\(Y = y\\\\),\nby:\n\n\\begin{align}\n  p\\_{X|Y}(x|y) &= P\\left\\\\{ X=x | Y=y \\right\\\\} \\\\\\\\\\\\\n               &= \\frac{P(X = x, Y = y)}{P( Y = y)} \\\\\\\\\\\\\n               &= \\frac{p(x,y)}{p\\_Y(y)}\n\\end{align}\n\nfor all values of \\\\(y\\\\) such that \\\\(P(Y = y) > 0\\\\). Similarly, we can\ndefine \\\\(F\\{X|Y}(x|y) = \\sum\\{a\\le x}p\\_{X|Y}(x|y)\\\\).\n\nFinally, the conditional expectation of \\\\(X\\\\) given that \\\\(Y = y\\\\) is\ndefined by:\n\n\\begin{align}\n  E[X|Y = y] &= \\sum\\_{x} P\\left\\\\{ X = x | Y = y \\right\\\\} \\\\\\\\\\\\\n             &= \\sum\\{x} x p\\{X|Y} (x|y)\n\\end{align}\n\nComputing Expectations by Conditioning {#computing-expectations-by-conditioning}\n\nLet us denote by \\\\(E[X|Y]\\\\) that function of the random variable \\\\(Y\\\\)\nwhose value at \\\\(Y=y\\\\) is \\\\(E[X|Y=y]\\\\). An extremely important property of\nconditional expectation is that for all random variables \\\\(X\\\\) and \\\\(Y\\\\):\n\n\\begin{equation}\n  E[X] = E\\left[ E[X|Y] \\right]\n\\end{equation}\n\nIndependent Events {#independent-events}\n\nTwo events \\\\(E\\\\) and \\\\(F\\\\) are independent if:\n\n\\begin{equation}\n  P(EF) = P(E) P(F)\n\\end{equation}\n\nThis also implies that \\\\(P(E|F) = P(E)\\\\).\n\nBayes' Formula {#bayes-formula}\n\nLet \\\\(E\\\\) and \\\\(F\\\\) be events. We may express \\\\(E\\\\) as \\\\(E = EF \\cup EF^c\\\\).\nSince \\\\(EF\\\\) and \\\\(EF^c\\\\) are mutually exclusive, we have:\n\n\\begin{align}\n  P(E) &= P(EF) + P(EF^c) \\\\\\\\\\\\\n       &= P(E|F)P(F) + P(E|F^c)P(F^c) \\\\\\\\\\\\\n  &= P(E|F)P(F) + P(E|F^c)\\left( 1 - P(F) \\right)\n\\end{align}\n",
        "tags": []
    },
    {
        "uri": "/zettels/productivity",
        "title": "Productivity",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/prog_lang",
        "title": "Programming Languages",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/programming_methodology",
        "title": "Programming Methodology",
        "content": "\nSource Specification {#source-specification}\n\nPredefined in global env: undefined, NaN,  Infinity\n\n| fn                 | description                                                                                                |\n|--------------------|------------------------------------------------------------------------------------------------------------|\n| Math.*           | Functions dealing with Math. (eg. Math.pow, Math.sqrt, Math.E, Math.PI)                            |\n| parseInt(string) | Takes a string and parses it into an integer.                                                              |\n| equal(x,y)       | Returns true if x and y have the same structure, and the nodes are all identical. false otherwise. |\n| is_number(x)     | Returns true if x is a number, false otherwise.                                                      |\n\nList {#list}\n\n| fn                            | description                                                                      | I/R | Time | Space |\n|-------------------------------|----------------------------------------------------------------------------------|-----|------|-------|\n| is_pair(x)                  | Returns true if x is a pair, false otherwise.                              |     |      |       |\n| is_list(x)                  | Returns true if x is a pair, false otherwise.                              |     |      |       |\n| isemptylist(x)            | Returns true if x is an empty list, false otherwise.                         |     |      |       |\n| pair(x,y)                   | Returns a pair of x and y.                                                   |     |      |       |\n| head(p)                     | Returns the head of a pair.                                                      |     |      |       |\n| tail(p)                     | Returns the tail of a pair.                                                      |     |      |       |\n| set_head(p, n)              | Sets the head of pair p to n.                                                |     |      |       |\n| set_tail(p, n)              | Sets the tail of pair p to n.                                                |     |      |       |\n| list(x1,x2,...,xn)          | Returns a list of n elements.                                                  |     | O(n) | O(n)  |\n| length(xs)                  | Returns the length of the list xs.                                             | I   | O(n) | O(1)  |\n| map(f,xs)                   | Returns a list with each element having f applied to it.                       | R   | O(n) | O(n)  |\n| build_list(n,f)             | Make a list of elements with unary function f applied to numbers 0 to n-1. | R   | O(n) | O(n)  |\n| for_each(f,xs)              | map, but use only for side effects. Returns true.                              | I   | O(n) | O(1)  |\n| listtostring(xs)          | Returns string representation of list xs.                                      |     |      |       |\n| reverse(xs)                 | Returns list xs in reverse order.                                              | I   | O(n) | O(n)  |\n| append(xs,ys)               | Returns a list with list ys appended to list xs.                             | R   | O(n) | O(n)  |\n| member(x, xs)               | Returns first postfix sublist whose head is identical to x.                    | I   | O(n) | O(1)  |\n| remove(x, xs)               | Returns a list by removing the first item identical to x.                      | R   | O(n) | O(n)  |\n| remove_all(x,xs)            | Returns a list by removing all elements identical to x.                        | R   | O(n) | O(n)  |\n| filter(pred,xs)             | Returns a list containing only elements in xs for which pred returns true. | R   | O(n) | O(n)  |\n| enumlist(start,end)        | enumlist(0, 4) becomes [0, [1, [2, [3, [4, []]]]]].                         | R   | O(n) | O(n)  |\n| list_ref(xs, n)             | Returns the element in xs at position n.                                     | I   | O(n) | O(1)  |\n| accumulate(op, initial, xs) | op(x1, op(x2, op(x3, ... op(xn, initial)))).                                   | R   | O(n) | O(n)  |\n\nStream {#stream}\n\n| fn                       | description                                                                                     | Lazy? |\n|--------------------------|-------------------------------------------------------------------------------------------------|-------|\n| stream_tail(s)         | returns result of applying nullary function at tail.                                            | Y     |\n| is_stream(s)           | returns true if s is a stream, false otherwise.                                           | N     |\n| stream(x1,x2,...,xn)   | Returns a stream with n elements.                                                             | N     |\n| listtostream(xs)     | Transforms a list into a stream.                                                                | Y     |\n| streamtolist(s)      | Transform a stream into a list.                                                                 | N     |\n| stream_length(s)       | Returns the length of the stream s.                                                           | N     |\n| stream_map(f,s)        | Returns a stream from stream s by element-wise application of f.                            | Y     |\n| build_stream(n,f)      | Makes a stream of n elements, by applying the unary function f to numbers 0 to n-1.     | Y     |\n| streamforeach(f,s)   | Applies f to every element of the stream s, and returns true.                               | N     |\n| stream_reverse(s)      | Returns a finite stream s in reverse order.                                                   | N     |\n| stream_append(xs,ys)   | Returns a stream that results from appending ys to xs.                                      | Y     |\n| stream_member(x,s)     | Returns first postfix substream whose head is identical to x.                                 | P     |\n| stream_remove(x,s)     | Returns a stream that results from removing the first element identical to x from stream s. | Y     |\n| streamremoveall(x,s) | Returns a stream that results from removing all elements identical to x from stream s.      | Y     |\n| stream_filter(pred,s)  | Returns a stream that contains only elements which return true on unary predicate pred.     | Y     |\n| enumstream(start,end) | Similar to enumlist.                                                                         | Y     |\n| integers_from(n)       | Constructs an infinite stream of integers starting at n.                                      | Y     |\n| eval_stream(s,n)       | Constructs a list of the first n elements of s.                                               | P     |\n| stream_ref(s,n)        | Returns the element of stream s at position n.                                              | P     |\n\nMetacircular {#metacircular}\n\n| fn                                              | description                                                                                                            |\n|-------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|\n| evaluate(stmt,env)                            | Classifies stmt and directs the evaluation. Handles primitive forms, special forms and combinations.                 |\n| apply(fun, args)                              | Primitive functions: calls applyprimitivefunction. Compound functions: sequentially eval exps in new env created.  |\n| lookupvariablevalue(var,env)                | returns value bound to the symbol var, or signals an error if unbound.                                               |\n| define_variable(var,value,env)                | adds to the first frame of env a binding of var to value.                                                        |\n| extendenvironment(variables,values,baseenv) | returns a new environment, with a new frame extended from base_env, with the corresponding variables and values. |\n| setvariablevalue(var,value,env)             | changes the binding of var in env to value, signals error if unbound.                                            |\n\nMutations {#mutations}\n\nfunction mutable_reverse(xs) {\n  function helper(prev,xs) {\n    return prev;\n  } else {\n    var rest = tail(xs);\n    set_tail(xs, prev);\n    return helper(xs,rest);\n  }\n\n  return helper([],xs);\n}\n\nfunction mutable_reverse(xs) {\n  if (isemptylist(xs) ||\n      isemptylist(tail(xs))) {\n    return xs;\n  } else {\n    var temp = mutable_reverse(tail(xs));\n    set_tail(tail(xs), xs);\n    set_tail(xs,[]);\n    return temp;\n  }\n}\n\nfunction makecircularcopy(xs) {\n  function helper(rem,front_ptr) {\n    if (isemptylist(rem)) {\n      return front_ptr;\n    } else {\n      return pair(head(rem),\n                  helper(tail(rem),\n                         front_ptr));\n    }\n  }\n\n  if (isemptylist(xs)) {\n    return [];\n  } else {\n    var ys = pair(head(xs), []);\n    set_tail(y, helper(tail(xs),ys));\n    return ys;\n  }\n}\n\nfunction mergeB(xs,ys) {\n  if (isemptylist(xs) && isemptylist(ys)) {\n    return [];\n  } else if (isemptylist(xs) ||\n             head(xs) = head(ys)) {\n    set_tail(xs, mergeB(tail(xs), ys));\n    return xs;\n  }\n}\n\nPermutations and Combinations {#permutations-and-combinations}\n\npermutations {#permutations}\n\nfunction permutations(xs) {\n  if (isemptylist(xs)) {\n    return list([]);\n  } else {\n    return accumulate(function(e, acc) {\n      return append(map(function(x) {\n        return pair(e, x);\n      }, permutations(remove(e, xs))), acc);\n    }, [], xs);\n  }\n}\n\nn_permutations {#n-permutations}\n\nfunction n_permutations(xs, n) {\n  if(n === 0) {\n    return list([]);\n  } else {\n    return accumulate(function(e, acc) {\n      return append(\n        map(function(x) {\n          return pair(e, x);\n        }, n_permutations(remove(e, xs),\n                          n - 1)),\n        acc);\n    }, [], xs);\n  }\n}\n\nn_combinations {#n-combinations}\n\nfunction n_combinations(xs, n) {\n  if (n === 0) {\n    return list([]);\n  } else if (isemptylist(xs)) {\n    return [];\n  } else {\n    return append(\n      map(function(e) {\n        return pair(head(xs), e);\n      }, n_combinations(tail(xs), n-1)),\n      n_combinations(tail(xs), n));\n  }\n}\n\nOOP {#oop}\n\nfunction Vector2D (x,y) {\n  this.x = x;\n  this.y = y;\n}\n\nVector2D.prototype.length = function() {\n  return Math.sqrt(this.x * this.x +\n                   this.y * this.y);\n}\n\nfunction Thrust (x,y, tag) {\n  Vector2D.call(this,x,y);\n  this.tag = tag;\n}\n\nThrust.Inherits(Vector2D);\n\nStreams {#streams}\n\nRecursively defined streams {#recursively-defined-streams}\n\nfunction fibgen(a,b) {\n  return pair(a,b function() {\n    return fibgen(b, a+b);\n  });\n}\n\nvar ones = pair(1, function() {\n  return ones;\n});\n\nvar integers = pair(1, function() {\n  return add_streams(integers, ones);\n});\n\n// Visualization:\nones:     1 1 1 1 1 1\nintegers:   1 2 3 4 5\n\nintegers: 1 2 3 4 5 6\n\nStream of primes {#stream-of-primes}\n\nfunction sieve(s) {\n  return pair(head(s), function() {\n    return sieve(stream_filter(function() {\n      return !is_divisible(x,head(s));\n    }, stream_tail(s)));\n  });\n}\n\nvar primes = sieve(integers_from(2));\n\nIterations with streams {#iterations-with-streams}\n\nfunction improve_guess(guess,x) {\n  return average(guess, x/guess);\n}\n\nfunction sqrt_iter(guess,x) {\n  if (good_enough(guess,x)){\n    return guess;\n  } else {\n    return sqrt_iter(improve(guess,x),x);\n  }\n}\n\nfunction sqrt(x) {\n  return sqrt_iter(1.0, x);\n}\n\nfunction sqrt_stream(x) {\n  var guesses = pair(1, function() {\n    return stream_map(function(guess) {\n      return improve(guess,x);\n    }, guesses);\n  });\n\n  return guesses;\n}\n\nInterleave {#interleave}\n\nfunction interleave(s1,s2) {\n  return pair(head(s1), function() {\n    return pair(head(s2), function() {\n      return interleave(stream_tail(s1),\n                        stream_tail(s2));\n    });\n  });\n}\n\nCartesian Product {#cartesian-product}\n\nfunction pairs(s1,s2){\n  if (isemptylist(s1) || isemptylist(s2)) {\n    return [];\n  } else {\n    return pair(\n      pair(head(s1), head(s2)),\n      function() {\n        return interleave(\n          stream_map(function(x) {\n            return pair(head(s1),x);\n          }, stream_tail(s2)),\n          pairs(stream_tail(s1), s2));\n      });\n  }\n}\n\nMisc {#misc}\n\nTowers of Hanoi {#towers-of-hanoi}\n\nfunction hanoi(disks, source,dest,aux) {\n  if (disks === 0) {\n    return [];\n  } else {\n    hanoi(disks-1,source,aux,dest);\n    display(\"Move disk from \" +\n            source + \" to \" + dest);\n    hanoi(disks-1,aux,dest,source);\n  }\n}\n\nCount Change {#count-change}\n\n// denoms is a list of coin denominations:\n// eg. list(50,20,10,5)\nfunction count_change(amt, denoms) {\n  if (isemptylist(denoms) || amt = amount) {\n      balance = balance - amount;\n      return balance;\n    } else {\n      return \"Insufficient Funds\";\n    }\n  };\n}\n// Pic 1\nvar w = make_withdraw(100);     // Pic 2\nw(50);                          // Pic 3\n// Pic 4\n\nMetacircular Interpreter {#metacircular-interpreter}\n\nReverse Application Order {#reverse-application-order}\n\nfunction listofvalues(exps.env) {\n  if (no_operands(exps)) {\n    return [];\n  } else {\n    var r = listofvalues(rest_operands(exps),\n                           env);\n    return pair(evaluate(first_operand(exps),\n                         env),\n                r);\n  }\n}\n\nThunking {#thunking}\n\nfunction listofvalues(exps,env) {\n  if (no_operands(exps)) {\n    return [];\n  } else {\n    return pair(\n      makethunk(firstoperand(exps), env),\n      listofvalues(rest_operands(exps), env)\n    );\n  }\n}\n\nfunction make_thunk(expr,env) {\n  return {\n    tag: \"thunk\",\n    expression: expr,\n    environment: env\n  };\n}\n\nfunction force(v) {\n  if (is_thunk(v)) {\n    return force(evaluate(thunk_expression(v),\n                          thunk_environment(v)));\n  } else {\n    return v;\n  }\n}\n\nfunction lookupvariablevalue(variable,env) {\n  function env_loop(env){\n    if (isemptyenvironment(env)) {\n      error(\"Unbound Variable\");\n    } else if (hasbindingin_frame(\n      variable,\n      first_frame(env))) {\n      var val = force(first_frame(env)[variable]);\n      first_frame(env)[variable] = val;\n      return val;\n    } else {\n      return envloop(enclosingenvironment(env));\n    }\n  }\n\n  var val = env_loop(env);\n  return val;\n}\n",
        "tags": []
    },
    {
        "uri": "/zettels/progressive_summarization",
        "title": "Progressive Summarization",
        "content": "\ntags\n: §productivity\n\nThe idea of progressive summarization is to create notes such that\ntheir purpose is comprehensible in the future.\n\nLayer 1\n: When you encounter something interesting, capture it\n\nLayer 2\n: Bold the most interesting parts\n\nLayer 3\n: Highlight the most interesting bolded sections\n\nLayer 4\n: Summarize the bolded portions and the note in your own words\n\nLayer 5\n: Turn your notes into something new: a tweet, a blog post, even a book\n\nAll of these steps are optional. I think of these as distillation,\nextracting key points from each note.\n\nResources {#resources}\n\nImplementing A Second Brain in Emacs and Org-Mode – Tasshin\n\nRelated {#related}\n\n§para\\_method\n",
        "tags": []
    },
    {
        "uri": "/zettels/python",
        "title": "Python",
        "content": "\nPython is an interpreted, general-purpose, high-level [programming\nlanguage]({{}}). Some properties of Python include:\n\nwhitespaces being significant\ndynamically-typed\ngarbage-collected\n\nPython Books {#python-books}\n\nTODO Fluent Python {#fluent-python}\n\nTODO Effective Python {#effective-python}\n\nTODO High Performance Python {#high-performance-python}\n\nSetting up Python Environment {#setting-up-python-environment}\n\nIf not using Nix/NixOS, versioning Python is (in my opinion best)\nhandled with pyenv. Some further packages may need to be installed to\nbuild a full Python distribution:\n\nsudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev libbz2-dev\n\nPackage management is traditionally accomplished via pip, but poetry\nis getting more popular.\n\nPython Default Values {#python-default-values}\n\nPython executes some very interesting behaviour with regards to\ndefault values:\n\ndef a(v=[]):\n    v.append(1)\n    print(v)\n\na()\na()\na()\na()\n\nWhy does this happen? Default parameter values are always evaluated\nwhen, and only when, the “def” statement they belong to is executed.\nAlso note that “def” is an executable statement in Python, and that\ndefault arguments are evaluated in the “def” statement’s environment.\nIf you execute “def” multiple times, it’ll create a new function\nobject (with freshly calculated default values) each time.\n\nPython Data Structures {#python-data-structures}\n\nHeap {#heap}\n\nHeaps are provided by the heapq library in python.\n\nimport heapq\n\na = [1,2,3,4,5,6]\nHeapq defaults to min heap\nheapq.heapify(a) # makes a into a min heap object\nheapq.heappop(a)\n\nheapq.heapifymax(a) # Makes a into max heap\nheapq.heappopmax(a)\n\nPriority Queue {#priority-queue}\n\nReference\n\nThe queue module provides FIFO, LIFO and Priority Queues. Among them,\nqueue.PriorityQueue is perhaps the most commonly used in interview\nquestions.\n\nclass queue.PriorityQueue(maxsize=0)\n\nimport queue\npq = queue.PriorityQueue()\n\npq.put((10, 'ten'))\npq.put((1, 'one'))\npq.put((5, 'five'))\n\nx = []\nwhile not pq.empty():\n    print(pq.get())\n\nStrings {#strings}\n\nSome important string operators and functions are:\n\ns[3]\nlen(s)\ns + t\ns[2:4]\ns in t\ns.strip()\ns.startswith(prefix)\ns.endswith(prefix)\ns.split(',')\n3 \\* '01'\n','.join([\"A\",\"B\",\"C\"])\ns.tolower()\n\nStrings are immutable, so operations create new array of characters\nthat are then assigned back to s.\n\nTricks {#tricks}\n\nTilde Operator {#tilde-operator}\n\nThe tilde operator ~ is the bit-wise complement operator, and it\nessentially calculates the -x-1. This comes in useful in arrays, where\n\\\\(s[~x] = s[-x-1]\\\\). An example usage would be the \\\\(O(n)\\\\) time\npalindrome function:\n\ndef is_palindrome(s):\n    return all(s[i] == s[~i] for i in range(len(s) // 2))\n\nFor loop else clause {#for-loop-else-clause}\n\nfor loop has else clause, that runs when the for loop successfully\ncompletes without break.\n\nString translation tables {#string-translation-tables}\n\nstr.maketrans creates a translation table that makes string\nsubstitution quick.\n\nChecking if character unique in string {#checking-if-character-unique-in-string}\n\ns.index(x) == s.rindex(x) to check if character is only one in the string\n\nReference Python Applications {#reference-python-applications}\n\nmahmoud/awesome-python-applications\n\nStyle Guides {#style-guides}\n\nstyleguide | Style guides for Google-originated open-source projects\npsf/black\n\nOptimizing Python {#optimizing-python}\n\nGregory Szorc's Digital Home | What I've Learned About Optimizing Python\n\nProfiling Python {#profiling-python}\n\nbenfred/py-spy: Sampling profiler for Python programs\n",
        "tags": [
            "python",
            "proglang"
        ]
    },
    {
        "uri": "/zettels/q_learning",
        "title": "Q-Learning",
        "content": "\ntags\n: Machine Learning Algorithms, Temporal Difference Learning, Reinforcement Learning ⭐\n\nThe Actor-Critic algorithm fits 2 function approximators: one for the\npolicy, and one for the value function. A key problem with the policy\ngradient method is the high variance in the gradient update. _Can we\nomit Policy Gradients completely?_\n\nKey Ideas {#key-ideas}\n\nIf we have a policy \\\\(\\pi\\\\), and we know \\\\(Q^{\\pi}(s\\t, a\\t)\\\\), then we\n    can improve \\\\(\\pi\\\\) by setting \\\\(\\pi'(a|s) = 1\\\\) where \\\\(a =\n       \\mathrm{argmax}\\_aQ^{\\pi}(s , a)\\\\).\nWe can compute the gradient to increase the probability of good\n    actions \\\\(a\\\\): if \\\\(Q^{\\pi}(s, a) > V^{\\pi}(s)\\\\), then a is better than\n    average (\\\\(V\\\\) is expectation of \\\\(Q\\\\) over \\\\(\\pi(a|s)\\\\)).\nIt can be shown that in a fully-observed MDP, there is a policy\n    that is both deterministic and optimal\n\nPolicy Iteration {#policy-iteration}\n\nevaluate \\\\(A^\\pi (s, a)\\\\)\nset \\\\(\\pi \\leftarrow \\pi'\\\\) where \\\\(\\pi' = 1\\\\) if \\\\(a\\_t =\n       \\mathrm{argmax}\\{a\\t}A^\\pi(s\\t, a\\t)\\\\) and \\\\(0\\\\) otherwise\n\nValue Iteration {#value-iteration}\n\nSkips the explicit policy representation altogether. Key idea:\nargmax of advantage function is the same as argmax of Q-function.\n\nset \\\\(Q(s,a) \\leftarrow r(s,a) + \\gamma E\\left[V(s')\\right]\\\\)\nset \\\\(V(s) \\leftarrow \\mathrm{argmax}\\_a Q(s,a)\\\\)\n\nThis simplifies the dynamic programming problem in policy iteration.\n\nFitted Value Iteration {#fitted-value-iteration}\n\nWe can use any most function approximators to represent \\\\(V(s)\\\\).\n\nset \\\\(y\\i \\leftarrow \\mathrm{max}\\{a\\i} (r(s\\i, a\\i) + \\gamma E\\left[V\\{\\phi}(s\\_i')\\right])\\\\)\nset \\\\(\\phi \\leftarrow \\mathrm{argmin}\\_{\\phi} \\frac{1}{2}\n       \\sum\\i |V\\{\\phi}(s\\i) - y\\i |^2\\\\)\n\nThis algorithm suffers from the curse of dimensionality. When the\nstate space is large, the algorithm is computationally expensive. To\nget around this, we may sample states instead, with little change to\nthe algorithm.\n\nFitted Q Iteration {#fitted-q-iteration}\n\nNotice the \\\\(\\mathrm{max}\\\\) over \\\\(a\\_i\\\\). This means that we need to know\nthe outcomes for different actions! But what if we don't know the\ntransition dynamics? If we use a Q-table, then we arrive at an\nalgorithm without needing to know the transition dynamics!\n\nset \\\\(y\\i \\leftarrow r(s\\i, a\\i) + \\gamma E\\left[V\\{\\phi}(s\\_i')\\right]\\\\)\nset \\\\(\\phi \\leftarrow \\mathrm{argmin}\\_{\\phi} \\frac{1}{2}\n       \\sum\\i |Q\\{\\phi}(s\\i, a\\i) - y\\_i |^2\\\\)\n\nThere is still a \"max\" hiding in \\\\(V\\\\phi(s\\i')\\\\). To get around this,\nwe approximate \\\\(E\\left[V(s\\i')\\right] \\approx \\mathrm{max}\\{a'}\nQ(s\\i', a\\i')\\\\):\n\nCollect dataset \\\\(\\left\\\\{(s\\i, a\\i, s\\i', r\\i)\\right\\\\}\\\\) using some policy\n    1.1. set \\\\(y\\i \\leftarrow r(s\\i, a\\i) + \\gamma \\mathrm{max}\\{a'} Q(s\\i', a\\i')\\\\)\n    1.2. set \\\\(\\phi \\leftarrow \\mathrm{argmin}\\_{\\phi} \\frac{1}{2}\n          \\sum\\i |Q\\{\\phi}(s\\i, a\\i) - y\\_i |^2\\\\)\n    1.3. goto 1.1 or 1\n\nThis works, even for off-policy samples (unlike §actor\\_critic). In\naddition, there is only one network, hence no high-variance policy\ngradient methods. However, _there are no convergence guarantees with\nnon-linear function approximators_!\n\nWhy is it off-policy? The algorithm doesn't assume anything about the\npolicy: given \\\\(s\\\\) and \\\\(a\\\\), the transition is independent of \\\\(\\pi\\\\).\n\nIn step 1.3, if the algorithm is off-policy, why would we ever need to\ngo back to collect more samples in step 1? This is because on a\nrandom, poorly performing policy, we might not access some interesting\nstates, that we would get after learning a better policy through\nQ-iteration.\n\nExploration in Q-learning {#exploration-in-q-learning}\n\nThe policy used in Q-learning is deterministic. To get around this, we\nuse a different, stochastic policy in step 1, when sampling actions to\ntake. An example of such a policy is the epsilon-greedy, or Boltzmann\nexploration policy.\n\nNon-Tabular Value Function Learning {#non-tabular-value-function-learning}\n\nIn the tabular case, we have a Bellman contraction \\\\(BV\\\\) such that \\\\(B\\\\)\nis a contraction w.r.t. tho infinity-norm:\n\n\\begin{equation}\n |BV - B\\overline{V}| \\le \\gamma |V - \\overline{V}| \\_{\\infty}\n\\end{equation}\n\n When we do fitted value iteration, we have another contraction \\\\(\\Pi\\\\)\nthat is a contraction wr.t. the \\\\(l\\_2\\\\) norm (if we do the l2 norm\nregression):\n\n\\begin{equation}\n |\\Pi V - \\Pi\\overline{V}| \\le |V - \\overline{V}| \\_{\\infty}\n\\end{equation}\n\nHowever, \\\\(\\Pi B\\\\) is not a contraction of any kind! _All convergence\nguarantees is lost!_\n\nQ-learning is not gradient descent! {#q-learning-is-not-gradient-descent}\n\nThere are several problems with the regular Q-iteration algorithm:\n\nSamples are temporally correlated\nTarget values are always changing\nThere is a no gradient through the target value, even though it\n    seems we are doing a single gradient update step.\nSingle-sample updates\n\nWith 1 and 2, it's possible to repeatedly overfit to the current sample.\n\nDealing with correlated samples {#dealing-with-correlated-samples}\n\nWe can follow the same technique from actor-critic\n(synchronous/asynchronous parallel Q-learning) to alleviate correlated\nsamples. The samples are however still temporally correlated. A better\nsolution is to use a replay buffer.\n\nReplay buffer {#replay-buffer}\n\nWe have a buffer \\\\(B\\\\) that stores samples of \\\\((s\\i, a\\i, s\\i', r\\i)\\\\)\nEach time we do an update, we sample a batch i.i.d from \\\\(B\\\\), resulting in a\nlower-variance gradient. The i.i.d results in decorrelated samples. In\npractice, we periodically update the replay buffer.\n\nDealing with the moving target {#dealing-with-the-moving-target}\n\nIn the online Q-learning algorithm, the target Q moves. To resolve\nthis we can use a target network:\n\n\\begin{equation}\n  \\phi \\leftarrow \\phi - \\alpha \\sum\\_i\n  \\frac{dQ\\\\phi}{d\\phi}(s\\i,a\\i)(Q\\\\phi(s\\i,a\\i) - [r(s\\i,a\\i) +\n  \\gamma Q\\{\\phi '}(s\\i', a\\_i')])\n\\end{equation}\n\nThe use of the target network \\\\(Q\\_{\\phi '}\\\\) results in targets not\nchanging in the inner loop.\n\nDQN {#dqn}\n\nDQN is the result of using a replay buffer, target network and some\ngradient clipping. See Playing Atari with Deep RL.\n\nDouble DQN {#double-dqn}\n\n{{}}\n\nIt has been shown imperatively that the learnt Q-values are\nnumerically much higher than the true Q-values. Practically, this\nisn't much of an issue: as the predicted Q-value increases,\nperformance also increases.\n\nThe intuition behind why this happens, is that our target value \\\\(y\\_j\\\\)\nis given by:\n\n\\begin{equation}\n  y\\j = r\\j + \\gamma \\mathrm{max}\\{a\\j'}Q\\{\\phi '}(s\\j', a\\_j')\n\\end{equation}\n\nIt is easy to show that:\n\n\\begin{equation}\n  E\\left[ \\mathrm{max}(X\\1, X\\2) \\right] \\ge \\mathrm{max}(E[X\\1], E[X\\2])\n\\end{equation}\n\n\\\\(Q\\_{\\phi '}(s', a')\\\\) overestimates the next value, because it is\nnoisy! The solution is to use 2 Q-functions, decorrelating the errors:\n\n\\begin{equation}\n  \\mathrm{max}\\{a'}Q\\{\\phi '}(s', a') = Q\\{\\phi '}(s', \\mathrm{argmax}\\{a'}(s',a'))\n\\end{equation}\n\nbecomes:\n\n\\begin{equation}\n  Q\\{\\phi\\A} (s,a) \\leftarrow r + \\gamma Q\\{\\phi\\B}(s', \\mathrm{argmax}\\{a'}Q\\{\\phi\\_A}(s',a'))\n\\end{equation}\n\n\\begin{equation}\n  Q\\{\\phi\\B} (s,a) \\leftarrow r + \\gamma Q\\{\\phi\\A}(s', \\mathrm{argmax}\\{a'}Q\\{\\phi\\_B}(s',a'))\n\\end{equation}\n\nTo get 2 Q-functions, we use the current and target networks:\n\n\\begin{equation}\n  y = r + \\gamma Q\\{\\phi '}(s', \\mathrm{argmax}\\{a'} Q\\_\\phi(s',a'))\n\\end{equation}\n\nQ-learning with stochastic optimization {#q-learning-with-stochastic-optimization}\n\nTaking max over a continuous action space can be expensive. A simple\napproximation is:\n\n\\begin{equation}\n  \\mathrm{max}\\{a} Q(s,a) \\approx \\mathrm{max}\\left\\\\{ Q(s,a\\1), \\dots,\n  Q(s,a\\_N)\\right\\\\}\n\\end{equation}\n\nwhere \\\\((a\\1, \\dots, a\\N)\\\\) is sampled from some distribution. A more\naccurate solution is to use the cross-entropy method.\n\nAnother option is to use a function class that is easy to maximize\n(e.g. using a quadratic function). This option is simple, but loses\nrepresentational power.\n\nThe final option is to learn an approximate maximizer (e.g. DDPG). The\nidea is to train another network \\\\(\\mu\\_{\\phi}(s) \\approx\n\\mathrm{argmax}\\{a}Q\\{\\phi}(s,a)\\\\), by solving \\\\(\\theta \\leftarrow\n\\mathrm{argmax} Q\\\\phi(s, \\mu\\\\theta(s))\\\\)\n\nQ-learning {#q-learning}\n\nQ-learning learns an action-utility representation instead of learning\nutilities. We will use the notation \\\\(Q(s,a)\\\\) to denote the value of\ndoing action \\\\(a\\\\) in state \\\\(s\\\\).\n\n\\begin{equation}\n  U = max\\_a Q(s, a)\n\\end{equation}\n\n**A TD agent that learns a Q-function does not need a model of the form\n\\\\(P(s' | s, a)\\\\), either for learning or for action selection.**\nQ-learning is hence called a model-free method. We can write a\nconstraint equation as follows:\n\n\\begin{equation}\n  Q(s,a) = R(s) + \\gamma \\sum\\{s'} P(s' | s, a) max\\{a'} Q(s', a')\n\\end{equation}\n\nHowever, this equation requires a model to be learnt, since it depends\non \\\\(P(s' | s, a)\\\\). The TD approach requires no model of state\ntransitions.\n\nThe updated equation for TD Q-learning is:\n\n\\begin{equation}\n  Q(s, a) \\leftarrow Q(s, a) + \\alpha (R(s) + \\gamma max\\_{a'} Q(s',\n  a') - Q(s,a))\n\\end{equation}\n\nwhich is calculated whenever action \\\\(a\\\\) is executed in state \\\\(s\\\\)\nleading to state \\\\(s'\\\\).\n\nQ-learning has a close relative called SARSA\n(State-Action-Reward-State-Action). The update rule for SARSA is as\nfollows:\n\n\\begin{equation}\n  Q(s, a) \\leftarrow Q(s, a) + \\alpha (R(s) + \\gamma Q(s', a') - Q(s, a))\n\\end{equation}\n\nwhere \\\\(a'\\\\) is the action actually taken in state \\\\(s'\\\\). The rule is\napplied at the end of each \\\\(s, a, r, s', a'\\\\) quintuplet, hence the\nname.\n\nWhereas Q-learning backs up the best Q-value from the state reached in\nthe observed transition, SARSA waits until an action is actually taken\nand backs up the Q-value for that action. For a greedy agent that\nalways takes the action with best Q-value, the two algorithms are\nidentical. When exploration is happening, they differ significanty.\n\nBecause Q-learning uses the best Q-value, it pays no attention to the\nactual policy being followed - it is an off-policy learning algorithm.\nHowever, SARSA is an on-policy algorithm.\n\nQ-learning is more flexible in the sense that a Q-learning agent can\nlearn how to behave well even when guided by a random or adversarial\nexploration policy. On the other hand, SARSA is more realistic: for\nexample if the overall policy is even partly controlled by other\nagents, it is better to learn a Q-function for what will actually\nhappen rather than what the agent would like to happen.\n\nQ-learning has been shown to be sample efficient in the tabular\nsetting (Jin et al., 2018).\n\nQ-learning with function approximation {#q-learning-with-function-approximation}\n\nTo generalize over states and actions, parameterize Q with a function\napproximator, e.g. a neural net:\n\n\\begin{equation}\n  \\delta = r\\t + \\gamma \\mathrm{max}\\a Q(s\\{t+1}, a; \\theta) - Q(s\\t,\na ; \\theta)\n\\end{equation}\n\nand turn this into an optimization problem minimizing the loss on the\nTD error:\n\n\\begin{equation}\nJ(\\theta) = \\left| \\delta \\right|^2\n\\end{equation}\n\nThe key problem with Q-learning is stability, coined the \"deadly\ntriad\".\n\nOff-policy learning\nflexible function approximation\nBootstrapping\n\nIn the presence of all three, learning is unstable. DQN is the first\nalgorithm that stabilized deep Q-learning ([Playing Atari with Deep\nRL]({{}})).\n\nBibliography\nJin, C., Allen-Zhu, Z., Bubeck, S., & Jordan, M. I., Is Q-Learning Provably Efficient?, In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 4863–4873) (2018). : Curran Associates, Inc. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/quantization",
        "title": "Quantization",
        "content": "\ntags\n: Model Compression, Machine Learning\n\nQuantization refers to techniques for performing computations and\nstoring tensors at lower bitwidths than floating point precision.\n\nQuantization is useful in reducing model size, and memory\nrequirements.\n\nPytorch supports two libraries for quantization:\n\nFBGEMM\nQNNPACK\n\nQuestions {#questions}\n\nCan we quantize §spiking\\neural\\networks? They already do binary\n    precision computation, what about the weights?\n",
        "tags": []
    },
    {
        "uri": "/zettels/rademacher",
        "title": "Rademacher Complexity",
        "content": "\ntags\n: §machine\\_learning\n\nIn PAC Learning, We have shown that [uniform convergence is a\nsufficient condition for learnability]({{}}). Rademacher complexity measures\nthe rate of uniform convergence. Rademacher complexity can also be\nused to provide generalization bounds.\n\nDefinition {#definition}\n\nLet us denote:\n\n\\begin{equation}\n  \\mathcal{F} \\overset{\\mathrm{def}}{=} l \\circ \\mathcal{H}\n  \\overset{\\mathrm{def}}{=} \\left\\\\{ z \\rightarrow l(h,z) : h \\in \\mathcal{H} \\right\\\\}\n\\end{equation}\n\ngiven \\\\(f \\in \\mathcal{F}\\\\), we also define:\n\n\\begin{equation}\n  L\\D(f) = \\mathbb{E}\\{z \\sim D} \\left[ f(z) \\right], L\\_S(f) =\n  \\frac{1}{m} \\sum\\{i=1}^{m} f(z\\i)\n\\end{equation}\n\nWe define the representativeness of \\\\(S\\\\) with respect to \\\\(\\mathcal{F}\\\\)\nas the largest gap between the true error of a function \\\\(f\\\\), and its\nempirical error:\n\n\\begin{equation}\n  \\mathrm{Rep}\\_D(\\mathcal{F}, S) \\overset{\\mathrm{def}}{=}\n  \\mathrm{sup}\\{f \\in \\mathcal{F}} (L\\D(f) - L\\_S(f))\n\\end{equation}\n\nSuppose we would like to estimate the representativeness of \\\\(S\\\\) using\nthe sample \\\\(S\\\\) only. One simple idea is to split \\\\(S\\\\) into 2 disjoint\nsets, \\\\(S = S\\1 \\cup S\\2\\\\) ; refer to \\\\(S\\_1\\\\) as the validation set and\n\\\\(S\\_2\\\\) as the training set. We can then estimate the representativeness\nof \\\\(S\\\\) by:\n\n\\begin{equation}\n  \\mathrm{sup}\\{f \\in \\mathcal{F}} (L\\{S\\1}(f) - L\\{S\\_2}(f))\n\\end{equation}\n\nIf we define \\\\(\\mathbf{\\sigma} = (\\sigma\\1, \\dots, \\sigma\\m) \\in\n\\left\\\\{ \\pm 1\\right\\\\}^m\\\\), to be a vector such that \\\\(S\\1 = \\\\{ z\\i :\n\\sigma\\i = 1\\\\}\\\\) and \\\\(S\\2 = \\\\{ z\\i : \\sigma\\i = -1\\\\}\\\\). If we further\nassume \\\\(|S\\1| = |S\\2|\\\\), then:\n\n\\begin{equation}\n  \\frac{2}{m} \\mathrm{sup}\\{f \\in \\mathcal{F}} \\sum\\{i=1}^{m} \\sigma\\i f(z\\i)\n\\end{equation}\n\nThe Rademacher complexity measure captures this idea by considering\nthe expectation of the above with respect to a random choice of\n\\\\(\\mathcal{\\sigma}\\\\). Formally, let \\\\(\\mathcal{F} \\circ S\\\\) be the set of\nall possible evaluations a function \\\\(f \\in \\mathcal{F}\\\\) can achieve on\nsample S, namely:\n\n\\begin{equation}\n  \\mathcal{F} \\circ S = \\left\\\\{ (f(z\\1), \\dots, f(z\\m)) : f \\in \\mathcal{F} \\right\\\\}\n\\end{equation}\n\nLet the variables in \\\\(\\mathbf{\\sigma}\\\\) be distributed i.i.d. according\nto \\\\(\\mathbb{P}[\\sigma\\i = 1] = \\mathbb{P}[\\sigma\\i = -1] =\n\\frac{1}{2}\\\\). Then the Rademacher complexity of \\\\(\\mathcal{F}\\\\) with\nrespect to \\\\(S\\\\) is defined as:\n\n\\begin{equation}\n  R(\\mathcal{F} \\circ S) \\overset{def}{=} \\frac{1}{m}\n  \\mathbb{E}\\{\\mathbf{\\sigma} \\in \\\\{ \\pm 1\\\\}^m} \\left[ \\mathrm{sup}\\{f\n    \\in \\mathcal{F}} \\sum\\{i=1}^{m} \\sigma\\i f(z\\_i) \\right]\n\\end{equation}\n\nMore generally, given a set of vectors \\\\(A \\subset \\mathbb{R}^m\\\\), we\ndefine\n\n\\begin{equation}\n  R(A) \\overset{\\mathrm{def}}{=} \\frac{1}{m}\n  \\mathbb{E}\\{\\mathbf{\\sigma}} \\left[ \\mathrm{sup}\\{f \\in \\mathcal{F}}\n  \\sum\\{i=1}^{m} \\sigma\\i f(z\\_i) \\right]\n\\end{equation}\n\nThe following lemma bounds the expected value of the\nrepresentativeness of \\\\(S\\\\) by twice the expected Rademacher complexity.\n\n\\begin{equation}\n  \\mathbb{E}\\{S \\sim \\mathcal{D}^m} \\left[ \\mathrm{Rep}\\{\\mathcal{D}}\n    (\\mathcal{F}, S) \\right] \\le 2 \\mathbb{E}\\_{S \\sim \\mathcal{D}^m}\n  R(\\mathcal{F} \\circ S)\n\\end{equation}\n\nThis lemma yields that, in expectation, the ERM rule finds a\nhypothesis which is close to the optimal hypothesis in \\mathcal{H}.\n\n\\begin{equation}\n  \\mathbb{E}\\{S \\sim \\mathcal{D}^m} \\left[ L\\D(ERM\\_{\\mathcal{H}}(S)) -\n  L\\S(ERM\\{\\mathcal{H}}(S))\\right] \\le 2 \\mathbb{E}\\_{S \\sim\n  \\mathcal{D}^m} (l \\circ \\mathcal{H} \\circ S)\n\\end{equation}\n\nFurthermore, for any \\\\(h^\\* \\in \\mathcal{H}\\\\)\n\n\\begin{equation}\n  \\mathbb{E}\\{S \\sim \\mathcal{D}^m} \\left[ L\\D(ERM\\_{\\mathcal{H}}(S)) -\n  L\\D(h^\\*)\\right] \\le 2 \\mathbb{E}\\{S \\sim\n  \\mathcal{D}^m} (l \\circ \\mathcal{H} \\circ S)\n\\end{equation}\n\nFurthermore, if \\\\(h^\\* = \\mathrm{argmin}\\h L\\{\\mathcal{D}}(h)\\\\) then for\neach \\\\(\\delta \\in (0,1)\\\\) with probability of at least \\\\(1 - \\delta\\\\) over\nthe choice of \\\\(S\\\\), we have:\n\n\\begin{equation}\n  L\\{\\mathcal{D}} (ERM\\{\\mathcal{H}}(S) - L\\_{\\mathcal{D}}(h^\\*)) \\le\n  \\frac{2 \\mathbb{E}\\_{S' \\sim \\mathcal{D}^m} R(l \\circ \\mathcal{H}\n    \\circ S')}{\\delta}\n\\end{equation}\n\nUsing McDiarmid's Inequality, we can derive bounds with better\ndependence on the confidence parameter:\n\nAssume that for all \\\\(z\\\\) and \\\\(h \\in \\mathcal{H}\\\\) we have that \\\\(|l(h,z)\n\\le c|\\\\). Then,\n\nWith probability at least \\\\(1 - \\delta\\\\), for all \\\\(h \\in\n       \\mathcal{H}\\\\),\n\n\\begin{equation}\n  L\\{\\mathcal{D}} (h) - L\\S(h) \\le 2 \\mathbb{E}\\_{S' \\sim\n    \\mathcal{D}^m} R(l \\circ \\mathcal{H} \\circ S') + c \\sqrt{\\frac{2 \\ln(2/\\delta)}{m}}\n\\end{equation}\n\nIn particular, this holds for \\\\(h = ERM\\_{\\mathcal{H}}(S)\\\\).\n\nWith probability at least \\\\(1 - \\delta\\\\), for all \\\\(h \\in\n       \\mathcal{H}\\\\),\n\n\\begin{equation}\n  L\\{\\mathcal{D}} (h) - L\\S(h) \\le 2 R(l \\circ \\mathcal{H} \\circ S) +\n  4c\\sqrt{\\frac{2 \\ln(4/\\delta)}{m}}\n\\end{equation}\n\nFor any \\\\(h^\\*\\\\) , with probability at least \\\\(1 - \\delta\\\\),\n\n\\begin{equation}\n  L\\{\\mathcal{D}} (ERM\\{\\mathcal{H}} (S)) - L\\_D(h^\\*) \\le 2 R(l \\circ \\mathcal{H} \\circ S) +\n  5c\\sqrt{\\frac{2 \\ln(8/\\delta)}{m}}\n\\end{equation}\n\nMassart's lemma states that the Rademacher complexity of a finite set\ngrows logarithmically with the size of the set.\n\nLet \\\\(A = \\\\{a\\1, \\dots, a\\N\\\\}\\\\) be a finite set of vectors in\n\\\\(\\mathbb{R}^m\\\\). Define \\\\(\\bar{a} = \\frac{1}{N} \\sum\\{i=1}^N a\\i\\\\).\nThen,\n\n\\begin{equation}\n  R(A) \\le \\mathrm{max}\\_{a \\in A} \\lVert a - \\bar{a} \\rVert\n  \\frac{\\sqrt{2 \\log(N)}}{m}\n\\end{equation}\n\nThe contraction lemma shows that composing \\\\(A\\\\) with a Lipschitz\nfunction does not blow up the Rademacher complexity.\n\nRademacher complexity of linear classes {#rademacher-complexity-of-linear-classes}\n\nWe define 2 linear classes:\n\n\\begin{equation}\n  \\mathcal{H}\\_1 = \\left\\\\{x \\rightarrow \\langle w,x  \\rangle : \\lVert w\n      \\rVert\\_1 \\le 1\\right\\\\}\n\\end{equation}\n\n\\begin{equation}\n  \\mathcal{H}\\_2 = \\left\\\\{x \\rightarrow \\langle w,x  \\rangle : \\lVert w\n      \\rVert\\_2 \\le 1\\right\\\\}\n\\end{equation}\n\n\\\\(\\mathcal{H}\\_2\\\\) is bounded by the following lemma:\n\nLet \\\\(S = (x\\1, \\dots, x\\m)\\\\) be vectors in an Hilbert space. Define\n\\\\(\\mathcal{H}\\2 \\circ S = \\left\\\\{( \\langle w, x\\1 \\rangle), \\langle w,\nx\\2 \\rangle), \\dots, \\langle w, x\\m \\rangle) : \\lVert w \\rVert\\_2 \\le 1\n\\right\\\\}\\\\). Then,\n\n\\begin{equation}\n  R(\\mathcal{H}\\2 \\circ S) \\le \\frac{\\mathrm{max}\\i \\lVert x\\i \\rVert\\2}{\\sqrt{m}}\n\\end{equation}\n\nThe following lemma \\\\(\\mathcal{H}\\_1\\\\):\n\nLet \\\\(S = (x\\1, \\dots, x\\m)\\\\) be vectors in \\\\(\\mathbb{R}^n\\\\). Then,\n\n\\begin{equation}\n  R(\\mathcal{H}\\1 \\circ S) \\le \\mathrm{max}\\i \\lVert x\\i \\rVert\\\\infty\n  \\sqrt{\\frac{2 \\log(2n)}{m}}\n\\end{equation}\n",
        "tags": []
    },
    {
        "uri": "/zettels/random_variables",
        "title": "Random Variables",
        "content": "\ntags\n: §statistics\n\nIntroduction {#introduction}\n\nIt frequently occurs that in performing an experiment, we are mainly\ninterested in some function of the outcome rather than the outcome\nitself. These quantities of interest, are real-valued functions\ndefined on the sample space \\\\(\\mathcal{S}\\\\), known as random variables.\n\nDiscrete Random Variables {#discrete-random-variables}\n\nFor a discrete random variable, we define the probability mass\nfunction \\\\(p(a)\\\\) of \\\\(X\\\\) by:\n\n\\begin{equation} \\label{dfn:pmf}\n  p(a) = P(X = a)\n\\end{equation}\n\nThe probability mass function \\\\(p(a)\\\\) is positive for at most a\ncountable number of values of \\\\(a\\\\).\n\nSince \\\\(X\\\\) must take on one of the values \\\\(x\\_i\\\\), we have\n\\\\(\\sum\\{i=1}^{\\infty} p(x\\i) = 1\\\\).\n\nThe cumulative distribution function \\\\(F\\\\) can be expressed in terms of\n\\\\(p(a)\\\\) by:\n\n\\begin{equation} \\label{dfn:cdf}\n  F(a) = \\sum\\{\\text{all } x\\i \\le a} p(x\\_i)\n\\end{equation}\n\nBernoulli Random Variable {#bernoulli-random-variable}\n\nA random variable \\\\(X\\\\) is said to be a Bernoulli random variable if its\nprobability mass function is given by:\n\n\\begin{equation}\n  p(0) = P(X = 0) = 1 - p, p(1) = P(X = 1) = p\n\\end{equation}\n\nThis corresponds to the outcome of a trial with binary outcomes.\n\nBinomial Random Variable {#binomial-random-variable}\n\nSuppose \\\\(n\\\\) independent trials, each of which have binary outcomes and\nresult in a success with probability \\\\(p\\\\). If \\\\(X\\\\) represents the number\nof successes that occur in the \\\\(n\\\\) trials, then \\\\(X\\\\) is a binomial\nrandom variable denoted by \\\\(Bin(n, p)\\\\).\n\nThe pmf of a binomial random variable is given by:\n\n\\begin{equation}\n  p(i) = P(X = i) = {n \\choose i}p^i (1-p)^i, i = 0,1,\\dots,n\n\\end{equation}\n\nGeometric Random Variable {#geometric-random-variable}\n\nSuppose that independent trials, each having probability \\\\(p\\\\) of being\na success, are performed until a success occurs. If we let \\\\(X\\\\) be the\nnumber of trials required until the first success, then \\\\(X\\\\) is said to\nbe a geometric variable with parameter \\\\(p\\\\).\n\n\\begin{equation}\n  p(n) = P(X = n) = (1-p)^{n-1}p\n\\end{equation}\n\nPoisson Random Variable {#poisson-random-variable}\n\nA random variable \\\\(X\\\\), taking on one of the values \\\\(0, 1, 2, \\dots\\\\) is\nsaid to be a poisson random variable with parameter \\\\(\\lambda\\\\) if for\nsome \\\\(\\lambda > 0\\\\),\n\n\\begin{equation}\n  p(i) = p(X = i) = e^{-\\lambda}\\frac{\\lambda^i}{i!}, i = 0, 1, \\dots\n\\end{equation}\n\nAn important property of the Poisson random variable is that it may be\nused to approximate a binomial random variable when the binomial\nparameter \\\\(n\\\\) is large and \\\\(p\\\\) is small.\n\nContinuous Random Variables {#continuous-random-variables}\n\nContinuous random variables have an uncountable set of possible\nvalues.\n\n\\begin{equation}\n  P[X \\in B] = \\int\\_{B}f(x)dx\n\\end{equation}\n\nThe cumulative distribution \\\\(F(\\cdot)\\\\) and the probability density\nfunction is expressed by:\n\n\\begin{equation}\n  F(a) = P(X \\in (-\\infty, a]) = \\int\\_{-\\infty}^{a} f(x) dx\n\\end{equation}\n\nUniform Random Variable {#uniform-random-variable}\n\n\\\\(X\\\\) is a uniform random variable on the interval \\\\((a, b)\\\\) if its\nprobability density function is given by:\n\n\\begin{equation}\n  f(x) = \\begin{cases}\n    \\frac{1}{b - a} & a 0} x p(x)\n\\end{equation}\n\nand for a continuous random variable is defined similarly:\n\n\\begin{equation}\n  E(X) = \\int\\_{-\\infty}^{\\infty} x f(x) dx\n\\end{equation}\n\nSuppose we are interested in getting the expected value of a function\nof a random variable \\\\(E(g(X))\\\\). One way is to obtain the distribution\nof \\\\(g(X)\\\\), and compute \\\\(E(g(X))\\\\) by definition of expectation.\n\n\\begin{equation}\n  E(g(X)) = \\sum\\_{x:p(x)>0} g(x) p(x)\n\\end{equation}\n\nWe also have linearity of expectations:\n\n\\begin{equation}\n  E(aX + b) = aE(X) + b\n\\end{equation}\n\n\\\\(E(X)\\\\) is the first moment of \\\\(X\\\\), and \\\\(E(X^n)\\\\) is the nth moment of\n\\\\(X\\\\).\n\nThe variance of \\\\(X\\\\), \\\\(Var(X)\\\\) measures the expected square of the\ndeviation of \\\\(X\\\\) from its expected value:\n\n\\begin{equation}\n  Var(X) = E((X-E(X))^2) = E(X^2) - (E(X))^2\n\\end{equation}\n\nJointly Distributed Random Variables {#jointly-distributed-random-variables}\n\nThe joint cumulative probability distribution function of \\\\(X\\\\) and \\\\(Y\\\\)\nis given by:\n\n\\begin{equation}\n  F(a, b) = P(X \\le a, Y \\le b), -\\infty < a , b < \\infty\n\\end{equation}\n\nThe distribution of \\\\(X\\\\) can then be obtained from the joint\ndistribution of \\\\(X\\\\) and \\\\(Y\\\\) as follows:\n\n\\begin{align}\n  F\\_X(a) &= P(X \\le a, Y \\le \\infty) \\\\\\\\\\\\\n         &= F(a, \\infty)\n\\end{align}\n\nCovariance and Variance of Sums of Random Variables {#covariance-and-variance-of-sums-of-random-variables}\n\nThe covariance of any 2 random variables \\\\(X\\\\) and \\\\(Y\\\\), denoted by\n\\\\(Cov(X,Y)\\\\), is defined by:\n\n\\begin{align}\n  Cov(X,Y) &= E\\left[ (X - E[X])(Y - E[Y]) \\right] \\\\\\\\\\\\\n           &= E[XY] - E[X]E[Y]\n\\end{align}\n\nIf \\\\(X\\\\) and \\\\(Y\\\\) are independent, then \\\\(Cov(X,Y) = 0\\\\).\n\nSome properties of covariance:\n\n\\\\(Cov(X,X) = Var(X)\\\\)\n\\\\(Cov(X,Y) = Cov(Y,X)\\\\)\n\\\\(Cov(cX, Y) = c Cov(X,Y)\\\\)\n\\\\(Cov(X, Y+Z) = Cov(X,Y) + Cov(X,Z)\\\\)\n",
        "tags": [
            "statistics"
        ]
    },
    {
        "uri": "/zettels/range_finder_model",
        "title": "Range Finder Model",
        "content": "\nBeam Model for Range Finders {#beam-model-for-range-finders}\n\n4 types of measurement errors are incorporated, all essential to\nmaking it work:\n\nsmall measurement noise\nerrors due to unexpected objects\nerrors due to failure to detect objects\nrandom unexplained noise\n\nThe desired model is a mixture of four densities, each of which\ncorresponds to a particular type of error.\n\nSmall Measurement Noise {#small-measurement-noise}\n\nSuppose the true range of the object is \\\\(z\\_t^{k\\*}\\\\). The small\nmeasurement noise is typically modelled as a narrow Gaussian with mean\n\\\\(z\\t^{k\\*}\\\\) and standard deviation \\\\(\\sigma\\{\\text{hit}}\\\\). We denote\nthis Gaussian as \\\\(p\\_{\\text{hit}}\\\\).\n\nIn practice, the values of the range sensor are limited to the\ninterval \\\\([0; z\\{\\text{max}}]\\\\), where \\\\(z\\{\\text{max}}\\\\) denotes the\nmaximum sensor range. Hence, the measurement probability is given by:\n\n\\begin{equation}\n  p\\{\\mathrm{hit}}\\left(z\\{t}^{k} | x\\{t}, m\\right)=\\left\\\\{\\begin{array}{ll}{\\eta \\mathcal{N}\\left(z\\{t}^{k} ; z\\{t}^{k \\*}, \\sigma\\{\\mathrm{hit}}^{2}\\right)} & {\\text { if } 0 \\leq z\\{t}^{k} \\leq z\\{\\max }} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n\\end{equation}\n\nUnexpected Objects {#unexpected-objects}\n\nThe environment of mobile robots are dynamics, whereas the map of the\nenvironment is static. Examples of moving objects include humans in\nthe environment.\n\nOne method of handling these is to include them in the state vector\nand estimate their position. The simpler way is to treat them as\nsensor noise. These objects cause ranges to be shorter than\n\\\\(z\\_t^{k\\*}\\\\), not longer.\n\nSince the likelihood of sensing unexpected objects decreases with\nrange, this probability can be described by an exponential\ndistribution, parameterized by \\\\(\\lambda\\_{\\text{short}}\\\\).\n\n\\begin{equation}\n  p\\{\\text {short }}\\left(z\\{t}^{k} | x\\{t}, m\\right)=\\left\\\\{\\begin{array}{ll}{\\eta \\lambda\\{\\text {short }} e^{-\\lambda\\{\\text {short }} z\\{t}^{k}}} & {\\text { if } 0 \\leq z\\{t}^{k} \\leq z\\{t}^{k \\*}} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right\n\\end{equation}\n\nFailure to detect objects {#failure-to-detect-objects}\n\nThis is the situation where objects are missed altogether. This\nhappens often where the object is beyond the sensor's maximum range.\nWe can model this as a point-mass distribution centered at\n\\\\(z\\_{\\text{max}}\\\\).\n\n\\begin{equation}\n  p\\{\\max }\\left(z\\{t}^{k} | x\\{t}, m\\right)=I\\left(z=z\\{\\max }\\right)=\\left\\\\{\\begin{array}{ll}{1} & {\\text { if } z=z\\_{\\max }} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n\\end{equation}\n\nMixture of the four {#mixture-of-the-four}\n\nThe four distributions are mixed by a weighted average:\n\n\\begin{equation}\n  p\\left(z\\{t}^{k} | x\\{t}, m\\right)=\\left(\\begin{array}{c}{z\\{\\text {hit }}} \\\\ {z\\{\\text {short }}} \\\\ {z\\{\\text {max }}} \\\\ {z\\{\\text {rand }}}\\end{array}\\right)^{T} \\cdot\\left(\\begin{array}{c}{p\\{\\text {hit }}\\left(z\\{t}^{k} | x\\{t}, m\\right)} \\\\ {p\\{\\text {short }}\\left(z\\{t}^{k} | x\\{t}, m\\right)} \\\\ {p\\{\\text {max }}\\left(z\\{t}^{k} | x\\{t}, m\\right)} \\\\ {p\\{\\text {rand }}\\left(z\\{t}^{k} | x\\{t}, m\\right)}\\end{array}\\right)\n\\end{equation}\n\n{{}}\n\nParameters can be learnt from data via maximum likelihood estimation.\n\nIssues {#issues}\n\nCan be unsmooth in the presence of many small obstacles: this\n    poses a problem for ML estimation\nComputationally expensive\n\nAlternatives {#alternatives}\n\n§likelihood\\field\\model\n",
        "tags": []
    },
    {
        "uri": "/zettels/react",
        "title": "React",
        "content": "\ntags\n: §javascript\n\nReact Frameworks {#react-frameworks}\n",
        "tags": []
    },
    {
        "uri": "/zettels/recommender_systems",
        "title": "Recommender Systems",
        "content": "\ntags\n: Machine Learning\n",
        "tags": []
    },
    {
        "uri": "/zettels/regression",
        "title": "Regression",
        "content": "\ntags\n: §statistics, §bayesian\\_statistics\n\nIntroduction {#introduction}\n\nRegression analysis is a conceptually simple method for investigating\nfunctional relationships between variables. The relationship is\nexpressed in the form of an equation or a model connecting the\nresponse or dependent variable and one or more explanatory or\npredictor variables.\n\nWe denote the response variable by \\\\(Y\\\\), and the set of predictor\nvariables by \\\\(X\\1, X\\2, \\dots, X\\_p\\\\), where \\\\(p\\\\) denotes the number of\npredictor variables.\n\nThe general regression model is specified as:\n\n\\begin{equation} \\label{dfn:general\\_regression}\n  Y = f(X\\1, X\\2, \\dots, X\\_p) + \\epsilon\n\\end{equation}\n\nAn example is the linear regression model:\n\n\\begin{equation}\n  Y = \\beta\\0 + \\beta\\1X\\1 + \\dots + \\beta\\pX\\_p + \\epsilon\n\\end{equation}\n\nwhere \\\\(\\beta\\0, \\dots, \\beta\\p\\\\) called the regression paramaters or\ncoefficients, are unknown constants to be estimated from the data.\n\\\\(\\epsilon\\\\) is the random error representing the discrepancy in the\napproximation.\n\nSteps in Regression Analysis {#steps-in-regression-analysis}\n\n1. Statement of the problem\n\nFormulation of the problem includes determining the questions to be\naddressed.\n\n2. Selection of Potentially Relevant Variables\n\nWe select a set of variables that are thought by the experts in the\narea of study to explain or predict the response variable.\n\nEach of these variables could be qualitative or quantitative. A\ntechnique used in casse where the response variable is binary is\ncalled logistic regression. If all predictor variables are\nqualitative, the techniques used in the analysis of the data are\ncalled the analysis of variance techniques. If some of the predictor\nvariables are quantitative while others are qualitative, regression\nanalysis in these cases is called the analysis of covariance.\n\n| Type                   | Conditions                                                                                                                            |\n|------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n| Univariate             | Only 1 quantitative RV                                                                                                                |\n| Multivariate           | 2 or more quantitative RV                                                                                                             |\n| Simple                 | Only 1 predictor variable                                                                                                             |\n| Multiple               | 2 or more predictor variables                                                                                                         |\n| Linear                 | All parameters enter the equation linearly, possibly after transformation of the data                                                 |\n| Non-linear             | The relationship between the response and some of the predictors is non-linear (no transformation to make parameters appear linearly) |\n| Analysis of variance   | All predictors are qualitative variables                                                                                              |\n| Analysis of covariance | Some predictors are quantitative variables, others are qualitative                                                                    |\n| Logistic               | The RV is qualitative                                                                                                                 |\n\n3. Model specification\n\nThe form of the model that is thought to relate the response variable to\nthe set of predictor variables can be specified initially by experts\nin the area of study based on their knowledge or their objective\nand/or subjective judgments.\n\nWe need to select the form of the function \\\\(f(X\\1, X\\2, \\dots, X\\_p)\\\\)\nin .\n\n4. Method of Fitting\n\nWe want to perform parameter estimation or model fitting after\ndefining the model and collecting the data. the most commonly used\nmethod of estimation is called the least squares method. Other\nestimation methods we consider are the maximum likelihood method,\nridge regression and the principal components method.\n\n5. Model Fitting\n\nThe estimates of the regression parameters \\\\(\\beta\\0, \\beta\\1, \\dots,\n\\beta\\p\\\\) are denoted by \\\\(\\hat{\\beta\\0}, \\hat{\\beta\\_1}, \\dots,\n\\hat{\\beta\\_p}\\\\). the obtained \\\\(\\hat{Y}\\\\) denotes the predicted value.\n\n6. Model Criticism and Selection\n\nThe validity of statistical methods depend on certain assumptions, about\nthe data and the model. We need to address the following questions:\n\nWhat are the required assumptions?\nFor each of these assumptions, how do we determine if they are valid?\nWhat can be done in cases where assumptions do not hold?\n\n{{}}\n\nSimple Linear Regression {#simple-linear-regression}\n\nSimple linear regression is a straightforward approach for predicting\na quantitative response \\\\(Y\\\\) on the basis of a single predictor\nvariable \\\\(X\\\\). Mathematically, we write this linear relationship as:\n\n\\begin{equation} \\label{eqn:slr}\n  Y \\approx \\beta\\0 + \\beta\\1 X\n\\end{equation}\n\nWe describe this as regressing \\\\(Y\\\\) on \\\\(X\\\\).\n\nWe wish to measure both the direction and strength of the relationship\nbetween \\\\(Y\\\\) and \\\\(X\\\\). Two related measures, known as the covariance and\nthe correlation coefficient are developed later.\n\nWe use our training data to produce estimates \\\\(\\hat{\\beta\\_0}\\\\) and\n\\\\(\\hat{\\beta\\_1}\\\\) for the model coefficients, and we can predict outputs\nby computing:\n\n\\begin{equation}\n  \\hat{y} = \\hat{\\beta\\0} + \\hat{\\beta\\1} X\n\\end{equation}\n\nLet \\\\(\\hat{y\\i} = \\hat{\\beta\\0} + \\hat{\\beta\\_1}\\\\) be the prediction of\n\\\\(Y\\\\) based on the ith value of \\\\(X\\\\). Then \\\\(e\\i = y\\i - \\hat{y\\_i}\\\\)\nrepresents the ith residual. We can define the residual sum of squares\n(RSS) as:\n\n\\begin{equation} \\label{eqn:dfn:rss}\n  \\mathrm{RSS} = e\\1^2 + e\\2^2 + \\dots + e\\_n^2\n\\end{equation}\n\nThe least squares approach chooses \\\\(\\hat{\\beta\\0}\\\\) and \\\\(\\hat{\\beta\\1}\\\\)\nto minimise the RSS.\n\nIf \\\\(f\\\\) is to be approximated by a linear function, then we can write\nthe relationship as:\n\n\\begin{equation}\n  Y = \\beta\\0 + \\beta\\1 X + \\epsilon\n\\end{equation}\n\nwhere \\\\(\\epsilon\\\\) is an error term -- the catch-all for what we miss\nwith this simple model. For example, the true relationship is probably\nnot linear, and other variables cause variation in \\\\(Y\\\\). It is\ntypically assumed that the error term is independent of \\\\(X\\\\).\n\nWe need to assess the accuracy of our estimates. How far off will a\nsingle estimation of \\\\(\\hat{\\mu}\\\\) be? In general, we can answer this\nquestion by computing the standard error of \\\\(\\hat{\\mu}\\\\), written as\n\\\\(\\mathrm{SE}(\\hat{\\mu})\\\\). This is given by:\n\n\\begin{equation} \\label{eqn:dfn:se}\n  \\mathrm{Var}(\\hat{\\mu}) = \\mathrm{SE}(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}\n\\end{equation}\n\nwhere \\\\(\\sigma\\\\) is the standard deviation of each of the realizations\n\\\\(y\\_i\\\\) of \\\\(Y\\\\).\n\nStandard errors can be used to compute confidence intervals. A 95%\nconfidence interval is defined as a range of values such that with 95%\nprobability, the range will contain the true unknown value of the\nparameter. For linear regression, the 95% confidence interval for\n\\\\(\\beta\\_1\\\\) approximately takes the form:\n\n\\begin{equation}\n  \\hat{\\beta\\1} \\pm 2 \\cdot SE(\\hat{\\beta\\1})\n\\end{equation}\n\nStandard errors can also be used to perform hypothesis tests on the\ncoefficients. The most common hypothesis test involves testing the\nnull hypothesis of:\n\n\\begin{equation} \\label{eqn:dfn:null\\_hyp}\n  H\\0 : \\mathrm{There is no relationship between X and Y} (\\beta\\1 = 0)\n\\end{equation}\n\nversus the alternative hypothesis:\n\n\\begin{equation} \\label{eqn:dfn:alt\\_hyp}\n  H\\a : \\mathrm{There is a relationship between X and Y} (\\beta\\1 \\ne 0)\n\\end{equation}\n\nTo test the null hypothesis, we need to determine whether\n\\\\(\\hat{\\beta\\_1}\\\\) is sufficiently far from zero that we can be\nconfident that \\\\(\\beta\\_1\\\\) is non-zero. How far is far enough? This\ndepends on the accuracy of \\\\(\\hat{\\beta\\_1}\\\\), or\n\\\\(\\mathrm{SE}(\\hat{\\beta\\1})\\\\). In practice, we compute a t-statistic_,\ngiven by:\n\n\\begin{equation} \\label{eqn:dfn:t-statistic}\n  t = \\frac{\\hat{\\beta\\1} - 0}{\\mathrm{SE}(\\hat{\\beta\\1})}\n\\end{equation}\n\nwhich measures the number of standard deviations that \\\\(\\hat{\\beta\\_1}\\\\)\nis away from 0. If there really is no relationship between \\\\(X\\\\) and\n\\\\(Y\\\\), then we expect that Eq.  will have a\nt-distribution with \\\\(n-2\\\\) degrees of freedom. The t-distribution has a\nbell shape and for values of \\\\(n\\\\) greater than approximately 30 it is\nsimilar to the normal distribution.\n\nIt is a simple matter to compute the probability of observing any\nnumber equal to \\\\(\\lVert t \\rVert\\\\) or larger in absolute value,\nassuming \\\\(\\beta\\1 = 0\\\\). We call this probability the p-value_. A small\np-value indicates that it is unlikely to observe such a substantial\nassociation between the predictor and the response due to chance.\n\nAssessing the accuracy of the model {#assessing-the-accuracy-of-the-model}\n\nOnce we have rejected the null hypothesis in favour of the alternative\nhypothesis, it is natural to want to quantify the extent to which the\nmodel fits the data.\n\nResidual Standard Error (RSE)\n\n    After we compute the least square estimates of the parameters of a\n    linear model, we can compute the following quantities:\n\n    \\begin{align}\n    \\mathrm{SST} &= \\sum(y\\_i - \\bar{y})^2 \\\\\\\\\\\\\n    \\mathrm{SSR} &= \\sum(\\hat{y\\_i} - \\bar{y})^2 \\\\\\\\\\\\\n    \\mathrm{SSE} &= \\sum(y\\i - \\hat{y\\i})^2\n    \\end{align}\n\n    A fundamental equality in both simple and multiple regressions is\n    given by \\\\(\\mathrm{SST} = \\mathrm{SSR} + \\mathrm{SSE}\\\\). This can be\n    interpreted as: The deviation from the mean is equal to the deviation\n    due to fit, plus the residual.\n\n\\\\(R^2\\\\) statistic\n\n    The RSE provides an absolute measure of lack of fit of the model to\n    the data. But since it is measured in the units of \\\\(Y\\\\), it is not\n    always clear what constitutes a good RSE. The \\\\(R^2\\\\) statistic provides\n    an alternative measure of fit. It takes a form of a proportion, and\n    takes values between 0 and 1, independent of the scale of \\\\(Y\\\\).\n\n    \\begin{equation} \\label{eqn:dfn:r\\_squared}\n      R^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} = 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\n    \\end{equation}\n\n    where \\\\(\\mathrm{SST} = \\sum(y\\i - \\bar{y})^2\\\\) is the total sum of\n    squares_. SST measures the total variance in the response \\\\(Y\\\\), and can\n    be thought of as the amount of variability inherent in the response\n    before the regression is performed. Hence, \\\\(R^2\\\\) measures the\n    proportion of variability in \\\\(Y\\\\) that can be explained using \\\\(X\\\\).\n\n    Note that the correlation coefficient \\\\(r = \\mathrm{Cor}(X, Y)\\\\) is\n    related to the \\\\(R^2\\\\) in the simple linear regression setting: \\\\(r^2 =\n    R^2\\\\).\n\nMultiple Linear Regression {#multiple-linear-regression}\n\nWe can extend the simple linear regression model to accommodate\nmultiple predictors:\n\n\\begin{equation}\n  Y = \\beta\\0 + \\beta\\1 X\\1 + \\beta\\2 X\\2 + \\dots + \\beta\\p X\\_p + \\epsilon\n\\end{equation}\n\nWe choose \\\\(\\beta\\0, \\beta\\1, \\dots, \\beta\\_p\\\\) to minimise the sum of\nsquared residuals:\n\n\\begin{equation}\n  \\mathrm{RSS} = \\sum\\{i=1}^{n} (y\\i - \\hat{y\\_i})^2\n\\end{equation}\n\nUnlike the simple regression estimates, the multiple regression\ncoefficient estimates have complicated forms that are most easily\nrepresented using matrix algebra.\n\nInterpreting Regression Coefficients {#interpreting-regression-coefficients}\n\n\\\\(\\beta\\0\\\\), the constant coefficient, is the value of \\\\(Y\\\\) when \\\\(X\\1 =\nX\\2 \\dots = X\\p = 0\\\\), as in simple regression. The regression\ncoefficient \\\\(\\beta\\_j\\\\) has several interpretations.\n\nFirst, it may be interpreted as the change in \\\\(Y\\\\) corresponding to a\nunit change in \\\\(X\\_j\\\\) when all other predictor variables are held\nconstant. In practice, predictor variables may be inherently related,\nand it is impossible to hold some of them constant while varying\nothers. The regression coefficient \\\\(\\beta\\_j\\\\) is also called the\npartial regression coefficient, because \\\\(\\beta\\_j\\\\) represents the\ncontribution of \\\\(X\\_j\\\\) to the response variable \\\\(Y\\\\) adjusted for other\npredictor variables.\n\nCentering and Scaling {#centering-and-scaling}\n\nThe magnitudes of the regression coefficients in a regression equation\ndepend on the unit of measurements of the variables. To make the\nregression coefficients unit-less, one may first center or scale the\nvariables before performing regression computations.\n\nWhen dealing with constant term models, it is convenient to center and\nscale the variables, but when dealing with no-intercept models, we\nneed only to scale the variables.\n\nA centered variable is obtained by subtracting from each observation\nthe mean of all observations. For example, the centered response\nvariable is \\\\((Y - \\bar{y})\\\\), and the centered jth predictor variable\nis \\\\((X\\j - \\bar{x}\\j)\\\\). The mean of a centered variable is 0.\n\nThe centered variables can also be scaled. Two types of scaling are\nusually performed: unit-length scaling and standardizing.\n\nUnit-length scaling of response variable \\\\(Y\\\\) and the jth predictor\nvariable \\\\(X\\_j\\\\) is obtained as follows:\n\n\\begin{align}\n  \\tilde{Z}\\y &= (Y - \\bar{y})/L\\y \\\\\\\\\\\\\n  \\tilde{Z}\\j &= (X - \\bar{x}\\j)/L\\_j \\\\\\\\\\\\\n\\end{align}\n\nwhere:\n\n\\begin{equation}\n  L\\y = \\sqrt{\\sum\\{i=1}^{n}(y\\i - \\bar{y})^2} \\text{ and } L\\j =\n  \\sqrt{\\sum\\{i=1}^{n}(x\\{ij} - \\bar{x}\\_j)^2}\\text{ , } j = 1,2,\\dots,p\n\\end{equation}\n\nThe quantities \\\\(L\\_y\\\\) is referred to as the length of the centered\nvariable \\\\(Y\\_\\bar{y}\\\\) because it measures the size or the magnitudes of\nthe observations in \\\\(Y - \\bar{y}\\\\). \\\\(L\\_j\\\\) has a similar interpretation.\n\nUnit length scaling has the following property:\n\n\\begin{equation}\n  \\mathrm{Cor}(X\\j, X\\k) = \\sum\\{i=1}^{n}z\\{ij}z\\_{ik}\n\\end{equation}\n\nThe second type of scaling is called standardizing, which is defined\nby:\n\n\\begin{align}\n  \\tilde{Y} &= \\frac{Y - \\bar{y}}{s\\_y} \\\\\\\\\\\\\n  \\tilde{X}\\j &= \\frac{X\\j - \\bar{x}\\j}{s\\j} \\text{ , } j = 1, \\dots, p\n\\end{align}\n\nwhere \\\\(s\\y\\\\) and \\\\(s\\j\\\\) are the standard deviations of the response and\njth predictor variable respectively. The standardized variables have\nmean zero and unit standard deviations.\n\nSince correlations are unaffected by centering or scaling, it is\nsufficient and convenient to deal with either unit-length scaled or\nstandardized models.\n\nProperties of Least-square Estimators {#properties-of-least-square-estimators}\n\nUnder certain regression assumptions, the least-square estimators have\nthe following properties:\n\nThe estimator \\\\(\\hat{\\beta}\\_j\\\\) is an unbiased estimate of\n    \\\\(\\hat{\\beta\\j}\\\\) and has a variance of \\\\(\\sigma^2 c\\{jj}\\\\), where\n    \\\\(c\\_{jj}\\\\) is the jth diagonal element of the inverse of a matrix\n    known as the corrected sums of squares and products matrix. The\n    covariance between \\\\(\\hat{\\beta}\\i\\\\) and \\\\(\\hat{\\beta}\\j\\\\) is \\\\(\\sigma^2\n       c\\_{ij}\\\\). For all unbiased estimates that are linear in the\n    observations the least squares estimators have the smallest\n    variance.\n\nThe estimator \\\\(\\hat{\\beta}\\_j\\\\), is normally distributed with mean\n    \\\\(\\beta\\j\\\\) and variance \\\\(\\sigma^2 c\\{jj}\\\\).\n\n\\\\(W = SSE/\\sigma^2\\\\) has a \\\\(\\chi^2\\\\) distribution with \\\\(n - p -1\\\\)\n    degrees of freedom, and \\\\(\\hat{\\beta}\\_j\\\\) and \\\\(\\hat{\\sigma}^2\\\\) are\n    distributed independently from each other.\n\nThe vector \\\\(\\hat{\\beta} = (\\hat{\\beta}\\0, \\hat{\\beta\\1}, \\dots,\n       \\hat{\\beta\\_p})\\\\) has a $(p+1)$-dimensional normal distribution with\n    mean vector \\\\(\\beta = (\\beta\\0, \\beta\\1, \\dots, \\beta\\_p)\\\\) and\n    variance-covariance matrix with elements \\\\(\\sigma^2  c\\_{ij}\\\\).\n\nImportant Questions in Multiple Regression Models {#important-questions-in-multiple-regression-models}\n\nWe can answer some important questions using the multiple regression\nmodel:\n\n1. Is there a relationship between the response and the predictors?\n\nThe strength of the linear relationship between \\\\(Y\\\\) and the set of\npredictors \\\\(X\\1, X\\2, \\dots X\\_p\\\\) can be assessed through the\nexamination of the scatter plot of \\\\(Y\\\\) versus \\\\(\\hat{Y}\\\\), and the\ncorrelation coefficient \\\\(\\mathrm{Cor}(Y, \\hat{Y})\\\\). The coefficient of\ndetermination \\\\(R^2 = [\\mathrm{Cor}(Y, \\hat{Y})]^2\\\\) may be interpreted\nas the proportion of total variability in the response variables \\\\(Y\\\\)\nthat can be accounted for by the set of predictor variables \\\\(X\\1, X\\2,\n\\dots, X\\_p\\\\).\n\nA quantity related to \\\\(R^2\\\\) knows as the adjusted R-squared, \\\\(R\\_a^2\\\\),\nis also used for judging the goodness of fit. It is defined as:\n\n\\begin{align}\n  R\\_a^2 &= 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)} \\\\\\\\\\\\\n        &= 1 - \\frac{n-1}{n-p-1}(1-R^2)\n\\end{align}\n\n\\\\(R\\_a^2\\\\) is sometimes used to compared models having different numbers\nof predictor variables.\n\nIf we do a hypothesis test on \\\\(H\\1 : \\beta\\j \\ne \\beta\\_j^0\\\\), we can do\na t-test:\n\n\\begin{equation}\n  t\\j = \\frac{\\hat{\\beta\\j} - \\beta\\j^0}{s.e.(\\hat{\\beta}\\j)}\n\\end{equation}\n\nwhich has a Stundent's t-distribution with \\\\(n-p-1\\\\) degrees of freedom.\nThe test is carried out by comparing the observed value with the\nappropriate critical value \\\\(t\\_{(n-p-1), \\alpha/2}\\\\).\n\nIf we are comparing \\\\(H\\0\\\\)  and \\\\(H\\a\\\\)\n, and we do so by computing the F-statistic:\n\n\\begin{equation} \\label{eqn:dfn:f-statistic}\n  F = \\frac{(\\mathrm{TSS} - \\mathrm{RSS})/p}{\\mathrm{RSS}/(n-p-1)}\n\\end{equation}\n\nIf the linear model is correct, one can show that:\n\n\\begin{equation}\nE \\\\{RSS/(n-p-1)\\\\} = \\sigma^2\n\\end{equation}\n\nand that provided \\\\(H\\_0\\\\) is true,\n\n\\begin{equation}\nE \\\\{(\\mathrm{TSS}-\\mathrm{RSS})/p\\\\} = \\sigma^2\n\\end{equation}\n\nHence, when there is no relationship between the response and the\npredictors, one would expect the F-statistic to be close to 1. if\n\\\\(H\\_a\\\\) is true, then we expect \\\\(F\\\\) to be greater than 1.\n\n2. Deciding on important variables\n\nIt is possible that all of the predictors are associated with the\nresponse, but it is more often the case that the response is only\nrelated to a subset of the predictors. The task of determining which\npredictors are associated is referred to as variable selection.\nVarious statistics can be used to judge the quality of a model. These\ninclude Mallow's \\\\(C\\_p\\\\), Akaike information criterion (AIC), Bayesian\ninformation criterion (BIC), and adjusted \\\\(R^2\\\\).\n\nThere are \\\\(2^p\\\\) models that contains a subset of \\\\(p\\\\) variables. Unless\n\\\\(p\\\\) is small, we cannot consider all \\\\(2^p\\\\) models, and we need an\nefficient approach to choosing a smaller set of models to consider.\nThere are 3 classical approaches for this task:\n\nForward selection: We begin with the null model -- a model that\n    contains an intercept but no predictors. We then fit p simple\n    linear regressions and add to the null model the variable that\n    results in the lowest RSS. We then add to that model the variable\n    that results in the lowest RSS for the new two-variable model, and\n    repeat.\nBackward selection: We start with all variables in the model, andd\n    remove the variable with the largest p-value -- that is the\n    variable that is the least statistically significant. The new\n    (p-1)-variable model is fit, and the variable with the largest\n    p-value is removed, and repeat.\nMixed selection. This is a combination of forward and\n    backward-selection. We once again start with the null model. The\n    p-values of the variables can become larger as new variables are\n    added to the model. Once the p-value of one of the variables in the\n    model rises above a certain threshold, they are removed.\n\n3. Model Fit\n\nTwo of the most common numerical measures of model fit are the RSE\nand \\\\(R^2\\\\), the fraction of variance explained.\n\nIt turns out that \\\\(R^2\\\\) will always increase when more variables are\nadded to the model, even if those variables are only weakly associated\nwith the response. This is because adding another variable to the\nleast squares equations must allow us to fit the training data more\naccurately. Models with more variables can have higher RSE if the\ndecrease in RSS is small relative to the increase in \\\\(p\\\\).\n\n4. Predictions\n\nOnce we have fit the multiple regression model, it is straightforward\nto predict the response \\\\(Y\\\\) on the basis of a set of values for\npredictors \\\\(X\\1, X\\2, \\dots, X\\_p\\\\). However, there are 3 sorts of\nuncertainty associated with this prediction:\n\nThe coefficient estimates \\\\(\\hat{\\beta\\0}, \\hat{\\beta\\1}, \\dots,\n       \\hat{\\beta\\p}\\\\) are estimates for \\\\(\\beta\\0, \\beta\\_1, \\dots,\n       \\beta\\_p\\\\).\n\nThat is, the least squares plane:\n\n\\begin{equation}\n  \\hat{Y} = \\hat{\\beta\\0} + \\hat{\\beta\\1}X\\1 + \\dots + \\hat{\\beta\\p}X\\_p\n\\end{equation}\n\n is only an estimate for the true\npopulation regression plane:\n\n\\begin{equation}\n  f(X) = \\beta\\0 + \\beta\\1 X\\1 + \\dots + \\beta\\p X\\_p\n\\end{equation}\n\n This inaccuracy is related to the reducible error. We can compute a\nconfidence interval in order to determine how close \\\\(\\hat{Y}\\\\) will be\nto \\\\(f(X)\\\\).\n\nIn practice, assuming a linear model for \\\\(f(X)\\\\) is almost always an\n    approximation of reality, so there is an additional source of\n    potentially reducible error which we call model bias.\n\nEven if we knew \\\\(f(X)\\\\), the response value cannot be predicted\n    perfectly because of the random error \\\\(\\epsilon\\\\) is the model. How\n    mich will \\\\(Y\\\\) vary from \\\\(\\hat{Y}\\\\)? We use prediction intervals to\n    answer the question. Prediction intervals are always wider than\n    confidence intervals, because they incorporate both the error in\n    the estimate for \\\\(f(X)\\\\), and the irreducible error.\n\nQualitative Variables {#qualitative-variables}\n\nThus far our discussion had been limited to quantitative variables.\nHow can we incorporate qualitative variables such as gender into our\nregression model?\n\nFor variables that take on only two values, we can create a dummy\nvariable of the form (for example in gender):\n\n\\begin{equation}\n  x\\_i = \\begin{cases}\n    1 & \\text{if ith person is female} \\\\\\\\\\\\\n    0 & \\text{if ith person is male}\n    \\end{cases}\n\\end{equation}\n\nand use this variable as a predictor in the equation. We can also use\nthe {-1, 1} encoding. For qualitative variables that take on more than\n2 values, a single dummy variable cannot represent all values. We can\nadd additional variables, essentially performing a one-hot encoding.\n\nExtending the Linear Model {#extending-the-linear-model}\n\nThe standard linear regression model provides interpretable results\nand works quite well on many real-world problems. However, it makes\nhighly restrictive assumptions that are often violated in practice.\n\nThe two most important assumptions of the linear regression model are\nthat the relationship between the response and the predictors are:\n\nadditive: the effect of changes in a predictor \\\\(X\\_j\\\\) on the\n    response \\\\(Y\\\\) are independent of the values of the other predictors.\nlinear: the change in the response \\\\(Y\\\\) due to a one-unit change in\n    \\\\(X\\j\\\\) is constant, regardless of the value of \\\\(X\\j\\\\)\n\nHow can we remove the additive assumption? We can add an interaction\nterm for two variables \\\\(X\\i\\\\) and \\\\(X\\j\\\\) as follows:\n\n\\begin{equation}\n\\hat{Y\\2} = \\hat{Y\\1} + \\beta\\{p+1} X\\i X\\_j\n\\end{equation}\n\nWe can analyze the importance of the interaction term by looking at\nits p-value. The hierarchical principle states that if we include an\ninteraction in a model, we should also include the main effects, even\nif the p-values associated with their coefficients are not\nsignificant.\n\nHow can we remove the assumption of linearity? A simple way is to use\npolynomial regression.\n\nPotential Problems {#potential-problems}\n\nWhen we fit a linear regression model to a particular data set, many\nproblems may occur. Most common among these are the following:\n\nNon-linearity of the response-predictor relationships\nCorrelation of error terms\nNon-constant variance of error terms\nOutliers\nHigh-leverage points\nCollinearity\n\nNon-linearity of the Data\n\n    The assumption of a linear relationship between response and\n    predictors aren't always true. Residual plots are a useful graphical\n    tool for identifying non-linearity. This is obtained by plotting the\n    residuals \\\\(e\\i = y\\i - \\hat{y\\i}\\\\) versus the predictor \\\\(x\\i\\\\). Ideally\n    the residual plot will show no discernible pattern. The presence of a\n    pattern may indicate a problem with some aspect of the linear model.\n\n    If the residual plots show that there are non-linear associations in\n    the data, then a simple approach is to use non-linear transformations\n    of the predictors, such as \\\\(\\log X\\\\), \\\\(\\sqrt{X}\\\\) and \\\\(X^2\\\\).\n\nCorrelation of Error Terms\n\n    An important assumption of the linear regression model is that the\n    error terms, \\\\(\\epsilon\\1, \\epsilon\\2, \\dots, \\epsilon\\_p\\\\) are\n    uncorrelated.  The standard errors for the estimated regression\n    coefficients are computed based on this assumption. This is mostly\n    mitigated by proper experiment design.\n\nNon-constant Variance of Error Terms\n\n    Variances of the error terms may increase with the value of the\n    response. One can identify non-constant variances in the errors, or\n    heteroscedasticity, from the presence of a funnel shape in the\n    residual plot.\n\n    {{(James et al., 2013)\" >}}\n\nOutliers\n\n    An outlier is a point from which \\\\(y\\_i\\\\) is far from the value predicted\n    by the model.  It is atypical for an outlier that does not have an\n    unusual predictor value to have little effect on the least squares\n    fit. However, it can cause other problems, such as dramatically\n    altering the computed values of RSE, \\\\(R^2\\\\) and p-values.\n\n    Residual plots can clearly identify outliers. One solution is to\n    simply remove the observation, but care must be taken to first\n    identify whether the outlier is indicative of a deficiency with the\n    model, such as a missing predictor.\n\nHigh Leverage Points\n\n    Observations with high leverage have an unusual value for \\\\(x\\_i\\\\).\n    High leverage observations typically have a substantial impact on the\n    regression line. These are easy to identify, by looking for values\n    outside the normal range of the observations. We can also compute the\n    leverage statistic. for a simple linear regression:\n\n    \\begin{equation}\n      h\\i = \\frac{1}{n} + \\frac{(x\\i - \\bar{x\\i})^2}{\\sum\\{i' =\n          1}^n(x\\_{i'} - \\bar{x})^2}\n    \\end{equation}\n\nCollinearity\n\n    Collinearity refers to the situation in which two or more predictor\n    variables are closely related to one another. The presence of\n    collinearity can pose problems in the regression context: it can be\n    difficult to separate out the individual effects of collinear\n    variables on the response. A contour plot of the RSS associated with\n    different possible coefficient estimates can show collinearity.\n\n    {{(James et al., 2013)\" >}}\n\n    Another way to detect collinearity is to look at the correlation\n    matrix of the predictors. An element of this matrix that is large in\n    absolute value indicates a pair of highly correlated variables, and\n    therefore a collinearity problem in the data.\n\n    Not all collinearity problems can be detected by inspection of the\n    correlation matrix: it is possible for collinearity to exist between\n    three or more variables even if no pairs of variables has a\n    particularly high correlation. This situation is called\n    multicollinearity. We instead compute the variance inflation factor\n    (VIF). The VIF is the ratio of the variance of \\\\(\\hat{\\beta\\_j}\\\\) when\n    fitting the full model divided by the variance of \\\\(\\hat{\\beta\\_j}\\\\) if\n    fit on its own. The smallest possible value for VIF is 1, indicating a\n    complete absence of collinearity. As a rule of thumb, a VIF exceeding\n    values of 5 or 10 indicates a problematic amount of collinearity.\n\n    \\begin{equation} \\label{eqn:dfn:vif}\n      \\mathrm{VIF}(\\hat{\\beta\\j}) = \\frac{1}{1-R^2\\{X\\j|X\\{-j}}}\n    \\end{equation}\n\n    where \\\\(R^2\\{X\\j|X\\{-j}}\\\\) is the regression of \\\\(X\\j\\\\) onto all of the\n    other predictors.\n\n    The data consists of \\\\(n\\\\) observations on a dependent or response\n    variable \\\\(Y\\\\), and \\\\(p\\\\) predictor or explanatory variables. The\n    relationship between \\\\(Y\\\\) and \\\\(X\\1, X\\2, \\dots, X\\_p\\\\) is represented by:\n\nLinear Basis Function Models {#linear-basis-function-models}\n\nWe can extend the class of models by considering linear combinations of fixed non-linear functions of the input variables, of the form:\n\n\\begin{equation}\n  y(\\mathbf{x}, \\mathbf{w}) = w\\0 + \\sum\\{j=1}^{M-1} w\\_j\n  \\phi\\_j(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})\n\\end{equation}\n\nThere are many choices on non-linear basis functions, such as the\nGaussian basis function:\n\n\\begin{equation}\n  \\phi\\j(x) = \\mathrm{exp}\\left\\\\{ - \\frac{(x - \\mu\\j)^2}{2s^2} \\right\\\\}\n\\end{equation}\n\nor the sigmoidal basis function:\n\n\\begin{equation}\n\\phi\\j(x) = \\sigma\\left( \\frac{x - \\mu\\j}{s} \\right)\n\\end{equation}\n\nRegression Diagnostics {#regression-diagnostics}\n\nIn fitting a model to a given body of data, we would like to ensure\nthat the fit is not overly determined by one or a few observations.\nThe distribution theory, confidence intervals, and tests of hypotheses\nare valid and have meaning only if the standard regression assumptions\nare satisfied.\n\nThe Standard Regression Assumptions {#the-standard-regression-assumptions}\n\nAssumptions about the form of the model: The model that relates the\n   response \\\\(Y\\\\) to the predictors \\\\(X\\1, X\\2, \\dots, X\\_p\\\\) is assumed to\n   be linear in the regression parameters \\\\(\\beta\\0, \\beta\\1, \\dots,\n   \\beta\\_p\\\\) respectively:\n\n\\begin{equation}\n  Y = \\beta\\0 + \\beta\\1X\\1 + \\dots + \\beta\\pX\\_p + \\epsilon\n\\end{equation}\n\nwhich implies that the ith observation can be written as:\n\n\\begin{equation}\n  y\\{i} = \\beta\\0 + \\beta\\1x\\{i1} + \\dots \\beta\\px\\{ip} + \\epsilon\\_i,\n  i = 1,2,\\dots,n\n\\end{equation}\n\nChecking the linearity assumption can be done by examining the scatter\nplot of \\\\(Y\\\\) versus \\\\(X\\\\), but linearity in multiple regression is more\ndifficult due to the high dimensionality of the data. In some cases,\ndata transformations can lead to linearity.\n\nAssumptions about the errors: The errors \\\\(\\epsilon\\1, \\epsilon\\2,\n  \\dots, \\epsilon\\_n\\\\) are assumed to be independently and identically\n  distributed normal random variables each with mean zero and a\n  common variance \\\\(\\sigma^2\\\\). Four assumptions are made here:\n\nThe error \\\\(\\epsilon\\_i\\\\) has a normal distribution. This normality\n    assumption can be validated by examining appropriate graphs of\n    the residuals.\n\nThe errors \\\\(\\epsilon\\_i\\\\) have mean 0.\n\nThe errors \\\\(\\epsilon\\_i\\\\) have the same (but unknown) variance\n    \\\\(\\sigma^2\\\\). This is the constant variance assumption, also known\n    as the homogeneity or homoscedasticity assumption. When this\n    assumption does not hold, the problem is called the heterogeneity\n    or the heteroscedasticity problem.\n\nThe errors \\\\(\\epsilon\\_i\\\\) are independent of each other. When this\n    assumption doesn't hold, we have the auto-correlation problem.\n\nAssumption about the predictors: Three assumptions are made here:\n\nThe predictor variables \\\\(X\\1, X\\2, \\dots, X\\_p\\\\) are nonrandom. This\n    assumption is satisfied only when the experimenter can set the\n    values of the predictor variables at predetermined levels. When\n    the predictor variables are random variables, all inferences are\n    conditional, conditioned on the observed data.\n\nThe values \\\\(x\\{1j}, x\\{2j}, \\dots, x\\_{nj}\\\\) are measured without\n    error. This assumption is hardly ever satisfied, and errors in\n    measurement will affect the residual variance, the multiple\n    correlation coefficient, and the individual estimates of the\n    regression coefficients. Correction for measurement errors of the\n    estimated regression coefficients, even in the simplest case\n    where all measurement errors are uncorrelated, requires a\n    knowledge of the ratio between the variances and the random\n    error. These quantities are seldom known.\n\nThe predictor variables \\\\(X\\1, X\\2, \\dots, X\\_p\\\\) are assumed to be\n    linearly independent of each other. This assumption is required\n    to guarantee the uniqueness of the least squares solution. If\n    this assumption is violated, the problem is referred to as the\n    collinearity problem.\n\nAssumption about the observations: All observations are equally\nreliable and have an approximately equal role in determining the\nregression results.\n\nA feature of the method of least squares is that minor violations of\nthe underlying assumptions do not invalidate the inferences or\nconclusions drawn from the analysis in a major way. However, gross\nviolations can severely distort conclusions.\n\nTypes of Residuals {#types-of-residuals}\n\nA simple method for detecting model deficiencies in regression\nanalysis is the examination of residual plots. Residual plots will\npoint to serious violations in one or more of the standard assumptions\nwhen they exist. The analysis of residuals may lead to suggestions of\nstructure or point to information in the data that might be missed or\noverlooked if the analysis is based only on summary statistics.\n\nWhen fitting the linear model to a set of data by least squares, we\nobtain the fitted values:\n\n\\begin{equation}\n  \\hat{y}\\i = \\hat{\\beta}\\0 + \\hat{\\beta}1 x\\_{i1} +\\dots +\n  \\hat{\\beta}\\p x\\{ip}\n\\end{equation}\n\nand the corresponding ordinary least squares residuals:\n\n\\begin{equation}\ne\\i = y\\i - \\hat{y}\\_i\n\\end{equation}\n\nThe fitted values can also be written in an alternative form as:\n\n\\begin{equation}\n\\hat{y\\i} = p\\{i1}y\\1 + p\\{i2}y\\2 + \\dots + p\\{in}y\\_n\n\\end{equation}\n\nwhere the \\\\(p\\_{ij}\\\\) are quantities that depend only on the values of\nthe predictor variables. In simple regression \\\\(p\\_{ij}\\\\) is given by:\n\n\\begin{equation}\n  p\\{ij} = \\frac{1}{n} + \\frac{\\left( x\\i + \\overline{x} \\right)\n    \\left( x\\j - \\overline{x} \\right)}{\\sum (x\\i - \\overline{x})^2}\n\\end{equation}\n\nIn multiple regression the \\\\(p\\_{ij}\\\\) are elements of matrix known as\nthe hat or projection matrix.\n\nThe value \\\\(p\\_{ii}\\\\) is called the leverage value for the ith\nobservation, because \\\\(\\hat{y}\\_i\\\\) is a weighted sum of all the\nobservations in \\\\(Y\\\\) and \\\\(p\\_{ii}\\\\) is the weight (leverage) given to\n\\\\(y\\i\\\\) in determining the ith fitted value \\\\(\\hat{y}\\i\\\\). Thus, we have\n\\\\(n\\\\) leverage values, and they are denoted by:\n\n\\begin{equation}\n  p\\{11}, p\\{22}, \\dots, p\\_{nn}\n\\end{equation}\n\nWhen the standard assumptions hold, the ordinary residuals, \\\\(e\\1, e\\2,\n\\dots, e\\_n\\\\) will sum to zero, but they will not have the same variance\nbecause:\n\n\\begin{equation}\n  \\textrm{Var}(e\\i) = \\sigma^2\\left( 1 - p\\{ii} \\right)\n\\end{equation}\n\nTo over come the problem of unequal variances, we standardize the ith\nresidual \\\\(e\\_i\\\\) by dividing by its standard deviation:\n\n\\begin{equation}\n  z\\i = \\frac{e\\i}{\\sigma \\sqrt{1 - p\\_{ii}}}\n\\end{equation}\n\nThis is called the ith standardized residual because it has mean zero\nand standard deviation 1. The standardized residuals depend on\n\\\\(\\sigma\\\\), the unknown standard deviation of \\\\(\\epsilon\\\\). An unbiased of\n\\\\(\\sigma^2\\\\) is given by:\n\n\\begin{equation}\n  \\hat{\\sigma}^2 = \\frac{\\sum e\\i^2}{n - p - 1} = \\frac{\\sum (y\\i -\n    \\hat{y}\\_i)^2}{n - p - 1} = \\frac{\\textrm{SSE}}{n -p-1}\n\\end{equation}\n\nAn alternative unbiased estimate of \\\\(\\sigma^2\\\\) is given by:\n\n\\begin{equation}\n\\hat{\\sigma}\\{(i)}^2 = \\frac{SSE\\{(i)}}{n - p -2}\n\\end{equation}\n\nwhere \\\\(SSE\\_{(i)}\\\\) is the sum of squared residuals when we fit the\nmodel to the n - 1 observations by omitting the ith observation.\n\nUsing \\\\(\\hat{\\sigma}\\\\) as an estamite of \\\\(\\sigma\\\\), we obtain:\n\n\\begin{equation}\n  r\\i = \\frac{e\\i}{\\hat{\\sigma}\\sqrt{1 - p\\_{ii}}}\n\\end{equation}\n\nwhich we term the internally studentized residual. Using teh other\nunbiased estimate, we get:\n\n\\begin{equation}\n  r\\i^\\* = \\frac{e\\i}{\\hat{\\sigma\\{(i)}}\\sqrt{1 - p\\{ii}}}\n\\end{equation}\n\nwhich is a monotonic transformation of \\\\(r\\_i\\\\). This is termed the\nexternally studentized residual. The standardized residuals do not sum\nto zero, but they all have the same variance.\n\nThe externally standardized residuals follow a t-distribution with n - p - 2 degrees\nof freedom, but the internally standardized residuals do not. However, with a\nmoderately large sample, these residuals should approximately have a standard\nnormal distribution. The residuals are not strictly independently distributed, but\nwith a large number of observations, the lack of independence may be\nignored.\n\nGraphical methods {#graphical-methods}\n\nBefore fitting the model\n\n    Graphs plotted before fitting the model serve as exploratory tools.\n    There are four categories of graphs:\n\n    1. One-dimensional graphs\n\n    One-dimensional graphs give a general idea of the distribution of each\n    individual variable. One of the following graphs may be used:\n\n    histogram\n    stem-and-leaf display\n    dot-plot\n    box-plot\n\n    These graphs indicate whether the variable is symmetric, or skewed.\n    When a variable is skewed, it should be transformed, generally\n    using a logarithmic transformation. Univariate graphs also point out\n    the presence of outliers in the variables. However, no observations\n    should be deleted at this stage.\n\n    2. Two-dimensional graphs\n\n    We can take the variables in pairs and look at the scatter plots of\n    each variable versus each other variable in the data set. These\n    explore relationships between each pair of variables and identify\n    general patterns. These pairwise plots can be arranged in a matrix\n    format, known as the draftsman's plot or the plot matrix. In simple\n    regression, we expect the plot of \\\\(Y\\\\) versus \\\\(X\\\\) to show a linear\n    pattern. However, scatter plots of \\\\(Y\\\\) versus each predictor variable\n    may or may not show linear patterns.\n\n    Beyond these, there are rotation plots and dynamic graphs which serve\n    as powerful visualizations of the data in more than 2 dimensions.\n\nGraphs after fitting a model\n\n    The graphs after fitting a model help check the assumptions and assess\n    the adequacy of the fit of a given model.\n\n    Graphs checking linearity and normality assumptions\n\n    When the number of variables is small, the assumption of linearity can\n    be checked by interactively and dynamically manipulating plots\n    discussed in the previous section. However, this quickly becomes\n    difficult with many predictor variables. Plotting the standardized\n    residuals can help check the linearity and normality assumptions.\n\n    Normal probability plot of the standardized residuals: This is a\n        plot of the ordered standardized residuals versus the normal scores.\n        The normal scores are what we would expect to obtain if we take a\n        sample of size \\\\(n\\\\) from a standard normal distribution. If the\n        residuals are normally distributed, the ordered residuals should be\n        approximately the same as the ordered normal scores. Under the\n        normality assumption, this plot should resemble a straight line with\n        intercept zero and slope of 1.\n\n    Scatter plots of the standardized residual against each of the\n        predictor variables: Under the standard assumptions, the\n        standardized residuals are uncorrelated with each of the predictor\n        variables. If the assumptions hold, the plot should be a random\n        scatter of points.\n\n    Scatter plot of the standardized residual versus the fitted values:\n        Under the standard assumptions, the standardized residuals are also\n        uncorrelated with the fitted values. Hence, this plot should also be\n        a random scatter of points.\n\n    Index plot of the standardized residuals: we display the\n        standardized residuals versus the observation number. If the order\n        in which the observations were taken is immaterial, this plot is not\n        needed. However, if the order is important, a plot of the residuals\n        in serial order may be used to check the assumption of the\n        independence of the errors.\n\n    {{}}\n\nReferences {#references}\n\n(Samprit Chatterjee \\& Ali Hadi, 2006), (James et al., 2013)\n\nBibliography\nJames, G., Witten, D., Hastie, T., & Tibshirani, R., An introduction to statistical learning (2013), : Springer. ↩\n\nChatterjee, S., & Hadi, A. S., Regression analysis by example (2006), : John Wiley \\& Sons, Inc. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/reinforcement_learning",
        "title": "Reinforcement Learning ⭐",
        "content": "\ntags\n: §machine\\_learning\n\nReinforcement Learning is the field of learning and decision-making\nunder uncertainty. An agent acts with a learnable behaviour policy in\nan environment with initially unknown dynamics and reward. The agent\nobserves the environment's state (sometimes partially: §pomdp) and\nchooses an action.\n\nbehaviour\n: \\\\(\\Pi(a | s)\\\\)\n\naction\n: \\\\(a\\_t \\in A\\\\)\n\nreward\n: \\\\(r\\_{t+1} \\in R\\\\)\n\nstate\n: \\\\(s\\_{t+1} \\in S\\\\)\n\ndynamics\n: \\\\(T(s\\{t+1} | s\\t, a\\_t)\\\\)\n\nreward\n: \\\\(R(r\\{t+1} | s\\t, a\\_t)\\\\)\n\nThe dynamics and rewards here reflect the §markovian\\_assumption.\nStates can be represented in many ways: in a tabular fashion (state 1\n\\\\(\\rightarrow\\\\) state 2) or as features.\n\nKey Challenges {#key-challenges}\n\nExploration vs Exploitation\nFunction approximation\nCredit Assignment\n\nMaking Complex Decisions {#making-complex-decisions}\n\nEarlier, we were concerned with environments with one-shot, episodic\ndecision problems. Sequential decision problems incorporate utilities,\nuncertainty and sensing. These include searching and planning problems\nas special cases.\n\nUtilities over time {#utilities-over-time}\n\nA finite horizon for decision making means that there is a fixed time\n\\\\(N\\\\) after which nothing matters. In these scenarios, the optimal\naction in a given state could change over time, i.e. the optimal\npolicy is non-stationary.\n\nIt turns out that under stationarity, there are only 2 coherent ways\nto assign utilities to sequences:\n\nadditive rewards\n: \\\\(U\\h([s\\0, s\\1, \\dots, s\\n]) = R(s\\0) + R(s\\1) + \\dots + R(s\\_n)\\\\)\n\ndiscounted rewards\n: \\\\(U\\h([s\\0, s\\1, \\dots, s\\n]) = R(s\\0) + \\gamma \\cdot R(s\\1) + \\dots +\n         \\gamma^2 \\cdot R(s\\_n)\\\\)\n\nThis discount factor \\\\(\\gamma\\\\) is a number between 0 and 1. Assuming\nstationarity has several problems. First, if the environment does not\ncontain a terminal state, then utilities of undiscounted rewards go to\ninfinite, and comparing two infinitely state sequences would be\nimpossible. With discounted rewards, the utility of an infinite\nsequence can be made finite.\n\nHowever, if the environment contains a terminal state, and the agent\nis guaranteed to reach a terminal state eventually, then this policy\nis called a proper policy, and the above issue goes away. Infinite\nsequences can be compared in terms of the average reward obtained per\ntime step.\n\nOptimal policies and the utilities of states {#optimal-policies-and-the-utilities-of-states}\n\nFirst, we can derive the expected utility of executing a policy \\\\(\\pi\\\\) in\n\\\\(s\\\\):\n\n\\begin{equation}\nU^\\pi (s) = \\mathbb{E} \\left[ \\sum\\{t=0}^\\infty \\gamma^t R(S\\t) \\right]\n\\end{equation}\n\nwhere the expectation is with respect to the probability distribution\nover state sequences. determined by \\\\(s\\\\) and \\\\(\\pi\\\\). Then $&pi;^\\*(s) =\nargmax\\_&pi; U^&pi; (s)\\*.\n\nA consequence of using discounted utilities with infinite horizons is\nthat the optimal policy is independent of the starting state. This\nallows us to compute the true utility of the state as \\\\(U^{\\pi^\\*} (s)\\\\).\nThe utility function allows the agent to select actions by using the\nprinciple of maximum expected utility from the earlier chapter: \\\\(\\pi^\\*(s)\n= argmax\\{a \\in A(s) } \\sum\\{s^{i}} P(s' |s, a)U(s')\\\\).\n\nSummary {#summary}\n\n| Problem    | Bellman Equation                                         | Algorithm                   |\n|------------|----------------------------------------------------------|-----------------------------|\n| Prediction | Bellman Expectation Equation                             | Iterative Policy Evaluation |\n| Control    | Bellman Expectation Equation + Greedy Policy Improvement | Policy Iteration            |\n| Control    | Bellman Optimality Equation                              | Value Iteration             |\n\nPassive Reinforcement Learning {#passive-reinforcement-learning}\n\nWe start with a passive learning agent using a state-based\nrepresentation in a fully observable environment.\n\nIn passive learning, the agent's policy \\\\(\\pi\\\\) is fixed: in state \\\\(s\\\\), it\nalways executes the action \\\\(\\pi(s)\\\\). Its goal is simply to learn how\ngood the policy is -- the utility function \\\\(U^\\pi (s)\\\\).\n\nThe passive learning task is similar to policy evaluation, but the\nagent does not know the transition model \\\\(P(s'|s, a)\\\\) and the reward\nfunction \\\\(R(s)\\\\).\n\nThe agent executes a number of trials using the policy \\\\(\\pi\\\\), and\nexperiences a sequence of state transitions. At each state its\npercepts receives the current state and the reward of the state.\n\nWe write the utility as:\n\n\\begin{equation}\n  U^\\pi (s) = E\\left[\\sum\\{t=0}^\\infty \\gamma^t R(S\\t) \\right]\n\\end{equation}\n\nDirect Utility Estimation (MC Learning) {#direct-utility-estimation--mc-learning}\n\nThe main idea of direct utility estimation is that the utility of a\nstate is the expected total reward from that state onward, and each\ntrial provides a sample of this quantity for each state visited.\n\nDirect utility estimation reduces the reinforcement learning problem\nto a supervised inductive learning problem, where each example has the\nstate as input, and the observed reward-to-go as output.\n\nHowever, it misses an important source of information: that the\nutility of states are not independent. This means it misses many\nopportunities for learning. For example, if a state has high expected\nutility, then neighbouring states should also have high expected\nutility.\n\nThe utility of each state equals its own reward plus the expected\nutility of its successor states: i.e. it obeys the Bellman Equation\nfor a fixed policy.\n\nWe can view directed utility estimation as searching for \\\\(U\\\\) in a\nhypothesis space that is much larger than it needs to be, since it\nincludes many functions that violate the Bellman equations.\n\nAdaptive Dynamic Programming {#adaptive-dynamic-programming}\n\nAn ADP agent takes advantage of the constraints among the utilities of\nstates by learning the transition model that connects them and solving\nthe corresponding MDP using a dynamic programming method.\n\nFor a passive learning agent, the task is as simply as plugging in the\nlearnt transition model and the rewards into the Bellman equations to\ncalculate the utility of each state.\n\nThe task of learning the model is easy, because the environment is\nfully observable. This means we have a supervised learning task where\nthe input is a state-action pair, and the output is the resulting\nstate. We keep track of how often each action outcome occurs and\nestimate the transition probability \\\\(P(s' | s, a)\\\\) from the frequency\nwith which \\\\(s'\\\\) is reached when executing \\\\(a\\\\) in \\\\(s\\\\).\n\nfunction PASSIVE-ADP_AGENT(percept) returns an action\n  inputs: percept, indicating state s' and reward signal r'\n  persistent: \\pi, a fixed policy\n    mdp: MDP with model P, rewards R, and discount \\gamma\n    U: a table of utilities, initially empty\n    N_{sa}: a table of frequencies for each state-action pair\n    N_{s'|s,a}: a table of outcome frequencies\n    s, a: the previous state and action\n  if s' is new then $U[s']\n  Code Snippet 1:\n  A passive RL agent based on ADP.\n\nThis approach is computationally intractable for large state spaces.\nIn addition, it uses the maximum-likelihood estimation for learning\nthe transition model.\n\nA more nuanced approach would be Bayesian reinforcement learning,\nwhich assumes a prior probability \\\\(P(h)\\\\) for each hypothesis \\\\(h\\\\) about\nwhat the true model is. The posterior probability \\\\(P(h|e)\\\\) is obtained\nvia Bayes' rule. Then \\\\(\\pi^\\* = argmax\\\\pi \\sum\\h P(h|e) u\\_h^\\pi\\\\).\n\nAnother approach, derived from robust control theory, allows for a set\nof possible models \\\\(H\\\\) and defines an optimal robust policy as one\nthat gives the best outcome in the worst case over \\\\(H\\\\): \\\\(\\pi^\\* =\nargmax\\\\pi min\\h u\\_h^\\pi\\\\).\n\nTemporal-difference Learning {#temporal-difference-learning}\n\nTD learning involves using the observed transitions to adjust the\nutilities such that the constraint equations are met.\n\nWhen a transition occurs from state \\\\(s\\\\) to state \\\\(s'\\\\), we apply the\nupdate rule:\n\n\\begin{equation}\n  U^\\pi(s') \\leftarrow U^\\pi(s) + \\alpha (R(s) + \\gamma U^\\pi(s') -U^\\pi(s))\n\\end{equation}\n\nWhere \\\\(\\alpha\\\\) is the learning rate. The difference in utilities gives rise\nto the name temporal-difference.\n\nfunction PASSIVE-TD-AGENT(percept) returns an action\n  inputs: percept, with current state s' and reward r'\n  persistent: \\pi, a fixed policy\n    U, a table of utilities, initially empty\n    N_s, a table of frequencies\n    s, a, r, the previous state, action and reward\n\n  if s' is new then U[s'] }})\n§mcts\n§deep\\_rl\n§td\\_learning\n§policy\\_gradients\n§actor\\_critic\n§q\\_learning\n",
        "tags": [
            "machine-learning"
        ]
    },
    {
        "uri": "/zettels/rejection_sampling",
        "title": "Rejection Sampling",
        "content": "\nGoal: To sample from unknown distribution \\\\(p(x)\\\\).\n\nAssumptions {#assumptions}\n\nwe cannot sample from \\\\(p(x)\\\\)\nwe have a simple proposal density \\\\(q(x)\\\\) we can evaluate within a\n    multiplicative factor \\\\(Z\\_q\\\\)\nWe know the value of a constant \\\\(c\\\\), such that \\\\(cq^\\star(x) >\n      p^\\star(x)\\\\) for all \\\\(x\\\\)\n\nMethod {#method}\n\nSample from proposal density \\\\(q^\\star(x)\\\\)\nEvaluate \\\\(c q^\\star(x)\\\\)\nGenerate a uniform distribution \\\\([0, c q^\\star(x)]\\\\) and sample \\\\(u\\\\)\n    from it\nEvaluate \\\\(p^\\star(x)\\\\)\nIf \\\\(u > p^\\star(x)\\\\), reject, else accept, and add \\\\(x\\\\) to the set of\n    samples\n\nDifficulties {#difficulties}\n\nWorks well only when \\\\(q(x)\\\\) is a good approximation to \\\\(p(x)\\\\),\n    keeping \\\\(c\\\\) small\nIn high-dimensional settings, \\\\(c\\\\) will generally be so large that\n    acceptances will be rare\n",
        "tags": []
    },
    {
        "uri": "/zettels/riken_aip_2019",
        "title": "Riken AIP Workshop 2019",
        "content": "\nWeakly Supervised Classification {#weakly-supervised-classification}\n\nMotivation {#motivation}\n\nMachine learning from big data is already successful\nIn some cases, massive labelled data is not available\nClassification from limited information\n\nSupervised Classification {#supervised-classification}\n\nA large number of labeled samples yield better classification\nperformance.\nOptimal convergence rate is \\\\(O(n^{-\\frac{1}{2}})\\\\).\n\nUnsupervised Classification {#unsupervised-classification}\n\nSince collecting labelled samples is costly, we can learn a classifier\nfrom unlabelled data. This is equivalent to clustering\n\nSemi-supervised Classification {#semi-supervised-classification}\n\nUse a large number of unlabelled samples and a small number of\n    labelled samples.\nFind a decision boundary along cluster structure induced by\n    unlabelled samples.\n\nPositive Unlabelled Classification {#positive-unlabelled-classification}\n\nGiven positive and unlabelled samples:\n\n\\begin{equation}\n{x\\i^P}\\{i=1}^{n\\_P} \\sim P(x | y = + 1)\n\\end{equation}\n\n\\begin{equation}\n  {x\\i^U}\\{i=1}^{n\\_U} \\sim P(x)\n\\end{equation}\n\nRisk of classifier can be decomposed into two terms:\n\nRisk for positive data\nRisk for negative data\n\nSince we do not have negative data in the positive unlabelled data in\nthe PU setting, the risk cannot be directly estimated.\n\nU-density is a mixture of positive and negative densities:\n\n\\begin{equation}\n  R(f) = \\pi E\\{p(x|y=+1)} \\left[ l(f(x)) \\right] + (1-\\pi) E\\{p(x|y=-1)}\\left[ l(-f(x)) \\right]\n\\end{equation}\n\nThrough this we can find an unbiased risk estimator.\n\nEstimating error bounds, we can show that PU learning can be better\nthan PN provided a large number of PU data.\n\nPNU Classification {#pnu-classification}\n\nTrain PU, PN, and NU classification, and combine them.\nUnlabelled data always helps without cluster assumptions\nUse unlabelled data for loss evaluation (reducing the bias), not for\n    regularisation.\n\nPconf Classification {#pconf-classification}\n\nOnly positive data is available:\n\ndata from rival companies cannot be obtained\nOnly successful examples are available\n\nIf we have positive data with confidence, we can train a classifier.\n\nOthers: Similar-unlabelled etc.\n\nFast Computation of Uncertainty in Deep Learning {#fast-computation-of-uncertainty-in-deep-learning}\n\nauthor\n: Emtiyaz Khan\n\nlinks\n:\n\nUncertainty quantifies the confidence in the prediction of a model,\ni.e., how much it does not know.\n\nUncertainty in Deep Learning {#uncertainty-in-deep-learning}\n\n\\begin{equation}\n  p(D|\\theta) = \\prod\\{i=1}^{N} p(y\\i | f\\\\theta (x\\i))\n\\end{equation}\n\nData given parameters,  output given NN(input)\n\nGenerate a prior distribution \\\\(\\theta \\sim p(\\theta)\\\\)\n\nApproximating Inference with Gradients {#approximating-inference-with-gradients}\n\n\\begin{equation}\n  p(\\theta | D) \\approx q(\\theta) = N(\\theta | \\mu, \\sigma^2)\n\\end{equation}\n\nFind the \\\\(\\mu\\\\) and \\\\(\\sigma^2\\\\) such that \\\\(q\\\\) is close to the posterior distribution.\n\n\\begin{equation}\n  max L(\\mu, \\sigma^2) = E\\_q\\left[ \\log \\frac{p(\\theta)}{q(\\theta)} \\right] +\n  \\sum\\{i=1}^N E\\q \\left[ \\log p(D\\_i|\\theta) \\right]\n\\end{equation}\n\nUsing natural-gradients leads to faster and simpler algorithm than\ngradients methods.\n\nData-efficient Probabilistic Machine Learning {#data-efficient-probabilistic-machine-learning}\n\nBryan Low\n\nGaussian Process (GP) Models for Big Data.\n\nGaussian Process {#gaussian-process}\n\nIs a rich class of Bayesian, non-parametric models\nA GP is a collection of rvs any finite subset of which belongs to a\n    univariate\n\nTask Setting {#task-setting}\n\nAgent explores unknown environment modelled by GP\nEvery location has a reward\n\nLipschitz Continuous Reward Functions {#lipschitz-continuous-reward-functions}\n\n\\begin{equation}\n  R(z\\t, s\\t) \\overset{\\Delta}{=}  R\\1(z\\t) + R\\2(z\\t) + R\\3(s\\t)\n\\end{equation}\n\nR\\_1 Lipschitz continuous (current measurement)\nR\\_2 Lipschitz continuous after convolution with Gaussian kernel (current measurement)\nR\\_3 Location History, independent of current measurement\n",
        "tags": []
    },
    {
        "uri": "/zettels/ritual",
        "title": "Daily Ritual",
        "content": "\n| Time             | Activity                      |\n|------------------|-------------------------------|\n| 10.00pm - 1.30am | Core Nap                      |\n| 1.30am - 2.00am  | Meditation                    |\n| 2.00am - 5.00am  | Side Projects                 |\n| 5.00am - 5.30am  | Nap 1                         |\n| 5.30am - 6.30am  | Morning Run + Washup          |\n| 6.30am - 7.00am  | Breakfast + Coffee            |\n| 7.00am - 7.30am  | Review day agenda/Clear Inbox |\n| 7.00am - 7.30am  | Nap 2                         |\n| 8.00am - 5.30pm  | School work (2pm Nap 3)       |\n| 5.30pm - Dinner  | Elements/Rings                |\n| Dinner - 9.00pm  | Reading                       |\n| 9.00pm - 10.00pm | Wash up/Wind down             |\n",
        "tags": []
    },
    {
        "uri": "/zettels/roam_research",
        "title": "Roam Research",
        "content": "\nwebpage\n: Roam Research\n\ntags\n: Productivity, Writing, Note-taking\n\nRoam research is a tool for networked-thought, created by [Conor\nWhite-Sullivan]({{}}). Roam adopts methods from Zettelkasten.\n\nRoam Media {#roam-media}\n\nAdam Keesling's Roam Thread\n[Building a Second Brain in Roam...And Why You Might Want To :\n    RoamResearch](https://reddit.com/r/RoamResearch/comments/eho7de/building%5Fa%5Fsecond%5Fbrain%5Fin%5Froamand%5Fwhy%5Fyou%5Fmight)\nRoam: Why I Love It and How I Use It - Nat Eliason\n\nRoam Tutorials {#roam-tutorials}\n\n[How to Use Roam to Outline a New Article in Under 20 Minutes -\n    YouTube](https://www.youtube.com/watch?v=RvWic15iXjk) by Nat Eliason\n    Basic Idea is to pull out related notes from other sources (books,\n        articles etc.)\n    Group them up into common themes (e.g. \"Ease is not the Goal\")\n    Polish after pulling out relevant notes\n[Roam for Personal Knowledge Management: Zettelkasten Method in Roam\n    Research ...](https://www.youtube.com/watch?v=ljyo%5FWAJevQ)\n    Highly similar to the above Nat Eliason video.\n    He uses file-tag metadata as well, \"in what context do I want to\n        see this idea?\"\n",
        "tags": []
    },
    {
        "uri": "/zettels/robot_kinematics",
        "title": "Robot Kinematics",
        "content": "\nKinematics is the calculus describing the effect of control actions on\nthe configuration of the robot. The configuration of a rigid mobile\nrobot is commonly described by 6 variables relative to an external\ncoordinate frame:\n\nthree Cartesian coordinates\nthree Euler angles (roll, pitch, yaw)\n\nThe robot's pose comprises its two-dimensional planar coordinates\nrelative to an external coordinate frame, along with its angular\norientation:\n\n\\begin{equation}\n  \\left(\\begin{array}{l}{x} \\\\ {y} \\\\ {\\theta}\\end{array}\\right)\n\\end{equation}\n\n{{}}\n\nThe motion model plays the role of the state transition model in\nmobile robotics. This model is the conditional density:\n\n\\begin{equation}\n  p(x\\t | u\\t, x\\_{t-1})\n\\end{equation}\n\nwhere \\\\(x\\t\\\\) and \\\\(x\\{t-1}\\\\) are both robot poses, and \\\\(u\\_t\\\\) is the\nmotion command. In implementations, \\\\(u\\_t\\\\) is sometimes provided by a\nrobot's odometry.\n\nOdometry models are generally more accurate than velocity models,\nbecause most commercial robots do not execute velocity commands with\nthe level of accuracy that can be obtained by measuring the revolution\nof the robot's wheels. The downside is that odometry is obtained\nafter-the-fact, meaning it cannot be used for motion planning. Hence,\nodometry models are used for state estimation, while velocity models\nfor motion planning.\n\nRelated {#related}\n\n§velocity\\motion\\model\n§odometry\\motion\\model\n",
        "tags": []
    },
    {
        "uri": "/zettels/robot_localization",
        "title": "Robot Localization",
        "content": "\nMobile robot localization is the problem of estimating the robots\ncoordinates in an external reference frame from sensor data, relative\nto a given map of the environment.\n\nThe robot's momentary estimate (belief) is represented by a\nprobability density function over the space of all locations. A\nuniform distribution as prior represents maximal uncertainty.\n\nLocalization can be seen as a problem of coordinate transformation.\nMaps are described in the global coordinate system, independent of the\nrobot's pose. Localization is the process of establishing\ncorrespondence between the map coordinate system and the robot's local\ncoordinate system.\n\nPose often cannot be sensed directly, and has to be inferred from\ndata. A single sensor measurement is usually insufficient, and\nintegrating data over time is required to determine the pose.\n\nTaxonomy {#taxonomy}\n\nLocal vs Global localization {#local-vs-global-localization}\n\nposition tracking\n: initial robot pose is known. Localization is\n    achieved by accommodating the noise in robot motion. The problem is\n    local, since the uncertainty is local and confined to the region\n    near the robot's true pose.\n\nglobal localization\n: initial robot pose is unknown. Robot is\n    initially placed somewhere in its environment, but it lacks\n    knowledge of where it is.\n\nkidnapped robot problem\n: During operation, the robot can get\n    kidnapped and teleported to some other location. The robot might\n    believe it knows where it is, while it does not.\n\nStatic vs Dynamic Environment {#static-vs-dynamic-environment}\n\nstatic environment\n: only variable quantity is the robot's pose.\n    Other objects remain at the same location forever.\n\ndynamic environment\n: there exists objects whose location or\n    configuration may change over time.\n\nPassive vs Active Approaches {#passive-vs-active-approaches}\n\npassive localization\n: localization module only observes the robot\n    operating: the robot is controlled via other means.\n\nactive localization\n: control the robot as to minimize the\n    localization error or costs arising from moving a poorly localized\n    robot into a hazardous place.\n\nSummary {#summary}\n\n{{}}\n\nRelated {#related}\n\n§markov\\_localization\n§ekf\\_localization\n§grid\\mc\\localization\n",
        "tags": []
    },
    {
        "uri": "/zettels/robot_motion",
        "title": "Robot Motion",
        "content": "\nProbabilistic robotics generalizes kinematics equations to the fact\nthat the outcome of a control is uncertain, due to control noise or\nexogenous effects. The outcome of a control is described as a\nposterior probability.\n\nRelated {#related}\n\n§robot\\_kinematics\n",
        "tags": []
    },
    {
        "uri": "/zettels/robotics_probabilistic_generative_laws",
        "title": "Robotics Probabilistic Generative Laws",
        "content": "\nNotation {#notation}\n\n\\\\(x\\_t\\\\)\n: world state at time \\\\(t\\\\)\n\n\\\\(z\\_t\\\\)\n: measurement data at time \\\\(t\\\\) (e.g. camera images)\n\n\\\\(u\\_t\\\\)\n: control data (change of state in the environment) at time\n    \\\\(t\\\\)\n\nState Evolution {#state-evolution}\n\n\\begin{equation}\n  p(x\\t | x\\{0:t-1} z\\{1:t-1}, u\\{1:t})\n\\end{equation}\n\nState Transition Probability {#state-transition-probability}\n\n\\begin{equation}\n  p(x\\t | x\\{0:t-1} z\\{1:t-1}, u\\{1:t}) = p ( x\\t | x\\{t-1}, u\\_t)\n\\end{equation}\n\nThe world state at the previous time-step is a sufficient summary of\nall that happened in previous time-steps.\n\nMeasurement Probability {#measurement-probability}\n\n\\begin{equation}\n  p(z\\t | x\\{0:t}, z\\{1:t-1}, u\\{1:t}) = p(z\\t | x\\t)\n\\end{equation}\n\nThe measurement at time-step \\\\(t\\\\) is often just a noisy projection of\nthe world state at time-step \\\\(t\\\\).\n",
        "tags": [
            "robotics"
        ]
    },
    {
        "uri": "/zettels/robotics",
        "title": "Robotics",
        "content": "\nTheory {#theory}\n\n§uncertainty\\in\\robotics\n§robotics\\probabilistic\\generative\\_laws\n§robot\\_localization\n§state\\_estimation\n§robot\\_motion\n§range\\finder\\model\n§occupancy\\grid\\mapping\n§slam\n\nTools {#tools}\n\n§ros\n§cartographer\n",
        "tags": [
            "robotics"
        ]
    },
    {
        "uri": "/zettels/ros",
        "title": "Robot Operating System (ROS)",
        "content": "\nIntroduction to ROS {#introduction-to-ros}\n\nWhat is ROS? @misc{nililrosintrodroswiki,\n  author =       {nil},\n  howpublished = {http://wiki.ros.org/ROS/Introduction},\n  note =         {Online; accessed 15 October 2019},\n  title =        {ROS/Introduction - ROS Wiki},\n  year =         {nil},\n} {#what-is-ros-a-id-b049e1028daa027cae7888fe4de0456c-href-nilil-ros-introd-ros-wiki-title-misc-nilil-ros-introd-ros-wiki-author-nil-howpublished-http-wiki-dot-ros-dot-org-ros-introduction-note-online-accessed-15-october-2019-title-ros-introduction-ros-wiki-year-nil-misc-nilil-ros-introd-ros-wiki-author-nil-howpublished-http-wiki-dot-ros-dot-org-ros-introduction-note-online-accessed-15-october-2019-title-ros-introduction-ros-wiki-year-nil-a}\n\nLeft     :B_column:\n\n    Meta-operating system, providing low level services:\n        process communication over a network\n        device control\n        hardware abstraction\n    Distributed framework of processes\n\nRight     :B_column:\n\n    {{}}\n\nWhy use ROS? {#why-use-ros}\n\n\"Lightweight\" framework that speeds up large-scale robotic\n    development\nMany libraries developed on top of this framework that can be\n    reused:\n    Physics simulation (Gazebo)\n    Movement + Navigation (ROS navigation)\n\nROS Concepts {#ros-concepts}\n\nComputational Graph\n\n    All computation is organized as a peer-to-peer network of communicating\n        processes.\n\nNodes\n\n    Processes that perform any form of computation.\n    Nodes can communicate with one another.\n    Example of nodes:\n        Publish sensor readings\n        Receiving teleop commands and running them\n    Written with ROS client libraries (rospy, roscpp)\n\nMaster (Primary) Node\n\n    Provides name registration, node lookup to all nodes in the\n        computational graph.\n    Enables communication between nodes.\n\nParameter Server\n\n    \"Distributed\" key-value store: all nodes can access data stored in\n        these keys.\n\nTopics\n\n    Nodes communicating via the publish-subscribe semantics do so by\n        publishing and subscribing to topics.\n    Every topic has a name, e.g. /sensors/temp1\n    No access permissions\n\nServices\n\n    Request-response semantics (think Web servers)\n    Requests are blocking\n\nExample Computational Graph {#example-computational-graph}\n\n{{}}\n\nGetting Started With ROS {#getting-started-with-ros}\n\nROS Environment Setup {#ros-environment-setup}\n\nHere I assume you have the ROS environment set up. If not, see [the\nappendix](#ros-installation).\n\nCreating a ROS Workspace {#creating-a-ros-workspace}\n\nCatkin is ROS' package manager, built on top of CMake.\n\nmkdir -p ~/catkin_ws/src        # Create the directories\ncd ~/catkin_ws/                 # Change to the directory\ncatkin_make                     # Initial setup\n\nExploring ROS shell commands  {#exploring-ros-shell-commands}\n\nrospack\n\n    rospack find locates ROS packages.\n\n        rospack find roscpp # /opt/ros/melodic/share/roscpp\n\nroscd\n\n    roscd changes you to the directory of the ros package.\n\n        roscd roscpp\n    pwd # /opt/ros/melodic/share/roscpp\n\nCreating a ROS package {#creating-a-ros-package}\n\nWe use the convenience script catkincreatepkg to instantiate our package.\n\ncd ~/catkin_ws/src\ncatkincreatepkg workshop std_msgs rospy roscpp\nCreated file workshop/CMakeLists.txt\nCreated file workshop/package.xml\nCreated folder workshop/include/workshop\nCreated folder workshop/src\nSuccessfully created files in /home/jethro/catkin_ws/src/workshop. Please adjust the values in package.xml.\n\nWhat's in a ROS package? {#what-s-in-a-ros-package}\n\nworkshop\n    CMakeLists.txt          # Build instructions\n    include                 # For cpp deps, if any\n       workshop\n    package.xml             # Details about the package\n    src                     # Contains source code\n\nStarting ROS {#starting-ros}\n\nWe initialize the ROS master node with roscore.\n\nroscore\n\n...\nprocess[master]: started with pid [16206]\nROSMASTERURI=http://jethro:11311/\n\nsetting /run_id to 05bf8c5e-efed-11e9-957b-382c4a4f3d31\nprocess[rosout-1]: started with pid [16217]\n\nTo kill it, press Ctrl-C in the same terminal.\n\nROS Nodes {#ros-nodes}\n\nrosnode\n\n    rosnode let's us inspect available nodes:\n\n        rosnode list                    # /rosout\n    rosnode info /rosout\n\n    What happens if master is not running?\n\n        rosnode list               # ERROR: Unable to communicate with master!\n\nRunning a ROS node\n\n    A ROS package may contain many ROS nodes.\n\n        rosrun turtlesim\ndrawsquare        mimic              turtlesimnode     turtleteleopkey\n\n        rosrun turtlesim turtlesim_node\n[ INFO] [1571214245.786246078]: Starting turtlesim with node name /turtlesim\n[ INFO] [1571214245.790986159]: Spawning turtle [turtle1] at x=[5.544445], y=[5.544445], theta=[0.000000]\n\n    Exercise: reinspect the node list.\n\nROS Topics {#ros-topics}\n\nNow we have a visual simulation of a turtle. How do we make it move?\n\nrosrun turtesim turtleteleopkey\n\nWhat's going on?\n\n    turtleteleopkey advertises on a ROS topic, and publishes each keystroke:\n\n        rostopic list\n    rostopic echo /turtle1/cmd_vel\n\nROS Messages\n\n    ROS messages are pre-defined formats. They are binarized and\n        compressed before they are sent over the wire.\n\n        rostopic type /turtle1/cmdvel   # geometrymsgs/Twist\n\nMonitoring the Topic\n\n    The rate at which messages is published is good to monitor (in Hz).\n    A topic that has too many messages can get congested, and buffer/drop\n        many messages, or congest the ROS network.\n\n        rostopic hz /turtle1/cmd_vel\nsubscribed to [/turtle1/cmd_vel]\naverage rate: 13.933\nmin: 0.072s max: 0.072s std dev: 0.00000s window: 2\n\nRosbag\n\n    A bag is subscribes to one or more topics, and stores serialized\n        data that is received (for logging/replay)\n\n        rosbag record /turtle1/cmd_vel\n[ INFO] [1571294982.145679913]: Subscribing to /turtle1/cmd_vel\n[ INFO] [1571294982.168808833]: Recording to 2019-10-17-14-49-42.bag\n\nROS Services {#ros-services}\n\nServices allow request-response interactions between nodes.\n\nrosservice list\nrosservice call /clear\nrosservice type /spawn | rossrv show\n\nROS Params {#ros-params}\n\nthe rosparams commandline interface allows us to store and manipulate\ndata on the ROS Parameter server.&nbsp;\n\nrosparam set            # set parameter\nrosparam get            # get parameter\nrosparam load           # load parameters from file\nrosparam dump           # dump parameters to file\nrosparam delete         # delete parameter\nrosparam list           # list parameter names\n\nPubsub @misc{wiki_pubsub,\n  author =       {nil},\n  howpublished =\n                  {http://wiki.ros.org/ROS/Tutorials/WritingPublisherSubscriber\n                  \\%28python \\%29},\n  note =         {Online; accessed 17 October 2019},\n  title =        {ROS/Tutorials/WritingPublisherSubscriber(python) -\n                  ROS Wiki},\n  year =         {nil},\n} {#pubsub-a-id-0eb03dca701c737878a6bf71343edd56-href-wiki-pubsub-title-misc-wiki-pubsub-author-nil-howpublished-http-wiki-dot-ros-dot-org-ros-tutorials-writingpublishersubscriber-28python-29-note-online-accessed-17-october-2019-title-ros-tutorials-writingpublishersubscriber--python--ros-wiki-year-nil-misc-wiki-pubsub-author-nil-howpublished-http-wiki-dot-ros-dot-org-ros-tutorials-writingpublishersubscriber-28python-29-note-online-accessed-17-october-2019-title-ros-tutorials-writingpublishersubscriber--python--ros-wiki-year-nil-a}\n\nWhen do we use topics? {#when-do-we-use-topics}\n\nPreviously we looked at ready-made ROS packages and how they used\ntopics and services. Now, we'll write our own publisher and\nsubscriber.\n\nThe pubsub interface is useful in situations where a response for each\nrequest is not required:\n\nSensor readings\nLog info\n\nA Simple Publisher {#a-simple-publisher}\n\nWe use rospy, but roscpp is fine as well. We create a new file in our\nworkshop package workshop/src/talker.py:\n\n#!/usr/bin/env python\nimport rospy\nfrom std_msgs.msg import String\n\npub = rospy.Publisher('mytopic', String, queuesize=10) # initializes topic\nrospy.init_node('talker', anonymous=True) # required to talk to Master\n\nwhile not rospy.is_shutdown():\n    pub.publish(\"Hello\")\n\nExecuting the Publisher Node {#executing-the-publisher-node}\n\nWe need to make our Python file executable:\n\nchmod +x talker.py\n\nrosrun workshop talker.py\n\nExercise: monitor the output. What's wrong? (hint: Hz)\n\nSetting the rate of publishing {#setting-the-rate-of-publishing}\n\nWe use the Rate object, and the rate.sleep() to set the rate of\npublishing:\n\nrate = rospy.Rate(10)           # 10 hz\n...\nrate.sleep()\n...\n\nGood Practice {#good-practice}\n\nWe often wrap all our logic in a function, and catch the\nROSInterruptException exception:\n\n#!/usr/bin/env python\nimport rospy\nfrom std_msgs.msg import String\n\ndef talker():\n    pub = rospy.Publisher('mytopic', String, queuesize=10) # initializes topic\n...\n\ntry:\n    talker()\nexcept rospy.ROSInterruptException:\n    pass\n\nExercise: Write a time publisher (5 minutes) {#exercise-write-a-time-publisher--5-minutes}\n\nGoal: publish the current date-time onto a topic /datetime.\n\nHint: Python has a datetime library.\n\nSubscriber {#subscriber}\n\nWe create a listener in workshop/src/listener.py\n\n#!/usr/bin/env python\nimport rospy\nfrom std_msg.msg import String\n\ndef echo(data):\n    print(data.data)\n\ndef listener():\n    rospy.init_node(\"listener\", anonymous=True)\n    rospy.Subscriber(\"my_topic\", String, echo)\n    rospy.spin() # prevents python from exiting\n\nlistener()\n\nSummary {#summary}\n\nrospy.init_node(name)           # create node\nrospy.Publisher(topicname, msgtype) # create publisher\nrospy.Subscriber(topicname, msgtype, callback) # create subscriber\nrospy.Rate(10)                  # rate object\nrospy.spin()                    # spin\n\nServices @misc{wiki_service,\n  author =       {nil},\n  howpublished =\n                  {http://wiki.ros.org/ROS/Tutorials/WritingServiceClient\n                  \\%28python \\%29},\n  note =         {Online; accessed 17 October 2019},\n  title =        {ROS/Tutorials/WritingServiceClient(python) - ROS\n                  Wiki},\n  year =         {nil},\n} {#services-a-id-d9f8b17a885ac23c956840df53cacd3f-href-wiki-service-title-misc-wiki-service-author-nil-howpublished-http-wiki-dot-ros-dot-org-ros-tutorials-writingserviceclient-28python-29-note-online-accessed-17-october-2019-title-ros-tutorials-writingserviceclient--python--ros-wiki-year-nil-misc-wiki-service-author-nil-howpublished-http-wiki-dot-ros-dot-org-ros-tutorials-writingserviceclient-28python-29-note-online-accessed-17-october-2019-title-ros-tutorials-writingserviceclient--python--ros-wiki-year-nil-a}\n\nMsg and Srv {#msg-and-srv}\n\nmsg\n: message files that define the format of a ROS message. These\n    generate source code for different languages (think Apache Thrift,\n    Protobuf).\n\nsrv\n: describes a service (request/response)\n\nCreating a msg {#creating-a-msg}\n\nmkdir -p workshop/msg\n\nCreate a file workshop/msg/Num.msg:\n\nint64 num\n\nCompiling the msg {#compiling-the-msg}\n\nIn package.xml:\n\nmessage_generation\nmessage_runtime\n\nIn CMakeLists.txt:\n\nfind_package(catkin REQUIRED COMPONENTS\n   roscpp\n   rospy\n   std_msgs\n   message_generation\n)\n\ncatkin_package(\n  ...\n  CATKINDEPENDS messageruntime ...\n  ...)\n\naddmessagefiles(\n  FILES\n  Num.msg\n)\n\ngenerate_messages()\n\nCompile the message:\n\ncd ~/catkin_ws\ncatkin_make\ncatkin_make install\n...\n[100%] Built target workshopgeneratemessages_cpp\n[100%] Built target workshopgeneratemessages_py\n[100%] Built target workshopgeneratemessages_eus\nScanning dependencies of target workshopgeneratemessages\n[100%] Built target workshopgeneratemessages\n\nUsing the ROS msg {#using-the-ros-msg}\n\nrosmsg list                     # ... workshop/Num\nrosmsg show workshop/Num        # int64 num\n\nCreating a ROS srv {#creating-a-ros-srv}\n\nmkdir -p workshop/srv\n\nIn workshop/srv/SumInts.srv:\n\nint64 a\nint64 b\n\nint64 sum\n\nCompiling the ROS srv {#compiling-the-ros-srv}\n\nSince srv files are also compiled, the setup is similar to compiling msgs.\n\nWriting a Service Node {#writing-a-service-node}\n\nWe can create a server that uses the service file we defined earlier:\n\n#!/usr/bin/env python\nfrom workshop.srv import SumInts, SumIntsResponse\nimport rospy\n\ndef handler(req):\n    return SumIntsResponse(req.a + req.b)\n\ndef sumints_server():\n    rospy.initnode(\"sumintsserver\")\n    s = rospy.Service(\"sumints\", SumInts, handler)\n    rospy.spin()\n\nsumints_server()\n\nWriting a Client {#writing-a-client}\n\n#!/usr/bin/env python\nimport sys\nimport rospy\nfrom workshop.srv import SumInts\n\ndef sumints_client(x, y):\n    rospy.waitforservice('sumints')\n    try:\n        sumints = rospy.ServiceProxy('sumints', SumInts)\n        resp1 = sumints(x, y)\n        return resp1.sum\n    except rospy.ServiceException, e:\n        print \"Service call failed: %s\"%e\n\nx = int(sys.argv[1])\ny = int(sys.argv[2])\nprint \"%s + %s = %s\"%(x, y, sumints_client(x, y))\n\nrosrun workshop sumint_client.py 1 2\n1 + 2 = 3\n\nExercise: Time Service (15 minutes) {#exercise-time-service--15-minutes}\n\nWrite a service that:\n\nrequests nothing\nresponds with the current time\n\nWrite a client that sends the request and prints this response.\n\nWhat's Next? {#what-s-next}\n\nWhat's Next? {#what-s-next}\n\nRun a simulator, model the robot using URDF\nLook at community ROS packages\n    tf2: maintain robotic coordinate frames (pose estimation)\n    gmapping/slam etc.: navigation\nLook at ROS 2\n\nAppendix {#appendix}\n\nCommon Pitfalls {#common-pitfalls}\n\nNot sourcing your devel/setup.bash:\n\nsource devel/setup.bash\n\nThis is necessary to make available all the C++ and python ROS\n    packages that you have built\nI recommend using direnv, and sourcing it every time you enter\n    the Catkin workspace.\n\nROS Installation {#ros-installation}\n\nUbuntu\n\n    Follow the instructions on ROS Wiki. @misc{nililinstalubuntroswiki,\n      author =       {nil},\n      howpublished = {http://wiki.ros.org/melodic/Installation/Ubuntu},\n      note =         {Online; accessed 16 October 2019},\n      title =        {melodic/Installation/Ubuntu - ROS Wiki},\n      year =         {nil},\n    }\n\nVM\n\n    Download the VM image and load it.\n\nReferences {#references}\n\nBibliography\nnil,  (nil). Ros/introduction - ros wiki. Retrieved from http://wiki.ros.org/ROS/Introduction. Online; accessed 15 October 2019. ↩\n\nnil,  (nil). Ros/tutorials/writingpublishersubscriber(python) - ros wiki. Retrieved from http://wiki.ros.org/ROS/Tutorials/WritingPublisherSubscriber \\%28python \\%29. Online; accessed 17 October 2019. ↩\n\nnil,  (nil). Ros/tutorials/writingserviceclient(python) - ros wiki. Retrieved from http://wiki.ros.org/ROS/Tutorials/WritingServiceClient \\%28python \\%29. Online; accessed 17 October 2019. ↩\n\nnil,  (nil). Melodic/installation/ubuntu - ros wiki. Retrieved from http://wiki.ros.org/melodic/Installation/Ubuntu. Online; accessed 16 October 2019. ↩\n\n: Almost all these commands have tab completion!\n: can also be done programatically\n",
        "tags": []
    },
    {
        "uri": "/zettels/running",
        "title": "Running",
        "content": "\nRunning Form {#running-form}\n\nMaintain a cadence of at least 160 steps per minute (aim for 180)\n    Stride length, cadence proportional to pace, and we do not want\n        stride length to compensate for lower cadence, as this leads to\n        feet landing in front of body, resulting in a damaging breaking\n        effect.\nWhether it is a heel or fore-foot strike is less important than\n    landing in the body's center of mass.\nKnee should be in line with the middle of foot so that when foot\n    strikes the ground, it’s right under the knee\n",
        "tags": []
    },
    {
        "uri": "/zettels/scala",
        "title": "Scala",
        "content": "\ntags\n: §prog\\_lang\n\nIntroduction to Scala {#introduction-to-scala}\n\nScala's primary signature is its blend of object-oriented and\nfunctional programming concepts.\n\nScala allows easy addition of new types. For example, Scala's actor\nmodel for concurrency is implemented as an abstraction on top of Java\nthreads.\n\nObject-oriented programming provides structure, that allows scaling up\nto large programs. Every value is an object, and every operation is a\nmethod call. Scala uses traits to compose objects. Traits act like\ninterfaces in Java, but can also have method implementations and even\nfields.\n\nIn Scala, functions are first-class values -- they can be passed as\narguments to other functions, be returned as results from functions,\nor be stored in variables. First-class functions provide a convenient\nmeans of abstracting over operations and creating new control\nstructures.\n\nMany of Scala's operations map input values to output values, rather\nthan mutate the data in place. Scala libraries define many more\nimmutable data types on top of those found in the Java APIs.\n\nWhy Scala? {#why-scala}\n\nScala has seamless interoperability with Java, compiling to JVM\n    bytecode. This means that it often has Java-like runtime performance.\n\nScala is concise: (1) it avoids boilerplate (2) it provides tooling\n    to define powerful libraries to factor out common behaviour\n\nScala is statically typed: it allows one to combine types, and hide\n    details of types with abstract types\n\nBasic Scala {#basic-scala}\n\n{{}}\n\nArrays are accessed with parantheses in Scala: a(0).  This is\nbecause arrays are simply instances of classes like any other class in\nScala. Scala will transform the code into an invocation of a method\nnamed apply on that variable: a.apply(0). Accessing an element of an\narray in Scala is a method call like any other.\n\nSimilarly, a(0) = 1 gets translated to a.update(0, 1).\n\nScaladoc: scala.collection.mutable.List\n",
        "tags": []
    },
    {
        "uri": "/zettels/security",
        "title": "Security",
        "content": "\nExploitation (Halvar Flake) {#exploitation--halvar-flake}\n\nPhrack\nStack Smashing\nHeap Overflow\nASLR\n\nWeird machines, exploitability, and provable unexploitability:\n\nFinite State Machines {#finite-state-machines}\n\nCPU States:\n\nSane\nTransitory\nWeird States\n\nExploitation Procedure {#exploitation-procedure}\n\nSetup (choose the right sane state)\nInstantiation (enter the weird state)\nProgramming (program the weird state)\n\nAttacker Specialization {#attacker-specialization}\n\nDifferent version of the same \"host\" may create similar weird machines.\n",
        "tags": []
    },
    {
        "uri": "/zettels/slam",
        "title": "Simultaneous Localization and Mapping (SLAM)",
        "content": "\nIn SLAM, the robot acquires a map of its environment while\nsimultaneously localizing itself relative to this map. It is a\nsignificantly more difficult problem compared to §robot\\_localization\nand §occupancy\\grid\\mapping.\n\nThere are 2 main forms of SLAM:\n\nOnline SLAM\n: estimating the posterior over the momentary pose\n    along with the map: \\\\(p(x\\t, m | z\\{1:t}, u\\_{1:t})\\\\)\n\nfull SLAM\n: posterior over the entire path \\\\(x\\_{1:t}\\\\) along with the\n    map: \\\\(p(x\\{1:t}, m | z\\{1:t}, u\\_{1:t})\\\\)\n\nThe online SLAM algorithm is the result of integrating out past poses\nin the full SLAM problem\n\nEKF SLAM {#ekf-slam}\n\nEKF SLAM uses a number of approximations and limiting assumptions:\n\nFeature-based maps\n: Maps are composed of a small number of point\n    landmarks. The method works well when the landmarks are relatively unambiguous.\n\nGaussian Noise\n: The noise in motion and perception is assumed to\n    be Gaussian.\n\nPositive Measurements\n: It can only process positive sightings of\n    landmarks, ignoring negative information.\n\nSLAM with Known Correspondence {#slam-with-known-correspondence}\n\nThe key idea is to integrate landmark coordinates into the state\nvector. This corresponds to the continuous portion of the SLAM\nproblem. EKF SLAM combines the state vector as such:\n\n\\begin{aligned}\n  y\\{t} &=\\left(\\begin{array}{c}{x\\{t}} \\\\ {m}\\end{array}\\right)\n\\end{aligned}\n\nand calculates the online posterior:\n\n\\begin{equation}\n  p\\left(y\\{t} | z\\{1: t}, u\\_{1: t}\\right)\n\\end{equation}\n\nEKF SLAM with Unknown Correspondences {#ekf-slam-with-unknown-correspondences}\n\nIt uses an incremental maximum likelihood estimator to determine\ncorrespondences.\n\nFeature Selection and Map Management {#feature-selection-and-map-management}\n\nEKF SLAM requires several additional techniques to be robust in\npractice. First, one needs to deal with outliers in the measurement\nspace. One technique is to maintain a provisional landmark list.\nInstead of adding the landmark immediately, it is added to this list,\nand when the uncertainty has shrunk after repeated observations of the\nlandmark, it is added in.\n\nEIF SLAM {#eif-slam}\n\nUnlike EKF SLAM, the extended information form SLAM algorithm (EIF\nSLAM) solves the full SLAM problem. EIF represents the posterior\ngaussian in its canonical representation form, with the precision\nmatrix and information state vector (§information\\_filter).\n\nEIF SLAM is also not incremental: it calculates posteriors over a\nrobot path. It is best suited for problems where a map needs to be\nbuilt from data of fixed size, and can afford to hold the data in\nmemory until the map is built.\n\nSuppose we are given a set of measurements \\\\(z\\_{1:t}\\\\) with associated\ncorrespondence variables \\\\(c\\{1:t}\\\\), and a set of controls \\\\(u\\{1:t}\\\\).\nThen the EIF SLAM algorithm operates as follows:\n\nConstruct the information matrix and information vector fromt he\n    joint space of robot poses \\\\(x\\{1:t}\\\\) and map \\\\(m = \\\\{m\\j\\\\}\\\\).\nEach measurement leads to a local update of \\\\(\\Omega\\\\) and \\\\(\\xi\\\\).\n    This is because information is an additive quantity.\n\nThe key insight is that information is sparse. Specifically,\n\nMeasurements provide information of a feature relative to the\n    robot's pose at the time of measurement, forming constraints between\n    pairs of variables.\nMotion provides information between two subsequent poses, also\n    froming constraints\n\nEIF SLAM records all this information, through links that are defined\nbetween poses and features, and pairs of subsequent poses. However, this\ninformation representation does not provide estimates of the map or\nrobot path.\n\nMaps are recovered via an iterative procedure involving 3 steps:\n\nConstruction of a linear information form through Taylor expansion\nReduction of this linear information form\nSolving the resulting optimization problem\n\nSparse EIF SLAM {#sparse-eif-slam}\n\nThe sparse EIF SLAM only maintains a posterior over the present robot\npose and the map. Hence, they can be both run online, and are\nefficient. Unlike EKFs, they also maintain information representation\nof all knowledge.\n",
        "tags": []
    },
    {
        "uri": "/zettels/sleep",
        "title": "Sleep",
        "content": "\nEveryman {#everyman}\n\nArticles {#articles}\n\nAll forms of sleep control with an alarm clock will increase the overall demand for sleep. This means that:\n\nIf you use an alarm clock, either:\n\nyou will sleep longer, or\nyou will feel more miserable.\n\nArtificial sleep schedules will dramatically reduce your mental capacity. A healthy individual in normal conditions will find it difficult to fall asleep 4 hours after the main sleep episode unless that episode was unnaturally cut with an alarm clock resulting in sleep deprivation.\n\nWhat Happens to Your Body on No Sleep | Outside Online\n",
        "tags": []
    },
    {
        "uri": "/zettels/slice_sampling",
        "title": "Slice Sampling",
        "content": "\nSlice sampling has similarities to Gibbs sampling, rejection sampling\nand the Metropolis-Hastings method. The advantage over simple\nMetropolis-Hastings methods is that it is robust to the choice of\nparameters. Like rejection sampling, it asymptotically draws samples\nfrom volume under the curve \\\\(p^\\star(x)\\\\).\n\nA good treatment of slice sampling can be found in Mackay's Book\nInformation Theory, Inference and Learning Algorithms.\n",
        "tags": []
    },
    {
        "uri": "/zettels/smoothed_snn",
        "title": "Smoothed Spiking Neural Networks",
        "content": "\ntags\n: §spiking\\neural\\networks\n\nSmoothed SNNs ensure well-behaved gradients which are directly\nsuitable for optimization. They come in 4 categories:\n\nSoft non-linearity models\nProbabilistic models, where gradients are well defined in expectation\nModels relying on rate or\nSingle-spike temporal codes\n\nSoft non-linearity models {#soft-non-linearity-models}\n\nThis approach can be applied on all spiking neuron models which\ninclude a smooth spike generating process. These include:\n\nHodgkin-Huxley\nMorris-Lecar\nFitzHugh-Nagumo\n\nThe resultant network can be optimized using standard methods of BPTT.\nThese smoothed models compromise on a key feature of SNNs: binary\nspike propagation.\n\nProbabilistic Models {#probabilistic-models}\n\nStochasticity in the models effectively smooths out the discontinuous\nbinary nonlinearity, which provides well-defined gradients on\nexpectation values. These binary probabilistic models are commonly\nstudied in Restricted Boltzmann machines.\n\nWith probabilistic models, the log-likelihood of a spike train is a\nsmooth quantity, which can be optimized using gradient-descent.\n\nGradients in rate-coding networks {#gradients-in-rate-coding-networks}\n\nIn rate-coding networks, it is assumed that the spike rate carries the\nunderlying information. Rate-based approaches offer good performance\nin practice, but may be inefficient. Precise estimation of firing\nrates require averaging over a number of spikes. These require high\nfiring rates, or longer averaging times.\n\nProbabilistic network implementations also use rate-coding at the\noutput level.\n\nSingle-spike-time-coding networks {#single-spike-time-coding-networks}\n\nIn the temporal coding setting, individual spikes carry significantly\nmore information. This was first pioneered by SpikeProp. Firing times\nfor hidden units were linearized, allowing to analytically compute\napproximate hidden layer gradients.\n",
        "tags": []
    },
    {
        "uri": "/zettels/soft_skills",
        "title": "Soft Skills",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/software_engineering",
        "title": "Software Engineering",
        "content": "\nObject-Oriented Programming {#object-oriented-programming}\n\nEvery object has both state (data) and behaviour (operations on\n    data).\nEvery object has an interface and an implementation.\n    Interface are for other objects to interact with.\n    Implementations support the interface, and may not be accessible\n        to other objects.\n\nBasic UML Notation {#basic-uml-notation}\n\nThe class is denoted with 3 parts: The class name, its attributes, and\nits methods.\n\n{{}}\n\nInstances of a class are denoted with 3 parts: its name (and class),\nand its attribute values.\n\n{{}}\n\nClass-level attributes and variables are denoted by underlines. In\nthe class diagram below, totalStudents and getTotalStudents are\nclass-level.\\_\n\n{{}}\n\nUML Notation for enumerations:\n\n{{}}\n\nAssociations {#associations}\n\nA solid line indicates an association between 2 objects.\nThe concept of Navigability refers to whether the association\n    knows about the other class. Arrow heads are used to indicate the\n    navigability of the association.\n\n{{}}\n\nMultiplicity is denoted on each end of the association.\n\n{{}}\n\nDependencies are weaker associations where interactions between\n    objects do not result in a long-term relationship. A dashed arrow\n    is used to show dependencies.\n\n{{}}\n\nComposition represents a strong whole-part relationship. When the\n    whole is destroyed, parts are destroyed too. There cannot be\n    cyclical links in composition.\n\n{{}}\n\nAggregation represents a container-contained relationship.\n\n{{}}\n\nAn association class represents additional information about an\n    association. It is a normal class but plays a special role from a\n    design point of view.\n\n{{}}\n\nInheritance {#inheritance}\n\nInheritance allows you to define a new class based on an existing\nclass. This helps group common parts among classes. This is denoted by\nan arrow.\n\n{{}}\n\nInterfaces {#interfaces}\n\nAn interface is a behaviour specification. If a class implements the\ninterface, it is able to support the behaviours specified by the\ninterface. A class implementing an interface results in an is-a\nrelationship. In the example below, AcademicStaff is a\nSalariedStaff.\n\n{{}}\n\nAn abstract method is the method interface without the implementation.\nIt is denoted with the {abstract} annotation in the UML diagram.\n\n{{}}\n\nPolymorphism {#polymorphism}\n\nPolymorphism is the ability of different objects to respond, each\nit its own way, to identical messages. The mechanisms that enable\npolymorphism are:\n\nSubstitutability\n: write code that expects parent class, yet use\n    that code with objects of child classes.\n\nOverriding\n: Operations in the super class need to be overridden in\n    each of the subclasses.\n\nDynamic binding\n: Calls to overridden methods are bound to the\n    implementation of the actual object's class dynamically during\n    runtime.\n\nModelling Behaviour {#modelling-behaviour}\n\nActivity Diagrams {#activity-diagrams}\n\nActions\n: Rectangles with rounded edges (Steps)\n\nControl flows\n: Lines with arrowheads (Flow of control from one\n    action to another)\n\nAlternate paths\n: Diamond shapes\n    Branch or merge nodes\n    Each control flow leaving branch node has guard condition\n    Only 1 alernative path can be taken at any time.\n\nParallel paths\n: bar\n    Forks and join\n    Indicate start and end of concurrent flows of control\n\nPart of Activity\n: rakes\n    Indicate that part of activity is given as separate diagram\n    In actions\n\nActor partitions\n: swimlanes\n    Partition activity diagram to show who is doing which action (Who\n        label at the top, as columns)\n\n{{}}\n\n{{}}\n\nSequence Diagrams {#sequence-diagrams}\n\nMethod calls\n: Solid arrows\n\nMethod returns\n: Dotted arrows (optional)\n\nLoops\n: labeled boxes\n\nActivation bar (optional)\n    Method is running and in charge of execution\n    Constructor is active\n    Dotted lines after activation bar shows a lifeline, i.e. it is\n        still alive\n\nDeletion\n: Use a X at end of lifeline of an object\n\nSelf Invocation\n: Draw a second bar within the activation bar for\n    inner method and an arrow to show self invocation\n\nAlternative paths\n: Alt frames (boxes) with dotted horizontal\n    lines to separate alternative paths\n\nOptional paths\n: Opt frames\n\nReference frames\n: frames\n    ref frame to omit details/ Show frame in another sequence\n        diagram\n    sd frame to show details\n\nParallel paths\n: For multi-threading, as multiple things are being\n    done at the same time\n\nNote:\n\nNo underlined object names (e.g. :Object)\n\n{{}}\n\nSoftware Requirements {#software-requirements}\n\nRequirements come from stakeholders: parties that are directly\naffected by the software project.\n\nfunctional requirements\n: specify what the system should do\n    data requirements: availability etc.\n\nnon-functional requirements\n: specify the constraints under which\n    system is developed\n    business and domain rules: the size of the group cannot be more\n        than 5\n    constraints: should be backwards compatible\n    technical requirements: should work on 32/64-bit environments\n\nGood requirements are: unambiguous, testable, clear, correct,\nunderstandable, feasible, independent, atomic, necessary,\nimplementation-free\n\nUser stories follow the format: As a , I can  so that _. They\noccur on different levels, high-level stories are called epics.\n\nDesign {#design}\n\nSoftware design has 2 main aspects:\n\nproduct/external design: designing the external behaviour of the\n    product to meet the user requirements.\nimplementation/internal design: designing how the product will be\n    implemented to meet the required external behaviour.\n\ntechnique for dealing with complexity, establishes a\n    level of complexity we are interested in, and\n    suppressing more complex details below that level.\n\nCoupling {#coupling}\n\nCoupling is the measure of the degree of dependence between\ncomponents. Highly coupled components are:\n\nharder to maintain: change in one module can cause changes to other modules\nharder to integrate: multiple components have to be integrated at\n    the same time\nharder to test: dependence on other modules\n\nCoupling comes in various forms:\n\nContent Coupling\n: one module modifies or relies on the internal\n    workings of another module.\n\nCommon/Global Coupling\n: two modules share the same global data\n\nControl Coupling\n: one module controls the flow of the other\n\nData Coupling\n: one module sharing data with another module (e.g.\n    passing params)\n\nExternal Coupling\n: two modules share an externally imposed convention\n\nSubclass Coupling\n: a class inherits from another class\n\nTemporal Coupling\n: two actions are bundled together because they\n    happen to occur at the same time\n\nCohesion {#cohesion}\n\nCohesion is a **measure of how strongly-related and focused the\nvarious responsibilities of a component are**. Low cohesion can:\n\nimpede the understandability of modules\nlower maintainability because  a module can be modified due to\n    unrelated causes\nlowers reusability because they do not represent logical units of\n    functionality\n\nCohesion can be present in many forms:\n\nCode related to the same concept are kept together\nCode invoked close together in time are kept together\nCode manipulating the same data structure are kept together\n\nSoftware Architecture {#software-architecture}\n\nSoftware architecture shows the **overall organization of the system\nand can be viewed as a very high-level design**.\n\nArchitectural Styles {#architectural-styles}\n\nn-tier\n    n layer\n    Higher layer communicates to lower tier\n    Must be independent\n\nClient-server\n    At least one client component and one server component\n    Commonly used in distributed apps\n\nEvent-driven Style\n    Detect events from emitters and communicating to event consumers\n\nTransaction processing style\n    Divides workload down to a number of transactions which are given\n        to a dispatcher which controls the execution for each transaction\n\nService-oriented architecture (SOA)\n    Combining functionalities packaged by programmatically accessible\n        services\n    e.g. Creating an SOA app that uses Amazon web services\n\nPipes and Filters pattern\n    Break down processing tasks by modules (streams) into separate\n        components(filters), each into 1 task\n    Combine them into a pipeline by standardising format of data each\n        component sends and receives\n    Bottleneck - Slowest filter\n    Components can be run independently\n    Used when processing steps by an application have different\n        scalability requirements\n\nBroker pattern\n    Broker component coordinates communication, such as forwarding\n        requests, as well as for transmitting results and exceptions\n    Used to structure distributed software systems with decoupled\n        components interacting by remote service invocations\n\nPeer-to-peer\n    Partitions workload between peers (both 'client' and 'server' to\n        other nodes)\n\nMessage-driven processing\n    Client sends service requests in specially-formatted messages to\n        request brokers(programs)\n    Request brokers maintain queues of requests (and maybe replies) to\n        screen their details\n\nSoftware Design Patterns {#software-design-patterns}\n\nSoftware Design Patterns are **elegant reusable solutions to commonly\nrecurring problems within a given context in software design**.\n\nDesign patterns are specified with: context, problem, solution,\nanti-patterns, consequences and other useful information.\n\nSingleton {#singleton}\n\nContext: certain classes should have no more than 1 instance.\nProblem: a normal class can be instantiated multiple times by\n    invoking the constructor\nSolution: make the constructor of the singleton class private,\n    provide a public class-level method to access the single instance.\nPros:\n    Easy to apply\n    Effective with minimal work\n    Access singleton from anywhere\nCons:\n    Global variable, increases coupling\n    Hard to test as they cannot be replaced with stubs\n    Singletons carry data from one test to another\n\nAbstraction Occurrence {#abstraction-occurrence}\n\nContext: Group of similar entities that appear to be occurrences\n    of the same thing, sharing a lot of common information, but differ\n    in many ways.\nProblem: representing objects as a single class would result in\n    duplication of data, leading to inconsistencies in data.\nSolution: Let a copy of the entity be represented by multiple\n    objects, separating the common and unique information into 2\n    classes.\n\nFacade Pattern {#facade-pattern}\n\nContext: Components need access to functionality deep inside other\n    components\nProblem: Access to component should be allowed without exposing\n    internal details\nSolution: Create a Facade class that sits between the component\n    internals and users of the component that access the component\n    happens through the facade class.\n\n{{}}\n\nCommand Pattern {#command-pattern}\n\nContext: A system is required to execute a number of commands,s\n    each doing a different task.\nProblem: Prefer to have code executing command to not have to know\n    each command type\nSolution: Have  a general Command object that can be passed\n    around, stored and executed without knowing the type of command.\n\nMVC Pattern {#mvc-pattern}\n\nContext: applications support storage/retrieval of information,\n    display, and updating stored information\nProblem: want to reduce coupling between interlinked nature of the\n    above features\nSolution: View displays data, interacts with the user.\n    Controller detects UI events, and updates the model/view when\n    necessary. Model stores and maintains the data, updates the views\n    if necessary.\n\nObserver Pattern {#observer-pattern}\n\nContext: An object is interested in getting notified when a change\n    happens to another object\nProblem: the observed object does not want to be coupled to\n    objects that are 'observing' it\nSolution: Force the communication through an interface know to\n    both parties.\n\n{{}}\n\nImplementation {#implementation}\n\nDebugging\n: process of discovering defects in the program.\n    inserting temporary print statements incur extra effort, manually\n        tracing through code is difficult and time consuming. We should\n        use a debugger tool, which allows pausing and stepping through\n        execution of the code.\n\nCode Quality {#code-quality}\n\nThere are various dimensions of code quality, including **run-time\nefficiency, security, and robustness**. The most important perhaps is\nreadability.\n\nSome basic guidelines:\n\nAvoid long methods\nAvoid deep nesting\nAvoid complicated expressions\nAvoid Magic numbers\nMake the code obvious, e.g. by using explicity type conversion\nStructure code logically\nDo not trip up the reader, with things like unused parameters in\n    the method signature\nPractice KISSing\nAvoid premature optimizations\nMake the happy path prominent\nSLAP (Single level of abstraction per method) hard\nMake the happy path prominent\n\nIt is also good to follow a coding standard.\n\nComments should explain the what and why, and not the how. Write\ncomments minimally but sufficiently, not repeating the obvious and\nwriting with the reader in mind.\n\nRefactoring {#refactoring}\n\nRefactoring **improves a program's internal structure in small steps without\nmodifying its external behaviour.** It is not rewriting, and not bug-fixing.\n\nCommon refactors include:\n\nConsolidate duplicate conditional fragments\nextract method\n\nError Handling {#error-handling}\n\nExceptions are events that occur during the execution of a program,\nthat disrupt the normal flow of the program's instructions.\n\nException objects encapsulate the unusual situation so that another\npiece of code can catch it and deal with it. Exception objects\npropagate up the  method call hierarchy until it is dealt with.\n\nAssertions {#assertions}\n\nAssertions are used to define assumptions about the program state so\nthat the runtime can verify them. If the runtime detects an assertion\nfailure, it typically takes some drastic action, such as terminating\nthe program. Assertions can be disabled without modifying the code.\n\nLogging {#logging}\n\nLogging is **The deliberate recording of certain information during\n   program execution for future reference**, and is useful for\n   troubleshooting problems. Most languages come with a logging\n   mechanism, and the logger has different levels: SEVERE,\n   INFO, WARNING etc.\n\nBuild automation {#build-automation}\n\nGradle\n    Automates tasks such as:\n        Running tests\n        Manage library dependencies\n        Analyse code for style compliance\n\n    Gradle configuration is defined in build script build.gradle\n    Gradle commands are run in gradlew (wrapper) which runs the\n        following commands by default:\n        clean\n        headless\n        allTests\n        coverage\n\n    Dependencies are updated automatically by other relevant Gradle\n        tasks\n\nContinuous Integration (CI) {#continuous-integration-ci}\n\nIntegration, building and testing happens automatically after code\n    change\nTravis CI\nContinuous Deployment (CD) - Changes are integreated, and deployed to\n    end-users at the same time (e.g. Travis)\n\nDefensive Programming {#defensive-programming}\n\nLeave no room for things to go wrong\n\nEnforce compulsory associations(perform checks for null)\nEnforce 1-to-1 associations (Initialise an association as a new\n    object first, before assignment)\nEnforce referential integrity (Inconsistency in object references)\n    (invoke the peer method when one is called)\n\nDesign-by-Contract {#design-by-contract}\n\nDbC is an \\*approach for designing software that requires defining\n  formal, precise and verifiable interface specifications for software\n  components\\*.\n\nMeet interface specifications for different components\n    (preconditions must be met) to fulfil contract\n\nIntegration Approaches {#integration-approaches}\n\nLate and one-time\n    Wait till all components are completed and integrate all finished\n        components near end of project\n    Not recommended due to possible component incompatabilities, which\n        can lead to delivery delays\nEarly and frequent\n    Integrate early and evolve each part in parallel, in small steps,\n        re-integrating frequently\nBig-Bang vs Incremental Integration\n    Big-bang can lead to many problems at the same time\nTop-Down vs Bottom-Up\n    Top-Down require stubs\n    Bottom-up require drivers\n    Sandwich for both to 'meet' in the middle\n\nReuse {#reuse}\n\nBy reusing tried and tested components, the robustness of a new\nsoftware system can be enhanced while reducing the manpower and time\nrequirement. There are costs associated with reuse.\n\nAPIs {#apis}\n\nAn Application  Programming Interface (API) specifies the interface\nthrough which other programs can interact with a software component.\n\nLibraries and Frameworks {#libraries-and-frameworks}\n\nA library is a collection of modular code that is general and can be\nused by other programs. A software framework is a reusable\nimplementation of a software providing generic functionality that can\nbe selectively customized to produce a specific application. Libraries\nare meant to be used 'as is' while frameworks are meant to be\ncustomized/extended. Your code calls the library code while the\nframework code calls your code.\n\nPlatforms {#platforms}\n\nA platform provides a runtime environment for applications. A\nplatform is often bundled with libraries, tools and frameworks.\n\nQuality Assurance {#quality-assurance}\n\nQA ensures that the software being built has the required levels of\nquality. This is achieved through:\n\ncode reviews\n: the systematic examination of code with the\n    intention of finding where the code can be improved\n\nstatic analysis\n: analysis of the code without actually executing\n    the code (e.g. Linters)\n\nformal verification\n: mathematical techniques, used to prove the\n    correctness of a program. it can only be used to prove the\n    absence of errors, but only proves compliance with the\n    specification, and not the actual utility of the software.\n\nTesting {#testing}\n\nWhen testing, we execute a set of test cases, containing the input\nand the expected behaviour. Test cases can be determined based on\nthe specification.\n\nUnit Testing {#unit-testing}\n\ntesting individual units to ensure each piece works correctly. In\n    OOP, this includes writing one or more unit tests for each public\n    method of a class.\nA proper unit test requires the unit to be testing in isolation,\n    hence stubs are created for the dependencies.\nDependency injection is the process of replacing current\n    dependencies with another object, commonly seen with stubs.\n    Polymorphism can be used to implement this.\n\nIntegration Testing {#integration-testing}\n\ntesting whether different parts of the software work together as\n    expected. It aims to discover bugs in the \"glue code\" related to how\n    components interact with each other.\n\nSystem Testing {#system-testing}\n\nTakes the whole system and tests it against the system specification\nSystem test cases are based on the specified external behaviour of\n    the system\nSystem testing includes testing against non-functional requirements\n\nOthers {#others}\n\nalpha testing is performed by the users, under controlled conditions\n    set by the software development team\nbeta testing is performed by a selected subset of users of the\n    system in their natural work setting\ndogfooding is the creators of the product using their own product\ndeveloper testing is done by the developers themselves, so as to\n    locate the cause of test case failure or fixing bugs\nregression testing is the retesting the SUT to detect regressions when a system is modified.\n\nExploratory vs Scripted Testing {#exploratory-vs-scripted-testing}\n\nExploratory testing devises test cases on-the-fly, creating new test\n    cases based on the results of past test cases\n    dependent on the tester's prior experience and intuition\nScripted testing is a set of test cases based on the expected\n    behaviour of the SUT\n    more systematic, and hence likely to discover more bugs given\n        sufficient time\n\nAcceptance Testing {#acceptance-testing}\n\ntest the delivered system to ensure it meets the user requirements\n\n| System Testing                        | Acceptance Testing                                 |\n|---------------------------------------|----------------------------------------------------|\n| done against the system specification | Done against the requirements specification        |\n| done by testers on the project team   | done by a team that represents the customer        |\n| done on the development environment   | done on the deployment site, or a close simulation |\n| both negative and positive test cases | focus on positive test cases                       |\n\nCoverage {#coverage}\n\nCoverage is the metric used to measure the extent to which  testing\nexercises the code.\n\nfunction/method coverage\n: based on the functions executed\n\nstatement coverage\n: based on the number of lines of code executed\n\ndecision/branch coverage\n: based on the decision points exercised\n\ncondition coverage\n: based on the boolean sub-expressions\n\npath coverage\n: in terms of possible paths through a given part of\n    the code executed\n\nentry/exit coverage\n: in terms of possible calls to and exits from\n    the operations in the SUT\n\nTest Case Design {#test-case-design}\n\nblack-box\n: designed exclusively based on the SUT's specified\n    external behaviour\n\nwhite-box\n: test cases are designed based on what is known about\n    the SUT's implementation\n\ngray-box\n: uses some important information about the\n    implementation.\n\nEquivalence partitions are **groups of test inputs that are likely to\nbe processed by the SUTs in the same way**. This can be determined by\nidentifying:\n\ntarget object of method call\ninput parameters of method call\nother data objects accessed by the method, such as global\n    variables.\n\nBoundary Value analysis is a \\*test case design heuristic that is\nbased on the observation that bugs often result from incorrect\nhandling of boundaries of equivalence partitions\\*.\n\nOther heuristics include:\n\neach valid input at least once in a positive test case\nno more than 1 invalid input in a test case\n\nSoftware Engineering Principles {#software-engineering-principles}\n\nLaw of Demeter {#law-of-demeter}\n\nAn object should have limited knowledge of another object\nAn object should have limited interaction with closely related\n    classes, if foo is coupled to bar, which is coupled to goo, foo\n    should not be coupled to goo\nReduces coupling\n\nSOLID {#solid}\n\nSingle Responsibility Principle\n: every module or class should\n    have responsibility over a single part of the functionality\n    provided by the software, and that responsibility should be\n    entirely encapsulated by the class.\n\nOpen-Closed Principle\n: software entities (classes, modules,\n    functions, etc.) should be open for extension, but closed for\n    modification\"; that is, such an entity can allow its behaviour to\n    be extended without modifying its source code.\n\nLiskov Substitution Principle\n: Functions that use pointers or\n    references to base classes must be able to use objects of derived\n    classes without knowing it.- Interface Segregation Principle ::  no client should be forced to\n    depend on methods it does not use.\n\nDependency Inversion Principle\n: high level modules should not\n    depend on low level modules; both should depend on abstractions.\n    Abstractions should not depend on details.\n\nYAGNI\n: a principle of extreme programming (XP) that states a\n    programmer should not add functionality until deemed\n    necessary.\n\nDRY\n: Don't repeat yourself, i.e. No duplicate implementations\n\nBrook's Law\n: Adding people to a late project makes it later\n\nSoftware Development Life Cycles {#software-development-life-cycles}\n\nSDLC consists of different stages such as:\n\nRequirements\nAnalysis\nDesign\nImplementation\nTesting\n\nSequential models {#sequential-models}\n\nSoftware development as linear process\nUseful for problems that are well-understood and stable\n    Rarely applicable in real-world projects\n\nEach stage provides artifacts for use in next stage\n\nIterative models {#iterative-models}\n\nSeveral iterations\nEach iteration is a new version\n    Each iteration is a complete product\n\nEither breadth-first (all major components in parallel) or\n    depth-first (Flesh out some components at a time)\nMost projects use both, i.e. iterative and incremental process\n\nAgile models {#agile-models}\n\nIndividuals and interactions over processes and tools\nWorking software over comprehensive documentation\nCustomer collaboration over contract negotiation\nResponding to change over following a plan\n\nRequirements based on needs of users, clarified regularly, factored\n    into developmental schedule when appropriate\nRough project plan, high level design that evolves as the project\n    goes on\nStrong emphasis on transparency and responsibility sharing among\n    members\n\nPopular SDLC process models {#popular-sdlc-process-models}\n\nScrum {#scrum}\n\nScrum master\nDevelopment team\nProduct Owner\n\nDivided into Sprints (basic unit of development)\n    Preceded by planning meeting\n    Potentially deliverable product increment is done during Sprint\n    Creates self-organising teams by encouraging co-location of team\n        members\n    Customers can change their minds about their wants and needs\n    Sprint backlog\n        To do\nDaily scrums\n    What did you do?\n    What will you do?\n    Are there any impediments?\n\nExtreme Programming (XP) {#extreme-programming--xp}\n\nStresses customer satisfaction\nEmpowers developers to respond to changing customer requirements\nEmphasises teamwork\nCompletes software project via:\n    Communication\n    Simplicity\n    Feedback\n    Respect\n    Courage\n\nUnified process {#unified-process}\n\nInception\n    Understand problem and requirements\n    Communicate\n    Plan\nElaboration\n    Refine and expands requirements\nConstruction\n    Major implementation to support use cases\n    Refine and flesh out design models\n    Testing of all levels\n    Multiple releases\nTransition\n    Ready system for actual production use\n    Familiarise end users with the system\n\nCMMI (Capability Maturity Model Integration) {#cmmi--capability-maturity-model-integration}\n\nDetermine if process of an organisation is at a certain maturity level\nInitial\n    Processes unpredictable, poorly controlled and reactive\nManaged\n    Processes characterized for projects and reactive\nDefined\n    Processes characterized for organisations and proactive\nQuantitatively Managed\n    Processes measured and controllers\nOptimized\n    Focus on process improvement\n",
        "tags": []
    },
    {
        "uri": "/zettels/spaced_repetition",
        "title": "Spaced Repetition",
        "content": "\ntags\n: Productivity\n\n> what srs does is makes it easier to reconstruct a graph on the fly, by\n> memorizing certain nodes once you have certain node density, you can\n> do a lossless reconstruction\n>\n> -- Siaw Young Lau (02/08/2020)\n\nSRS Software {#srs-software}\n\nAnki\nl3kn/org-fc\n",
        "tags": []
    },
    {
        "uri": "/zettels/spark",
        "title": "Spark",
        "content": "\ntags\n: §data\\_science\n\nWhat is Apache Spark? {#what-is-apache-spark}\n\nA cluster computing platform designed to be fast and general-purpose.\nIt extends the MapReduce to support more types of computations, and\ncovers a wide range of workloads that previously required separate\ndistributed systems.\n\nSpark is a computational engine that is responsible for scheduling,\ndistributing and monitoring applications consisting of many\ncomputational tasks across many worker machines.\n\nThe Spark Stack {#the-spark-stack}\n\nSpark Core\n: contains basic functionality, including memory\n    management, scheduling, fault recovery and interacting\n    with storage systems. It contains the API for =RDD=s,\n    which represent a collection of items distributed\n    across many nodes that can be manipulated in parallel.\n\nSpark SQL\n: allows querying of data via SQL, and supports many\n    sources of data.\n\nSpark Streaming\n: Provides support for processing live streams of data.\n\nMLLib\n: Contains basic ML functionality, such as classification\n    and regression.\n\nGraphX\n: Library for manipulating graphs, and contains common graph\n    algorithms like PageRank.\n\nCluster Managers\n: Library that enables auto-scaling via cluster\n    managers such as Hadoop YARN, Apache Mesos, and its own\n    Standalone Scheduler.\n\nFor Data Scientists, Spark's builtin libraries help them visualize\nresults of queries in the least amount of time. For Data Processing,\nSpark allows Software Engineers to build distributed applications,\nwhile hiding the complexity of distributed systems programming and\nfault tolerance.\n\nWhile Spark supports all files stored in the Hahoop distributed\nfilesystem (HDFS), it does not require Hadoop.\n\nGetting Started {#getting-started}\n\nEnter the shell with spark-shell, or pyspark.\n\nlines = sc.textFile(\"README.md\")\nlines.count()\nlines.first()\n\nCore Spark Concepts {#core-spark-concepts}\n\nEvery Spark application consists of a driver program that launches\nvarious parallel operations on a cluster. The driver program contains\nthe application's main function and defines distributed datasets on\nthe cluster.\n\nDriver programs access Spark thorugh a SparkContext object, which\nrepresents a connection to a computing cluster.\n\nOnce we have a SparkContext, we use it to create RDDs. Driver programs\ntypically manage a number of nodes called executors. A lot of\nSpark's API revolves around passing functions to its operators to run\nthem on the cluster.\n\nlines = sc.textFile(\"README.md\")\nlines.filter(lambda line: \"Machine\" in line)\n\nRunning a Python script on Spark {#running-a-python-script-on-spark}\n\nbin/spark-submit includes the Spark dependencies, setting up the\nenvironment for Spark's Python API to function. To run a python\nscript, simply run spark-submit script.py.\n\nAfter linking an application to Spark, we need to create a\nSparkContext.\n\nfrom pyspark import SparkConf, SparkContext\n\nconf = SparkConf().setMaster(\"local\").setAppName(\"My app\")\nsc = SparkContext(conf=conf)\n\nProgramming with RDDs {#programming-with-rdds}\n\nAn RDD is a distributed collection of elements. All work is expressed\nas either creating new RDDs, transforming existing RDDs or calling\noperations on RDDs to compute a result.\n\nRDDs are created by: (1) loading an external dataset, or (2) creating\na collection of objects in the driver program.\n\nSpark's RDDs are by default recomputed every time an action is run. To\nreuse an RDD in multiple actions,  we can use rdd.persist. In\npractice, persist() is often used to load a subset of data into\nmemory to be queried repeatedly.\n\nlines = sc.parallelize([\"pandas\", \"i like pandas\"])\n\nRDD Operations {#rdd-operations}\n\nRDDs support transformations and actions. Transformations are\noperations on RDDs that return a new RDD (e.g. map and filter).\nActions are operations that return a result to the driver program, or\nwrite it to storage,and kick off a computation.\n\nerrorsRDD = inputRDD.filter(lambda x: \"error\" in x)\nwarningsRDD = inputRDD.filter(lambda x: \"warning\" in x)\nbadLinesRDD = errorsRDD.union(warningsRDD)\n\nSpark keeps track of RDD dependencies from various transformations in\na lineage graph, that way if a RDD is lost, it can be recreated from\nits dependencies.\n\nTransformations on RDDs are lazily evaluated, so Spark will not\nexecute until an action is seen.\n\nWhen passing a function that is a member of an object, or contains\nreferences to fields in an object, Spark sends the entire object to\nworker nodes, which can be larger than the information you need.  This\ncan also cause the program to fail, if the class contains objects that\nPython cannot pickle.\n\nBasic Transformations:\n\nmap(), flatMap()\npseudo-set operations: distinct(), union(), intersection(),\n    subtract()\ncartesian()\n\nActions:\n\nreduce(lambda x, y: f(x,y))\nfold(zero)(fn)  is reduce, but takes an additional zeroth-value parameter\n=take(n)=, =top(n)=, takeOrdered(n)(ordering),\n    takeSample(withReplacement, num, [seed])\naggregate(zero)(seqOp, combOp) is similar reduce, but used to return\n    a different type\nforeach(fn)\n\nPersistence {#persistence}\n\nDifferent level of persistence helps with making Spark jobs faster. If\na node with persisted data goes down, Spark will recreate the RDD from\nthe lineage graph.\n\n{{}}\n\nWorking with Key/Value Pairs {#working-with-key-value-pairs}\n\nSpark provides special operations on RDDs with KV pairs, called pair\nRDDs.\n\nFor Python and Scala, the RDD needs to be composed of tuples:\n\npairs = lines.map(lambda x: (x.split(\" \")[0], x))\n\nval pairs = lines.map(x => (x.split(\" \")(0), x))\n\nJava does not have a built-in tuple type, so it uses the\nscala.Tuple2 class.\n\nTransformations on Pair RDDs {#transformations-on-pair-rdds}\n\n| Function              | Purpose                                                                |\n|-----------------------|------------------------------------------------------------------------|\n| reduceByKey(func)     | Combines values with the same key                                      |\n| groupByKey()          | Group values with the same key                                         |\n| combineByKey(a,b,c,d) | Combine values with the same key using a different result type         |\n| mapValues(func)       | Apply a function to each value of a pair RDD without changing the key  |\n| flatMapValues(func)   | Apply a function that returns an iterator to each value of a pair RDD. |\n| keys()                | Returns an RDD for just the keys.                                      |\n| values()              | Returns an RDD of just the values                                      |\n| sortByKey()           | Returns an RDD sorted by the key.                                      |\n\nSet transformations:\n\n| Function       | Purpose                                                                        |\n|----------------|--------------------------------------------------------------------------------|\n| subtractByKey  | Remove elements with a key present in the other RDD.                           |\n| join           | Perform an inner join between the 2 RDDs                                       |\n| rightOuterJoin | Performs a join between 2 RDDs where the key must be present in the first RDD. |\n| leftOuterJoin  | Perform a join between 2 RDDs where the key must be in the other RDD.          |\n| cogroup        | Group data from both RDDs sharing the same key.                                |\n\nActions:\n\n| Function       | Purpose                                             |\n|----------------|-----------------------------------------------------|\n| countByKey()   | Count the number of elements for each key.          |\n| collectAsMap() | Collect the result as a map to provide easy lookup  |\n| lookup(key)    | Return all values associated with the provided key. |\n\nData Partitioning {#data-partitioning}\n\nSpark programs can choose to control their RDDs' partition to reduce\ncommunication. Sparks's partitioning is available on all RDDs of\nkey/value pairs, and cause the system to group elements based on a\nfunction of each key.\n\nWe use partitionBy() to return a new RDD that partitions the Spark\nframe efficiently. Below are the operations that benefit from\npartitioning:\n\ncogroup\ngroupWith\njoin\nleftOuterJoin\nrightOuterJoin\ngroupByKey\nreduceByKey\ncombineByKey\nlookup\n\nImplementing a custom partitioner in Python is relatively simple:\n\nimport urlparse\n\ndef hash_domain(url):\n    return hash(urlparse.urlparse(url).netloc)\n\nrdd.partitionBy(20, hash_domain)\n\nThe hash function will be compared by identity to that of other RDDs,\nso a global function object needs to be passed, rather than creating a\nnew lambda.\n\nLoading and Saving Your Data {#loading-and-saving-your-data}\n\nFor data stored in a local or distributed filesystem such as NFS,\nHDFS, or S3, Spark can access a variety of file formats including\ntext, JSON, SequenceFiles and protocol buffers. Spark also provides\nstructured data sources through SparkSQL, and allows connections to\ndatabases like Cassandra, HBase, Elasticsearch and JDBC databases.\n\nSequenceFiles are a popular Hadoop format composed of flat files with\nkey/value pairs. They have sync markers that allow Spark to seek to a\npoint in the file and then resynchronize with the record boundaries,\nallowing Spark to efficiently read them in parallel from multiple nodes.\n\ndata = sc.sequenceFile(inFile, # input file\n                       \"org.apache.hadoop.io.Text\", # key Class\n                       \"org.apache.hadoop.io.IntWritable\", # value Class\n                       10 # min partitions\n)\n\ndata = sc.parallelize(((\"Panda\", 3), (\"Kay\", 6)))\ndata.saveAsSequenceFile(outputFile)\n\nSparkSQL {#sparksql}\n\nSparkSQL can load any table supported by Apache Hive.\n\nfrom pyspark.sql import HiveContext\n\nhiveCtx = HiveContext(sc)\nrows = hiveCtx.sql(\"SELECT name, age FROM users\")\nfirstRow = rows.first()\n\nprint firstRow.name\n\nIt even supports loading JSON files, if the JSON data has a consistent\nschema cross records.\n\n{\"user\": {\"name\": \"Holden\", \"location\": \"SF\"}, \"text\": \"Nice\"}\n\ntweets = hiveCtx.jsonFile(\"tweets.json\")\ntweets.registerTempTable(\"tweets\")\nresults = hiveCtx.sql(\"SELECT user.name, text FROM tweets\")\n\nAdvanced Spark Programming {#advanced-spark-programming}\n\nIn this section, we look at some techniques that were not previously\ncovered, in particular shared variables: accumulators to aggregate\ninformation and broadcast variables to efficiently distribute large\nvalues.\n\n{\n  \"address\": \"address here\",\n  \"band\": \"40m\",\n  \"callsign\": \"KK6JLK\",\n  \"city\": \"SUNNYVALE\",\n  \"contactlat\": \"37.384733\",\n  \"contactlong\": \"-122.032164\",\n  \"county\": \"Santa Clara\",\n  \"dxcc\": \"291\",\n  \"fullname\": \"MATTHEW McPherrin\",\n  \"id\": 57779,\n  \"mode\": \"FM\",\n  \"mylat\": \"37.751952821\",\n  \"mylong\": \"-122.4208688735\"\n}\n\nAccumulators {#accumulators}\n\nWhen we normally pass functions to Spark, they can use variables\ndefined outside of them in the Spark program, but updates to these\nvariables are not progagated to the driver. Spark's shared variables\nrelax this restriction for two common typess of communication\npatterns: aggregation of results and broadcasts.\n\nfile = sc.textFile(inputFile)\n\nblankLines = sc.accumulator(0)\n\ndef extractCallSigns(line):\n    global blankLines # Make the global variable accessible\n    if (line == \"\"):\n        blankLines += 1\n    return line.split(\" \")\n\ncallSigns = file.flatMap(extractCallSigns)\n\nTasks on worker nodes cannot access the accumulator's value. This\nallows accumulators to be implemented efficiently without having to\ncommunicate every update.\n\nFor accumulators used in actions, Spark applies each task's update to\neach accumulator only once. Thus, for a reliable absolute value\ncounter, the accumulator should be in an action such as foreach(). For\naccumulators used in RDD tarnsformations instead of actions, this\nguarantee does not exist.\n",
        "tags": []
    },
    {
        "uri": "/zettels/spike_train_metrics",
        "title": "Spike Train Metrics",
        "content": "\ntags\n: Spiking Neural Networks\n\nWe study spike train metrics to quantify differences between event\nsequences. These metrics apply at both the single-neuron level and the\nmulti-neuronal level. Studying these metrics helps us identify\ncandidate features for neuronal codes. (Victor, 2005)\n\nSpike Trains as Point Processes {#spike-trains-as-point-processes}\n\nAction potentials are propagated without loss and result in the\nrelease of neurotransmitter. Hence, sequences of action potentials\nemitted by individual neurons are the natural focus of brain activity.\nThe choice of representing spike trains as point processes means we do\nnot have some algebraic operations defined. If a vector representation\n(neuronal activity as continuous voltage records) were chosen instead,\nvector-space operations like addition, dot-product would be\nimmediately available.\n\nOne issue with choosing a vectorial representation is that in vector\nspace, linearity plays a fundamental role, but this is at odds with\nthe nature of neural dynamics. Choosing to represent spike trains as\npoint processes prevents us from artificially limiting ourselves in\nthis manner.\n\nSpike Train Distance {#spike-train-distance}\n\nWe consider the dissimilarity of spike trains \\\\(d(A,B)\\\\), without the\nneed to define addition and multiplication on these spike trains.\nTypical metric spaces often do not have Euclidean geometry, and are\nmore general than vector spaces.\n\nThese distances are required to be a metric. This means:\n\n\\\\(d(A, B) > 0\\\\) with equality when \\\\(A = B\\\\)\n\\\\(d(A,B) = d(B,A)\\\\)\n\\\\(d(A,C) \\le d(A,B) + d(B, C)\\\\)\n\nEdit-distance Metrics {#edit-distance-metrics}\n\nOne simple way to derive a metric is to consider the total cost of\ntransforming \\\\(A\\\\) to \\\\(B\\\\) via elementary steps:\n\n\\begin{equation}\n  d(A, B)=\\min \\left\\\\{\\sum\\{j=0}^{n-1} c\\left(X\\{j}, X\\_{j+1}\\right)\\right\\\\}\n\\end{equation}\n\nwhere \\\\(\\left\\\\{X\\{0}, X\\{1}, \\dots, X\\_{n}\\right\\\\}\\\\) is a sequence of\nspike trains. These metrics often have an efficient\ndynamic-programming algorithm.\n\nSpike Train Metrics {#spike-train-metrics}\n\nWe know that the timings of individual spikes are crucial. To capture\nthis dependence, we consider 2 elementary steps:\n\nInserting or deleting a spike has a cost of 1 -- this ensures that\n    every spike train can be transformed to any other spike train by\n    some path\nThe cost of moving a single spike is proportional to the time that\n    the spike is moved: $c(X,Y) = q| t\\X - t\\Y| for some parameter \\\\(q\\\\).\n    This introduces the sensitivity to spike timing.\n\n\\\\(q\\\\) is a tunable parameter that needs to be experimented with.\n\nSpike Interval Metrics {#spike-interval-metrics}\n\nSimilarly, we can define a metric that is sensitive to patterns of\nspike intervals, rather than individual times. We again introduce 2\nelementary steps:\n\nInsertion or deletion of an interspike interval, having a cost\n    of 1.\nShortening or lengthening an existing interspike interval. This is\n    equal to \\\\(q \\Delta T\\\\) where \\\\(\\Delta T\\\\) is the amount of time by\n    which the interval has been lengthened or shortened.\n\nMulti-neuronal Cost-based Metrics {#multi-neuronal-cost-based-metrics}\n\nTo extend spike-train metrics to multiple neurons, an additional\nelementary step is added:\n\nChanging the label associated with an event, with cost \\\\(k\\\\)\n\nWhen \\\\(k = 0\\\\), the metric ignores the label associated with each event.\nWhen \\\\(k = 2\\\\), the effect of the label is maximal, since it costs as\nmuch to delete a spike and reinsert it with a new label.\n\nThere exists other kinds of elementary steps, or tweaks of the\nexisting elementary steps. These cost-based metrics can be thought of\nas formalizing a hypothesis that certain aspects of spike train\nstructure are meaningful.\n\nOne can find algorithms implementing these metrics in the public\ndomain:\n\nSpike Train Metrics on Vector-Space Embeddings {#spike-train-metrics-on-vector-space-embeddings}\n\nAnother class of metrics embed the spike trains into a vector space.\nTypically, the embedding is linear, and the resulting metric respects\nlinearity. However, this is not a prerequisite.\n\nBy expressing a spike train as a function of time \\\\(A(t)\\\\), we abstract\nwhat happens at a synapse. Vector-space metrics are also simpler to\ncalculate.\n\nSteps {#steps}\n\nExpress the event sequence as a sum of dirac-delta functions:\n\n\\begin{equation}\n  \\delta\\{A}(t)=\\sum\\{j=1}^{M(A)} \\delta\\left(t-t\\_{j}\\right)\n\\end{equation}\n\nConvolve the sum with a kernel function \\\\(K(t)\\\\):\n\n\\begin{equation}\n  A(t)=\\left(\\delta\\{A} \\* K\\right)(t)=\\int\\{-\\infty}^{\\infty} \\delta\\{A}(\\tau) K(t-\\tau) d \\tau=\\sum\\{j=1}^{M(A)} K\\left(t-t\\_{j}\\right)\n\\end{equation}\n\nAny vector-space distance can then be used to define a distance. The\n$L^p$-norm yields the distance:\n\n\\begin{equation}\n  d(A, B)=\\left(\\int\\_{-\\infty}^{\\infty}|A(t)-B(t)|^{p} d t\\right)^{1 / p}\n\\end{equation}\n\nThe van Rossum distance uses the $L^2$-distance, and the exponential\nkernel:\n\n\\begin{equation}\n  K^{V R}\\left(t\\{c} ; t\\right)=\\left\\\\{\\begin{array}{ll}{\\frac{1}{\\sqrt{t\\{c}}} e^{-t / t\\_{c}},} & {t \\geq 0} \\\\ {0,} & {t\\\\) is the $L^p$-norm\nbetween their associated temporal functions:\n\n\\begin{equation}\nd(A, B)=\\left(\\int\\{-\\infty}^{\\infty} \\sum\\{l=1}^{L}\\left|A\\{l}(t)-B\\{l}(t)\\right|^{p} d t\\right)^{1 / p}\n\\end{equation}\n\nfor each of their components.\n\nReferences {#references}\n\nBibliography\nVictor, J. D., Spike train metrics, Current opinion in neurobiology, 15(5), 585–592 (2005).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/spike_train_mutual_information",
        "title": "Spike Train Mutual Information",
        "content": "\nWould be nice to compute the mutual information between multi-neuronal\nspike trains in §spiking\\neural\\networks. Perhaps we can learn some\nlow-level spike representation, and relate this somehow to the\n§information\\bottleneck\\dnn theory.\n",
        "tags": []
    },
    {
        "uri": "/zettels/spiking_datasets",
        "title": "Spiking Datasets",
        "content": "\nDatasets for evaluating §spiking\\neural\\networks.\n\nSpiking Heidelberg Digits (SHD) dataset and the Spiking Speech Command (SSC) dataset\nN-Caltech101, N-MNIST and Characters\n",
        "tags": []
    },
    {
        "uri": "/zettels/spiking_neural_networks",
        "title": "Spiking Neural Networks",
        "content": "\ntags\n: §machine\\_learning\n",
        "tags": []
    },
    {
        "uri": "/zettels/spiking_neurons_lit_review",
        "title": "Spiking Neurons (Literature Review)",
        "content": "\ntags\n: §spiking\\neural\\networks\n\nIntroduction to Spiking Neural Networks  {#introduction-to-spiking-neural-networks}\n\nWhile the project is equal part reinforcement learning and spiking\nneural networks, reinforcement learning is a popular field and has\nbeen extensively covered by researchers worldwide\n(Ivanov \\& D'yakonov, 2019), (Li, 2018).\nHence, I have chosen instead to review the literature around spiking\nneural networks.\n\nThe Generations of Neural Networks {#the-generations-of-neural-networks}\n\nNeural network models can be classified into three generations,\naccording to their computational units: perceptrons, non-linear\nunits, and spiking neurons (\"Wolfgang Maass\", 1997).\n\nPerceptrons can be composed to produce a variety of models, including\nBoltzmann machines and Hopfield networks. Non-linear units are\ncurrently the most widely used computational unit, responsible for the\nexplosion of progress in machine learning research, in particular, the\nsuccess of deep learning. These units traditionally apply\ndifferentiable, non-linear activation functions such across a weighted\nsum of input values.\n\nThere are two reasons second-generation computational units have seen\nso much success. First, the computational power of these units is\ngreater than that of first-generation neural networks. Networks built\nwith second-generation computational units with one hidden layer are\nuniversal approximators for any continuous function with a compact\ndomain and range (\"Cybenko, 1989). Second, networks built with these\nunits are trainable with well-researched gradient-based methods, such\nas backpropagation.\n\nThe third generation of neural networks use computational units called\nspiking neurons. Much like our biological neurons, spiking neurons are\nconnected to each other at synapses, receiving incoming signals at the\ndendrites and sending spikes to other neurons via the axon. Each\ncomputational unit stores some state: in particular, it stores its\nmembrane potential at any point in time. Rather than fire at each\npropagation cycle, these computational units fire only when their\nindividual membrane potentials crosses its firing threshold. A simple\nspiking neuron model is given in .\n\nFrom this section onwards, we shall term second-generation neural\nnetworks Artificial Neural Networks (ANNs), and third-generation\nneural networks Spiking Neural Networks (SNNs).\n\nA Spiking Neuron Model  {#a-spiking-neuron-model}\n\nIn spiking neural networks, neurons exchange information via spikes,\nand the information received depends on:\n\nFiring frequencies\n: The relative timing of pre and post-synaptic\n    spikes, and neuronal firing patterns\n\nIdentity of synapses used\n: Which neurons are connected, whether their\n    synapses are inhibitory or excitatory, and synaptic strength\n\nEach neuron has a corresponding model that encapsulates its state: the\ncurrent membrane potential. As with the mammalian brain, incoming\nspikes increase the value of membrane potential. The membrane\npotential eventually decays to resting potential in the absence of\nspikes. These dynamics are often captured via first-order differential\nequations. Here we define the Spike Response Model (SRM), a simple but\nwidely-used model describing the momentary value of a neuron \\\\(i\\\\).\n\nWe define for presynaptic neuron \\\\(j\\\\), \\\\(\\epsilon\\{ij}(t) = u\\{i}(t) -\nu\\_{\\text{rest}}\\\\). For a few input spikes, the membrane potential responds\nroughly linearly to the input spikes:\n\n\\begin{equation}\nu\\i{t} = \\sum\\{j}\\sum\\{f} \\epsilon\\{ij}(t - t\\j^{(f)}) + u\\{\\text{rest}}\n\\end{equation}\n\nSRM describes the membrane potential of neuron \\\\(i\\\\) as:\n\n\\begin{equation}\nu\\i{t} = \\eta (t - \\hat{t\\i}) + \\sum\\{j}\\sum\\{f} \\epsilon\\{ij}(t - t\\j^{(f)}) + u\\_{\\text{rest}}\n\\end{equation}\n\nwhere \\\\(\\hat{t\\_i}\\\\) is the last firing time of neuron \\\\(i\\\\).\n\nWe refer to moment when a given neuron emits an action potential as\nthe firing time of that neuron. We denote the firing times of neuron\n\\\\(i\\\\) by \\\\(t\\_i^{(f)}\\\\) where \\\\(f = 1,2,\\dots\\\\) is the label of the spike.\nThen we formally denote the spike train of a neuron \\\\(i\\\\) as the\nsequence of firing times:\n\n\\begin{equation}\n  S\\i(t) = \\sum\\{f} \\delta\\left( t - t\\_i^{(f)} \\right)\n\\end{equation}\n\nwhere \\\\(\\delta(x)\\\\) is the Dirac-delta function with \\\\(\\delta(x) = 0\\\\)\nfor \\\\(x \\ne 0\\\\) and \\\\(\\int\\_{-\\infty}^{\\infty} \\delta(x)dx = 1\\\\). Spikes\nare thus reduced to points in time.\n\ndendrites\n: input device\n\nsoma\n: central processing unit (non-linear processing step). If the\n    total input exceeds a certain threshold, an output signal is generated\n\naxon\n: output device, delivering signal to other neurons\n\nsynapse\n: junction between two neurons\n\npost/presynaptic cells\n: If a neuron is sending a signal across a\n    synapse, the sending neuron is the presynaptic cell, and the\n    receiving neuron is the postsynaptic cell\n\n{{}}\n\naction potentials/spikes\n: short electrical pulses, typically of\n    amplitude about 100mV and a duration of 1-2ms\n\nspike train\n: a chain of action potentials (sequence of stereotyped\n    events) that occur at intervals. Since all spikes of\n    a given neuron look the same, the form of the spike\n    does not matter: the number and timing of the spikes\n    encode the information.\n\nabsolute refractory period\n: minimal distance between two spikes.\n    Spike are well separated, and it is impossible to excite a second\n    spike within this refractory period.\n\nrelative refractory period\n: follows the absolute refractory\n    period -- a period where it is difficult to excite an action\n    potential\n\nWe define for presynaptic neuron \\\\(j\\\\), \\\\(\\epsilon\\{ij}(t) = u\\{i}(t) -\nu\\_{rest}\\\\). For a few input spikes, the membrane potential responds\nroughly linearly to the input spikes:\n\n\\begin{equation}\n  u\\i{t} = \\sum\\{j}\\sum\\{f} \\epsilon\\{ij}(t - t\\j^{(f)}) + u\\{rest}\n\\end{equation}\n\nIf \\\\(u\\_i(t)\\\\) reaches threshold \\\\(\\vartheta\\\\) from below, neuron \\\\(i\\\\) fires\na spike.\n\nFrom the above, we can define the Spike Response Model describing the momentary\nvalue of the membrane potential of neuron \\\\(i\\\\):\n\n\\begin{equation}\n  u\\i{t} = \\eta (t - \\hat{t\\i}) + \\sum\\{j}\\sum\\{f} \\epsilon\\{ij}(t - t\\j^{(f)}) + u\\_{rest}\n\\end{equation}\n\nwhere \\\\(\\hat{t\\_i}\\\\) is the last firing time of neuron \\\\(i\\\\).\n\nWe refer to moment when a given neuron emits an action potential as\nthe firing time of that neuron. We denote the firing times of neuron\n\\\\(i\\\\) by \\\\(t\\_i^{(f)}\\\\) where \\\\(f = 1,2,\\dots\\\\) is the label of the spike.\nThen we formally denote the spike train of a neuron \\\\(i\\\\) as the\nsequence of firing times:\n\n\\begin{equation}\n  S\\i(t) = \\sum\\{f} \\delta\\left( t - t\\_i^{(f)} \\right)\n\\end{equation}\n\nwhere \\\\(\\delta(x)\\\\) is the Dirac \\\\(\\delta\\\\) function with \\\\(\\delta(x) = 0\\\\)\nfor \\\\(x \\ne 0\\\\) and \\\\(\\int\\_{-\\infty}^{\\infty} \\delta(x)dx = 1\\\\). Spikes\nare thus reduced to points in time.\n\nSRM only takes into account the most recent spike, and cannot capture\nadaptation.\n\nNeuronal Coding {#neuronal-coding}\n\nHow do spike trains encode information? At present, a definite answer\nto this question is not known.\n\nTemporal Coding\n\n    Traditionally, it had been thought that information was contained in\n    the mean firing rate of a neuron:\n\n    \\begin{equation}\n      v = \\frac{n\\_{sp}(T)}{T}\n    \\end{equation}\n\n    measured over some time window \\\\(T\\\\), counting the number of the spikes\n    \\\\(n\\\\). The primary objection to this is that if we need to compute a\n    temporal average to transfer information, then our reaction times\n    would be a lot slower.\n\n    From the point of view of rate coding, spikes are a convenient wa of\n    transmitting the analog output variable \\\\(v\\\\) over long spikes. The\n    optimal scheme is to transmit the value of rate \\\\(v\\\\) by a regular spike\n    train at intervals \\\\(\\frac{1}{v}\\\\), allowing the rate to be reliably\n    measured after 2 spikes. Therefore, irregularities in real spike\n    trains must be considered as noise.\n\nRate as spike density (average over several runs)\n\n    this definition works for both stationary and time-dependent stimuli.\n    The same stimulation sequence is repeated several times, and the\n    neuronal response is reported in a peri-stimulus-time histogram\n    (PSTH). We can obtain the spike density of the PSTH by:\n\n    \\begin{equation}\n      \\rho(t) =  \\frac{1}{\\Delta t} \\frac{n\\_K(t; t + \\Delta t)}{K}\n    \\end{equation}\n\n    where \\\\(K\\\\) is the number of repetitions of the experiment. We can\n    smooth the results to get a continuous rate.\n\n    The problem with this scheme is that it cannot be the decoding scheme\n    of the brain. This measure makes sense if there is always a population\n    of neurons with the same stimulus. This leads to population coding.\n\nRate as population activity (average over several neurons)\n\n    This is a simple extension of the spike density measure, but adding\n    activity across a population of neurons. Population activity varies\n    rapidly and can reflect changes in the stimulus nearly\n    instantaneously, an advantage over temporal coding. However, it\n    requires a homogeneous population of neurons, which is hardly\n    realistic.\n\nSpike Codes {#spike-codes}\n\nThese are coding strategies based on spike timing.\n\nTime-to-first-spike\n\n    A neuron which fires shortly after the reference signal (an abrupt\n    input, for example) may signal a strong stimulation, and vice-versa.\n    This estimate has been successfully used in an interpretation of\n    neuronal activity in primate motor cortex.\n\n    The argument is that the brain does not have time to evaluate more\n    than one spike per neuron per processing step, and hence the first\n    spike should contain most of the relevant information.\n\nPhase\n\n    Oscillations are common in the olfactory system, and other areas of\n    the brain. Neuronal spike trains could then encode information in the\n    phase of a pulse, with respect to the background oscillation.\n\nCorrelations and Synchrony\n\n    Synchrony between any pairs of neurons could signify special events\n    and convey information not contained in the firing rate of the\n    neurons.\n\nSpikes or Rates? {#spikes-or-rates}\n\nA code based on time-to-first-spike is consistent with a rate code: if\nthe mean firing rate of a neuron is high, then the time to first spike\nis expected to occur early. Stimulus reconstruction with a linear\nkernel can be seen as a special instance of a rate code. It is\ndifficult to draw a clear borderline between pulse and rate codes. The\nkey consideration in using any code is the ability for the system to\nreact quickly to changes in the input. If pulse coding is relevant,\ninformation processing in the brain must be based on spiking neuron\nmodels. For stationary input, spiking neuron models can be reduced to\nrate models, but in other cases, this reduction is not possible.\n\nMotivating Spiking Neural Networks {#motivating-spiking-neural-networks}\n\nSince second-generation neural networks have excellent performance,\nwhy bother with spiking neural networks? In this section, we motivate\nspiking neural networks from various perspectives.\n\nInformation Encoding\n\n    To directly compare ANNs and SNNs, one can consider the real-valued\n    outputs of ANNs to be the firing rate of a spiking neuron in steady\n    state. In fact, such rate coding has been used to explain\n    computational processes in the brain (Pfeiffer \\& Pfeil, 2018). Spiking\n    neuron models encode information beyond the average firing rate: these\n    models also utilize the relative timing between spikes\n    (Robert G\\\"utig, 2014), or spike phases (in-phase or\n    out-of-phase). These time-dependent codes are termed temporal codes,\n    and play an important role in biology. First, research has shown that\n    different actions are taken based on single spikes\n    (Martin Stemmler, 1996). Second, relying on the average firing rate\n    would greatly increase the latency of the brain, and our brain often\n    requires decision-making long before several spikes are accumulated.\n    It has also been successfully demonstrated that temporal coding\n    achieves competitive empirical performance on classification tasks for\n    both generated datasets, as well as image datasets like MNIST and\n    CIFAR (Comsa et al., 2019).\n\nBiological Plausibility\n\n    A faction of the machine learning and neurobiology community strives\n    for emulation of the biological brain. There are several\n    incompatibilities between ANNs and the current state of neurobiology\n    that are not easily reconciliated.\n\n    First, neurons in ANNs communicate via continuous-valued activations.\n    This is contrary to neurobiological research, which shows that\n    communication between biological neurons communicate by broadcasting\n    spike trains: trains of action potentials to downstream neurons. The\n    spikes are to a first-order approximation of uniform amplitude, unlike\n    the continuous-valued activations of ANNs.\n\n    Second, backpropagation as a learning procedure also presents\n    incompatibilities with the biological brain (\"Amirhossein Tavanaei et al., 2019).\n    Consider the chain rule in backpropagation:\n\n    \\begin{equation} \\label{chainrule}\n      \\delta\\{j}^{\\mu}=g^{\\prime}\\left(a\\{j}^{\\mu}\\right) \\sum\\{k} w\\{k j} \\delta\\_{k}^{\\mu}\n    \\end{equation}\n\n    \\\\(\\delta\\{j}^{\\mu}\\\\) and \\\\(\\delta\\{k}^{\\mu}\\\\) denote the partial\n    derivatives of the cost function for input pattern \\\\(\\mu\\\\) with respect\n    to the net input to some arbitrary unit \\\\(j\\\\) or \\\\(k\\\\). Unit \\\\(j\\\\) projects\n    feed-forward connections to the set of units indexed by \\\\(k\\\\).\n    \\\\(g(\\cdot)\\\\) is the activation function applied to the net input of unit\n    \\\\(j\\\\), denoted \\\\(a\\j^{\\mu}\\\\), \\\\(w\\{kj}\\\\) are the feedforward weights\n    projecting from unit \\\\(j\\\\) to the set of units indexed by \\\\(k\\\\).\n\n    The chain rule formulation presents two problems. First, the\n    gradients \\\\(g'(\\cdot)\\\\) requires derivatives, but \\\\(g(\\cdot)\\\\) in spiking\n    neurons is represented by sum of Dirac delta functions, for which\n    derivatives do not exist. Second, the expression \\\\(\\sum\\{k} w\\{k j}\n    \\delta\\_{k}^{\\mu}\\\\) uses feedforward weights in a feedback fashion. This\n    mean that backpropagation is only possible in the presence of\n    symmetric feedback weights, but these do not exist in the brain. In\n    addition, during backpropagation the error assignment for each neuron\n    is computed using non-local information.\n\nNeuromorphic Hardware\n\n    In a traditional Von Neumann architecture, the logic core operates on\n    data fetched sequentially from memory. In contrast, in neuromorphic\n    chips both computation and memory are distributed across computational\n    units that are connected via synapses. The neuronal architecture and\n    parameters hence play a key role in information representation and\n    define the computations that are performed.\n\n    It has also been observed that spike-trains in the mammalian brain are\n    often sparse in time, suggesting that timing and relative timings of\n    spikes encode large amounts of information. Neuromorphic chips\n    implement this same sparse, low-precision communication protocol\n    between neurons on the chip, and by offering the same asynchronous,\n    event-based parallelism paradigm that the brain uses, are able to\n    perform certain workloads with much less power than Von Neumann chips.\n\n    These integrated circuits are typically programmed with spiking neural\n    networks. Examples of such chips include IBM's TrueNorth\n    (Merolla et al., 2014) and Intel's Loihi (Davies et al., 2018). Because\n    spiking neural networks have not yet been successfully trained on many\n    tasks, neuromorphic chips has seen little practical use. These chips\n    have only recently been successfully used in robotic navigation\n    (\"Tang et al., 2019), and solving graph problems by manual construction of the\n    network graph (\"William Severa et al., 2016).\n\nTraining Spiking Neural Networks {#training-spiking-neural-networks}\n\nAs explained in , it is desirable to train spiking\nneural networks to perform arbitrary tasks, utilizing power-efficient\nneuromorphic chips that break the Von Neumann bottleneck. We classify\nthe training strategies by their usage of gradients, and discuss\ncertain optimization techniques.\n\nNon-gradient based methods\n\n    Spiking neurons communicate via spikes, hence, unlike ANNs, gradients\n    are non-existent. In addition, backpropagation is not biologically\n    plausible (see ). This motivates the use of\n    plasticity-based methods and evolutionary strategies for training\n    SNNs.\n\n    One category of learning rules used in SNNs are local learning rules.\n    These rules include Hebbian learning (neurons that fire together wire\n    together), and its extension: the spike-timing-dependent-plasticity\n    rule (STDP). Inspired by experiments in neuroscience, central to these\n    learning rules is the theme that neuron spike ordering and their\n    relative timings encode information. STDP adjusts the strength of\n    connections between neurons using the relative timing of a neuron's\n    output and its input potentials (hence, spike-timing dependent).\n\n    In machine learning terminology, the weights of the synapses are\n    adjusted according to fixed rules for each training example. Each\n    synapse is given a weight \\\\(0 \\le w \\le w\\_{max}\\\\) , characterizing its\n    strength, and its change depends on the exact moments \\\\(t\\_{pre}\\\\) of\n    pre-synaptic spikes and \\\\(t\\_{post}\\\\) of post-synaptic spikes\n    (Alexander Sboev et al., 2018):\n\n    \\begin{equation}\n      \\Delta w=\\left\\\\{\\begin{array}{l}{-\\alpha \\lambda \\cdot \\exp\n                        \\left(-\\frac{t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}}{\\tau\\_{-}}\\right),\n                        \\text {if } t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}>0}\n                        \\\\ {\\lambda \\cdot \\exp\n                        \\left(-\\frac{t\\{\\mathrm{post}}-t\\{\\mathrm{pre}}}{\\tau\\_{+}}\\right),\n                        \\text {if }\n                        t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}(Hazan et al., 2018) that simulate SNNs on Von Neumann\n    computers implementing these rules. Recent attempts have been made to\n    combine Reinforcement Learning and STDP: both in solving RL problems\n    (Hazan et al., 2018), and using the reinforcement learning\n    framework to train SNN\n    (Bing et al., 2019), (Lee et al., 2018). However, SNNs\n    trained using the STDP learning rule have yet to achieve comparable\n    performance compared to ANNs on relatively simple datasets like MNIST\n    (\"Amirhossein Tavanaei et al., 2019).\n\nGradient-based methods\n\n    Performance is important for practical applications, and\n    gradient-based training methods such as backpropagation has shown\n    competitive performance. It is thus desirable to train spiking neural\n    networks with these gradient-based methods.\n\n    There are several problems with spike-compatible gradient-based\n    methods. First, most of these methods cannot train neurons in the\n    hidden layers: they can only train neurons at the final layer, that\n    receive the desired target output pattern\n    (Robert Urbanczik \\& Walter Senn, 2009), (Lee et al., 2016).\n    Second, the discontinuous, binary nature of spiking output needs to be\n    addressed. For example, SpikeProp approximates the membrane\n    threshold function at a local area with a linear function, introducing\n    gradients and computing the exact formulae for error backpropagation\n    for synaptic weights and spike times (Bohte et al., 2000). Others have\n    modified the threshold function with a gate function\n    (Huh \\& Sejnowski, 2018), used the alpha transfer function to derive\n    gradient update rules (Comsa et al., 2019),\n    and approximate the dirac-delta spikes with a probability density\n    function (Shrestha \\& Orchard, 2018).\n\n    Another approach is converting trained ANN models into SNNs\n    (Rueckauer et al., 2016). Common ANN layers such\n    as softmax, batch normalization and max-pooling layers have their\n    corresponding spiking counterparts.\n\n    Equilibrium Propagation was recently proposed to solve the\n    neurobiological incompatibilities of backpropagation\n    (Scellier \\& Bengio, 2017). Because the gradients are defined only\n    in terms of local perturbations, the synaptic updates correspond to\n    the standard form of STDP. The propagated signal encodes the gradients\n    of a well-defined objective function on energy-based models, where the\n    goal is to minimize the energy of the model. To resolve the issue of\n    communication using binary-valued signals, step-size annealing was\n    used to train spiking neural networks with Equilibrium Propagation\n    (O'Connor et al., 2019).\n\nFuture Research Areas\n\n    A nascent area is local learning on neuromorphic chips. Thus far\n    spiking neural networks are simulated and trained before deployment on\n    a neuromorphic chip. In Intel's Loihi chip, each core contains a\n    learning engine that can update synaptic weights using the 4-bit\n    microcode-programmed learning rules that are associated with that\n    synapse. This opens up areas for online learning.\n\n    Neural network models can be classified into three generations,\n    according to their computational units: perceptrons, non-linear\n    units, and spiking neurons (\"Wolfgang Maass\", 1997).\n\n    Perceptrons can be composed to produce a variety of models, including\n    Boltzmann machines and Hopfield networks. Non-linear units are\n    currently the most widely used computational unit, responsible for the\n    explosion of progress in machine learning research, in particular, the\n    success of deep learning. These units traditionally apply\n    differentiable, non-linear activation functions such across a weighted\n    sum of input values.\n\n    There are two reasons second-generation computational units have seen\n    so much success. First, the computational power of these units is\n    greater than that of first-generation neural networks. Networks built\n    with second-generation computational units with one hidden layer are\n    universal approximators for any continuous function with a compact\n    domain and range (\"Cybenko, 1989). Second, networks built with these\n    units are trainable with well-researched gradient-based methods, such\n    as backpropagation.\n\n    The third generation of neural networks use computational units called\n    spiking neurons. Much like our biological neurons, spiking neurons are\n    connected to each other at synapses, receiving incoming signals at the\n    dendrites and sending spikes to other neurons via the axon. Each\n    computational unit stores some state: in particular, it stores its\n    membrane potential at any point in time. Rather than fire at each\n    propagation cycle, these computational units fire only when their\n    individual membrane potentials crosses its firing threshold. A simple\n    spiking neuron model is given in .\n\n    From this section onwards, we shall term second-generation neural\n    networks Artificial Neural Networks (ANNs), and third-generation\n    neural networks Spiking Neural Networks (SNNs).\n\nA Spiking Neuron Model  {#a-spiking-neuron-model}\n\nIn spiking neural networks, neurons exchange information via spikes,\nand the information received depends on:\n\nFiring frequencies\n: The relative timing of pre and post-synaptic\n    spikes, and neuronal firing patterns\n\nIdentity of synapses used\n: Which neurons are connected, whether their\n    synapses are inhibitory or excitatory, and synaptic strength\n\nEach neuron has a corresponding model that encapsulates its state: the\ncurrent membrane potential. As with the mammalian brain, incoming\nspikes increase the value of membrane potential. The membrane\npotential eventually decays to resting potential in the absence of\nspikes. These dynamics are often captured via first-order differential\nequations. Here we define the Spike Response Model (SRM), a simple but\nwidely-used model describing the momentary value of a neuron \\\\(i\\\\).\n\nWe define for presynaptic neuron \\\\(j\\\\), \\\\(\\epsilon\\{ij}(t) = u\\{i}(t) -\nu\\_{\\text{rest}}\\\\). For a few input spikes, the membrane potential responds\nroughly linearly to the input spikes:\n\n\\begin{equation}\nu\\i{t} = \\sum\\{j}\\sum\\{f} \\epsilon\\{ij}(t - t\\j^{(f)}) + u\\{\\text{rest}}\n\\end{equation}\n\nSRM describes the membrane potential of neuron \\\\(i\\\\) as:\n\n\\begin{equation}\nu\\i{t} = \\eta (t - \\hat{t\\i}) + \\sum\\{j}\\sum\\{f} \\epsilon\\{ij}(t - t\\j^{(f)}) + u\\_{\\text{rest}}\n\\end{equation}\n\nwhere \\\\(\\hat{t\\_i}\\\\) is the last firing time of neuron \\\\(i\\\\).\n\nWe refer to moment when a given neuron emits an action potential as\nthe firing time of that neuron. We denote the firing times of neuron\n\\\\(i\\\\) by \\\\(t\\_i^{(f)}\\\\) where \\\\(f = 1,2,\\dots\\\\) is the label of the spike.\nThen we formally denote the spike train of a neuron \\\\(i\\\\) as the\nsequence of firing times:\n\n\\begin{equation}\n  S\\i(t) = \\sum\\{f} \\delta\\left( t - t\\_i^{(f)} \\right)\n\\end{equation}\n\nwhere \\\\(\\delta(x)\\\\) is the Dirac-delta function with \\\\(\\delta(x) = 0\\\\)\nfor \\\\(x \\ne 0\\\\) and \\\\(\\int\\_{-\\infty}^{\\infty} \\delta(x)dx = 1\\\\). Spikes\nare thus reduced to points in time.\n\nMotivating Spiking Neural Networks {#motivating-spiking-neural-networks}\n\nSince second-generation neural networks have excellent performance,\nwhy bother with spiking neural networks? In this section, we motivate\nspiking neural networks from various perspectives.\n\nInformation Encoding\n\n    To directly compare ANNs and SNNs, one can consider the real-valued\n    outputs of ANNs to be the firing rate of a spiking neuron in steady\n    state. In fact, such rate coding has been used to explain\n    computational processes in the brain (Pfeiffer \\& Pfeil, 2018). Spiking\n    neuron models encode information beyond the average firing rate: these\n    models also utilize the relative timing between spikes\n    (Robert G\\\"utig, 2014), or spike phases (in-phase or\n    out-of-phase). These time-dependent codes are termed temporal codes,\n    and play an important role in biology. First, research has shown that\n    different actions are taken based on single spikes\n    (Martin Stemmler, 1996). Second, relying on the average firing rate\n    would greatly increase the latency of the brain, and our brain often\n    requires decision-making long before several spikes are accumulated.\n    It has also been successfully demonstrated that temporal coding\n    achieves competitive empirical performance on classification tasks for\n    both generated datasets, as well as image datasets like MNIST and\n    CIFAR (Comsa et al., 2019).\n\nBiological Plausibility\n\n    A faction of the machine learning and neurobiology community strives\n    for emulation of the biological brain. There are several\n    incompatibilities between ANNs and the current state of neurobiology\n    that are not easily reconciliated.\n\n    First, neurons in ANNs communicate via continuous-valued activations.\n    This is contrary to neurobiological research, which shows that\n    communication between biological neurons communicate by broadcasting\n    spike trains: trains of action potentials to downstream neurons. The\n    spikes are to a first-order approximation of uniform amplitude, unlike\n    the continuous-valued activations of ANNs.\n\n    Second, backpropagation as a learning procedure also presents\n    incompatibilities with the biological brain (\"Amirhossein Tavanaei et al., 2019).\n    Consider the chain rule in backpropagation:\n\n    \\begin{equation} \\label{chainrule}\n      \\delta\\{j}^{\\mu}=g^{\\prime}\\left(a\\{j}^{\\mu}\\right) \\sum\\{k} w\\{k j} \\delta\\_{k}^{\\mu}\n    \\end{equation}\n\n    \\\\(\\delta\\{j}^{\\mu}\\\\) and \\\\(\\delta\\{k}^{\\mu}\\\\) denote the partial\n    derivatives of the cost function for input pattern \\\\(\\mu\\\\) with respect\n    to the net input to some arbitrary unit \\\\(j\\\\) or \\\\(k\\\\). Unit \\\\(j\\\\) projects\n    feed-forward connections to the set of units indexed by \\\\(k\\\\).\n    \\\\(g(\\cdot)\\\\) is the activation function applied to the net input of unit\n    \\\\(j\\\\), denoted \\\\(a\\j^{\\mu}\\\\), \\\\(w\\{kj}\\\\) are the feedforward weights\n    projecting from unit \\\\(j\\\\) to the set of units indexed by \\\\(k\\\\).\n\n    The chain rule formulation presents two problems. First, the\n    gradients \\\\(g'(\\cdot)\\\\) requires derivatives, but \\\\(g(\\cdot)\\\\) in spiking\n    neurons is represented by sum of Dirac delta functions, for which\n    derivatives do not exist. Second, the expression \\\\(\\sum\\{k} w\\{k j}\n    \\delta\\_{k}^{\\mu}\\\\) uses feedforward weights in a feedback fashion. This\n    mean that backpropagation is only possible in the presence of\n    symmetric feedback weights, but these do not exist in the brain. In\n    addition, during backpropagation the error assignment for each neuron\n    is computed using non-local information.\n\nNeuromorphic Hardware\n\n    In a traditional Von Neumann architecture, the logic core operates on\n    data fetched sequentially from memory. In contrast, in neuromorphic\n    chips both computation and memory are distributed across computational\n    units that are connected via synapses. The neuronal architecture and\n    parameters hence play a key role in information representation and\n    define the computations that are performed.\n\n    It has also been observed that spike-trains in the mammalian brain are\n    often sparse in time, suggesting that timing and relative timings of\n    spikes encode large amounts of information. Neuromorphic chips\n    implement this same sparse, low-precision communication protocol\n    between neurons on the chip, and by offering the same asynchronous,\n    event-based parallelism paradigm that the brain uses, are able to\n    perform certain workloads with much less power than Von Neumann chips.\n\n    These integrated circuits are typically programmed with spiking neural\n    networks. Examples of such chips include IBM's TrueNorth\n    (Merolla et al., 2014) and Intel's Loihi (Davies et al., 2018). Because\n    spiking neural networks have not yet been successfully trained on many\n    tasks, neuromorphic chips has seen little practical use. These chips\n    have only recently been successfully used in robotic navigation\n    (\"Tang et al., 2019), and solving graph problems by manual construction of the\n    network graph (\"William Severa et al., 2016).\n\nTraining Spiking Neural Networks {#training-spiking-neural-networks}\n\nAs explained in , it is desirable to train spiking\nneural networks to perform arbitrary tasks, utilizing power-efficient\nneuromorphic chips that break the Von Neumann bottleneck. We classify\nthe training strategies by their usage of gradients, and discuss\ncertain optimization techniques.\n\nNon-gradient based methods\n\n    Spiking neurons communicate via spikes, hence, unlike ANNs, gradients\n    are non-existent. In addition, backpropagation is not biologically\n    plausible (see ). This motivates the use of\n    plasticity-based methods and evolutionary strategies for training\n    SNNs.\n\n    One category of learning rules used in SNNs are local learning rules.\n    These rules include Hebbian learning (neurons that fire together wire\n    together), and its extension: the spike-timing-dependent-plasticity\n    rule (STDP). Inspired by experiments in neuroscience, central to these\n    learning rules is the theme that neuron spike ordering and their\n    relative timings encode information. STDP adjusts the strength of\n    connections between neurons using the relative timing of a neuron's\n    output and its input potentials (hence, spike-timing dependent).\n\n    In machine learning terminology, the weights of the synapses are\n    adjusted according to fixed rules for each training example. Each\n    synapse is given a weight \\\\(0 \\le w \\le w\\_{max}\\\\) , characterizing its\n    strength, and its change depends on the exact moments \\\\(t\\_{pre}\\\\) of\n    pre-synaptic spikes and \\\\(t\\_{post}\\\\) of post-synaptic spikes\n    (Alexander Sboev et al., 2018):\n\n    \\begin{equation}\n      \\Delta w=\\left\\\\{\\begin{array}{l}{-\\alpha \\lambda \\cdot \\exp\n                        \\left(-\\frac{t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}}{\\tau\\_{-}}\\right),\n                        \\text {if } t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}>0}\n                        \\\\ {\\lambda \\cdot \\exp\n                        \\left(-\\frac{t\\{\\mathrm{post}}-t\\{\\mathrm{pre}}}{\\tau\\_{+}}\\right),\n                        \\text {if }\n                        t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}(Hazan et al., 2018) that simulate SNNs on Von Neumann\n    computers implementing these rules. Recent attempts have been made to\n    combine Reinforcement Learning and STDP: both in solving RL problems\n    (Hazan et al., 2018), and using the reinforcement learning\n    framework to train SNN\n    (Bing et al., 2019), (Lee et al., 2018). However, SNNs\n    trained using the STDP learning rule have yet to achieve comparable\n    performance compared to ANNs on relatively simple datasets like MNIST\n    (\"Amirhossein Tavanaei et al., 2019).\n\nGradient-based methods\n\n    Performance is important for practical applications, and\n    gradient-based training methods such as backpropagation has shown\n    competitive performance. It is thus desirable to train spiking neural\n    networks with these gradient-based methods.\n\n    There are several problems with spike-compatible gradient-based\n    methods. First, most of these methods cannot train neurons in the\n    hidden layers: they can only train neurons at the final layer, that\n    receive the desired target output pattern\n    (Robert Urbanczik \\& Walter Senn, 2009), (Lee et al., 2016).\n    Second, the discontinuous, binary nature of spiking output needs to be\n    addressed. For example, SpikeProp approximates the membrane\n    threshold function at a local area with a linear function, introducing\n    gradients and computing the exact formulae for error backpropagation\n    for synaptic weights and spike times (Bohte et al., 2000). Others have\n    modified the threshold function with a gate function\n    (Huh \\& Sejnowski, 2018), used the alpha transfer function to derive\n    gradient update rules (Comsa et al., 2019),\n    and approximate the dirac-delta spikes with a probability density\n    function (Shrestha \\& Orchard, 2018).\n\n    Another approach is converting trained ANN models into SNNs\n    (Rueckauer et al., 2016). Common ANN layers such\n    as softmax, batch normalization and max-pooling layers have their\n    corresponding spiking counterparts.\n\n    Equilibrium Propagation was recently proposed to solve the\n    neurobiological incompatibilities of backpropagation\n    (Scellier \\& Bengio, 2017). Because the gradients are defined only\n    in terms of local perturbations, the synaptic updates correspond to\n    the standard form of STDP. The propagated signal encodes the gradients\n    of a well-defined objective function on energy-based models, where the\n    goal is to minimize the energy of the model. To resolve the issue of\n    communication using binary-valued signals, step-size annealing was\n    used to train spiking neural networks with Equilibrium Propagation\n    (O'Connor et al., 2019).\n\nFuture Research Areas\n\n    A nascent area is local learning on neuromorphic chips. Thus far\n    spiking neural networks are simulated and trained before deployment on\n    a neuromorphic chip. In Intel's Loihi chip, each core contains a\n    learning engine that can update synaptic weights using the 4-bit\n    microcode-programmed learning rules that are associated with that\n    synapse. This opens up areas for online learning.\n\nProbabilistic SNNs {#probabilistic-snns}\n\nA probabilistic model defines the outputs of all spiking neurons as\njointly distributed binary random processes. The joint distribution is\ndifferentiable in the synaptic weights, and principled learning\ncriteria from statistics and information theory such as likelihood and\nmutual information apply. The maximization of such criteria do not\nrequire the implementation of the backpropagation mechanism, and often\nrecover as special cases known biologically plausible algorithms.\n\nGraphical Representation {#graphical-representation}\n\nA SNN consists of a network of \\\\(N\\\\) spiking neurons. At any time \\\\(t =\n0,1,2, \\dots\\\\) each neouron \\\\(i\\\\) outputs a binary signal \\\\(s\\_{i,t} =\n\\\\{0,1\\\\}\\\\), with value \\\\(s\\_{i,t} = 1\\\\) corresponding to a spike emitted at\ntime \\\\(t\\\\). We collect in vector \\\\(s\\{t} = \\left( s\\{i,t}: i \\in V \\right)\\\\)\nthe binary signals emitted by all neurons at time \\\\(t\\\\), where \\\\(V\\\\) is\nthe set of all neurons. Each neuron \\\\(i \\in V\\\\) receives the signals\nemitted by a subset \\\\(P\\_i\\\\) of neurons through directed links, known as\nsynapses. Neurons in a set \\\\(P\\i\\\\) are referred to as pre-synaptic_ for\npost-synaptic neuron \\\\(i\\\\).\n\nThe internal, analog state of each spiking neuron \\\\(i \\in V\\\\) at time\n\\\\(t\\\\) is defined by its membrane potential \\\\(u\\_{i,t}\\\\).\n\nLong short-term memory and learning-to-learn in networks of spiking neurons (Bellec et al., 2018) {#long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons}\n\nKey contribution: Inclusion of adapting neurons into recurrent SNN\nmodels (RSNNs) increases computing and learning capability. By using a\nlearning algorithm that combines BPTT with a rewiring algorithm that\noptimizes the network architecture, performance comes close to LSTM\nANNs.\n\nModel composition: LSNNs consist of a populaction \\\\(R\\\\) of\nintegrate-and-fire (LIF) neurons (excitatory and inhibitory), and a\nsecond population \\\\(A\\\\) of LIF excitatory neurons whose excitability is\ntemporarily reduced through preceding firing activity. \\\\(R\\\\) and \\\\(A\\\\)\nreceive spike trains from a population \\\\(X\\\\) of external input neurons.\nResults of computations are read out by a population \\\\(Y\\\\) of external\nlinear readout neurons.\n\n{{}}\n\nBPTT is done by replacing the non-continuous membrane potential  with\na pseudo derivative that smoothly increases from 0 to 1.\n\nLearning to Learn LSNNs {#learning-to-learn-lsnns}\n\n> LSTM networks are especially suited for L2L since they can\n> accommodate two levelsof learning and representation of learned\n> insight: Synaptic connections and weights can encode,on a higher\n> level, a learning algorithm and prior knowledge on a large time-scale.\n> The short-termmemory of an LSTM network can accumulate, on a lower\n> level of learning, knowledge during thecurrent learning task\n\nGradient Descent for Spiking Neural Networks (Huh \\& Sejnowski, 2017) {#gradient-descent-for-spiking-neural-networks}\n\nkey idea: Replacing the non-differentiable model for membrane\npotential:\n\n\\begin{equation}\n  \\tau \\dot{s} = -s + \\sum\\{k} \\delta (t - t\\k)\n\\end{equation}\n\nwith\n\n\\begin{equation}\n\\tau \\dot{s} = -s + g \\dot{v}\n\\end{equation}\n\nfor some gate function \\\\(g\\\\), and \\\\(\\dot{v}\\\\) is the time derivative of\nthe pre-synaptic membrane voltage.\n\nExact gradient calculations can be done with BPTT, or real-time\nrecurrent learning. The resultant gradients are similar to\nreward-modulated spike-time dependent plasticity.\n\nTODO Surrogate Gradient Learning in Spiking Neural Networks (Neftci et al., 2019) {#surrogate-gradient-learning-in-spiking-neural-networks}\n\nTODO Theories of Error Back-Propagation in the Brain (James Whittington \\& Rafal Bogacz, 2019) {#theories-of-error-back-propagation-in-the-brain}\n\n§comsa2019\\temp\\coding {#comsa2019-temp-coding--comsa2019-temp-coding-dot-md}\n\nSTDP {#stdp}\n\nSTDP is a biologically inspired long-term plasticity model, in which\neach synapse is given a weight \\\\(0 \\le w \\le w\\_{maxx}\\\\) , characterizing its\nstrength, and its change depends on the exact moments \\\\(t\\_{pre}\\\\) of\npresynaptic spikes and \\\\(t\\_{post}\\\\) of postsynaptic spikes:\n\n\\begin{equation}\n  \\Delta w=\\left\\\\{\\begin{array}{l}{-\\alpha \\lambda \\cdot \\exp\n                    \\left(-\\frac{t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}}{\\tau\\_{-}}\\right),\n                    \\text {if } t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}>0}\n                    \\\\ {\\lambda \\cdot \\exp\n                    \\left(-\\frac{t\\{\\mathrm{post}}-t\\{\\mathrm{pre}}}{\\tau\\_{+}}\\right),\n                    \\text {if }\n                    t\\{\\mathrm{pre}}-t\\{\\mathrm{post}}(Alexander Sboev et al., 2018)\n\nLoihi {#loihi}\n\nDescribes SNNs as a weighted, directed graph \\\\( G(V, E)\\\\) where the\n    vertices \\\\(V\\\\) represent compartments, and the weighted edges \\\\(E\\\\)\n    represent synapses.\nBoth compartments and synapses maintain internal state and\n    communicate only via discrete spike impulses.\nUses a variant of the CUBA model for the neuron model, which is\n    defined as a set of first-order differential equation using traces,\n    evaluated at discrete algorithmic time steps.\n\nLearning must follow the sum-of-products form:\n\n\\begin{equation}\n  Z(t) = Z(t-1) + \\sum\\m S\\m \\prod\\n F\\n\n\\end{equation}\n\nwhere \\\\(Z\\\\) is the synaptic state variable defined for the source\ndestination neuron pair being updated, and \\\\(F-N\\\\) may be a synaptic\nstate variable, a pre-synaptic trace or a post-synaptic trace defined\nfor the neuron pair.\n\nGenerating Spike Trains {#generating-spike-trains}\n\nPoisson Model (Heeger, 2000) {#poisson-model}\n\nIndependent spike hypothesis: the generation of each spike is\nindependent of all other spikes. If the underlying instantaneous\nfiring rate \\\\(r\\\\) is constant over time, it is a homogeneous Poisson\nprocess.\n\nWe can write:\n\n\\begin{equation}\n  P(\\textrm{1 spike during } \\delta t) \\approx r \\delta t\n\\end{equation}\n\nWe divide time into short, discrete intervals \\\\(\\delta t\\\\). Then, we\ngenerate a sequence of random numbers \\\\(x[i]\\\\) uniformly between 0\nand 1. For each interval, if \\\\(x[i] \\le r \\delta t\\\\), generate a spike.\n\n{#}\n\nBibliography\nIvanov, S., & D'yakonov, A., Modern Deep Reinforcement Learning Algorithms, CoRR, (),  (2019).  ↩\n\nLi, Y., Deep Reinforcement Learning, CoRR, (),  (2018).  ↩\n\nMaass, W., Networks of spiking neurons: the third generation of neural network models, Neural Networks, 10(9), 1659–1671 (1997).  http://dx.doi.org/https://doi.org/10.1016/S0893-6080(97)00011-7 ↩\n\nCybenko, G., Approximation by superpositions of a sigmoidal function, Mathematics of Control, Signals and Systems, 2(4), 303–314 (1989).  http://dx.doi.org/10.1007/BF02551274 ↩\n\nPfeiffer, M., & Pfeil, T., Deep learning with spiking neurons: opportunities and challenges, Frontiers in neuroscience, 12(),  (2018).  ↩\n\nRobert G\\\"utig, To spike, or when to spike?, Current Opinion in Neurobiology, 25(nil), 134–139 (2014).  http://dx.doi.org/10.1016/j.conb.2014.01.004 ↩\n\nStemmler, M., A single spike suffices: the simplest form of stochastic resonance in model neurons, Network: Computation in Neural Systems, 7(4), 687–716 (1996).  http://dx.doi.org/10.1088/0954-898x74_005 ↩\n\nComsa, I. M., Potempa, K., Versari, L., Fischbacher, T., Gesmundo, A., & Alakuijala, J., Temporal coding in spiking neural networks with alpha synaptic function, CoRR, (),  (2019).  ↩\n\nTavanaei, A., Ghodrati, M., Kheradpisheh, S. R., Masquelier, T., & Maida, A., Deep learning in spiking neural networks, Neural Networks, 111(), 47–63 (2019).  http://dx.doi.org/https://doi.org/10.1016/j.neunet.2018.12.002 ↩\n\nMerolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada, J., Akopyan, F., Jackson, B. L., …, A million spiking-neuron integrated circuit with a scalable communication network and interface, Science, 345(6197), 668–673 (2014).  http://dx.doi.org/10.1126/science.1254642 ↩\n\nDavies, M., Srinivasa, N., Lin, T., Chinya, G., Cao, Y., Choday, S. H., Dimou, G., …, Loihi: a neuromorphic manycore processor with on-chip learning, IEEE Micro, 38(1), 82–99 (2018).  ↩\n\nTang, G., Shah, A., & Michmizos, K. P., Spiking neural network on neuromorphic hardware for energy-efficient unidimensional slam, CoRR, (),  (2019).  ↩\n\nSevera, W., Parekh, O., Carlson, K. D., James, C. D., & Aimone, J. B., Spiking network algorithms for scientific computing, 2016 IEEE International Conference on Rebooting Computing (ICRC), (), 1–8 (2016).  ↩\n\nSboev, A., Vlasov, D., Rybka, R., & Serenko, A., Spiking neural network reinforcement learning method based on temporal coding and stdp, Procedia Computer Science, 145(nil), 458–463 (2018).  http://dx.doi.org/10.1016/j.procs.2018.11.107 ↩\n\nHazan, H., Saunders, D. J., Khan, H., Patel, D., Sanghavi, D. T., Siegelmann, H. T., & Kozma, R., Bindsnet: a machine learning-oriented spiking neural networks library in python, Frontiers in Neuroinformatics, 12(), 89 (2018).  http://dx.doi.org/10.3389/fninf.2018.00089 ↩\n\nBing, Z., Baumann, I., Jiang, Z., Huang, K., Cai, C., & Knoll, A., Supervised learning in snn via reward-modulated spike-timing-dependent plasticity for a target reaching vehicle, Frontiers in Neurorobotics, 13(), 18 (2019).  http://dx.doi.org/10.3389/fnbot.2019.00018 ↩\n\nLee, C., Panda, P., Srinivasan, G., & Roy, K., Training deep spiking convolutional neural networks with stdp-based unsupervised pre-training followed by supervised fine-tuning, Frontiers in Neuroscience, 12(), 435 (2018).  http://dx.doi.org/10.3389/fnins.2018.00435 ↩\n\nUrbanczik, R., & Senn, W., A gradient learning rule for the tempotron, Neural Computation, 21(2), 340–352 (2009).  http://dx.doi.org/10.1162/neco.2008.09-07-605 ↩\n\nLee, J., Delbruck, T., & Pfeiffer, M., Training Deep Spiking Neural Networks Using Backpropagation, Frontiers in Neuroscience, 10(),  (2016).  http://dx.doi.org/10.3389/fnins.2016.00508 ↩\n\nBohte, S., Kok, J., & Poutré, J., Spikeprop: backpropagation for networks of spiking neurons., In ,  (pp. 419–424) (2000). : . ↩\n\nHuh, D., & Sejnowski, T. J., Gradient descent for spiking neural networks, In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 1433–1443) (2018). : Curran Associates, Inc. ↩\n\nShrestha, S. B., & Orchard, G., Slayer: spike layer error reassignment in time, In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 1412–1421) (2018). : Curran Associates, Inc. ↩\n\nRueckauer, B., Lungu, I., Hu, Y., & Pfeiffer, M., Theory and tools for the conversion of analog to spiking convolutional neural networks, CoRR, (),  (2016).  ↩\n\nScellier, B., & Bengio, Y., Equilibrium propagation: bridging the gap between energy-based models and backpropagation, Frontiers in Computational Neuroscience, 11(), 24 (2017).  http://dx.doi.org/10.3389/fncom.2017.00024 ↩\n\nO'Connor, P., Gavves, E., & Welling, M., Training a spiking neural network with equilibrium propagation, In K. Chaudhuri, & M. Sugiyama, Proceedings of Machine Learning Research (pp. 1516–1523) (2019). : PMLR. ↩\n\nBellec, G., Salaj, D., Subramoney, A., Legenstein, R., & Maass, W., Long short-term memory and learning-to-learn in networks of spiking neurons, CoRR, (),  (2018).  ↩\n\nHuh, D., & Sejnowski, T. J., Gradient descent for spiking neural networks, CoRR, (),  (2017).  ↩\n\nNeftci, E. O., Mostafa, H., & Zenke, F., Surrogate gradient learning in spiking neural networks, CoRR, (),  (2019).  ↩\n\nWhittington, J. C., & Bogacz, R., Theories of error back-propagation in the brain, Trends in Cognitive Sciences, 23(3), 235–250 (2019).  http://dx.doi.org/10.1016/j.tics.2018.12.005 ↩\n\nHeeger, D., Poisson model of spike generation, Handout, University of Standford, 5(), 1–13 (2000).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/spiking_software",
        "title": "SNN Software",
        "content": "\nSoftware for Spiking Neural Networks.\n\nPython {#python}\n\nGitHub - BindsNET/bindsnet: Simulation of spiking neural networks (SNNs) usin...\nGitHub - norse/norse: A library to do deep learning with spiking neural netwo...\nBrian 2 documentation — Brian 2 2.3 documentation\n\nPaper Implementations {#paper-implementations}\n\nSurrogate Gradient Learning · GitHub\nGitHub - bamsumit/slayerPytorch: PyTorch implementation of SLAYER for trainin...\nGitHub - uber-research/backpropamine: Train self-modifying neural networks wi...\nGitHub - fzenke/pub2018superspike: Code from our paper: SuperSpike: Supervise...\nGitHub - nmi-lab/dcll\n",
        "tags": []
    },
    {
        "uri": "/zettels/ssnlp",
        "title": "SSNLP Conference Notes",
        "content": "\ntags\n: §conferences, §nlp\n\nAliNLP: the Text Processing Engine that Powers Alibaba's Business Applications {#alinlp-the-text-processing-engine-that-powers-alibaba-s-business-applications}\n\nLinlin Li\n\nAliNLP is a large-scale NLP technology platform for the entire Alibaba\necosystem.\n\nUtilizes behaviour data instead of demanding human annotations of\n    NLP algorithms\nUtilizing multiple correlated tasks for improving effectiveness of\n    individual tasks of the complex Alibaba eco-system\n\nNLP Data:\n\nlanguage dictionary\ntoken & POS corpus\nEntity Corpus\nTreebank\nSentiment Corpus\nNews Category Corpus\nWord Relation Graph\nKnowledge Graph\n\nNLP Foundations:\n\nLexical Analysis\nSyntactical Analysis\nSemantic Analysis\ntext Analysis\nDeep Learning\n    Word2vec\n    Graph2vec\n\nChallenges in Chinese word segmentation application\n\nLimited amount of human annotated training data\nFrequent appearance of out-of-vocabulary words\n\nTaobao has billions of user search logs available\n    auto semi-annotated data\n\nMulti-task learning : Combine human annotated data and automatically\n\nMachine Reading Comprehension {#machine-reading-comprehension}\n\nAim at answering a question about a given domain/paragraph\nUsing IR based selection method to choose the relevant document/paragraph\nNo Pre-knowledge or FAQ needed\n\nEncode Layer (BiLSTM) -> Attention Layer -> Match Layer (Bilinear\nMatch) -> Output Layer (Pointer Network)\n\nUsed in AliMe, Timi, Lazada Chatbot\n\nIdo Dagan: {#ido-dagan}\n\nNews Tweet Illustration: What is being said here?\n\nStructured Knowledge Graphs (freebase knowledge graph)\n\n\"Natural Representations\": Open IE - isolates propositions as\npredicate--argument tuple\n\nQA-SRL: Question answering - semantic role labeling\n\nEach QA pair represents one predicate-argument relation\nQA roles: Gunman takes own life after killing thrtee in Wisconsin spa\nshooting. Form QASemDep Graph.\n\nco-referring nodes/edges collapsed.\n\nNoah Smith: Synchretizing Structured and Learned Representations {#noah-smith-synchretizing-structured-and-learned-representations}\n\nBroad-coverage semantic parsing: baselines\nBuilding SCAFFOLD from alternative tasks\nPutting a SPIGOT on the pipeline\n\nNL Semantics : Two Operationalizations {#nl-semantics-two-operationalizations}\n\ndependencies: a graph, entities/concepts/events as nodes,\n    relations as edges\n    Turboparser (Martins et al)\n    JAMR (Flanigan et. al)\nspans: segmentation into labeled parts\n\nBoth associate words with predicates and arguments\n\nLinguistic Structure Prediction {#linguistic-structure-prediction}\n\nx -> differentiable -> y (conventional NN)\nx -> differentiable -> \\\\(argmax\\_{y\\in Y}S^Ty\\\\) -> structured output\n(input text) -> (part representations) -> dynamic programming/spanning\ntree/inference method (trained with loss e.g. hinge loss)\n\nNeurboparser\nOpen SESAME\n\njoint learning\nUse low-rank, sparse tensors\nPromoting sparsity :\n\n\\begin{equation}\n  \\lambda \\sum\\{y\\i, z\\j \\in C} \\left|S\\left( y\\i, z\\_j \\right) \\right|\n\\end{equation}\n\ngave 14x speedup\n\nGreedy, Joint Syntactic-Semantic Parsing with Stack LSTMs\n\nStructured Attention, Straight-through estimator (Hinton 2012)\n\nSPIGOT (structured projection of intermediate gradients)\n\n\\\\(\\hat{Z}\\\\), the convex hull of intermediate parses\n\nSentence as graph vs sentence as sequence\n\ndependency tree vs dependency semantics\n\nAutomatic Essay Scoring {#automatic-essay-scoring}\n\nRegression, classification, preference ranking\n    Prompt Independent Features\n        length, syntax, style etc.\n    Argumentation Features\n        argument components\n        argument relations\nNeural Models\n    No need for feature engineering: word (vectors) as features\n    Hard to interpret results\n\nEssay scoring engines provides no feedback to the student on how to\nimprove the essay.\n\nQuestion Answering & QANet {#question-answering-and-qanet}\n\n-end-to-end models\n\nSQuAD dataset\n\nBase Model {#base-model}\n\nBiDAF\n\nIdea #1: Combine Convolution and Self-Attention\nConvolution: Captures local context\n    But Global interaction requires \\\\(O(\\log\\_kN)\\\\) layers, and\n        interactions become weaker as it goes deeper\n\nPosition Encoding -> repeat(Separable Convolution) -> Self Attention\n-> Feed Forward\n\nAugmentation:\n\nNMT, en - de -en  to get new QA pair\n\nDeep Embedding through transfer learning:\n\nELMo (words have different embeddings depending on context)\n",
        "tags": []
    },
    {
        "uri": "/zettels/state_estimation",
        "title": "State Estimation",
        "content": "\nState estimation seeks to recover state variables from the data.\nProbabilistic state estimation computes belief distributions over\npossible world states.\n\nExamples of state estimation include §robot\\_localization.\n\n§bayes\\_filter\n",
        "tags": []
    },
    {
        "uri": "/zettels/statistical_learning",
        "title": "Statistical Learning",
        "content": "\ntags\n: §machine\\_learning, §statistics\n\nIntroduction {#introduction}\n\nStatistical learning refers to a vast set of tools for _understanding\ndata_. It involves building a statistical model for predicting, or\nestimating, an output based on one or more input.\n\nOur goal is to apply a statistical learning method to the training data\nin order to estimate the unknown function \\\\(f\\\\). Broadly speaking, most\nstatistical learning methods for this task can be characterized as\neither parametric or non-parametric.\n\nParametric Methods {#parametric-methods}\n\nParametric methods involve a two-step model-based approach:\n\nFirst, we make an assumption about the functional form, or shape,\n    of \\\\(f\\\\). For example, one simple assumption is that \\\\(f\\\\) is linear in\n    \\\\(X\\\\):\n\n\\begin{equation}\nf(X) = \\beta\\0 + \\beta\\1X\\1 + \\dots + \\beta\\p X\\_p\n\\end{equation}\n\nThis is a linear model, and with this assumption the problem of\nestimating \\\\(f\\\\) is greatly simplified. Instead of estimating an\narbitrary p-dimensional function \\\\(f(X)\\\\), one only needs to estimate\nthe \\\\(p+1\\\\) coefficients \\\\(\\beta\\0, \\dots, \\beta\\p\\\\).\n\nAfter a model has been selected, we need a procedure that uses the\ntraining data to fit or train the model. In the case of the linear\nmodel, we need to estimate the parameters \\\\(\\beta\\0, \\beta\\1, \\dots,\n\\beta\\_p\\\\). The most common approach to fitting the model is referred to\nas the (ordinary) least squares.\n\nThe model-based approach just described is referred to as parametric;\nit reduces the problem of estimating \\\\(f\\\\) down to one of estimating a\nset of parameters. The potential disadvantage of a parametric approach\nis that the model we choose will usually not match the true unknown\nform of \\\\(f\\\\). If the chosen model is too far from the true \\\\(f\\\\), then\nour estimate will be poor. Flexible models require a large amount of\nparameters, and complex models are also susceptible to overfitting.\n\nNon-parametric Methods {#non-parametric-methods}\n\nNon-parametric methods do not make explicit assumptions about the\nfunctional form of \\\\(f\\\\). Instead, they seek an estimate of \\\\(f\\\\) that\ngets as close to the data points as possible without being too rough\nor wiggly. By avoiding assumptions of a particular form of \\\\(f\\\\),\nnon-parametric approaches can possibly fit the wider range of possible\nshapes of \\\\(f\\\\). However, they do not reduce the problem of estimating\n\\\\(f\\\\) to a small number of parameters, and a large number of\nobservations is required to obtain an accurate estimate for \\\\(f\\\\). An\nexample of a non-parametric model is the thin-plate spline.\n\nThe Trade-Off Between Prediction Accuracy and Model Interpretability {#the-trade-off-between-prediction-accuracy-and-model-interpretability}\n\n{{(James et al., 2013)\" >}}\n\nThere are several reasons to choose a restrictive model over a\nflexible approach. First, if the interest is mainly in inference,\nrestrictive models tend to be much more interpretable. For example\nlinear models make it easy to understand the associations between\nindividual predictors and the response. Even when predictions are the\nonly concern, highly flexible models are susceptible to overfitting,\nand restrictive models can often outperform them.\n\nMeasuring Quality of Fit {#measuring-quality-of-fit}\n\nIn order to evaluate the performance of a statistical learning method\non a given data set, we need some way to measure how well its\npredictions actually match the observed data.\n\nIn the regression setting, the most commonly-used measure is the mean\nsquared error (MSE), given by:\n\n\\begin{equation} \\label{eqn:dfn:mse}\n  \\mathrm{MSE} = \\frac{1}{n} \\mathop{\\sum}\\{i=1}^{n} (y\\i - \\hat{f}(x\\_i))^2\n\\end{equation}\n\nThe MSE will be small if the predicted responses are very close to the\ntrue responses.\n\nBias-Variance Trade-Off {#bias-variance-trade-off}\n\nIt can be shown taht the expected test MSE, for a given value \\\\(x\\_0\\\\),\ncan always be decomposed into the sum of 3 fundamental qualities: the\nvariance of \\\\(\\hat{f}(x\\0)\\\\), the squared bias of \\\\(\\hat{f}(x\\0)\\\\), and\nthe variance of the error terms \\\\(\\epsilon\\\\):\n\n\\begin{equation} \\label{eqn:bvtrade}\n  \\mathrm{E} (y\\0 - \\hat{f}(x\\0))^2 = \\mathrm{Var}(\\hat{f}(x\\_0)) + \\left[\n  \\mathrm{Bias}(\\hat{f}(x\\_0)) \\right]^2 + \\mathrm{Var}(\\epsilon)\n\\end{equation}\n\nThe expected test MSE refers to the average test MSE that we would\nobtain if we repeatedly estimated \\\\(f\\\\) using a large number of training\nsets, and tested each at \\\\(x\\_0\\\\).\n\nThe variance of a statistical learning method refers to the amount by\nwhich \\\\(\\hat{f}\\\\) would change if we estimated it using a different\ntraining data set. Since the training data are used to fit the\nstatistical learning method, different training data sets will result\nin a different \\\\(\\hat{f}\\\\). In general, more flexible statistical\nmethods have higher variance.\n\nBias refers to the error that is introduced by approximating a\nreal-life problem, which may be extremely complicated, by a much\nsimpler model. In general, more flexible approaches have lower bias.\n\nThe Bayes Classifier {#the-bayes-classifier}\n\nWhen the test error rate is minimized, the classifier assigns each\nobservation to the most likely class, given its predictor values. This\nis known as the Bayes classifier. The Bayes classifier produces the\nlowest possible error rate, known as the Bayes error rate, given by:\n\n\\begin{equation} \\label{eqn:bayes\\error\\rate}\n  1 - \\mathrm{E} \\left( \\mathop{\\mathrm{max}}\\_{j} \\mathrm{Pr}(Y = j | X) \\right)\n\\end{equation}\n\nIn theory, we would always like to predict qualitative responses using\nthe Bayes classifier. However, for real data, we do not know the\nconditional distribution of \\\\(Y\\\\) given \\\\(X\\\\), and computing the Bayes\nclassifier would be impossible.\n\nK-Nearest Neighbours {#k-nearest-neighbours}\n\nThe Bayes classifier serves as the unattainable gold standard against\nwhich to compare other methods. Many approaches attempt to estimate\nthe conditional distribution of \\\\(Y\\\\) given \\\\(X\\\\), and then classify the\ngiven observation to the class with highest estimated probability. One\nsuch method is the K-nearest neighbours classifier.\n\nGiven a positive integer \\\\(K\\\\) and a test observation \\\\(x\\_0\\\\), the KNN\nclassifier first identifies the K points in the training data that are\nclosest to \\\\(x\\0\\\\), represented by \\\\(N\\0\\\\). It then estimates the\nconditional probability for class \\\\(j\\\\) as the fraction of points in\n\\\\(N\\_0\\\\) whose response values equal \\\\(j\\\\):\n\n\\begin{equation} \\label{eqn:knn}\n  \\mathrm{Pr}(Y=j | X = x\\0) = \\frac{1}{K} \\sum\\{i\\in N\\0}I(y\\i = j)\n\\end{equation}\n\nFinally, KNN applies Bayes rule and classifies the test observation\n\\\\(x\\_0\\\\) to the class with the largest probability.\n\nThe Statistical Learning Framework {#the-statistical-learning-framework}\n\nConsider the problem of classifying a papaya into 2 bins: tasty or not\ntasty. We've chosen 2 features:\n\nThe papaya's colour, ranging from dark green through orange and red\n    to dark brown\nThe papaya's softness, ranging from rock hard to mushy\n\nThe learner's input consists of:\n\nDomain set: An arbitrary set, \\\\(\\mathcal{X}\\\\). This is the set of objects\n    that we may wish to label. The domain set in our example will be\n    the set of all papayas. Usually, these domain\n    represented by a vector of features (like colour and softness). We\n    also refer to domain points as instances and to \\\\(\\mathcal{X}\\\\) as the\n    instance space.\nLabel set: The label set is restricted in our example to a\n    two-element set, usually {0, 1} or {-1, +1}.\nTraining data: \\\\(S = ((x\\1, y\\1) \\dots (x\\m, y\\m))\\\\) is a finite\n    sequence of pairs in \\\\(\\mathcal{X} \\times \\mathcal{Y}\\\\). This is the\n    input that the learner has access to. Such labeled examples are\n    often called training examples. \\\\(S\\\\) is also sometimes referred to\n    as the training set.\nThe learner's output: The learner is requested to output a\n    prediction rule \\\\(h: \\mathcal{X} \\rightarrow \\mathcal{Y}\\\\). This\n    function is also called a predictor, a hypothesis, or a classifier.\n    The predictor can be used to predict the label of new domain\n    points.\nA simple data-generation model: This explains how the training data\n    is generated. First, we assume that the instances are generated by\n    some probability distribution. We denote the probability\n    distribution over \\\\(\\mathcal{X}\\\\) by \\\\(\\mathcal{D}\\\\). we do not assume\n    that the learner knows anything about this distribution.\nMeasures of success: We define the error of a classifier to be the\n    probability that it does not predict the correct label on a random\n    data point generated by the underlying distribution. That is, the\n    error of \\\\(h\\\\) is the probability to draw a random instance \\\\(x\\\\) from\n    \\\\(\\mathcal{D}\\\\), such that \\\\(h(x) \\ne f(x)\\\\), where \\\\(f(x)\\\\) is the true\n    labelling function:\n\n    \\begin{equation} \\label{eqn:dfn:error}\n      L\\{\\mathcal{D}, f} (h) \\overset{\\mathrm{def}}{=} \\mathop{P}\\{x \\sim \\mathcal{D}} \\left[ h(x) \\ne f(x) \\right] \\overset{\\mathrm{def}}{=}\n    \\mathcal{D} (\\\\{ x: h(x) \\ne f(x) \\\\} )\n    \\end{equation}\n\nThe learner is blind to the underlying probability distribution\n\\\\(\\mathcal{D}\\\\) over the world, and to the labelling function \\\\(f\\\\). The\nlearner can only interact with the environment through the training set.\n\nClassification {#classification}\n\nThe linear regression model assumes that the response variable \\\\(Y\\\\) is\nquantitative. However, in many cases the response variable is\nqualitative. Classification encompasses approaches that predict\nqualitative responses. 3 of the most widely-used classifiers include:\nlogistic regression, linear discriminant analysis, and K-nearest\nneighbours. More computer-intensive methods include generalized\nadditive models, trees, random forests, boosting, and support vector\nmachines.\n\nWhy not Linear Regression? {#why-not-linear-regression}\n\nEncoding non-binary categorical variables as a dummy variable using\nintegers can lead to a unwanted encoding of a relationship between the\ndifferent options. With binary outcomes, linear regression does do a\ngood job as a classifier: in fact, it is equivalent to linear\ndiscriminant analysis.\n\nSuppose we encode the outcome \\\\(Y\\\\) as follows:\n\n\\begin{equation}\n  Y = \\begin{cases}\n    0 & \\text{if No} \\\\\\\\\\\\\n    1 & \\text{if Yes} \\\\\\\\\\\\\n    \\end{cases}\n\\end{equation}\n\nThen the population \\\\(E(Y|X = x) = \\mathrm{Pr}(Y=1|X=x)\\\\), which may\nseem to imply that regression is perfect for the task. However, linear\nregression may produce probabilities less than zero or bigger than\none, hence logistic regression is more appropriate.\n\nLogistic Regression {#logistic-regression}\n\nRather than modelling the response \\\\(Y\\\\) directly, logistic regression\nmodels the probability that \\\\(Y\\\\) belongs to a particular category.\n\nHow should we model the relationship between \\\\(p(X) =\n\\mathrm{Pr}(Y=1|X)\\\\) and \\\\(X\\\\)? In the linear regression model, we used\nthe formula:\n\n\\begin{equation}\np(X) = \\beta\\0 + \\beta\\1 X\n\\end{equation}\n\nThis model for \\\\(p(X)\\\\) is not suitable because any time a straight line\nis fit to a binary response that is coded as a 0 or 1, in principle we\ncan always predict \\\\(p(X)  1\\\\)\nfor others.\n\nIn logistic regression, we use the logistic function:\n\n\\begin{equation}\n  p(X) = \\frac{e^{\\beta\\0 + \\beta\\1 X}}{1 + e^{\\beta\\0 + \\beta\\1 X}}\n\\end{equation}\n\nThis restricts values of \\\\(p(X)\\\\) to be between 0 and 1. A bit of\nrearrangement gives:\n\n\\begin{equation}\n\\log \\left( \\frac{p(X)}{1-p(X)} \\right) = \\beta\\0 + \\beta\\1 X\n\\end{equation}\n\nAnd this monotone transformation is called the log odds or logit\n transformation of \\\\(p(X)\\\\).\n\nWe use maximum likelihood to estimate the parameters:\n\n\\begin{equation}\n  l(\\beta\\0, \\beta) = \\prod\\{i:y\\i=1} p(x\\i) \\prod\\{i:y\\i=0} (1 - p(x\\_i))\n\\end{equation}\n\nThis likelihood gives the probability of the observed zeros and ones\nin the data. We pick \\\\(\\beta\\0\\\\) and \\\\(\\beta\\1\\\\) to maximize the\nlikelihood of the observed data.\n\nAs with linear regression, we can compute the coefficient values, the\nstandard error of the coefficients, the z-statistic, and the p-value.\nThe z-statistic plays the same role as the t-statistic. A large\nabsolute value of the z-statistic indicates evidence against the null\nhypothesis.\n\nMultiple Logistic Regression {#multiple-logistic-regression}\n\nIt is easy to generalize the formula to multiple logistic regression:\n\n\\begin{equation}\n\\log \\left( \\frac{p(X)}{1-p(X)} \\right) = \\beta\\0 + \\beta\\1 X\\_1 +\n\\dots + \\beta\\p X\\p\n\\end{equation}\n\n\\begin{equation}\np(X) = \\frac{e^{\\beta\\0 + \\beta\\1X\\1 + \\dots + \\beta\\pX\\p}}{1 + e^{\\beta\\0 + \\beta\\1X\\1 + \\dots + \\beta\\pX\\p}}\n\\end{equation}\n\nSimilarly, we use the maximum likelihood method to estimate the\ncoefficient.\n\nTODO Case Control Sampling {#case-control-sampling}\n\nCase control sampling is most effective when the prior probabilities of the classes are very unequal.\n\nLinear Discriminant Analysis {#linear-discriminant-analysis}\n\nLogistic regression involves directly modelling \\\\(\\mathrm{Pr}(Y=k|X=x)\\\\)\nusing the logistic function. We now consider an alternative and less\ndirect approach to estimating these probabilities. We model the\ndistribution of the predictors \\\\(X\\\\) separately in each of the response\nclasses (i.e. given \\\\(Y\\\\)), and then use Bayes' theorem to flip these\naround into estimates for \\\\(\\mathrm{Pr}(Y=k|X=x)\\\\).\n\nWhen these distributions are assumed to be normal, it turns out that\nthe model is very similar in form to logistic regression.\n\nWhy do we need another method?\n\nWhen the classes are well-separated, the parameter estimates for\n    the logistic regression model are surprisingly unstable. LDA does\n    not suffer from this issue.\nIf n is small, and the distribution of the predictors \\\\(X\\\\) is\n    approximately normal in each of the classes, the LDA model is more\n    stable than the logistic regression model.\nLDA is more popular when we have more than 2 response classes.\n\nWe first state Bayes' theorem, and write it differently for\ndiscriminant analysis:\n\n\\begin{equation} {eqn:dfn:bayes}\n  \\mathrm{Pr}(Y=k|X=x) = \\frac{\\mathrm{Pr}(X=x|Y=k) \\cdot \\mathrm{Pr}(Y=k)}{\\mathrm{Pr}(X=x)}\n\\end{equation}\n\n\\begin{equation}\n  \\mathrm{Pr}(Y=k|X=x) = \\frac{\\pi\\k f\\k(x)}{\\sum\\{l=1}^{K}\\pi\\lf\\_l(x)}\n\\end{equation}\n\nwhere \\\\(f\\_k(x) = \\mathrm{Pr}(X=x|Y=k)\\\\) is the density for \\\\(X\\\\) in class \\\\(k\\\\), and\n\\\\(\\pi\\_k = \\mathrm{Pr}(Y=k)\\\\) is the prior probability for class \\\\(k\\\\).\n\nWe first discuss LDA when \\\\(p = 1\\\\). The Gaussian density has the form:\n\n\\begin{equation}\n  f\\k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma\\k}e^{2\\frac{1}{2}\\left( \\frac{x-\\mu\\k}{\\sigma\\k} \\right)^2}\n\\end{equation}\n\nWe can plug this into Bayes formula and get a complicated expression\nfor \\\\(p(x)\\\\). To classify at the value \\\\(X = x\\\\), we just need to see\nwhich of \\\\(p\\_k(x)\\\\) is largest. Taking logs, and discarding terms that\ndo not depend on \\\\(k\\\\), we see that this is equivalent to assigning \\\\(x\\\\)\nto the class with the largest discriminant score:\n\n\\begin{equation}\n  \\partial\\k(x) = x \\cdot \\frac{numerator}{\\mu\\k}{\\sigma^2} -\n  \\frac{\\mu\\k^2}{2\\sigma^2}+ \\log(\\pi\\k)\n\\end{equation}\n\nNote that \\\\(\\partial\\_k(x)\\\\) is a linear function of \\\\(x\\\\). If there are\n\\\\(K=2\\\\) classes, and \\\\(\\pi\\1 = \\pi\\2 = 0.5\\\\), we can see that the decision\nboundary becomes \\\\(x = \\frac{\\mu\\1 + \\mu\\2}{2}\\\\).\n\nWe can estimate the parameters:\n\n\\begin{equation}\n  \\hat{\\pi\\k} = \\frac{n\\k}{n}\n\\end{equation}\n\n\\begin{equation}\n  \\hat{\\mu\\k} = \\frac{1}{n\\k}\\sum\\{i:y\\i=k}x\\_i\n\\end{equation}\n\n\\begin{equation}\n  \\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum\\{k=1}^{K}\\sum\\{i:y\\i=k} (x\\i - \\hat{\\mu\\_k})^2\n\\end{equation}\n\nWe can extend Linear Discriminant Analysis to the case of multiple\npredictors. To do that, we will assume that \\\\(X = (X\\1, X\\2, \\dots,\nX\\_p)\\\\) is drawn from a multivariate Gaussian distribution, with a\nclass-specific mean vector and a common covariance matrix.\n\nThe multivariate Gaussian distribution assumes that each individual\npredictor follows a one-dimensional normal distribution, with some\ncorrelation between each pair of predictors. Formally, the\nmultivariate Gaussian density is defined as:\n\n\\begin{equation}\n  f(x) = \\frac{1}{(2\\pi)^{p/2|\\Sigma|^{1/2}}} \\mathrm{exp} \\left( -\\frac{1}{2}(x\n    \\mu)^T \\Sigma^{-1}(x - \\mu) \\right)\n\\end{equation}\n\nIn the case of \\\\(p > 1\\\\) predictors, the LDA classifier assumes that the\nobservations in the kith class are drawn from a multivariate Gaussian\ndistribution \\\\(N(\\mu\\_k, \\Sigma)\\\\), where \\\\(\\Sigma\\\\) is common to all\nclasses. With find that the Bayes classifier assigns an observation\n\\\\(X = x\\\\) to the class for which:\n\n\\begin{equation}\n  \\sigma\\k(x) = x^T \\Sigma^{-1}\\mu\\k -\n  \\frac{1}{2}\\mu\\k^T\\Sigma^{-1}\\mu\\k + \\log \\pi\\_k\n\\end{equation}\n\nThe LDA model has the lowest error rate the Gaussian model is correct,\nsince it approximates the Bayes classifier. However,\nmisclassifications can still happen, and a good way to visualize them\nis through a confusion matrix. The probability threshold can also be\ntweaked to reduce the error rates for incorrect classification to a\nsingle class.\n\nThe ROC (Receiver Operating Characteristics) curve is a popular\ngraphic for simultaneously displaying the two types of errors for all\npossible thresholds. An ideal ROC curve will hug the top left corner,\nso the larger the AUC (Area Under Curve) the better the classifier.\nThe overall performance of a classifier, summarized over all possible\nthresholds, is given by this value.\n\nVarying the classifier threshold also changes its true positive and\nfalse negative rate. These are also called the sensitivity, and 1 -\nspecificity of the classifier.\n\nQuadratic Discriminant Analysis {#quadratic-discriminant-analysis}\n\nIn LDA with multiple predictors, we assumed that observations are\ndrawn from a multivariate Gaussian distribution with a class-specific\nmean vector and a common covariance matrix. Quadratic Discriminant\nAnalysis (QDA) assumes that each class has its own covariance matrix.\nUnder this assumption, the Bayes classifier assigns an observation\n\\\\(X = x\\\\) to the class for which:\n\n\\begin{equation}\n  \\partial\\k(x) = -\\frac{1}{2}(x-\\mu\\k)^T \\Sigma\\k^{-1}(x - \\mu\\k) -\n  \\frac{1}{2} \\log |\\Sigma\\k| + \\log \\pi\\k\n\\end{equation}\n\nWhen would one prefer LDA to QDA, or vice-versa? The answer lies in\nthe bias-variance trade-off. When there are \\\\(p\\\\) predictors, estimating\na covariance matrix requires estimating \\\\(p(p+1)/2\\\\) variables. In QDA\nwith \\\\(K\\\\) predictors, we need to estimate \\\\(Kp(p+1)/2\\\\) parameters, which\ncan quickly get big. Hence LDA is much less flexible, and has a lower\nvariance. On the other hand, if the assumption of a common covariance\nmatrix is bad, then LDA will perform poorly.\n\nComparison of Classification Methods {#comparison-of-classification-methods}\n\nLogistic Regression and LDA produce linear decision boundaries. The\nonly difference between the two approaches is that in logistic\nregression the coefficients are estimated using maximum likelihood,\nwhile in LDA the coefficients are approximated via the estimated mean\nand variance from a normal distribution.\n\nSince logistic regression and LDA differ only in their fitting\nprocedures, one might expect the two approaches to give similar\nresults. Logistic regression can outperform LDA if the Gaussian\nassumptions are not met. On the other hand, LDA can show improvements\nover logistic regression if they are.\n\nKNN takes a completely different approach from the classifiers seen in\nthis chapter. In order to make a prediction for an observation \\\\(X = x\\\\)\n, the \\\\(K\\\\) training observations that are closest to \\\\(x\\\\) are\nidentified. Then \\\\(X\\\\) is assigned to the class to which the plurality\nof these observations belong. Hence KNN is a completely non-parametric\napproach: no assumptions are made about the shape of the decision\nboundary. KNN does not tell us which predictors are important, but can\noutperform LDA and logistic regression if the decision boundary is\nhighly non-linear.\n\nThough not as flexible as the KNN, QDA can perform better in the\npresence of a limited number of training observations, because it does\nmake some assumptions about the form of the decision boundary.\n\nReference Textbooks {#reference-textbooks}\n\nAn introduction to statistical learning (James et al., 2013)\nUnderstanding Machine Learning (Shalev-Shwartz \\& Ben-David, 2014)\n\nBibliography\nJames, G., Witten, D., Hastie, T., & Tibshirani, R., An introduction to statistical learning (2013), : Springer. ↩\n\nShalev-Shwartz, S., & Ben-David, S., Understanding machine learning: from theory to algorithms (2014), New York, NY, USA: Cambridge University Press. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/statistics",
        "title": "Statistics",
        "content": "\nBasic Properties {#basic-properties}\n\n\\\\(E(X) = \\sum x p(x)\\\\)\n\\\\(Var(X) = \\sum (x-\\mu)^2f(x)\\\\)\nX is around \\\\(E(X)\\\\), give or take \\\\(SD(X)\\\\)\n\\\\(E(aX + bY) = aE(X) + bE(Y)\\\\)\n\\\\(Var(aX + bY) = a^2Var(X) + b^2Var(Y)\\\\)\n\\\\(Var(X) = E(X^2) - [E(X)]^2\\\\)\n\\\\(Cov(X\\1, X\\2) = E(X\\1X\\2) - E(X\\1)E(X\\2)\\\\)\nif \\\\(X\\\\), \\\\(Y\\\\) are independent:\n    \\\\(M\\{X+Y}(t) = M\\X(t)M\\_Y(t)\\\\)\n    \\\\(E(XY)=E(X)E(Y)\\\\), converse is true if \\\\(X\\\\) and \\\\(Y\\\\) are bivariate\n        normal, extends to multivariate normal\n\nApproximations {#approximations}\n\nLaw of Large Numbers {#law-of-large-numbers}\n\nLet \\\\(X\\1, X\\2, ..., X\\_n\\\\) be IID, with expectation \\\\(\\mu\\\\) and variance\n\\\\(\\sigma^2\\\\). \\\\(\\overline{X\\_n} =\n\\frac{1}{n}\\sum^{n}\\{i=1}X\\i\\xrightarrow[n]{\\infty}\\mu\\\\). Let \\\\(x\\_1,\nx\\2, ..., x\\n\\\\) be realisations of the random variable \\\\(X\\1, X\\2, ..., X\\_n\\\\),\nthen \\\\(\\overline{x\\n} = \\frac{1}{n}\\sum^{n}\\{i=1}x\\_n\n\\xrightarrow[n]{\\infty} \\mu\\\\)\n\nCentral Limit Theorem {#central-limit-theorem}\n\nLet \\\\(S\\n = \\sum^{n}\\{i=1}X\\i\\\\) where \\\\(X\\1, X\\2, ..., X\\n\\\\) IID.\n\\\\(\\frac{S\\_n - n\\mu}{\\sqrt{n}\\sigma} \\xrightarrow[n]{\\infty} \\mathcal{N}(0,1)\\\\)\n\nDistributions {#distributions}\n\nPoisson(\\\\(\\lambda\\\\)) {#poisson---lambda}\n\n\\\\(E(X) = Var(X) = \\lambda\\\\)\n\nNormal \\\\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\\\) {#normal--x-sim-mathcal-n--mu-sigma-2}\n\n\\\\(f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} exp\n\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), -\\infty 2\\\\), \\\\(E(W) = \\frac{n}{n-2}\\\\)\n\nSampling {#sampling}\n\nLet \\\\(X\\1, X\\2, ..., X\\_n\\\\) be IID \\\\(\\mathcal{N}(\\mu, \\sigma^2)\\\\).\n\n\\\\(\\text{sample mean, } \\overline{X} = \\frac{1}{n}\\sum^{n}\\{i=1}X\\i\\\\)\n\n\\\\(\\text{sample variance, } S^2 = \\frac{1}{n-1}\\sum^{n}\\{i=1}\\left(X\\i-\\overline{X}\\right)^2\\\\)\n\nProperties of \\\\(\\overline{X}\\\\) and \\\\(S^2\\\\) {#properties-of--overline-x--and--s-2}\n\n\\\\(\\overline{X}\\\\) and \\\\(S^2\\\\) are independent\n\\\\(\\overline{X} \\sim \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})\\\\)\n\\\\(\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi\\_{n-1}^2\\\\)\n\\\\(\\frac{\\overline{X} - \\mu}{S/\\sqrt{n}} \\sim t\\_{n-1}\\\\)\n\nSimple Random Sampling (SRS) {#simple-random-sampling--srs}\n\nAssume \\\\(n\\\\) random draws are made without replacement. (Not SRS, will\nbe corrected for later).\n\nSummary of Lemmas\n\n    \\\\(P(X\\i =\\xi\\j) = \\frac{n\\_j}{N}\\\\): Lemma A\n    For \\\\(i \\ne j\\\\), \\\\(Cov(X\\i, X\\j) = - \\frac{\\sigma^2}{N-1}\\\\): Lemma B\n\nEstimation Problem\n\n    Let \\\\(X\\1, X\\2, ..., X\\_n\\\\) be random draws with replacement. Then\n    \\\\(\\overline{X}\\\\) is an estimator of \\\\(\\mu\\\\). and the observed value of\n    \\\\(\\overline{X}\\\\), \\\\(\\overline{x}\\\\) is an estimate of \\\\(\\mu\\\\).\n\nStandard Error (SE)\n\n    SE of an \\\\(\\overline{X}\\\\) is defined to be \\\\(SD(\\overline{X})\\\\).\n\n    | param     | est                | SE                            | Est. SE                                     |\n    |-----------|--------------------|-------------------------------|---------------------------------------------|\n    | \\\\(\\mu\\\\) | \\\\(\\overline{X}\\\\) | \\\\(\\frac{\\sigma}{\\sqrt{n}}\\\\) | \\\\(\\frac{s}{\\sqrt{n}}\\\\)                    |\n    | \\\\(p\\\\)   | \\\\(\\hat{p}\\\\)      | \\\\(\\sqrt{\\frac{p(1-p)}{n}}\\\\) | \\\\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n-1}}\\\\) |\n\nWithout Replacement\n\n    SE is multiplied by \\\\(\\frac{N-n}{N-1}\\\\), because \\\\(s^2\\\\) is biased for\n    \\\\(\\sigma^2\\\\): \\\\(E(\\frac{N-1}{N}s^2) = \\sigma^2\\\\), but N is normally large.\n\nConfidence Interval\n\n    An approximate \\\\(1-\\alpha\\\\) CI for \\\\(\\mu\\\\) is\n\n    \\\\((\\overline{x} - z\\{\\alpha/2}\\frac{s}{\\sqrt{n}}, \\overline{x} + z\\{\\alpha/2}\\frac{s}{\\sqrt{n}})\\\\)\n\nBiased Measurements {#biased-measurements}\n\nLet \\\\(X = \\mu + \\epsilon\\\\), where \\\\(E(\\epsilon) = 0\\\\), \\\\(Var(\\epsilon) =\n\\sigma^2\\\\)\n\nSuppose X is used to measure an unknown constant a, \\\\(a \\ne \\mu\\\\). \\\\(X =\na + (\\mu - a) + \\epsilon\\\\), where \\\\(\\mu-a\\\\) is the bias.\n\nMean square error (MSE) is \\\\(E((X-a)^2) = \\sigma^2 + (\\mu - a)^2\\\\)\n\nwith n IID measurements, \\\\(\\overline{x} = \\mu + \\overline{\\epsilon}\\\\)\n\n\\\\(E((x - a)^2) = \\frac{\\sigma^2}{n} + \\left(\\mu - a\\right)^2\\\\)\n\n\\\\(\\text{MSE} = \\text{SE}^2 + \\text{bias}^2\\\\), hence\n\\\\(\\sqrt{\\text{MSE}}\\\\) is a good measure of the accuracy of the estimate\n\\\\(\\overline{x}\\\\) of a.\n\nEstimation of a Ratio {#estimation-of-a-ratio}\n\nConsider a population of \\\\(N\\\\) members, and two characteristics are\nrecorded: \\\\((X\\1, Y\\1), (X\\2, Y\\2), ... , (X\\n, Y\\n)\\\\), \\\\(r =\n\\frac{\\mu\\y}{\\mu\\x}\\\\).\n\nAn obvious estimator of r is \\\\(R = \\frac{\\overline{Y}}{\\overline{X}}\\\\)\n\n\\\\(Cov(\\overline{X},\\overline{Y}) = \\frac{\\sigma\\_{xy}}{n}\\\\), where\n\n\\\\(\\sigma\\{xy} := \\frac{1}{N}\\sum^{N}\\{i=1}(x\\i-\\mu\\x)(x\\i-\\mu\\y)\\\\) is\nthe population covariance.\n\nProperties\n\n    \\\\(Var( R) \\approx \\frac{1}{\\mu\\x^2}\\left(r^2\\sigma\\{\\overline{X}}^2 + \\sigma\\{\\overline{Y}}^2 - 2r\\sigma\\{\\overline{X}\\overline{Y}}\\right)\\\\)\n\n    Population coefficient \\\\(\\rho =\n    \\frac{\\sigma\\{xy}}{\\sigma\\{x}\\sigma\\_{y}}\\\\)\n\n    \\\\(E( R) \\approx r + \\frac{1}{n}\\left(\\frac{N-n}{N-1}\\right)\\frac{1}{\\mu\\x^2}\\left(r\\sigma\\x^2-\\rho\\sigma\\x\\sigma\\y\\right)\\\\)\n\n    \\\\(s\\{xy} = \\frac{1}{n-1}\\sum^{n}\\{i=1}\\left(X\\_i -\n    \\overline{X}\\right)\\left(Y\\_i - \\overline{Y}\\right)\\\\)\n\nRatio Estimates\n\n    \\\\(\\overline{Y}\\R = \\frac{\\mu\\x}{\\overline{X}}\\overline{Y} = \\mu\\_xR\\\\)\n\n    \\\\(Var(\\overline{Y}\\_R) \\approx\n    \\frac{1}{n}\\frac{N-n}{N-1}(r^2\\sigma\\x^2 + \\sigma\\y^2\n    -2r\\rho\\sigma\\x\\sigma\\y)\\\\)\n\n    \\\\(E(\\overline{Y}\\R) - \\mu\\y \\approx\n    \\frac{1}{n}\\frac{N-n}{N-1}\\frac{1}{\\mu\\x}\\left(r\\sigma\\x^2 -\\rho\\sigma\\x\\sigma\\y\\right)\\\\)\n\n    The bias is of order \\\\(\\frac{1}{n}\\\\), small compared to its standard error.\n\n    \\\\(\\overline{Y}\\_R\\\\) is better than \\\\(\\overline{Y}\\\\), having smaller\n    variance, when \\\\(\\rho > \\frac{1}{2}\\left(\\frac{C\\x}{C\\y}\\right)\\\\), where\n    \\\\(C\\i = \\sigma\\i/\\mu\\_i\\\\)\n\n    Variance of \\\\(\\overline{Y}\\_R\\\\) can be estimated by\n\n    \\\\(s\\{\\overline{Y}\\R}^2 =\n    \\frac{1}{n}\\frac{N-n}{N-1}\\left(R^2s\\x^2+s\\y^2-2Rs\\_{xy}\\right)\\\\)\n\n    An approximate \\\\(1-\\alpha\\\\) C.I. for \\\\(\\mu\\y\\\\) is \\\\(\\overline{Y}\\R \\pm\n    z\\{\\alpha/2}s\\{\\overline{Y}\\_R}\\\\)\n\nMethod of Moments {#method-of-moments}\n\nTo estimate \\\\(\\theta\\\\), express it as a function of moments\n\\\\(g(\\hat{\\mu}\\1,\\hat{\\mu}\\2,...)\\\\)\n\nMonte Carlo {#monte-carlo}\n\nMonte Carlo is used to generate many realisations of random\nvariable.\n\n\\\\(\\overline{X} \\xrightarrow[n]{\\infty} \\alpha/\\lambda, \\hat{\\sigma}^2\n \\xrightarrow[n]{\\infty}\\alpha/\\lambda^2\\\\), MOM estimators are\nconsistent (asymptotically unbiased).\n\n\\\\(\\text{Poisson}(\\lambda)\\\\): \\\\(\\text{bias} = 0, SE \\approx \\sqrt{\\frac{\\overline{x}}{n}}\\\\)\n\n\\\\(N(\\mu, \\sigma^2)\\\\): \\\\(\\mu = \\mu\\1\\\\), \\\\(\\sigma^2 = \\mu\\2 - \\mu\\_1^2\\\\)\n\n\\\\(\\Gamma(\\lambda, \\alpha)\\\\): \\\\(\\hat{\\lambda} =\n \\frac{\\hat{\\mu}\\1}{\\hat{\\mu}\\2-\\hat{\\mu}\\1^2}=\\frac{\\overline{X}}{\\hat{\\sigma}^2}, \\hat{\\alpha} = \\frac{\\hat{\\mu}\\1^2}{\\hat{\\mu}\\2-\\hat{\\mu}\\1^2}=\\frac{\\overline{X}^2}{\\hat{\\sigma}^2}\\\\)\n\nMaximum Likelihood Estimator (MLE) {#maximum-likelihood-estimator--mle}\n\nPoisson Case {#poisson-case}\n\n\\\\(L(\\lambda) = \\prod^n\\{i=1}\\frac{\\lambda^{x\\i}e^{-\\lambda}}{x\\i!} = \\frac{\\lambda\\sum^n\\{i=1}x\\ie^{-n\\lambda}}{\\prod^{n}\\{i=1}x\\_i!}\\\\)\n\n\\\\(l(\\lambda) = \\sum^{n}\\{i=1}x\\i\\log\\lambda - n\\lambda -\n\\sum^{n}\\{i=1}\\log x\\i!\\\\)\n\nML estimate of \\\\(\\lambda\\_0\\\\) is \\\\(\\overline{x}\\\\). ML estimator is\n\\\\(\\hat{\\lambda}\\_0 = \\overline{X}\\\\)\n\nNormal case {#normal-case}\n\n\\\\(l(\\mu, \\sigma) = -n\\log\\sigma - \\frac{n\\log 2\\pi}{2} - \\frac{\\sum^{n}\\{i=1}\\left(X\\i-\\mu\\right)^2}{2\\sigma^2}\\\\)\n\n\\\\(\\frac{\\partial l}{\\partial \\mu} = \\frac{\\sum \\left(X\\_i -\n\\mu\\right)}{\\sigma^2} \\implies \\hat{\\mu} = \\overline{x}\\\\)\n\n\\\\(\\frac{\\partial l}{\\partial \\sigma} =\n\\frac{\\sum^{n}\\{i=1}\\left(X\\i-\\mu\\right)^2}{\\sigma^3} -\n\\frac{n}{\\sigma} \\\\ \\implies \\hat{\\sigma^2} = \\frac{1}{n}\\sum^{n}\\{i=1}\\left(X\\i-\\overline{X}\\right)^2\\\\)\n\nGamma case {#gamma-case}\n\n\\\\(l(\\theta) = n\\alpha\\log\\lambda + (\\alpha -1)\\sum^{n}\\{i=1}\\log X\\i -\n\\lambda\\sum^{n}\\{i=1} X\\i - n\\log\\Gamma(\\alpha)\\\\)\n\n\\\\(\\frac{\\partial l}{\\partial \\alpha} = n\\log\\alpha + \\sum^{n}\\_{i=1}\\log\nX\\i - \\sum^{n}\\{i=1}X\\_i - \\frac{n}{\\Gamma(\\alpha)}\\Gamma '(\\alpha)\\\\)\n\n\\\\(\\frac{\\partial l}{\\partial \\lambda} = \\frac{n\\alpha}{\\lambda} -\n\\sum^{n}\\{i=1}X\\i\\\\)\n\n\\\\(\\hat{\\lambda} = \\frac{\\hat{\\alpha}}{\\hat{x}}\\\\)\n\nMultinomial Case {#multinomial-case}\n\n\\\\(f(x\\1, ..., x\\r) = {n \\choose {x\\1, x\\2, ... x\\r}} \\prod^{n}\\{i=1}\np\\i^{X\\i}\\\\)\n\nwhere \\\\(X\\_i\\\\) is the number of times the value occurs, and not the\nnumber of trials. and \\\\(x\\1, x\\2, ... x\\_r\\\\) are non-negative integers\nsumming to \\\\(n\\\\). \\\\(\\forall i\\\\):\n\n\\\\(E(X\\i) = np\\i, Var(X\\i)=np\\i(1-p\\_i)\\\\)\n\n\\\\(Cov(X\\i,X\\j) = -np\\ip\\j, \\forall i \\ne j\\\\)\n\n\\\\(l(p) = \\Kappa + \\sum^{r-1}\\{i=1}x\\i\\log p\\_i +\nx\\r\\log(1-p\\1-...-p\\_{r-1})\\\\)\n\n\\\\(\\frac{\\partial l}{\\partial p\\i} = \\frac{x\\i}{p\\i} - \\frac{x\\r}{p\\_r} =\n0 \\text{ assuming MLE exists}\\\\)\n\n\\\\(\\frac{x\\i}{\\hat{p}\\i} = \\frac{x\\r}{\\hat{p}\\r} \\implies \\hat{p}\\_i =\n\\frac{x\\i}{c}, c=\\frac{x\\r}{\\hat{p}\\_r}\\\\)\n\n\\\\(\\sum^r\\{i=1}\\hat{p}\\i = \\sum^r\\{i=1}\\frac{x\\i}{c} = 1 \\\\ \\implies c =\n\\sum^{r}\\{i=1}x\\i = n \\implies \\hat{p}\\i = \\frac{\\overline{x}\\i}{n}\\\\)\n\nsame as MOM estimator.\n\nCIs in MLE {#cis-in-mle}\n\n\\\\(\\frac{\\hat{X} - \\mu}{s/\\sqrt{n}} \\sim t\\_{n-1}\\\\)\n\nGiven the realisations \\\\(\\overline{x}\\\\) and \\\\(s\\\\), \\\\(\\overline{x} \\pm\nt\\{n-1, \\alpha/2}\\frac{s}{\\sqrt{n}},\\overline{x} + t\\{n-1,\n\\alpha/2}\\frac{s}{\\sqrt{n}}\\\\) is the exact \\\\(1-\\alpha\\\\) CI for \\\\(\\mu\\\\).\n\n\\\\(\\frac{n\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi\\_{n-1}^\\\\),\n\\\\(\\frac{n\\hat{\\sigma}^2}{\\chi\\_{n-1,\\alpha/2}^2},\n\\frac{n\\hat{\\sigma}^2}{\\chi\\_{n-1,1-\\alpha/2}^2}\\\\) is the exact\n\\\\(1-\\alpha\\\\) CI for \\\\(\\sigma\\\\).\n\nFisher Information {#fisher-information}\n\n\\\\(I\\left( \\theta \\right) = - E \\left( \\frac{\\partial}{\\partial \\theta^2} \\log\n    f\\left( x | \\theta \\right) \\right)\\\\)\n\n| Distribution         | MLE                        | Variance                         |\n|----------------------|----------------------------|----------------------------------|\n| Po(\\\\(\\lambda\\\\))    | \\\\(X\\\\)                    | \\\\(\\lambda\\\\)                    |\n| Be(\\\\(p\\\\))          | \\\\(X\\\\)                    | \\\\(p\\left(1-p\\right)\\\\)          |\n| Bin(\\\\(n\\\\),\\\\(p\\\\)) | \\\\(\\frac{X}{n}\\\\)          | \\\\(\\frac{p(1-p)}{n}\\\\)           |\n| HWE tri              | \\\\(\\frac{X\\2+2X\\3}{n}\\\\) | \\\\(\\frac{\\theta(1-\\theta)}{n}\\\\) |\n\nGeneral trinomial: \\\\(\\left(\\frac{X\\1}{n}, \\frac{X\\2}{n} \\right)\\\\)\n\n\\begin{equation\\*}\n\\begin{bmatrix} p\\1(1-p\\1) & -p\\1p\\2 \\\\ -p\\1p\\2 & p\\2(1-p\\2) \\end{bmatrix} \\frac{1}{n}\n\\end{equation\\*}\n\nIn all the above cases, \\\\(\\text{var}(\\hat{\\theta}) = I(\\theta)^{-1}\\\\).\n\nAsymptotic Normality of MLE {#asymptotic-normality-of-mle}\n\nAs \\\\(n \\rightarrow \\infty\\\\), \\\\(\\sqrt{nI(\\theta)}(\\hat{\\theta} -\n\\theta) \\rightarrow N(0,1)\\\\) in distribution, and hence \\\\(\\hat{\\theta}\n\\sim N\\left(\\theta, \\frac{I\\left( \\theta \\right)^{-1}}{n}\\right)\\\\)\n\nAs \\\\(\\hat{\\theta} \\xrightarrow[n]{\\infty} \\theta\\\\), MLE is consistent.\n\nSE of an estimate of \\\\(\\theta\\\\) is the SD of the estimator\n\\\\(\\hat{\\theta}\\\\), hence \\\\(SE = SD(\\hat{\\theta}) =\n\\sqrt{\\frac{I(\\theta)^{-1}}{n}} \\approx\n\\sqrt{\\frac{I(\\hat{\\theta})^{-1}}{n}}\\\\)\n\n\\\\(1-\\alpha \\text{ CI } \\approx \\hat{\\theta} \\pm\n  z\\_{\\alpha/2}\\sqrt{\\frac{I(\\theta)^{-1}}{n}}\\\\)\n\nEfficiency {#efficiency}\n\nCramer-Rao Inequality: if \\\\(\\theta\\\\) is unbiased, then \\\\(\\forall \\theta\n\\in \\Theta\\\\) , \\\\(var(\\hat{\\theta}) \\ge I(\\hat{\\theta})^{-1}/n\\\\), if =\nthen \\\\(\\hat{\\theta}\\\\) is efficient.\n\n\\\\(eff(\\hat{\\theta}) = \\frac{I(\\hat{\\theta})^{-1}/n}{var(\\hat{\\theta})} \\mu\\0\\\\):  Critical region \\\\(\\\\{\\bar{x} > \\mu\\0 +\nz\\_\\alpha\\frac{\\sigma}{\\sqrt{n}}\\\\}\\\\), the power is a function of \\\\(\\mu\\\\),\nand this is uniformly the most powerful test for size \\\\(\\le \\alpha\\\\).\n\n\\\\(H\\1 : \\mu \\ne \\mu\\0\\\\): Critical region \\\\(\\\\{|\\bar{x}-\\mu\\_0| > c\\\\}, c =\nz\\_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}}\\\\), but not uniformly most\npowerful.\n\nThe \\\\((1-\\alpha)\\\\) CI for \\\\(\\mu\\\\) consists of precisely the values \\\\(\\mu\\_0\\\\)\nfor which \\\\(H\\0: \\mu = \\mu\\0\\\\) is not rejected against \\\\(H\\_1: \\mu \\ne\n\\mu\\_0\\\\). Exact for normal with known variance, approx. in others.\n\np-value {#p-value}\n\nthe probability under \\\\(H\\_0\\\\) that the test statistic is more extreme\nthan the realisation. (A, B): \\\\(p = p\\_0(\\bar{X} > \\bar{x}) =\nP(Z>\\frac{\\bar{x} - \\mu\\_0}{\\sigma/\\sqrt{n}})\\\\). (C): \\\\(p =\nP\\0(|\\bar{X} - \\mu\\0| > |\\bar{x} - \\mu\\_0|)\\\\). The smaller the p-value,\nthe more suspicious one should be about \\\\(H\\_0\\\\). If size is smaller than\np-value, do not reject \\\\(H\\_0\\\\).\n\nGeneralized Likelihood Ratio {#generalized-likelihood-ratio}\n\n\\\\(\\Lambda^\\* = \\frac{\\text{max}\\_{\\theta \\in\n\\omega\\0}L(\\theta)}{\\text{max}\\{\\theta\\in\\Omega}L(\\theta)}\\\\), \\\\(\\Omega =\n\\omega\\0 \\cup \\omega\\1\\\\). The closer \\\\(\\Lambda\\\\) is to 0, the stronger\nthe evidence for \\\\(H\\_1\\\\).\n\nLarge-sample null distribution of \\\\(\\Lambda\\\\) {#large-sample-null-distribution-of--lambda}\n\nUnder \\\\(H\\0\\\\), when n is large, \\\\(-2\\log\\Lambda = \\chi\\k^2\\\\), where \\\\(k =\n\\text{dim}(\\Omega) - \\text{dim}(\\omega\\_0)\\\\).\n\nNormal (C): \\\\(p = P\\left(\\chi\\_1^2 > \\frac{(\\bar{x} -\n\\mu\\_0)^2}{\\sigma^2/n}\\right)\\\\)\n\nMultinomial: \\\\(\\Lambda = \\prod\\_{i=1}^{r}\n\\left(\\frac{E\\i}{X\\i}\\right)^{X\\i}\\\\) where \\\\(E\\i = np\\_i(\\hat{\\theta})\\\\) is\nthe expected frequency of the ith event under \\\\(H\\_0\\\\). \\\\(-2\\log\\Lambda\n\\approx \\sum\\{i=1}^{r}\\frac{(X\\i-E\\i)^2}{E\\i}\\\\), which is the Pearson\nchi-square statistic, written as \\\\(X^2\\\\).\n\nPoisson Dispersion Test {#poisson-dispersion-test}\n\nFor \\\\(i = 1 ... n\\\\) let \\\\(X\\i \\sim Poisson(\\lambda\\i)\\\\) are independent.\n\n\\\\(w\\0 = \\\\{ \\tilde{\\lambda} |  \\lambda\\1 = \\lambda\\_2 = ... =\n\\lambda\\_n\\\\}\\\\)\n\n\\\\(w\\1 = \\\\{\\tilde{\\lambda} | \\lambda\\i \\ne \\lambda\\_j \\text{ for some }\ni,j\\\\}\\\\)\n\n\\\\(-2\\log\\Lambda \\approx \\frac{\\sum\\{i=1}^{n}(X\\i-\\bar{X})^2}{\\bar{X}}\\\\).\nFor large n, the null distribution of \\\\(-2\\log\\Lambda\\\\) is approximately\n\\\\(\\chi\\_{n-1}^2\\\\)\n\nComparing 2 samples {#comparing-2-samples}\n\nNormal Theory: Same Variance {#normal-theory-same-variance}\n\n\\\\(X\\1, ..., X\\n\\\\) be i.i.d \\\\(N(\\mu\\X,\\sigma^2)\\\\) and \\\\(Y\\1,...,Y\\_m\\\\) be\ni.i.d \\\\(N(\\mu\\Y, \\sigma^2)\\\\), independent. \\\\(H\\0: \\mu\\X - \\mu\\Y = d\\\\)\n\nKnown Variance\n\n    \\\\(Z := \\frac{\\bar{X} - \\bar{Y} - (\\mu\\_X -\n    \\mu\\Y)}{\\sigma{\\sqrt{\\frac{1}{n} + \\frac{1}{m}}}}\\\\) and reject \\\\(H\\0\\\\)\n    when \\\\(|Z| > z\\_{\\alpha/2}\\\\)\n\nUnknown Variance\n\n    \\\\(s\\p^2 = \\frac{(n-1)s\\X^2 + (m-1)s\\Y^2}{m+n-2}\\\\) where \\\\(s\\X^2 =\n    \\frac{1}{n-1}\\sum\\{i=1}^{n}(X\\i-\\bar{X})^2\\\\). \\\\(s\\_p^2\\\\) is an unbiased\n    estimator of \\\\(\\sigma^2\\\\). \\\\(s\\X\\\\) within factor of 2 from \\\\(s\\Y\\\\).\n\n    \\\\(t := \\frac{\\bar{X} - \\bar{Y} - (\\mu\\_X -\n    \\mu\\Y)}{s\\p{\\sqrt{\\frac{1}{n} + \\frac{1}{m}}}}\\\\) follows a t\n    distribution with \\\\(m+n-2\\\\) d.f.\n\n    If two-sided: reject \\\\(H\\0\\\\) when \\\\(|t| > t\\{n+m-2,\\alpha/2}\\\\). If\n    one-sided, e.g \\\\(H\\1: \\mu\\X > \\mu\\Y\\\\), reject \\\\(H\\0\\\\) when \\\\(t >\n    t\\_{n+m-2,\\alpha}\\\\).\n\nCI\n\n    \\\\(\\frac{\\bar{X}-\\bar{Y}}\\pm z\\_{\\alpha/2} \\cdot \\sigma\n    \\sqrt{\\frac{1}{n} + \\frac{1}{m}}\\\\) if \\\\(\\sigma\\\\) is known, or\n    \\\\(\\frac{\\bar{X}-\\bar{Y}}\\pm t\\{m+n-2, \\alpha/2} \\cdot s\\p\n    \\sqrt{\\frac{1}{n} + \\frac{1}{m}}\\\\) if \\\\(\\sigma\\\\) is unknown.\n\nUnequal Variance\n\n    \\\\(Z := \\frac{\\bar{X} - \\bar{Y} - (\\mu\\_X -\n    \\mu\\Y)}{{\\sqrt{\\frac{\\sigma\\X^2}{n} + \\frac{\\sigma\\_Y^2}{m}}}}\\\\)\n\n    \\\\(t := \\frac{\\bar{X} - \\bar{Y} - (\\mu\\_X -\n    \\mu\\Y)}{{\\sqrt{\\frac{s\\X^2}{n} + \\frac{s\\_Y^2}{m}}}}\\\\), with \\\\(df =\n    \\frac{(a+b)^2}{\\frac{a^2}{n-1} + \\frac{b^2}{m-1}}\\\\) where \\\\(a =\n    \\frac{s\\X^2}{n}\\\\) and \\\\(b = \\frac{s\\Y^2}{m}\\\\)\n\nMann-Whitney Test {#mann-whitney-test}\n\nWe take the smaller sample of size \\\\(n\\_1\\\\), and sum the ranks in that\nsample. \\\\(R' = n\\1(m+n+1) -R\\\\), and \\\\(R\\* = min(R',R)\\\\), we reject \\\\(H\\0: F\n= G\\\\) if \\\\(R\\*\\\\) is too small.\n\nTest works for all distributions, and is robust to outliers.\n\nPaired Samples {#paired-samples}\n\n\\\\((X\\i, Y\\i)\\\\) are paired and related to the same individual. \\\\((X\\_i,\nY\\i)\\\\) is independent from \\\\((X\\j, Y\\j)\\\\). Compute \\\\(D\\i = Y\\i - X\\i\\\\), To\ntest \\\\(H\\0 : \\mu\\D = d\\\\), \\\\(t = \\frac{\\bar{D} - \\mu\\D}{s\\D/\\sqrt{n}}\\\\).\n\n\\\\(1-\\alpha\\\\) CI: \\\\(\\bar{D}\\pm t\\{n-1,\\alpha/2}S\\D/\\sqrt{n}\\\\)\n\nRanked Test {#ranked-test}\n\n\\\\(W\\+\\\\) is the sum of ranks among all positive \\\\(D\\i\\\\) and \\\\(W\\_i\\\\) is the\nsum of ranks among all negative \\\\(D\\i\\\\). We want to reject \\\\(H\\0\\\\) if\n\\\\(W = min(W\\+, W\\-)\\\\) is too large.\n",
        "tags": []
    },
    {
        "uri": "/zettels/stochastic_processes",
        "title": "Stochastic Processes",
        "content": "\nA stochastic process \\\\(X(t), t \\in T\\\\) is a collection of random\nvariables. For each \\\\(t \\in T\\\\), \\\\(X(t)\\\\) is a random variable. The index\n\\\\(t\\\\) is often interpreted as time, and as a result, we refer to \\\\(X(t)\\\\)\nas the state of the process at time \\\\(t\\\\).\n\nThe set \\\\(T\\\\) is called the index set of the process. When \\\\(T\\\\) is a\ncountable set, the stochastic is a discrete-time process. If \\\\(T\\\\) is an\ninterval of the real line, the process is said to be a continuous-time\nprocess.\n\nFocus: Discrete time, discrete state space Markov Chain\n\nStochastic = random\nA stochastic process describes random phenomena that change over\n    time\n\nvalues that \\\\(X\\_t\\\\)'s take\nset of all possible states, denoted by \\\\(\\mathcal{S}\\\\).\ncan be thought of as time. If \\\\(T = \\\\{0, 1, 2, \\dots \\\\}\\\\)\n    then it is a discrete-time process. If \\\\(T\\\\) is an\n    interval, it is a continuous time process.\n\nEach \\\\(X\\_t\\\\) is a random variable.\n\nExample of stochastic process: Gambler's ruin\n\nA gambler starts with an initial fortune of \\\\(k\\\\) dollars.\nThe gambler plays against \\\\(B\\\\) with an initial fortune of \\\\(N-k\\\\) dollars.\nEach game he bets $1, wins with probability \\\\(p\\\\)\nLet \\\\(\\\\{X\\_t = t = 0,1,2 \\dots\\\\}\\\\) represent his fortune as the\n    betting goes on.\nGame only stops when either gambler or \\\\(B\\\\) is ruined.\nHere \\\\(\\mathcal{S} = \\\\{0,1,\\dots,N\\\\}\\\\)\nFor a realization of the results of the first 10 games, (here\n    \\\\(p=1/2\\\\)):\n\nsample(c(-1, 1), 10, replace=T)\n\nReference Textbooks {#reference-textbooks}\n\n(Ross, 2014), (Pinsky \\& Karlin, 2010)\n\nBibliography\nRoss, S. M., Introduction to probability models (2014), : Academic press. ↩\n\nPinsky, M., & Karlin, S., An introduction to stochastic modeling (2010), : Academic press. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/stock19_and_bit_goes_down",
        "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks",
        "content": "\ntags\n: Model Compression\n\npaper\n: (Stock et al., 2019)\n\nThis method minimizes the loss reconstruction error for in-domain\ninputs, and does not require any labelled data.\n\n{{}}\n\nThis method exploits the high correlation in the convolutions in\nResNet-like architectures by the use of product quantization (PQ). The\napproach here focuses on reconstructing the activations, and not the\nweights. This results in better in-domain reconstruction, and does not\nrequire any supervision.\n\nVector Quantization (VQ) and Product Quantization (PQ) decompose the\nhigh-dimensional space into a cartesian product of subspaces that are\nquantized separately. These are typically studied under the context of\nnearest neighbour search.\n\n{#}\n\nBibliography\nStock, P., Joulin, A., Gribonval, R\\'emi, Graham, B., & J\\'egou, Herv\\'e, And the bit goes down: revisiting the quantization of neural networks, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/studying",
        "title": "Studying",
        "content": "\ntags\n: §soft\\_skills\n\nThe optimal studying strategy {#the-optimal-studying-strategy}\n\nGlance at past year paper questions\n\nRationale: understand the kind of questions to be asked, prime the\nbrain for chunking, mental pump\n\nGlance through relevant textbook chapters\n\nSame rationale. Understand the big picture.\n\nfor n from 1..N\n\nGlance at tutorial n\n\nUnderstand the problems that typically come out from this chapter.\n\nGlance at textbook chapter subheadings/structure.\n\nForm big picture.\n\nRead textbook with 2 passes minimum.\n    First pass to get general feel, less math\n    Second pass to understand deeper\n\nWork through the example problems at least once after first pass, when\nencountered during second pass. Attempt to recall during break, in a\ndifferent location. Try to solve the problem without referencing\nsolution.\n\nRework this problem after the break.\n",
        "tags": []
    },
    {
        "uri": "/zettels/sufficient_statistics",
        "title": "Sufficient Statistics",
        "content": "\ntags\n: §exponential\\_family, §statistics\n\nA statistic \\\\(t\\\\) is called a sufficient statistic for \\\\(\\theta\\\\) for a\ngiven \\\\(\\boldsymbol{y}\\\\) if:\n\n\\begin{equation}\n  p(\\boldsymbol{y} | t, \\theta)=p(\\boldsymbol{y} | t)\n\\end{equation}\n\nLet \\\\(Y\\_{i} \\sim \\text { Bernoulli }(\\theta)\\\\) for \\\\(i = 1, \\dots, n\\\\),\nand \\\\(T=\\sum\\{i=1}^{n} Y\\{i}\\\\). Then it can be shown that\n\\\\(t=\\sum\\{i=1}^{n} y\\{i}\\\\) is a sufficient statistic for \\\\(\\theta\\\\) given\n\\\\(y=\\left(y\\{1}, \\ldots, y\\{n}\\right)\\\\).\n\nFisher-Neyman Theorem {#fisher-neyman-theorem}\n\nThe Fisher-Neyman theorem, or the factorization theorem, helps us\nfind sufficient statistics more readily. It states that:\n\nA statistic \\\\(t\\\\) is sufficient for \\\\(\\theta\\\\) if and only if there are\nfunctions \\\\(f\\\\) and \\\\(g\\\\) such that:\n\n\\begin{equation}\n  p(\\boldsymbol{y} | \\theta)=f(t, \\theta) g(\\boldsymbol{y})\n\\end{equation}\n\nwhere \\\\(t=t(\\boldsymbol{y})\\\\).\n",
        "tags": []
    },
    {
        "uri": "/zettels/swift",
        "title": "Swift",
        "content": "\ntags\n: §prog\\_lang\n\nThe Swift Programming Language book\n",
        "tags": []
    },
    {
        "uri": "/zettels/synaptic_current_model",
        "title": "Synaptic Current Model",
        "content": "\nSynaptic currents are generated by synaptic currents triggered by\narrival of presynaptic spikes \\\\(S\\_{j}^{(l)}(t)\\\\). Spike trains\n\\\\(S\\_{j}^{(l)}(t)\\\\) are denoted as a sum of Dirac delta functions\n\\\\(S\\{j}^{(l)}(t)=\\sum\\{s \\in C\\_{j}^{(l)}} \\delta(t-s)\\\\), where \\\\(s\\\\) runs\nover the firing times \\\\(C\\_j^{(l)}\\\\) of neuron \\\\(j\\\\) in layer \\\\(l\\\\).\n\nA good first-order approximation of the synaptic current is one of\nexponential decay. Synaptic currents are also assumed to sum linearly.\n\n\\begin{equation} \\label{eq:scm}\n  \\frac{\\mathrm{d} I\\{i}^{(l)}}{\\mathrm{d} t}=-\\underbrace{\\frac{I\\{i}^{(l)}(t)}{\\tau\\{\\mathrm{syn}}}}\\{\\mathrm{exp} . \\text { decay }}+\\underbrace{\\sum\\{j} W\\{i j}^{(l)} S\\{j}^{(l-1)}(t)}\\{\\text {feed-forward }}+\\underbrace{\\sum\\{j} V\\{i j}^{(l)} S\\{j}^{(l)}(t)}\\{\\text {recurrent }}\n\\end{equation}\n\nA single LIF neuron can be simulated with 2 linear differential\nequations whose initial conditions change instantaneously when a spike\noccurs. Combining the reset term with the equation for the\n§leaky\\integrate\\and\\_fire model, we get:\n\n\\begin{equation} \\label{eq:lif\\with\\reset}\n  \\frac{\\mathrm{d} U\\{i}^{(l)}}{\\mathrm{d} t}=-\\frac{1}{\\tau\\{\\mathrm{mem}}}\\left(\\left(U\\{i}^{(l)}-U\\{\\mathrm{rest}}\\right)+R I\\{i}^{(l)}\\right)+S\\{i}^{(l)}(t)\\left(U\\_{\\mathrm{rest}}-\\vartheta\\right)\n\\end{equation}\n\nThe solutions to Equations eq:scm and eq:lifwithreset are\napproximated numerically by discretizing time, and expressing the\noutput spike-train \\\\(S\\_{i}^{(l)}(n)\\\\) of neuron \\\\(i\\\\) in layer \\\\(l\\\\) at\ntime-step \\\\(n\\\\) as a non-linear function of the membrane voltage\n\\\\(S\\i^{(l)}(n) \\equiv \\Theta(U\\i^{(l)(n)}  - \\theta)\\\\) where \\\\(\\theta\\\\) is\nthe Heaviside step function, and \\\\(\\theta\\\\) is the firing threshold.\n\nSetting \\\\(U\\_{\\text{rest}} = 0\\\\), \\\\(R=1\\\\), \\\\(\\theta=1\\\\), and using some small\nsimulation time step \\\\(\\delta t > 0\\\\), we get:\n\n\\begin{equation}\nI\\{i}^{(l)}[n+1]=\\alpha I\\{i}^{(l)}[n]+\\sum\\{j} W\\{i j}^{(l)} S\\{j}^{(l)}[n]+\\sum\\{j} V\\{i j}^{(l)} S\\{j}^{(l)}[n]\n\\end{equation}\n\nwith decay strength \\\\(\\alpha \\equiv \\exp\n\\left(-\\frac{\\Delta\\{t}}{\\tau\\{\\mathrm{syn}}}\\right)\\\\). Equation\neq:lifwithreset can then be expressed as:\n\n\\begin{equation}\n  U\\{i}^{(l)}[n+1]=\\beta U\\{i}^{(l)}[n]+I\\{i}^{(l)}[n]-S\\{i}^{(l)}[n]\n\\end{equation}\n\nwith \\\\(\\beta \\equiv \\exp\n\\left(-\\frac{\\Delta\\{t}}{\\tau\\{\\operatorname{mem}}}\\right)\\\\). These two\nequations characterise the dynamics of a RNN. Specifically, the state\nof neuron \\\\(i\\\\) is given by the instantaneous synaptic currents \\\\(I\\_i\\\\)\nand the membrane voltage \\\\(U\\_i\\\\).\n\nReferences {#references}\n\n(Neftci et al., 2019)\n\nBibliography\nNeftci, E. O., Mostafa, H., & Zenke, F., Surrogate gradient learning in spiking neural networks, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/system_design",
        "title": "System Design",
        "content": "\ntags\n: §coding\\interview, §software\\engineering\n\nBasics {#basics}\n\nSQL, noSQL\nConcurrency\n    Threads, deadlock, starvation\n    read/write locks\nNetworking\n    Routers/Switches\n    TCP vs UDP\nFile Systems\n    OS, file system, database\n    levels of caching in modern OS\n\nTerminologies {#terminologies}\n\nReplication {#replication}\n\nReplication refers to frequently copying the data across multiple\nmachines. Post replication, multiple copies of the data exists across\nmachines. This might help in case one or more of the machines die due\nto some failure.\n\nConsistency {#consistency}\n\nAssuming you have a storage system which has more than one machine,\nconsistency implies that the data is same across the cluster, so you\ncan read or write to/from any node and get the same data. Eventual\nconsistency : Exactly what the name suggests. In a cluster, if\nmultiple machines store the same data, an eventual consistent model\nimplies that all machines will have the same data eventually. Its\npossible that at a given instance, those machines have different\nversions of the same data ( temporarily inconsistent ) but they will\neventually reach a state where they have the same data.\n\nAvailability {#availability}\n\nIn the context of a database cluster, Availability refers to the\nability to always respond to queries ( read or write ) irrespective of\nnodes going down.\n\nPartition Tolerance {#partition-tolerance}\n\nIn the context of a database cluster, cluster continues to function\neven if there is a “partition” (communications break) between two\nnodes (both nodes are up, but can’t communicate).\n\nVertical scaling and Horizontal scaling {#vertical-scaling-and-horizontal-scaling}\n\nIn simple terms, to scale horizontally is adding more servers. To scale\nvertically is to increase the resources of the server ( RAM, CPU,\nstorage, etc. ). Example: Lets say you own a restaurant which is now\nexceeding its seating capacity. One way of accomodating more people (\nscaling ) would be to add more and more chairs (scaling vertically).\nHowever since the space is limited, you won’t be able to add more\nchairs once the space is full. Another way of scaling would be to open\nnew branches of the restaurant ( horizontal scaling ). Source :\n\nSharding {#sharding}\n\nWith most huge systems, data does not fit on a single machine. In such\ncases, sharding refers to splitting the very large database into\nsmaller, faster and more manageable parts called data shards.\n\nCAP Theorem {#cap-theorem}\n\nIt is impossible to simultaneously guarantee the following:\n\nConsistency\nAvailability\nPartition Tolerance\n\nApproaching System Design Questions {#approaching-system-design-questions}\n\nFeature Clarification (2 minutes)\nEstimations\n    Does the data fit in one machine/database?\n    Can the cache fit on one machine/database?\nDesign Goals\n    Can data loss be tolerated?\nSkeleton of design\n    Discuss high-level components; go into deep dive only on request\n\nCaching {#caching}\n\nTypes {#types}\n\nWrite-through -  write to both cache and db at the same time,\n    before confirming write completion\n    Write latency is higher\n    but re-reading writes and reads is fast\nWrite-around - write to db, missing cache\n    cache must fetch reads from db on first try\n    higher read latency\nWrite-back - I/O completion sent when data written to cache\n    cache writes to db\n    might lose data\n    allieviated with replicates\n\nImplementing LRU caching {#implementing-lru-caching}\n\nHashtable + Doubly-linked list\nKey -> Pointer to node in doubly linked list\nEach time it is accessed, move node to head of doubly-linked list\nEvicting keys:\nIf adding new item, and it is full, remove tail of list,\n\nSAS/SSD {#sas-ssd}\n\nused for I/O over SATA (7.5krpm)\n\nImplementing TinyURL {#implementing-tinyurl}\n\nFeature Clarification {#feature-clarification}\n\nShorten a URL\nExpand a slug into a URL\nAllow users to pick a custom URL\n\nData Estimation {#data-estimation}\n\nAssume tinyURL load, 100M new writes per month\nThen, in 5 years, 6B writes.\nTo handle 6B slugs, assuming we're using A-z[0-9] 62^k > 6\\*10^9\nslugs need just 6 characters, 6 bytes.\n    Slugs will take up 36GB.\nAssume 500 bytes for a URL, URLs will take up 3TB.\n    It is reasonable to store all of this on a single machine.\n    But large amounts of reads and writes going to one machine can\n        cause deadlock\n    Master-slave replication\n\nDesign Goals {#design-goals}\n\n| Latency | Consistency | Availability |\n|---------|-------------|--------------|\n| Yes     | Yes         | C > A        |\n\nDesign API {#design-api}\n\nshortenURL(url)\nexpandURL(hash)\n\nComputing the Hash\n\n    convert\\to\\base\\_62(md5(url + salt))[:6]\n\nStateless application servers\n\n    load balancers ensure application is available when a server dies, and\n    client knows which server to talk to\n\nImplementing Search {#implementing-search}\n\nImplementing a distributed key-store {#implementing-a-distributed-key-store}\n\nData can't fit onto one machine\nSo now the choice is between consistency and availability\nPerform some estimations\n",
        "tags": []
    },
    {
        "uri": "/zettels/systems_programming",
        "title": "Systems Programming",
        "content": "\ntags\n: §operating\\_systems, §linux\n\nMemory {#memory}\n\nTODO:\n\n(Drepper, 2007)\n\nBibliography\nDrepper, U., What every programmer should know about memory, , (),  (2007).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/td_learning",
        "title": "Temporal Difference Learning",
        "content": "\nObserve samples \\\\(\\left(s\\t, a\\t, r\\t, s\\{t+1} \\right)\\\\). If value\nestimates are accurate, the following must hold:\n\n\\begin{equation}\n  V(s\\t) = r\\t + \\gamma V(s\\_{t+1})\n\\end{equation}\n\nIf not, there is a TD error:\n\n\\begin{equation}\n  \\gamma = r\\t  + \\gamma V(s\\{t+1}) - V(s\\_t)\n\\end{equation}\n\nTo learn better estimates - minimize $&gamma; $ TD(0):\n\n\\begin{equation}\n  V(s) \\leftarrow V(s) + \\alpha \\left( r\\t + \\gamma V(s\\{t+1}) - V(s\\_t) \\right)\n\\end{equation}\n",
        "tags": []
    },
    {
        "uri": "/zettels/theory_of_computation",
        "title": "Theory Of Computation",
        "content": "\nIntroduction {#introduction}\n\nFinite automata are a useful model for many kinds of important\nsoftware and hardware. Automatas are found in compilers, software for\ndesigning digital circuits, and many more systems.\n\nGrammars provide useful models when designing software that processes\ndata with a recursive structure. The compiler's parser deals with\nrecursively nested features. Regular Expressions also denote the\nstructure of data, especially in text strings.\n\nThe study of automata addresses the following questions:\n\nWhat can a computer do?\nWhat can a computer do efficiently?\n\nAutomata theory also provides a tool for making formal proofs, of both\nthe inductive and deductive type.\n\nFormal Proofs {#formal-proofs}\n\nHere the more common proof requirements are here:\n\nProving set equivalence: we can approach this by rephrasing it into\n    an iff statement.\n    Prove that if x is in E, then x  is in F.\n    Prove that if x is in F, then x is in E.\n\nInductive Proofs {#inductive-proofs}\n\nSuppose we are given a statement \\\\(S(n)\\\\) to prove. The inductive\napproach involves:\n\nThe basis: where we show \\\\(S(i)\\\\) for a particular \\\\(i\\\\).\nThe inductive step: where we show if \\\\(S(k)\\\\) (or \\\\(S(i), S(i+1),\n       \\dots, S(k)\\\\)) then \\\\(S(k+1)\\\\).\n\nStructural Inductions {#structural-inductions}\n\nWe can sometimes prove statements by construction. This is often the\ncase with recursively defined structures, such as with trees and\nexpressions. This works because we the recursive definition is invoked\nat each step, so we are guaranteed that at each step of the\nconstruction, the construction \\\\(X\\_i\\\\) is valid.\n\nAutomata Theory {#automata-theory}\n\nDefinitions {#definitions}\n\nalphabet\n: An alphabet is a finite, nonempty set of symbols. E.g.\n    \\\\(\\Sigma = \\{0, 1\\}\\\\) represents the binary alphabet\n\nstring\n: A string is a finite sequence of symbols chosen from some\n    alphabet. For example, \\\\(01101\\\\) is a string from the binary\n    alphabet. The empty string is represented by &epsilon;, and\n    sometimes \\\\(\\Lambda\\\\). The\n    length of a string is denoted as such: \\\\(|001| = 3\\\\)\n\nPowers\n: \\\\(\\Sigma^k\\\\) represents strings of length \\\\(k\\\\).\n\nConcatenation\n: \\\\(xy\\\\) denotes the concatenation of strings \\\\(x\\\\) and\n    \\\\(y\\\\).\n\nProblem\n: a problem is the question of deciding whether a given\n    string is a member of some particular language.\n\nFinite Automata {#finite-automata}\n\nAn automata has:\n\na set of states\nits \"control\" moves from state to state in response to external inputs\n\nA finite automata is one where the automaton can only be in a single\nstate at once (it is deterministic). Non-determinism allows us to\nprogram solutions to problems using a higher-level language.\n\nDeterminism refers to the fact that on each input there is one and\nonly one state to which the automaton can transition from its current\nstate. Deterministic Finite Automata is often abbrieviated with DFA.\n\nDeterministic Finite Automata {#deterministic-finite-automata}\n\nA dfa consists of:\n\nA finite set of states, often denoted \\\\(Q\\\\).\nA finite set of input symbols, often denoted \\\\(\\Sigma\\\\).\nA transition function that takes as arguments a state and an input\n    symbol and returns a state, often denoted \\\\(\\delta\\\\). If \\\\(q\\\\) is a state,\n    and \\\\(a\\\\) is an input symbol, then \\\\(\\delta(q,a)\\\\) is that state \\\\(p\\\\) such\n    that there is an arc labeled \\\\(a\\\\) from \\\\(q\\\\) to \\\\(p\\\\).\nA start state, one of the states in \\\\(Q\\\\).\nA set of final or accepting states \\\\(F\\\\). The set \\\\(F\\\\) is a subset of\n    \\\\(Q\\\\).\n\nIn proofs, we often talk about a DFA in \"five-tuple\" notation:\n\n\\begin{equation}\n  A = \\left(Q, \\Sigma, \\delta, q\\_0, F \\right)\n\\end{equation}\n\nSimpler Notations {#simpler-notations}\n\nThe two preferred notation for describing automata are:\n\ntransition diagrams\n: a graph\n\n{{}}\n\ntransition table\n: a tubular listing of the \\\\(\\delta\\\\) function, which by\n    implication tells us the states and the input alphabet.\n\n{{}}\n\nLanguage of a DFA {#language-of-a-dfa}\n\nWe can define the language of a DFA \\\\(A = \\left(Q, \\Sigma, q\\_0, F\\right)\\\\).\nThis language is denoted \\\\(L(A)\\\\), and is defined by:\n\n\\begin{equation}\nL(A) = \\{ w | \\delta(q\\_0, w) \\text{ is in } F\\}\n\\end{equation}\n\nThe language of \\\\(A\\\\) is the set of strings \\\\(w\\\\) that take the start\nstate \\\\(q\\_0\\\\) to one of the accepting states.\n\nExtending Transition Function to Strings {#extending-transition-function-to-strings}\n\nBasis:\n\n\\begin{equation}\n\\hat{\\delta}\\left(q, \\epsilon\\right) = q\n\\end{equation}\n\nInduction:\n\n\\begin{equation}\n\\hat{\\delta}\\left(q, xa\\right) = \\delta \\left(\\hat{\\delta}\\left(q, x\\right), a \\right)\n\\end{equation}\n\nNondeterministic Finite Automata {#nondeterministic-finite-automata}\n\nA NFA has can be in several states at once, and this ability is\nexpressed as an ability to \"guess\" something about its input. It can\nbe shown that NFAs accept exactly the regular languages, just as DFAs\ndo. We can always convert an NFA to a DFA, although the latter may\nhave exponentially more states than the NFA.\n\nDefinition {#definition}\n\nAn NFA has:\n\nA finite set of states \\\\(Q\\\\).\nA finite set of input symbols \\\\(\\Sigma\\\\).\nA starting state \\\\(q\\_0 \\in Q\\\\),\nA set of final states \\\\(F \\subset Q\\\\).\nA transition function that takes a state in \\\\(Q\\\\) and an input symbol\n    in \\\\(\\Sigma\\\\) as arguments and returns a subset of \\\\(Q\\\\).\n\nThe Language of an NFA {#the-language-of-an-nfa}\n\nif \\\\(A = (Q, \\Sigma, \\delta, q\\_0, F)\\\\) is an NFA, then\n\n\\begin{equation}\nL(A) = \\{w | \\hat{\\delta}(q\\_0, w) \\cap F \\neq \\emptyset\\}\n\\end{equation}\n\nThat is, \\\\(L(A)\\\\) is the set of strings \\\\(w\\\\) in \\\\(\\Sigma^\\*\\\\) such that\n\\\\(\\hat{\\delta}(q\\_0, w)\\\\).\n\nThe Equivalence of DFA and NFA {#the-equivalence-of-dfa-and-nfa}\n\n{{}}\n\nFinite Automata with Epsilon-Transitions {#finite-automata-with-epsilon-transitions}\n\nTransitions on &epsilon;, the empty string, allow NFAs to make a transition\nspontaneously. This is sometimes referred to as &epsilon;-NFAs, and are\nclosely related to regular expressions.\n\n{{}}\n\nOf particular interest is the transition from \\\\(q\\0\\\\) to \\\\(q\\1\\\\), where the\n\\\\(+\\\\) and \\\\(-\\\\) sign is optional.\n\nEpsilon-Closures {#epsilon-closures}\n\nWe &epsilon;-close a state \\\\(q\\\\) by following all transitions out of \\\\(q\\\\) that\nare labelled &epsilon;, eventually finding all states that can be reached\nfrom \\\\(q\\\\) along any path whose arcs are all labelled &epsilon;.\n\n&epsilon;-closure allows us to explain easily what the transitions of an\n&epsilon;-NFA look like when given a sequence of (non-&epsilon;) inputs. Suppose\nthat \\\\(E = (Q, \\Sigma, \\delta, q\\_0, F)\\\\) is an &epsilon;-NFA. We first define \\\\(\\hat{\\delta}\\\\),\nthe extended transition function, to reflect what happens on a\nsequence of inputs.\n\nBASIS: \\\\(\\hat{\\delta}(q, \\epsilon) = ECLOSE(q)\\\\). If the label of the path is &epsilon;,\nthen we can follow only &epsilon;-labeled arcs extending from state \\\\(q\\\\).\n\nINDUCTION: Suppose \\\\(w\\\\) is of the form \\\\(xa\\\\), where \\\\(a\\\\) is the last\nsymbol of \\\\(w\\\\). Note that \\\\(a\\\\) is a member of \\\\(\\Sigma\\\\); it cannot be &epsilon;.\nThen:\n\n\\begin{align}\n  \\text{Let } & \\hat{\\delta}(q, x) = \\{p\\1, p\\2, \\dots, p\\_k\\} \\\\\\\\\\\\\n   & \\bigcup\\limits\\{i=1}^k \\delta(p\\i, a) = \\{r\\1, r\\2, \\dots, r\\_m\\} \\\\\\\\\\\\\n  \\text{Then } & \\hat{\\delta}(q,w) = \\bigcup\\limits\\{j=1}^m ECLOSE(r\\j)\n\\end{align}\n\nEliminating &epsilon;-Transitions {#eliminating-and-epsilon-transitions}\n\nGiven any &epsilon;-NFA \\\\(E\\\\), we can find a DFA \\\\(D\\\\) that accepts the same\nlanguage as \\\\(E\\\\).\n\nLet \\\\(E = (Q\\E, \\Sigma, \\delta\\E, q\\0, F\\E)\\\\), then the equivalent DFA \\\\(D = (Q\\_D, \\Sigma,\n\\delta\\D, q\\D, F\\_D)\\\\) is defined as follows:\n\n\\\\(Q\\D\\\\) is the set of subsets of \\\\(Q\\E\\\\). All accessible states of \\\\(D\\\\)\n    are &epsilon;-closed subsets of \\\\(Q\\E\\\\), i.e. \\\\(S \\subseteq Q\\K s.t. S =\n       ECLOSE(S)\\\\). Any &epsilon;-transition out of one of the states in \\\\(S\\\\)\n    leads to another state in \\\\(S\\\\).\n\\\\(q\\D = ECLOSE(q\\0)\\\\), we get the start state of \\\\(D\\\\) by closing the set\n    consisting of only the start state of \\\\(E\\\\).\n\\\\(F\\_D\\\\) is those set of states that contain at least one accepting\n    state of \\\\(E\\\\). \\\\(F\\D = \\{S | S \\text{ is in } Q\\D \\text{ and } S \\cap F\\_E\n       \\neq \\emptyset \\}\\\\)\n\\\\(\\delta\\D(S,a)\\\\) is computed for all \\\\(a\\\\) in \\\\(\\Sigma\\\\) and sets \\\\(S\\\\) in \\\\(Q\\D\\\\) by:\n    Let \\\\(S = \\{p\\1, p\\2, \\dots, p\\_k\\}\\\\)\n    Compute \\\\(\\bigcup\\limits\\{i=1}^{k}\\delta\\E(p\\i, a) = \\{r\\1, r\\2, \\dots, r\\m\\}\\\\)\n    Then \\\\(\\delta\\D(S, a) = \\bigcup\\limits\\{j=1}^{m}ECLOSE(r\\_j)\\\\)\n\nRegular Expressions {#regular-expressions}\n\nRegular expressions may be thought of as a \"programming language\", in\nwhich many important applications like text search applications or\ncompiler components can be expressed in.\n\nRegular expressions can define the exact same languages that various\nforms of automata describe: the regular languages. Regular expressions\ndenote languages. We define 3 operations on languages that the\noperators of regular expressions represent.\n\nThe union of two languages \\\\(L \\bigcup M\\\\), is the set of strings\n    that are either in \\\\(L\\\\) or \\\\(M\\\\).\nThe concatenation of languages \\\\(L\\\\) and \\\\(M\\\\) is the set of strings\n    that can be formed by taking any string in \\\\(L\\\\) and concatenating it\n    with any string in \\\\(M\\\\).\nThe closure (or star, or Kleene closure) is denoted \\\\(L^\\*\\\\) and\n    represents the set of those strings that can be formed by taking\n    any number of strings from \\\\(L\\\\), possibly with repetitions, and\n    concatenating all of them.\n\nWe can describe regular expressions recursively. For each expression\n\\\\(E\\\\), we denote the language it represents with \\\\(L(E)\\\\).\n\nBASIS:\n\nThe constants \\\\(\\epsilon\\\\) and \\\\(\\emptyset\\\\) are regular expressions, denoting the\n    languages \\\\(\\{\\epsilon\\}\\\\) and \\\\(\\emptyset\\\\) respectively.\nIf \\\\(a\\\\) is a symbol, then \\\\(\\mathbb{a}\\\\) is a regular expression. This\n    expression denotes the language \\\\(\\{a\\}\\\\).\n\nINDUCTION:\n\n\\\\(L(E) + L(F) = L(E) \\bigcup L(F)\\\\)\n\\\\(L(EF) = L(E)L(F)\\\\)\n\\\\(L(E^\\) = (L(E))^\\\\\\)\n\\\\(L((E)) = L(E)\\\\)\n\nPrecedence of regular expression operators {#precedence-of-regular-expression-operators}\n\nThe precedence in order of highest to lowest, is:\n\nstar\ndot (note that this operation is associative)\nunion (\\\\(\\plus\\\\) operator)\n\nEquivalence of DFA and Regular Expressions {#equivalence-of-dfa-and-regular-expressions}\n\nWe show this by showing that:\n\nEvery language defined by a DFA is also defined by a regular\n    expression.\nEvery language defined by a regular expression is also defined by a\n    $\\epilon$-NFA, which we have already shown is equivalent to a DFA.\n\n{{}}\n\nFrom DFA to Regular Expression {#from-dfa-to-regular-expression}\n\nWe can number the finite states in a DFA \\\\(A\\\\) with \\\\(1, 2, \\dots, n\\\\).\n\nLet \\\\(R\\_{ij}^{(k)}\\\\) be the name of a regular expression whose language is the\nset of strings \\\\(w\\\\) such that \\\\(w\\\\) is the label of a path from state \\\\(i\\\\)\nto state \\\\(j\\\\) in a DFA \\\\(A\\\\), and the path has no intermediate node whose\nnumber is greater than \\\\(k\\\\).\nTo construct the expression \\\\(R\\_{ij}^{(k)}\\\\), we use the following inductive\ndefinition, starting at \\\\(k= 0\\\\), and finally reaching \\\\(k=n\\\\).\n\nBASIS: \\\\(k=0\\\\).\nSince the states are numbered \\\\(1\\\\) or above, the restriction on paths\nis that the paths have no intermediate states at all. There are only 2\nkinds of paths that meet such a condition:\n\nAn arc from node (state) \\\\(i\\\\) to node \\\\(j\\\\).\nA path of length \\\\(0\\\\) that consists only of some node \\\\(i\\\\).\n\nIf \\\\(i \\ne j\\\\), then only case \\\\(1\\\\) is possible. We must examine DFA \\\\(A\\\\)\nand find input symbols \\\\(a\\\\) such that there is a transition from state\n\\\\(i\\\\) to state \\\\(j\\\\) on symbol \\\\(a\\\\).\n\nIf there is no such symbol \\\\(a\\\\), then \\\\(R\\_{ij}^{(0)} = \\emptyset\\\\).\nIf there is exactly one such symbol \\\\(a\\\\), then \\\\(R\\_{ij}^{(0)} = \\mathbb{a}\\\\)\nIf there are symbols \\\\(a\\1, a\\2, \\dots, a\\_k\\\\) that label arcs from\n    state \\\\(i\\\\) to state \\\\(j\\\\), then \\\\(R\\{ij}^{(0)} = \\mathbb{a\\1} + \\mathbb{a\\_2} +\n       \\dots + \\mathbb{a\\_k}\\\\)\n\nIn case (a), the expression becomes \\\\(\\epsilon\\\\), in case (c), the expression\nbecomes \\\\(\\epsilon + \\mathbb{a\\1} + \\mathbb{a\\2} + \\dots + \\mathbb{a\\_k}\\\\).\n\nINDUCTION: Suppose there is a path from state \\\\(i\\\\) to state \\\\(j\\\\) that\ngoes through no state higher than \\\\(k\\\\). Then either:\n\nThe path does not go through state \\\\(k\\\\) at all. In this case, the\n    label of the path is \\\\(R\\_{ij}^{(k-1)}\\\\).\nThe path goes through state \\\\(k\\\\) at least once. We can break the\n    path into several pieces:\n\n{{}}\n\nThen the set of labels for all paths of this type is represented by\nthe regular expression \\\\(R\\{ik}^{(k-1)}(R\\{kk}^{(k-1)})^\\*R\\_{kj}^{(k-1)}\\\\). Then, we can\ncombine the expressions for the paths of the two above:\n\n\\begin{equation}\nR\\{ij}^{(k)} = R\\{ij}^{(k-1)} + R\\{ik}^{(k-1)}(R\\{kk}^{(k-1)})^\\*R\\_{kj}^{(k-1)}\n\\end{equation}\n\nWe can compute \\\\(R\\_{ij}^{(n)}\\\\) for all \\\\(i\\\\) and \\\\(j\\\\), and the language of the\nautomaton is then the sum of all expressions \\\\(R\\_{ij}^{(n)}\\\\) such that state\n\\\\(j\\\\) is an accepting state.\n\nConverting DFAs to regular expressions by eliminating states {#converting-dfas-to-regular-expressions-by-eliminating-states}\n\nThe above method of conversation always works, but is expensive. \\\\(n^3\\\\)\nexpressions have to be constructed for an n-state automaton, but the\nlength of the expression can grow by a factor of 4 on the average,\nwith each of the \\\\(n\\\\) inductive steps, and the expressions themselves\ncould reach on the order of \\\\(4^n\\\\) symbols.\n\nThe approach introduced here avoids duplicating work at some points,\nby eliminating states. If we eliminate a state \\\\(s\\\\), then all paths\nthat went through \\\\(s\\\\) no longer exist in the automaton. To preserve\nthe language, we must include on an arc that goes directly from \\\\(q\\\\) to\n\\\\(p\\\\), the labels of paths that went from some state \\\\(q\\\\) to \\\\(p\\\\) through\n\\\\(s\\\\). This label now includes strings, but we can use a regular\nexpression to represent all such strings.\n\nHence, we can construct a regular expression from a finite automaton\nas follows:\n\nFor each accepting state \\\\(q\\\\), apply the reduction process to\n    produce an equivalent automaton with regular-expression labels on\n    the arcs. Eliminate all states except \\\\(q\\\\) and the start state \\\\(q\\_0\\\\).\nIf \\\\(q \\neq q\\_0\\\\), then a two-state automaton remains, as depicted. The\n    regular expression for the automaton is \\\\((R + SU^\\T)^\\SU^\\*\\\\).\n\n{{}}\n\nIf the start state is also an accepting state, then we must perform\n    a state-elimination from the original automaton that gets rid of\n    every state but the start state, leaving a one-state automaton,\n    which accepts \\\\(R^\\*\\\\).\n\n{{}}\n\nConverting regular expressions to automata {#converting-regular-expressions-to-automata}\n\nWe can show every language defined by a regular expression is also\ndefined by a finite automaton, and we do so by converting any regular\nexpression \\\\(R\\\\) to an $&epsilon;$-NFA \\\\(E\\\\) with:\n\nExactly one accepting state\nNo arcs into initial state\nNo arcs out of the accepting state\n\nThe proof is conducted by structural induction on R, following the\nrecursive definition of regular expressions.\n\nThe basis of the induction involves constructing automatons for\nregular expressions (a) \\\\(\\epsilon\\\\), (b) \\\\(\\emptyset\\\\) and (c) \\\\(\\mathbb{a}\\\\). They are displayed\nbelow:\n\n{{}}\n\nThe inductive step consists of 4 cases: (a) The expression is \\\\(R + S\\\\)\nfor some smaller expressions \\\\(R\\\\) and \\\\(S\\\\). (b) The expression is \\\\(RS\\\\)\nfor smaller expressions \\\\(R\\\\) and \\\\(S\\\\). (c) The expression is \\\\(R\\*\\\\) for\nsome smaller expression \\\\(R\\\\). (d) The expression is (R) for some\nexpression R. The automatons for (a), (b), and (c) are shown below:\n\n{{}}\n\nThe automaton for \\\\(R\\\\) also serves as the automaton for \\\\(( R)\\\\).\n\nAlgebraic law for regular expressions {#algebraic-law-for-regular-expressions}\n\ncommutativity\n: \\\\(x + y = y + x\\\\).\n\nassociativity\n: \\\\((x \\times y) \\times z = x \\times (y \\times z)\\\\).\n\ndistributive\n: \\\\(x \\times (y + z) = x \\times y + x \\times z\\\\)\n\n\\\\(L + M = M + L\\\\)\n\\\\((L + M) + N = L + (M + N)\\\\)\n\\\\((LM)N = L(MN)\\\\)\n\\\\(\\emptyset + L = L + \\emptyset = L\\\\). \\\\(\\emptyset\\\\) is the identity for union.\n\\\\(\\epsilon L = L \\epsilon = L\\\\). \\\\(\\epsilon\\\\) is the identity for concatenation.\n\\\\(\\emptyset L = L\\emptyset = \\emptyset\\\\). \\\\(\\emptyset\\\\) is the annihilator for concatenation.\n\\\\(L(M + N) = LM + LN\\\\) (left distributive)\n\\\\((M + N)L  = ML + NL\\\\) (right distributive)\n\\\\(L + L = L\\\\) (idempotence law)\n\\\\((L^\\)^\\ = L^\\*\\\\).\n\\\\(\\emptyset^\\* = \\epsilon\\\\)\n\\\\(\\epsilon^\\* = \\epsilon\\\\)\n\\\\(L^{+} = LL^\\* = L^\\*L\\\\).\n\\\\(L^\\* = L^{+} + \\epsilon\\\\)\n\\\\(L? = \\epsilon + L\\\\)\n\nDiscovering laws for regular expressions {#discovering-laws-for-regular-expressions}\n\nThe truth of a law reduces to the question of the equality of two\nlanguages. We show set equivalence: a string in one language must be\nin another, and vice-versa.\n\nProperties of Regular Languages {#properties-of-regular-languages}\n\nRegular languages exhibit the \"closure\" property. These properties let\nus build recognizers for languages that are constructed from other\nlanguages by certain operations. Regular languages also exhibit\n\"decision properties\", which allow us to make decisions about whether\ntwo automata define the same language. This means that we can always\nminimize an automata to have as few states as possible for a\nparticular language.\n\nPumping Lemma {#pumping-lemma}\n\nWe have established that the class of languages known as regular\nlanguages are accepted by DFAs, NFAs and by $&epsilon;$-NFAs.\n\nHowever, not every language is a regular language. An easy way to see\nthis is that the number of languages is infinite, but DFAs have finite\nnumber of states, and are finite.\n\nThe pumping lemma lets us show that certain languages are not regular.\n\nLet \\\\(L\\\\) be a regular language. Then there exists a constant \\\\(n\\\\) (which\ndepends on \\\\(L\\\\)) such that for every string \\\\(w\\\\) in \\\\(L\\\\) such that\n\\\\(| w |  \\ge n\\\\), we can break \\\\(w\\\\) into three strings \\\\(w = xyz\\\\)\nsuch that:\n\n\\\\(y \\ne \\epsilon\\\\)\n\\\\(| xy | \\le n\\\\)\nFor all \\\\(k \\ge 0\\\\), the string \\\\(xy^k z\\\\) is also in \\\\(L\\\\)\n\nThat is, we can always find a non-empty string \\\\(y\\\\) not too far from\nthe beginning of \\\\(w\\\\) that can be \"pumped\". This means repeating \\\\(y\\\\)\nany number of times, or deleting it, keeps the resulting string in the\nlanguage \\\\(L\\\\).\n\nNote that there has been other ways to prove irregularity of languages.\n\nClosure of Regular Languages {#closure-of-regular-languages}\n\nIf certain languages are regular, then languages formed from them by\ncertain operations are also regular. These are referred to as the\nclosure properties of regular languages. Below is a summary:\n\nUnion of 2 regular languages\nIntersection of 2 regular languages\nComplement of 2 regular languages\nDifference of 2 regular languages\nReversal of a regular language\nClosure (star) of a regular language\nThe concatenation of regular languages\nA homomorphism (substitution of strings for symbols) of a regular language\nThe inverse homomorphism of a regular language\n\nThe above are all regular.\n\nContext-free Grammars and Languages {#context-free-grammars-and-languages}\n\nContext-free languages are a larger class of languages, that have\ncontext-free grammars. We show how these grammars are defined, and how\nthey define languages.\n\nContext-free grammars are recursive definitions. For example, the\ncontext-free grammar for palindromes can be defined as:\n\n\\\\(P \\rightarrow \\epsilon\\\\)\n\\\\(P \\rightarrow 0\\\\)\n\\\\(P \\rightarrow 1\\\\)\n\\\\(P \\rightarrow 0P0\\\\)\n\\\\(P \\rightarrow 1P1\\\\)\n\nThere are four important components in a grammatical description of a\nlanguage:\n\nThere is a finite set of symbols that form the strings of the\n    language. These alphabets are called the terminals, or _terminal\n    symbols_.\nThere is a finite set of variables, or nonterminals or _syntactic\n    categories_. Each variable represents a language. In the language\n    above, the only variable is \\\\(P\\\\).\nOne of the variables represents the language being defined, called\n    the start symbol.\nThere is a finite set of productions or rules that represent the\n    recursive definition of a language. Each production rule consists:\n    A variable that is being defined by the production (called the head).\n    The production symbol \\\\(\\rightarrow\\\\).\n    A string of zero or more terminals and variables.\n\nWe can represent any CFG as these 4 components, we denote CFG \\\\(G = (V,\nT, P, S)\\\\).\n\nDerivations using a Grammar {#derivations-using-a-grammar}\n\nWe can apply the productions of a CFG to infer that certain strings\nare in the language of a certain variable.\n\nThe process of deriving strings by applying productions from head to\nbody requires the definition of a new relation symbol, \\\\(\\Rightarrow\\\\). Suppose \\\\(G\n= (V, T, P, S)\\\\) is a CFG> Let \\\\(\\alpha A \\beta\\\\) be a string of terminals and\nvariables, with \\\\(A\\\\) a variable. That is \\\\(\\alpha\\\\) and \\\\(\\beta\\\\) are strings in \\\\((V\n\\cup T)^\\*\\\\), and \\\\(A\\\\) is in \\\\(V\\\\). Let \\\\(A \\rightarrow \\gamma\\\\) be a production of \\\\(G\\\\). We say\nthat \\\\(\\alpha A \\beta \\Rightarrow\\_{G} \\alpha \\gamma B\\\\). If \\\\(G\\\\) is understood, we can omit the\nsubscript.\n\nWe may extend the \\\\(\\Rightarrow\\\\) relationship to represent zero, one or many\nderivation steps, similar to the extended transition function.\n\nLeftmost and Rightmost Derivations {#leftmost-and-rightmost-derivations}\n\nIn order to restrict the number of choices we have in deriving a\nstring, it is often useful to require that at each step, we replace\nthe leftmost variable by one of its production bodies. Such a\nderivation is called a leftmost derivation. Leftmost derivations are\nindicated with \\\\(\\Rightarrow\\{lm}\\\\) and \\\\(\\Rightarrow\\{lm}^\\*\\\\).\n\nSimilarly, it is possible to require that at each step the rightmost\nvariable is replaced by one of its bodies. These are called _rightmost\nderivations. These are similarly denoted \\\\(\\Rightarrow\\{rm}\\\\) and \\\\(\\Rightarrow\\_{rm}^\\*\\\\).\n\nThe language of a Grammar {#the-language-of-a-grammar}\n\nIf \\\\(G = (V, T, P, S)\\\\) is a CFG, then the language of \\\\(G\\\\), denoted\n\\\\(L(G)\\\\) is the set of terminal strings that have derivations from the\nstart symbol:\n\n\\begin{equation}\n  L(G) = \\right\\{w in T^\\* | S \\Rightarrow\\_{G}^\\* w \\left\\}\n\\end{equation}\n\nSentential Forms {#sentential-forms}\n\nDerivations from the start symbol produce strings that have a special\nrole. These are called sentential forms. We also denote\nleft-sentential and right-sentential forms for leftmost derivations\nand rightmost derivations respectively.\n\nParse Trees {#parse-trees}\n\nThere is a tree representation of derivations, that clearly show how\nsymbols of a terminal string are grouped into substrings, each of\nwhich belongs to the language of one of the variables of the grammar.\nThis tree is the data structure of choice when representing the source\nof a program.\n\nConstruction {#construction}\n\nThe parse trees for a CFG \\\\(G\\\\) are trees with the following conditions:\n\nEach interior node is labeled by variable in \\\\(V\\\\).\nEach leaf is labeled by either a variable, a terminal, or \\\\(\\epsilon\\\\).\n    However, if the leaf is labeled \\\\(\\epsilon\\\\), then it must be the only child\n    of its parent.\nIf an interior node is labeled \\\\(A\\\\), and its children are labeled\n    \\\\(X\\1, X\\2, \\dots, X\\k\\\\), respectively from the left, then \\\\(A \\rightarrow X\\1 X\\_2 \\dots\n       X\\_k\\\\) is a production in \\\\(P\\\\).\n\nThe yield {#the-yield}\n\nThe yield of the tree is the concatenation of the leaves of the parse\ntree from the left. This yield is a terminal string (all leaves are\nlabeled either with a terminal or with \\\\(\\epsilon\\\\)). The root is labeled by\nthe start symbol.\n\nInferences and derivations {#inferences-and-derivations}\n\nThe following statements are equivalent:\n\nThe recursive inference procedure determines that terminal string\n    \\\\(w\\\\) is in the language of variable \\\\(A\\\\).\n\\\\(A \\Rightarrow^\\* w\\\\)\n\\\\(A \\Rightarrow\\_{lm}^\\* w\\\\)\n\\\\(A \\Rightarrow\\_{rm}^\\* w\\\\)\nThere is a parse tree with root \\\\(A\\\\) and yield \\\\(w\\\\).\n\nWe can prove these equivalences using the following arcs:\n\n{{}}\n\nLinear Grammars {#linear-grammars}\n\nRight linear grammars have all the productions of form:\n\n\\\\(A \\rightarrow wB\\\\) for \\\\(B \\in V\\\\) and \\\\(w \\in T^\\*\\\\)\n\\\\(A \\rightarrow  w\\\\), for \\\\(w \\in T^\\*\\\\)\n\nEvery regular language can be generated by some right-linear grammar.\nSuppose \\\\(L\\\\) is accepted by DFA \\\\(A = (Q, \\Sigma, \\delta, q\\_0, F)\\\\), Then, let \\\\(G =\n(Q, \\Sigma, P, q\\_0)\\\\) where,\n\nFor \\\\(q, p \\in Q\\\\), \\\\(a \\in \\Sigma\\\\), if \\\\(\\delta(q, a) = p\\\\), then we have a\n    production in \\\\(P\\\\) of the form \\\\(q \\rightarrow ap\\\\).\nWe also have productions \\\\(q \\rightarrow \\epsilon\\\\) for each \\\\(q \\in F\\\\).\n\nWe can prove by induction on \\\\(|w|\\\\) that \\\\(\\hat{\\delta}(q\\0, w) = p\\\\)  iff \\\\(q\\0\n\\Rightarrow^\\* wp\\\\). This would give \\\\(\\hat{\\delta}(q\\0, w) \\in F\\\\)  iff \\\\(q\\0 \\Rightarrow^\\* w\\\\).\n\nTODO Ambiguous Grammars {#ambiguous-grammars}\n\nChomsky Normal Form {#chomsky-normal-form}\n\nChomsky normal form is useful in giving algorithms for working with\ncontext-free grammars. A context-free grammar is in Chomsky normal\nform if every rule is of the form:\n\n\\\\(A \\rightarrow BC\\\\)\n\\\\(A \\rightarrow a\\\\)\n\nweher \\\\(a\\\\) is any terminal and \\\\(A\\\\), \\\\(B\\\\), \\\\(C\\\\) are any variables, except\n\\\\(B\\\\) and \\\\(C\\\\) cannot be the start variable. \\\\(S \\rightarrow \\epsilon\\\\) is also allowed.\n\nAny context-free language is generated by a context-free grammar in\nChomsky normal form. This is because we can convert any grammar into\nChomsky normal form.\n\nPushdown Automata {#pushdown-automata}\n\nPushdown automata are equivalent in power to context-free grammars\nThis equivalence is useful because it gives us two options for proving\nthat a language is context-free. Certain languages are more easily\ndescribed in terms of recognizers, while others aremore easily\ndescribed in terms of generators.\n\nDefinition {#definition}\n\nIt is in essence a nondeterministic finite automaton with\n&epsilon;-transitions permitted, with one additional capability: a stack on\nwhich it can store a string of \"stack symbols\".\n\nWe can view the pushdown automaton informally as the device suggested\nbelow:\n\n{{}}\n\nA \"finite-state-control\" reads inputs, one symbol at a time. The\nautomaton is allowed to observe the symbol at the top of the stack,\nand to base its transition on its current state. It :\n\nConsumes from the input the symbol it uses in the transition. If &epsilon;\n    is used for the input, then no input symbol is consumed.\nGoes to a new state\nReplaces the symbol at the top of the stack by any string. This\n    corresponds to &epsilon;, which corresponds to a pop of the stack. It could\n    also replace the top symbol by one other symbol. Finally the top\n    stack symbol could be replaced by 2 or more symbols, which has the\n    effect of changing the top stack symbol, and pushing one or more\n    new symbols onto the stack.\n\nFormal Definition {#formal-definition}\n\nWe can specify a PDA \\\\(P\\\\) as follows:\n\n\\begin{equation}\n  P = (Q,\\Sigma, \\Gamma, \\delta, q\\0, Z\\0, F)\n\\end{equation}\n\n\\\\(Q\\\\) is the finite set of states\n\\\\(\\Sigma\\\\) is the finite set of input symbols\n\\\\(\\Gamma\\\\) is the finite stack alphabet\n\\\\(\\delta\\\\) is the transition function, taking a triple $&delta;(q,a,X), where:\n    \\\\(q\\\\) is a state in \\\\(Q\\\\)\n    \\\\(a\\\\) is either an input symbol in \\\\(\\Sigma\\\\) or \\\\(\\epsilon\\\\).\n    \\\\(X\\\\) is a stack symbol, that is a member of \\\\(\\Gamma\\\\)\n\\\\(q\\_0\\\\) the start state\n\\\\(Z\\_0\\\\) the start symbol. Initially, the PDA's stack consists of this\n    symbol, nothing else\n\\\\(F\\\\) is the set of accepting states\n\nThe formal definition of a PDA contains no explicit mechanism to allow\nthe PDA to test for an empty stack. The PDA is able to get the same\neffect by initially placing a special symbol $ on the stack. If it\nsees the $ again, it knows that the stack effectively is empty.\n\nWe can also draw transition diagrams for PDAs. An example is shown\nbelow.\n\n{{}}\n\nInstantaneous Descriptions {#instantaneous-descriptions}\n\nWe represent the configuration of a PDA by a triple \\\\((q, w, \\gamma)\\\\),\nwhere:\n\n\\\\(q\\\\) is the state.\n\\\\(w\\\\) is the remaining input\n\\\\(\\gamma\\\\) is the stack contents\n\nConventionally, we show the top of the stack at the left, and the\nbottom at the right. This triple is called the _instantaneous\ndescription_, or ID, of the pushdown automaton.\n\nFor finite automata, the \\\\(\\hat{\\gamma}\\\\) notation was sufficient to\nrepresent sequences of instantaneous descriptions through which a\nfinite automaton moved. However, for PDAs we need a notation that\ndescribes changes in the state, input and stack. Hence, we define\n\\\\(\\vdash\\\\) as follows. Supposed \\\\(\\delta(q, a, X)\\\\) contains \\\\((p, \\alpha)\\\\). Then for\nall stings \\\\(w\\\\) in \\\\(\\Sigma^\\\\\\) and \\\\(\\beta\\\\) in \\\\(\\Gamma^\\\\\\):\n\n\\begin{equation}\n  (q, aw, X\\beta) \\vdash (p, w, \\alpha\\beta)\n\\end{equation}\n\nWe use \\\\(\\vdash^\\*\\\\) to represent zero or more moves of the PDA.\n\nThe Languages of PDAs {#the-languages-of-pdas}\n\nThe class of languages for PDAs that accept by final state and accept\nby empty stack are the same. We can show how to convert between the\ntwo.\n\nAcceptance by Final State {#acceptance-by-final-state}\n\nLet \\\\(P = (Q, \\Sigma, \\Gamma, \\delta, q\\0, Z\\0, F)\\\\) be a PDA. Then \\\\(L(P)\\\\), the language\naccepted by P by final state, is:\n\n\\begin{equation}\n\\{w | (q\\0, w, Z\\0) \\vdash^\\* (q, \\epsilon, \\alpha) \\}\n\\end{equation}\n\nfor some state \\\\(q\\\\) in \\\\(F\\\\) and any stack string \\\\(\\alpha\\\\).\n\nAcceptance by Empty Stack {#acceptance-by-empty-stack}\n\nLet \\\\(P = (Q, \\Sigma, \\Gamma, \\delta, q\\0, Z\\0, F)\\\\) be a PDA. We define \\\\(N(P) = \\{w |\n(q\\0, w, Z\\0) \\vdash^\\* (q, \\epsilon, \\epsilon)\\}\\\\). That is \\\\(N(P)\\\\) is the set of inputs\n\\\\(w\\\\) that \\\\(P\\\\) can consume and at the same time empty its stack.\n\nFrom Empty Stack to Final State {#from-empty-stack-to-final-state}\n\nTheorem:\n\nIf \\\\(L = N(P\\N)\\\\) for some PDA \\\\(P\\N\\\\), then there is a PDA \\\\(P\\_F\\\\)\nsuch that \\\\(L = L(P\\_F)\\\\).\n\nProof:\n\nWe use a new symbol \\\\(X\\0\\\\), not a symbol of \\\\(\\Gamma\\\\); \\\\(X\\0\\\\) is both the start\nsymbol of \\\\(P\\_F\\\\) and a marker on the bottom of the stack that lets us\nknow when \\\\(P\\N\\\\) has reached an empty stack, then it knows that \\\\(P\\N\\\\)\nwould empty its stack on the same input.\n\nWe also use a new start state \\\\(p\\_0\\\\), whose sole function is to push\n\\\\(Z\\0\\\\), the start symbol of \\\\(P\\N\\\\), onto the top of the stack and enter\n\\\\(q\\0\\\\), the start state of \\\\(P\\N\\\\). \\\\(P\\F\\\\) simulates \\\\(P\\N\\\\), until the stack of\n\\\\(P\\N\\\\) is empty, which \\\\(P\\F\\\\) detects because it sees \\\\(X\\_0\\\\) on the top of\nthe stack.\n\n{{}}\n\nHence, we can specify \\\\(P\\F = (Q \\cup \\{p\\0, p\\f\\}, \\Sigma, \\Gamma \\cup \\{X\\0\\}, \\delta\\F, p\\0,\nX\\0, \\{p\\f\\}\\\\):\n\n\\\\(\\delta\\F(p\\0, \\epsilon, x\\0) = \\{(q\\0, Z\\0, X\\0\\}\\\\).\nFor all states \\\\(q\\\\) in \\\\(Q\\\\), inputs \\\\(a\\\\) in \\\\(\\Sigma\\\\) or \\\\(a = \\epsilon\\\\), and the\n    stack symbols \\\\(Y\\\\) in \\\\(\\Gamma\\\\), \\\\(\\delta\\_F(q,a,Y)\\\\) contains all the pairs in\n    \\\\(\\delta\\_N(qa,Y)\\\\).\n\\\\(\\delta\\F(q, \\epsilon, X\\0\\\\) contains \\\\((p\\f, \\epsilon)\\\\) iff w is in \\\\(N(P\\N)\\\\).\n\nFrom Final State to Empty Stack {#from-final-state-to-empty-stack}\n\nWhenever \\\\(P\\_F\\\\) enters an accepting state after consuming input \\\\(w\\\\),\n\\\\(P\\_N\\\\) will empty its stack after consuming \\\\(w\\\\).\n\n{{}}\n\nTo avoid simulating a situation where \\\\(P\\_F\\\\) accidentally empties its\nstack without accepting, \\\\(P\\N\\\\) also utilizes a marker \\\\(X\\0\\\\) on the\nbottom of its stack.\n\nThat is \\\\(P\\N = (Q \\cup \\{p\\0, p\\}, \\Sigma, \\Gamma \\cup \\{X\\0\\}, \\delta\\N, p\\0, X\\0)\\\\), where \\\\(\\delta\\_N\\\\)\nis:\n\n\\\\(\\delta\\N(p\\0, \\epsilon, x\\0) = \\{(q\\0, Z\\0, X\\0)\\}\\\\)\nFor all states \\\\(q\\\\) in \\\\(Q\\\\), input symbols \\\\(a\\\\) in \\\\(\\Sigma\\\\) or \\\\(a = \\epsilon\\\\), \\\\(Y\\\\)\n    in \\\\(\\Gamma\\\\), \\\\(\\delta\\N(q, a, Y)\\\\) contains every pair that is in \\\\(\\delta\\F(q, a Y)\\\\).\nFor all accepting states \\\\(q\\\\) in \\\\(F\\\\), and stack symbols \\\\(Y\\\\) in \\\\(\\Gamma\\\\)\n    or \\\\(Y = X\\0\\\\), \\\\(\\delta\\N(q, \\epsilon, Y)\\\\) contains \\\\((p, \\epsilon)\\\\). Whenever \\\\(P\\_F\\\\)\n    accepts, \\\\(P\\_N\\\\) can start emptying its stack without consuming any input.\nFor all stack symbols \\\\(Y\\\\) in \\\\(\\Gamma\\\\) or \\\\(Y = X\\0\\\\), \\\\(\\delta\\N(p, \\epsilon, Y) = \\{(p,\n       \\epsilon)\\}\\\\). Once in state \\\\(p\\\\), which only occurs when \\\\(P\\_F\\\\) is accepted,\n    \\\\(P\\_N\\\\) pops every symbol on its stack, until the stack is empty.\n\nEquivalence of CFG and PDA {#equivalence-of-cfg-and-pda}\n\nLet \\\\(A\\\\) be a CFL. From the definition we know that \\\\(A\\\\) has a CFG, \\\\(G\\\\),\ngenerating it. We show how to convert \\\\(G\\\\) into an equivalent PDA.\n\nThe PDA \\\\(P\\\\) we now describe will work by accepting its input \\\\(w\\\\), if\n\\\\(G\\\\) generates that input, by determining whether there is a derivation\nfor \\\\(w\\\\). Each step of the derivation yields an intermediate string of\nvariables and terminals. We design \\\\(P\\\\) to determine whether some\nseries of substitutions of \\\\(G\\\\) can lead from the start variable to\n\\\\(w\\\\).\n\nThe PDA \\\\(P\\\\) begins by writing the start variable on its stack. It goes\nthrough a series of intermediate strings, making one substitution\nafter another. Eventually it may arrive at a string that contains only\nterminal symbols, meaning that it has used the grammar to derive a\nstring. Then \\\\(P\\\\) accepts if this string is identical to the string it\nhas received as input.\n\nPlace the marker symbol $ and the start variable on the stack\nRepeat:\n    If the top of stack is a variable symbol \\\\(A\\\\), nondeterministically\n        select one of the rules for \\\\(A\\\\) and substitute \\\\(A\\\\) by the string\n        on the right-hand side of the rule\n    If the top of stack is a terminal symbol \\\\(a\\\\), read the next\n        symbol from the input and compare it to \\\\(a\\\\). If they match,\n        continue. Else, reject the branch of nondeterminism.\n    If the top of stack is the symbol $, enter the accept state.\n\nNow we prove the reverse direction. We have a PDA \\\\(P\\\\) and want to make\na CFG \\\\(G\\\\) that generates all the strings that \\\\(P\\\\) accepts.\n\nFor each pair of states \\\\(p\\\\) and \\\\(q\\\\), the grammar will have a variable\n\\\\(A\\_{pq}\\\\).\n\nFirst, we simplify the task by modifying P slightly to give it the\nfollowing three features.\n\nIt has a single accept state, \\\\(q\\_{accept}\\\\).\nIt empties its stack before accepting.\nEach transition either pushes a symbol onto the stack (a push move)\n    or pops one off the stack (a pop move), but it does not do both at\n    the same time.\n\nGiving \\\\(P\\\\) features 1 and 2 is easy. To give it feature 3, we replace\neach transition that simultaneously pops and pushes with a two\ntransition sequence that goes through a new state, and we replace each\ntransition that simultaneously pops and pushes with a two transition\nsequence that goes through a new state, and we replace each transition\nthat neither pops nor pushes with a two transition sequence that\npushes then pops an arbitrary stack symbol.\n\nTo design \\\\(G\\\\) so that \\\\(A\\_{pq}\\\\) generates all strings that take \\\\(P\\\\) from\n\\\\(p\\\\) to \\\\(q\\\\), regardless of the stack contents at \\\\(p\\\\), leaving the stack\nat \\\\(q\\\\) in the same condition as it was at \\\\(p\\\\).\n\nFirst, we simplify our task by modifying \\\\(P\\\\) slightly to give it the\nfollowing three features.\n\nDeterministic Pushdown Automata {#deterministic-pushdown-automata}\n\nDPDAs accept a class of languages between the regular languages and\nthe CFLs.\n\nWe can easily show that DPDAs accept all regular languages by making\nit simulate a DFA (ignoring the stack).\n\nWhile DPDAs cannot represent all CFLs, it is able to represent\nlanguages that have unambiguous grammars.\n\nProperties of Context-Free Languages {#properties-of-context-free-languages}\n\nSimplification of CFG {#simplification-of-cfg}\n\nThe goal is to reduce all CFLs to Chomsky Normal Form. To get there,\nwe must make several preliminary simplifications, which are useful in\ntheir own ways.\n\nElimination of useless symbols\n    Remove all non-reachable symbols: starting from \\\\(S\\\\), if it is\n        impossible to reach a symbol, then it is non-reachable and can be\n        removed. Example: \\\\(S \\rightarrow a, B \\rightarrow b\\\\), then \\\\(B\\\\) is not reachable.\n    Remove all non-generating symbols: for each symbol, check if the\n        symbol can ever generate a terminating string. If not, remove\n        it. Example: \\\\(A \\rightarrow A | Ab\\\\), \\\\(A\\\\) is non-generating and can be removed.\n    It is important to remove non-generating symbols first, because\n        it can lead to more non-reachable symbols.\n\nRemoval of unit productions\n    Unit productions look like \\\\(A \\rightarrow B\\\\). We do unit production\n        elimination to get a grammar into Chomsky Normal form.\n\n    To eliminate unit productions, we substitute the unit symbol\n        into the production.\n\nRemoval of \\\\(\\epsilon\\\\) productions\n    Start from any symbol, remove the \\\\(\\epsilon\\\\) production, by\n        substituting all possibilities. E.g. \\\\(S \\rightarrow AB | AC\\\\), and we\n        eliminate $&epsilon;$-productions from \\\\(B\\\\): \\\\(S \\rightarrow AB | A | AC\\\\).\n    Eliminate until no more $&epsilon;$-productions.\n    When an $&epsilon;$-production is eliminated from a symbol, it does not\n        need to be reapplied if it appears again in the same symbol.\n\nConversion to Chomsky Normal Form\n\nAfter performing the preliminary simplifications, we can then:\n\nArrange that all bodies of length 2 or more consist only of variables\nBreak bodies of length 3 or more into a cascade of productions\n\nPumping Lemma for CFLs {#pumping-lemma-for-cfls}\n\nIn any sufficiently long string in a CFL, it is possible to find at\nmost 2 short, nearby substrings, that we can pump in tandem. First, we\nuse several results, that we will state below.\n\nConversion to CNF converts a parse tree into a binary tree.\nFor a CNF grammar \\\\(G = (V, T, P, S)\\\\), if the length of the longest\n    path is \\\\(n\\\\), then \\\\(|w| \\le 2^{n-1}\\\\) for all terminal strings \\\\(w\\\\).\n\nLet \\\\(L\\\\) be a CFL. Then there exists a constant \\\\(n\\\\) (which\ndepends on \\\\(L\\\\)) such that for every string \\\\(z\\\\) in \\\\(L\\\\) such that\n\\\\(| z |  \\ge n\\\\), we can break \\\\(z\\\\) into three strings \\\\(z = uvwxy\\\\)\nsuch that:\n\n$ vx &ne; &epsilon;$\n\\\\(| vwx | \\le n\\\\)\nFor all \\\\(i \\ge 0\\\\), the string \\\\(uv^i wx^i y\\\\) is also in \\\\(L\\\\)\n\n{{}}\n\nIf \\\\(s\\\\) is sufficiently long, its derivation tree w.r.t. a Chomsky\nnormal form grammar must contain some nonterminal \\\\(N\\\\) twice on some\ntree path (upper picture). Repeating \\\\(n\\\\) times the derivation part \\\\(N\n\\Rightarrow \\dots \\Rightarrow vNx\\\\) obtains a derivation for \\\\(u v^n w x^n y\\\\).\n\nClosure Properties of CFLs {#closure-properties-of-cfls}\n\nFirst, we introduce the notion of substitutions. Let \\\\(\\Sigma\\\\) be an\nalphabet, and suppose that for every symbol \\\\(a\\\\) in \\\\(\\Sigma\\\\), we choose a\nlanguage \\\\(L\\_a\\\\). These chosen languages can be over any alphabets, not\nnecessarily \\\\(\\Sigma\\\\) and not necessarily the same. The choice of languages\ndefines a function \\\\(s\\\\) on \\\\(\\Sigma\\\\), and we shall refer to \\\\(L\\_a\\\\) as \\\\(s(a)\\\\)\nfor each symbol \\\\(a\\\\).\n\nIf \\\\(w = a\\1 a\\2 \\dots a\\_n\\\\) in \\\\(\\Sigma^\\*\\\\), then \\\\(s(w)\\\\) is the language of\nall strings \\\\(x\\1 x\\2 \\dots x\\n\\\\) such that the string \\\\(x\\i\\\\) is in the\nlanguage \\\\(s(a\\_i)\\\\). \\\\(s(L)\\\\) is the union of \\\\(s(w)\\\\) for all strings \\\\(w\\\\) in\n\\\\(L\\\\).\n\nThe substitution theorem states that if we can find a substitution\nfunction \\\\(a\\\\) on a CFL, then the resultant language \\\\(s(a)\\\\) is also a\nCFL.\n\nCFLs are closed under:\n\nUnion\n\n\\\\(L\\1 \\cup L\\2\\\\) is the language \\\\(s(L)\\\\), where \\\\(L\\\\) is the language \\\\(\\{1,\n2\\}\\\\), and \\\\(s(1) = L\\1\\\\) and \\\\(s(2) = L\\2\\\\).\n\nConcatenation\n\n\\\\(L\\1 L\\2\\\\) is the language \\\\(s(L)\\\\), \\\\(L = {12}\\\\), and \\\\(s(1) = L\\_1\\\\) and \\\\(s(2)\n= L\\_2\\\\).\n\nClosure, and positive closure (asterisk and plus)\n\n\\\\(L\\\\) is the language \\\\({1}^\\*\\\\), and \\\\(s\\\\) is the substitution \\\\(s(1) = L\\_1\\\\),\nthen \\\\(L\\_1^\\* = s(L)\\\\). Similarly, for positive closure, \\\\(L = \\{1\\}^+\\\\) and\n\\\\(L\\_1^+ = s(L)\\\\).\n\nHomomorphism\n\n\\\\(s(a) = \\{h(a)\\}\\\\), for all \\\\(a\\\\) in \\\\(\\Sigma\\\\). Then \\\\(h(L) = s(L)\\\\).\n\nThese are the base closure properties. CFLs are also closed under\nreversal. We can prove this by constructing a grammar for the reversed\nCFL.\n\nCFLs are not closed under intersection. However, CFLs are closed under\nthe operation of \"intersection with a regular language\". To prove\nthis, we use the PDA representation of CFLs, and FA representation of\nregular languages. We can run the FA in parallel with the PDA.\n\nCFLs are also closed under inverse homomorphism. The proof is similar\nto that of regular languages, but using a PDA, but is more complex\nbecause of the stack introduced in the PDA.\n\nDecision Properties of CFLs {#decision-properties-of-cfls}\n\nFirst, we consider the complexity of converting from a CFG to a PDA,\nand vice versa. Let \\\\(n\\\\) be the length of the entire representation of\na PDA or CFG.\n\nBelow, we list algorithms linear in the size of the input:\n\nConverting a CFG to a PDA\nConverting a PDA that accepts by final state to one that accepts by\n    empty stack\nConverting a PDA that accepts by empty stack to one that accepts by\n    final state\n\nThe running time of conversion from a PDA to a grammar is much more\ncomplex. The upper bound on the number of states and stack symbols is\n\\\\(n^3\\\\), so there cannot be more than \\\\(n^3\\\\) variables of the form \\\\([pXq\\\\)\nconstructed for the grammar. However, the running time of conversion\ncould still be exponential because there are no limits to the number\nof symbols put on the stack.\n\nHowever, we can break the pushing of a long string of stack symbols\ninto a sequence of at most \\\\(n\\\\) steps that each pushes one symbol. Then\neach transition has no more than 2 stack symbols, and the total length\nof all the transition rules has grown by at most a constant factor,\ni.e. it is still \\\\(O(n)\\\\). There are \\\\(O(n)\\\\) transition rules, and each\ngenerates \\\\(O(n^2)\\\\) productions, since there are only 2 states that need\nto be chosen in the productions that come from each rule. Hence, the\nconstructed grammar has length \\\\(O(n^3)\\\\), and can be constructed in\ncubic time.\n\nRunning Time of Conversion to CNF {#running-time-of-conversion-to-cnf}\n\nDetecting reachable and generating symbols can be done in \\\\(O(n)\\\\)\n    time, and removing useless symbols takes \\\\(O(n)\\\\) time and does not\n    increase the size of the grammar\nConstructing unit pairs and eliminating unit productions takes\n    \\\\(O(n^2)\\\\) time, and results in a grammar whose length is \\\\(O(n)\\\\).\nReplacing terminals by variables in production bodies takes \\\\(O(n)\\\\)\n    time and results in a grammar whose length is \\\\(O(n)\\\\).\nBreaking production bodies of length 3 or more takes \\\\(O(n)\\\\) time\n    and results in a grammar of length \\\\(O(n)\\\\).\n\nHowever eliminating $&epsilon;$-productions is tricky. If we have a production\nbody of length \\\\(k\\\\), we can construct from that one production \\\\(2^{k-1}\\\\)\nproductions for the new grammar, so this part of the construction\ncould take \\\\(O(2^n)\\\\) time. However, we can break all long production\nbodies into a sequence of productions with bodies of length 2. This\nstep takes \\\\(O(n)\\\\) time and grows only linearly, and makes eliminating\n$&epsilon;$-productions run in \\\\(O(n)\\\\) time.\n\nIn all, converting to CNF form is a \\\\(O(n^2)\\\\) algorithm.\n\nTesting for emptiness of CFL {#testing-for-emptiness-of-cfl}\n\nThis is equivalent to testing if \\\\(S\\\\) is generating. The algorithm goes\nas follows:\n\nWe maintain an array indexed by variable, which tells whether or not\nwe have established that a variable is generating.For each variable\nthere is a chain of all the positions in which the variable occurs\n(solid lines). The dashed lines suggest links from the productions to\ntheir counts.\n\n{{}}\n\nFor each production, we count the number of positions holding\nvariables whose ability to generate is not yet accounted for. Each\ntime the count for a head variable reaches 0, we put the variable on a\nqueue of generating variables whose consequences need to be explored.\n\nThis algorithm takes \\\\(O(n)\\\\) time:\n\nThere are at most \\\\(n\\\\) variables in a grammar of size \\\\(n\\\\), creation\n    and initialization of the array can be done in \\\\(O(n)\\\\) time.\nInitialization of the links and counts can be done in \\\\(O(n)\\\\) time.\nWhen we discover a production has count 0:\n    Discover a production has count \\\\(0\\\\), finding which variable is\n        at the head, checking whether it is already known to be\n        generating, and putting it on the queue if not. All these steps\n        are \\\\(O(1)\\\\) for each production so \\\\(O(n)\\\\) in total.\n    work done when visiting the production bodies that have the head\n        variable \\\\(A\\\\). This work is proportional to the number of\n        positions with \\\\(A\\\\). Hence, \\\\(O(n)\\\\).\n\nTesting Membership in a CFL {#testing-membership-in-a-cfl}\n\nFirst, we can easily see that algorithms exponential in \\\\(n\\\\) can\ndecide membership. We can convert the grammar to CNF form. As the\nparse trees are binary trees, there will be exactly \\\\(2n-1\\\\) nodes\nlabeled by variables in the tree for a string \\\\(w\\\\) of length \\\\(n\\\\). The\nnumber of possible trees and node-labelings is only exponential in\n\\\\(n\\\\).\n\nFortunately, more efficient techniques exist, based on the idea of\ndynamic programming. Once such algorithm is the CYK Algorithm.\n\nWe construct a triangular table, and begin the fill the table\nrow-by-row upwards. The horizontal axis corresponds to the positions\nof the string \\\\(w = a\\1 a\\2 \\dots a\\n\\\\), and the table entry \\\\(X\\ij\\\\) is the\nset of variables $A such that \\\\(A \\overset{\\*}{\\Rightarrow} a\\i a\\{i+1} \\dots a\\_j\\\\). We\nare interested in whether \\\\(S\\\\) is in the set \\\\(X\\_{1n}\\\\).\n\n{{}}\n\nIt takes \\\\(O(n)\\\\) time to compute any one entry of the table. Hence, the\ntable-construction process takes \\\\(O(n^3)\\\\) time.\n\nThe algorithm for computing \\\\(X\\_{ij}\\\\) is as such:\n\nBASIS: We compute the first row as follows. Since the string beginning\n and ending at position \\\\(i\\\\) is just the terminal \\\\(a\\_i\\\\), and the grammar\n is in \\\\(CNF\\\\),the only way to derive the string \\\\(a\\_i\\\\) is to use a\n production of the form \\\\(A \\rightarrow a\\i\\\\). Hence \\\\(X\\ii\\\\) is the set of variables\n \\\\(A\\\\) such that \\\\(A \\rightarrow a\\_i\\\\) is a production of \\\\(G\\\\).\n\nINDUCTION: To compute \\\\(X\\_{ij}\\\\) that is in row \\\\(j - i + 1\\\\), we would have\n computed all the \\\\(X\\\\) in the rows below i.e. we know about all strings\n shorter than \\\\(a\\i a\\{i+1} \\dots a\\_j\\\\), and we know all the proper prefix\n and proper suffixes of that string.\n\nFor \\\\(A\\\\) to be in \\\\(X\\_{ij}\\\\), we must find variables \\\\(B\\\\), \\\\(C\\\\), and integer\n\\\\(k\\\\) such that:\n\n\\\\(i \\le k }}\n\nInitially, the input, which is a finite-length string of symbols\nchosen from the input alphabet, is placed on the tape. All other tape\ncells, extending infinitely to the left and right, initially hold a\nspecial symbol called the blank. The blank is a tape symbol, but not\nan input symbol, and there may be other tape symbols.\n\nThere is a tape head that is always positioned at one of the tape\ncells. The Turing machine is said to be scanning that cell.\n\nA move of the Turing machine is a function of the state of the finite\ncontrol and the tape symbol scanned.\n\nChange state: The next state optionally may be the same as the\n    current state.\nWrite a tape symbol in the cell scanned. This tape symbol replaces\n    whatever symbol was in that cell.\nMove the tape head left or right.\n\nFormal Notation {#formal-notation}\n\nThe formal notation used for a Turing Machine (TM) is by a 7-tuple:\n\n\\begin{equation}\n  M = (Q, \\Sigma, \\Gamma, \\delta, q\\_0, B, F)\n\\end{equation}\n\nwhere:\n\nQ\n: The finite set of states of the finite control\n\n&Sigma;\n: The finite set of input symbols\n\n&Gamma;\n: The complete set of tape symbols. &Sigma; is a subset of &Gamma;.\n\n&delta;\n: The transition function. The arguments of \\\\(\\delta(q, X)\\\\) are a state \\\\(q\\\\)\n    and a tape symbol \\\\(X\\\\). The value of \\\\(\\delta(q, X)\\\\) is a triple\n    \\\\((p, Y, D)\\\\) where \\\\(p\\\\) is the next state, \\\\(Y\\\\) is the symbol\n    in \\\\(\\Gamma\\\\) written in the cell being scanned, \\\\(D\\\\) is a\n    direction, either left or right.\n\nq\\_0\n: The start state, a member of Q\n\nB\n: the blank symbol, in &Gamma; but not &Sigma;\n\nF\n: the set of final or accepting states\n\nInstantaneous Descriptions {#instantaneous-descriptions}\n\nWe use the string \\\\(X\\1 X\\2 \\dots X\\{i-1} q X\\i X\\{i+1} \\dots X\\n\\\\) to represent\nan ID in which:\n\n\\\\(q\\\\) is the state of the Turing machine.\nThe tape head is scanning the $i$th symbol from the left.\n\\\\(X\\1 X\\2 \\dots X\\_n\\\\) is the portion of the tape between the leftmost\n    and the rightmost nonblank. As an exception, if the head is to the\n    left of the leftmost nonblank, or to the right of the rightmost\n    nonblank, then some prefix or suffix of \\\\(X\\1 X\\2 \\dots X\\_n\\\\) will be\n    blank, and \\\\(i\\\\) will be 1 or n, respectively.\n\nWe describe moves of a Turing machine \\\\(M = (Q, \\Sigma, \\Gamma, \\delta, q\\_0, B, F)\\\\) by\nthe \\\\(\\vdash\\\\) notation that was used for PDAs. Suppose \\\\(\\delta(q, X\\_i) = (p,\nY, L)\\\\). Then \\\\(X\\1 X\\2 \\dots X\\{i-1} q X\\{i+1} \\dots X\\n \\vdash X\\1 X\\2 \\dots X\\{i-2}\np X\\{i-1} Y X\\{i+1} \\dots X\\_n\\\\).\n\nThere are 2 important exceptions, when \\\\(i=1\\\\) and M moves to the blank\nto the left of \\\\(X\\_1\\\\) and when \\\\(i=n\\\\) and \\\\(Y=B\\\\).\n\nTransition Diagrams {#transition-diagrams}\n\nTuring machines can be denoted graphically, as with PDAs.\n\nAn arc from state \\\\(q\\\\) to state \\\\(p\\\\) is labelled by one or more items of\nthe form \\\\(X/YD\\\\), where \\\\(X\\\\) and \\\\(Y\\\\) are tape symbols, and \\\\(D\\\\) is a\ndirection from either \\\\(L (\\leftarrow)\\\\) or \\\\(R (\\rightarrow)\\\\).\n\n{{}}\n\nThe Language of a TM {#the-language-of-a-tm}\n\n\\\\(L(M)\\\\) is the set of strings \\\\(w\\\\) in \\\\(\\Sigma^\\*\\\\) such that \\\\(q\\_0 w \\vdash \\alpha p \\beta\\\\)\nfor some state in \\\\(p\\\\) in \\\\(F\\\\) and any tape strings \\\\(\\alpha\\\\) and \\\\(\\beta\\\\). The set\nof languages we can accept using a TM is often called the _recursively\nenumerable languages_ or RE languages.\n\nTuring machines and halting {#turing-machines-and-halting}\n\nA TM halts if it enters a state \\\\(q\\\\), scanning a tape symbol \\\\(X\\\\), and\nthere is no move in this situation: \\\\(\\delta(q, X)\\\\) is undefined. This is\nanother notion of \"acceptance\". We can assume that a TM halts if it\naccepts. That is, without changing the language accepted, we can make\n\\\\(\\delta(q, X)\\\\) undefined whenever \\\\(q\\\\) is an accepting state. However, it is\nnot always possible to require that a TM halts even if it does not\naccept.\n\nLanguages that halt eventually, regardless of whether they accept, are\ncalled recursive. If an algorithm to solve a given problem exists,\nthen we say the problem is decidable, so TMs that always halt figure\nimportantly into decidability theory.\n\nProgramming Techniques for Turing Machines {#programming-techniques-for-turing-machines}\n\nStorage in the State {#storage-in-the-state}\n\nWe can use the finite control not only to represent a position in the\n\"program\" of a TM, but to hold a finite amount of data. We extend\nthe state as a tuple \\\\([q, A, B, C]\\\\), and having multiple tracks.\n\nMultiple Tracks {#multiple-tracks}\n\nOne can also think of the tape of a Turing machine as composed of\nseveral tracks. Each track can hold one symbol, and the tape alphabet\nof the TM consists of tuples, with each component for each \"track\". A\ncommon use of multiple tracks is to treat one track as holding the\ndata, and another track as holding a mark. We can check off each\nsymbol as we \"use\" it, or we can keep track of a small number of\npositions within the data by only marking these positions.\n\nSubroutines {#subroutines}\n\nA Turing machine subroutine is a set of states that perform some\nuseful process. This set of states includes a start state and another\nstate to pass control to whatever other set of states called the\nsubroutine. Since the TM has no mechanism for remembering a \"return\naddress\", that is, a state to go to after it finishes, should our\ndesign of a TM call for one subroutine to be called from several\nstates, we can make copies of the subroutine, using a new set of\nstates for each copy. The \"calls\" are made to the start states of\ndifferent copies of the subroutine, and each copy \"returns\" to a\ndifferent state.\n\nMultitape Turing Machines {#multitape-turing-machines}\n\nThe device has a finite control (state), and some finite number of\ntapes. Each tape is divided into cells, and each cell can hold any\nsymbol of the finite tape alphabet.\n\nInitially, the head of the first tape is at the left end of the input,\nbut all other tape heads are at some arbitrary cell.\n\n{{}}\n\nA move on the multitape TM depends on the state and symbol scanned by\neach of the tape heads. On each move:\n\nthe control enters a new state, which could be the same as a\n    previous state.\nOn each tape, a new tape symbol is written on the cell scanned.\n    This symbol could be the same as the previous symbol.\nEach tape head makes a move, which can be either left, right or stationary.\n\nEquivalence of one-tape and multitape TMs {#equivalence-of-one-tape-and-multitape-tms}\n\nSuppose language \\\\(L\\\\) is accepted by a k-tape TM \\\\(M\\\\). We simulate \\\\(M\\\\)\nwith a one-tape TM \\\\(N\\\\) whose tape we think of as having 2k tracks.\nHalf of these tracks hold the tapes of \\\\(M\\\\), and the other half of the\ntracks each hold only a single marker that indicates where the head\nfor the corresponding tape of \\\\(M\\\\) is currently located.\n\n{{}}\n\nTo simulate a move of \\\\(M\\\\), \\\\(N\\\\)'s head must visit the \\\\(k\\\\) head markers.\nSo that \\\\(N\\\\) does not get lost, it must remember how many head markers\nare to its left at all times. That count is stored as a component of\n\\\\(N\\\\)'s finite control. After visiting each head marker and storing the\nscanned symbol in a component of the finite control, \\\\(n\\\\) knows what\ntape symbols have been scanned by each of \\\\(M\\\\)'s heads. \\\\(N\\\\) also knows\nthe state of \\\\(M\\\\), which it stores in \\\\(N\\\\)'s own finite control. Thus,\n\\\\(N\\\\) knows what move \\\\(M\\\\) will make.\n\n\\\\(N\\\\) can now revisit each of the head markers on its tape, changing the\nsymbol in the track representing the corresponding tapes of \\\\(M\\\\), and\nmove the head markers left or right, if necessary. \\\\(N\\\\)'s accepting\nstates are all the states that record \\\\(M\\\\)'s states as one of the\naccepting states of \\\\(M\\\\). When the simulated \\\\(M\\\\) accepts, \\\\(N\\\\) also\naccepts, and \\\\(N\\\\) does not accept otherwise.\n\nThe time taken by the one-tape TM \\\\(N\\\\) to simulate \\\\(n\\\\) moves of the\nk-tape TM \\\\(M\\\\) is \\\\(O(n^2)\\\\).\n\nNon-deterministic Turing Machines {#non-deterministic-turing-machines}\n\nA NTM differs from the deterministic variety by having a transition\n\\\\(\\delta\\\\) such that for each state \\\\(q\\\\) and tape symbol \\\\(X\\\\), \\\\(\\delta(q,X)\\\\) is a\nset of triples \\\\(\\{(q\\1, Y\\1, D\\1), \\dots, (q\\k, Y\\k, D\\k)\\}\\\\).\n\nThe NTM can choose at each step any of the triples to be the next\nmove. We can show that NTM and TM are equivalent. The proof involves\nshowing that for every NTM \\\\(M\\N\\\\), we can construct a DTM \\\\(M\\D\\\\) that\nexplores the ID's that \\\\(M\\_N\\\\) can reach by any sequence of its choices.\nIf \\\\(M\\D\\\\) has an accepting state, then \\\\(M\\D\\\\) enters an accepting state of\nits own. \\\\(M\\_D\\\\) must be systematic, putting new ID's on a queue, rather\nthan a stack, so \\\\(M\\_D\\\\) would have simulated all sequences up to k moves\nof \\\\(M\\_N\\\\) after some finite time.\n\n\\\\(M\\D\\\\) is designed as a multi-tape TM. The first tape of \\\\(M\\D\\\\) holds a\nsequence of ID's of \\\\(M\\N\\\\), including the state of \\\\(M\\N\\\\). One ID of \\\\(M\\_N\\\\)\nis marked as the current ID, whose successor ID's are in the process\nof being discovered. All IDs to the left of the current one have been\nexplored and can be ignored subsequently.\n\n{{}}\n\nTo process the current ID, \\\\(M\\_D\\\\) does:\n\n\\\\(M\\_D\\\\) examines the state and scanned symbol of the current ID. Built\n    into the finite control of \\\\(M\\_D\\\\) is the knowledge of what choices of\n    move \\\\(M\\_N\\\\) has for each state and symbol. If the state in the\n    current ID is accepting, then \\\\(M\\D\\\\) accepts and simulates \\\\(M\\N\\\\) no further.\nHowever, if the state is not accepting, and the state-symbol\n    combination has \\\\(k\\\\) moves, then \\\\(M\\_D\\\\) uses its second tape to copy\n    the ID and the make k copies of that ID at the end of the sequence\n    of ID's on tape 1.\n\\\\(M\\_D\\\\) modifies each of those k ID's according to a different one of\n    the k choices of move that \\\\(M\\_N\\\\) has from its current ID.\n\\\\(M\\_D\\\\) returns to the marked current ID, erases the mark and moves to\n    the next ID to the right. The cycle the repeats with step (1).\n\nThis can be viewed as a breadth-first search on all possible IDs\nreached.\n\nRestricted Turing Machines {#restricted-turing-machines}\n\nTuring Machines with Semi-infinite Tapes {#turing-machines-with-semi-infinite-tapes}\n\nWe can assume the tape to be semi-infinite, having no cells to the\nleft of the initial head position, an still retain the full power of\nTMs.\n\nThe trick behind the construction is to use two tracks on the\nsemi-infinite tape. The upper track represents the cells of the\noriginal TM that are at or to the right of the initial head position,\nbut in reverse order. The upper track represents \\\\(X\\0 , X\\1 \\dots\\\\) where\n\\\\(X\\0\\\\) is the initial position of the head; \\\\(X\\1, X\\_2\\\\) so on are the cells\nto its right. The \\\\(\\*\\\\) on the leftmost cell bottom track serves as an\nend marker and prevents the head of the semi-infinite TM from falling\noff the end of the tape.\n\n{{}}\n\nAnother restriction we make is to never write a blank. This combined\nwith the semi-infinite tape restriction means that the tape is at all\ntimes a prefix of non-blank symbols followed by an infinity of blanks.\n\nWe can construct an equivalent TM \\\\(M\\2\\\\) from a TM \\\\(M\\1\\\\) by restricting\nthat \\\\(M\\_1\\\\) never writes a blank, by creating a new tape symbol \\\\(B'\\\\)\nthat functions as a blank, but is not the blank \\\\(B\\\\).\n\nMultistack Machines {#multistack-machines}\n\nA $k$-stack machine is a deterministic PDA with \\\\(k\\\\) stacks. The\nmultistack machine has a finite control, which is in one of a finite\nstate set of states. A move of the multistack machine is based on:\n\nThe state of the finite control\nThe input symbol read, which is chosen from the finite input\n    alphabet.\n\nEach move allows the multistack machine to:\n\nChange to a new state\nReplace the top symbol of each stack with a string of zero or more\n    stack symbols.\n\nIf a language is accepted by a TM, it is also accepted by a two-stack\nmachine.\n\nCounter Machines {#counter-machines}\n\nCounter machines have the same structure as the multistack machine,\nbut in place of each stack is a counter. Counters holdd any\nnon-negative integer, but we can only distinguish between zero and\nnon-zero counters. That is, the move of the counter machine depends on\nits state, input symbol, and which, if any, of the counters are zero.\n\nEach move can:\n\nChange state\nAdd or subtract 1 from any of its counters independently\n\nA counter machine can be thought of as a restricted multistack\nmachine, where:\n\nThere are only 2 stack symbols, which is the bottom-of-stack marker\n    \\\\(Z\\_0\\\\), and \\\\(X\\\\).\n\\\\(Z\\_0\\\\) is initially on each stack.\nWe may replace \\\\(Z\\0\\\\) only with \\\\(X^i Z\\0\\\\), \\\\(i \\ge 0\\\\).\nWe may replace \\\\(X\\\\) only with \\\\(X^i\\\\), \\\\(i \\ge 0\\\\).\n\nWe observe the following properties:\n\nEvery language accepted by a counter machine is recursively\n    enumerable.\nEvery language accepted by a one-counter machine is a CFL. This is\n    made immediately obvious by considering that it is a one-stack\n    machine (a PDA).\nEvery language accepted by a two-counter machine is RE.\n\nThe proof is done by showing that three counters is enough to simulate\na TM, and that two counters can simulate three counters.\n\nProof outline: Suppose there are \\\\(r-1\\\\) symbols used by the stack\nmachine. We can identify the symbols with the digits \\\\(1\\\\) through\n\\\\(r-1\\\\), and think of the stack as an integer in base \\\\(r\\\\). We use two\ncounters to hold the integers that represent each of the two stacks in\na two-stack machine. The third counter is used to adjust the other two\ncounter, by either dividing or multiplying a count by \\\\(r\\\\), where\n\\\\(r-1\\\\) tape symbols are used by the stack machine.\n\nTuring Machines and Computers {#turing-machines-and-computers}\n\nA computer can simulate a Turing machine, and\nA Turing machine can simulate a computer, and can do so in an\n    amount of time that is at most some polynomial in the number of\n    steps taken by the computer\n\n{{}}\n\nWe can prove the latter by using a TM to simulate the instruction\ncycle of the computer, as follows:\n\nSearch the first tape for an address matching the instruction\n    number on tape 2.\nExamine the instruction address value, if it requires the value of\n    some address, that address is part of the instruction. Mark the\n    position of the instruction using a second tape, not shown in the\n    picture. Search for the memory address on the first tape, an dcopy\n    its value onto tape 3, the tape holding the memory address\nExecute the instruction. Examples of instructions are:\n    Copying values to some other address\n    Adding value to some other address\n    The jump instruction\n\nRuntime of Computers vs Turing Machines {#runtime-of-computers-vs-turing-machines}\n\nRunning time is an important consideration, because we want to know\nwhat can be computed with enough efficiency. We divide between\ntractable and intractable problems, where tractable problems can be\nsolved efficiently.\n\nHence, we need to assure ourselves that a problem can be solved in\npolynomial time on a typical computer, can be solved in polynomial\ntime on a TM, and conversely.\n\nA TM can simulate \\\\(n\\\\) steps of a computer in \\\\(O(n^3)\\\\) time. Consider\nthe multiplication instruction: if a computer were to start with a\nword holding the integer 2, and multiply the word by itself \\\\(n\\\\) times,\nit would hold the number \\\\(2^{2^n}\\\\), and would take \\\\(2^n + 1\\\\) bits to\nrepresent, exponential in \\\\(n\\\\).\n\nTo resolve this issue, one can assert that words retain a maximum\nlength. We take another approach, imposing the restriction that instructions\nmay use words of any length, but can only produce words one bit longer\nthan its arguments.\n\nThe prove the polynomial time relationship, we note that after \\\\(n\\\\)\ninstructions have been executed, the number of words mentioned on the\nmemory tape of the TM is \\\\(O(n)\\\\), and hence requires \\\\(O(n)\\\\) TM cells to\nrepresent. The tape is hence \\\\(O(n^2)\\\\) cells long, and the TM can locate\nthe finite number of words required by one computer instruction in\n\\\\(O(n^2)\\\\) time.\n\nHence, if a computer:\n\nHas only instructions that increase the maximum word length by at\n    most 1, and\nHas only instructions that a multitape TM can perform on wordsd of\n    length \\\\(k\\\\) in \\\\(O(k^2)\\\\) steps or less, then\n\nthe Turing machine can simulate \\\\(n\\\\) steps of the computer in \\\\(O(n^3)\\\\)\nof its own steps.\n",
        "tags": []
    },
    {
        "uri": "/zettels/topic_modelling",
        "title": "Topic Modeling",
        "content": "\ntags\n: §machine\\learning, §data\\viz\n\nLDA survey - Github\n\nLDA {#lda}\n\nThe Little Book on LDA\n\nDirichlet Distribution\n\n    Dirichlet distribution is a family of continuous multivariate\n    probability distributions parameterized by a vector α of positive\n    reals.\n\n    \\begin{equation}\n      \\theta \\sim Dir(\\alpha)\n    \\end{equation}\n\n    \\begin{equation}\n      p(\\theta) = \\frac{1}{\\beta(\\alpha)} \\prod\\{i=1}^n \\theta\\i^{\\alpha\\_i-1} I(\\theta \\in S)\n    \\end{equation}\n\n    Where \\\\(\\theta = (\\theta\\1, \\theta\\2, \\dots, \\theta\\n), \\alpha = (\\alpha\\1, \\alpha\\2, \\dots, \\alpha\\n), \\alpha\\_i > 0\\\\) and\n\n    \\begin{equation}\n      S = \\left\\\\{x \\in \\mathbb{R}^n : x\\i \\ge 0, \\sum\\{i=1}^{n} x\\_i = 1 \\right\\\\}\n    \\end{equation}\n\n    and\n    \\\\(\\frac{1}{\\beta(\\alpha)} =\n    \\frac{\\Gamma(\\alpha\\0)}{\\Gamma(\\alpha\\1)\\Gamma(\\alpha\\2)\\dots\\Gamma(\\alpha\\n)}\\\\)\n\n    The infinite-dimensional generalization of the Dirichlet distribution\n    is the Dirichlet process.\n\n    The Dirichlet distribution is the conjugate prior distribution of the\n    categorical distribution (a generic discrete probability distribution\n    with a given number of possible outcomes) and multinomial distribution\n    (the distribution over observed counts of each possible category in a\n    set of categorically distributed observations). This means that if a\n    data point has either a categorical or multinomial distribution, and\n    the prior distribution of the distribution's parameter (the vector of\n    probabilities that generates the data point) is distributed as a\n    Dirichlet, then the posterior distribution of the parameter is also a\n    Dirichlet.\n\nExploring a Corpus with the posterior distribution\n\n    Quantities needed for exploring a corpus are the posterior\n    expectations of hidden variables. Each of these quantities are\n    conditioned on the observed corpus.\n\n    Visualizing a topic is done by visualizing the posterior topics\n    through their per-topic probabilities \\\\(\\hat{\\beta}\\\\).\n\n    Visualizing a document uses the posterior topic proportions\n    \\\\(\\hat{\\theta}\\_{d,k}\\\\) and the posterior topic assignments\n    \\\\(\\hat{z}\\_{d,k}\\\\).\n\n    Finding similar documents can be done through the _Hellinger\n    distance_:\n\n    \\begin{align\\*}\n      D\\{d,k} = \\sum\\{k=1}^K \\left( \\sqrt{\\hat{\\theta}\\{d,k}} - \\sqrt{\\hat{\\theta}\\{f,k}}\\right)^2\n    \\end{align\\*}\n\nPosterior Inference\n\n    Mean Field Variational Inference\n\n        Approximate intractable posterior distribution with a simpler\n        distribution containing free variational parameters. These parameters\n        are fit to approximate the true posterior.\n\n        In contrast to the true posterior, the mean field variational\n        distribution for LDA is one where the variables are independent of\n        each other, with and each governed by a different variational\n        parameter.\n\n        We fit the variational parameters to minimise the KL-divergence to the\n        true posterior.\n\n        The general approach to mean-field variational methods - update each\n        variational parameter with the parameter given by the expectation of\n        the true posterior under the variational distribution - is applicable\n        when the conditional distribution of each variable is the exponential\n        family.\n\nMarkov Chains\n\nShortcomings\n\n    strong, potentially invalid statistical assumptions:\n        topics have no correlation to one another (dirichlet assumes\n            nearly independent)\n            solution: CTM: use a logistic normal distribution\n        assumes order of documents don't matter\n            solution: DTM: use logistic normal distribution to model topics\n                evolving over time\n\nTopicRNN {#topicrnn}\n\nIn TopicRNN, latent topic models are used to capture global semantic\ndependencies so that the RNN can focus its modeling capacity on the\nlocal dynamics of the sequences\n\nPotential Research Topics {#potential-research-topics}\n\nTODO Visualization of Perplexity for topic models as a potential topic? {#visualization-of-perplexity-for-topic-models-as-a-potential-topic}\n",
        "tags": []
    },
    {
        "uri": "/zettels/transfer_learning",
        "title": "Transfer Learning",
        "content": "\ntags\n: Reinforcement Learning ⭐\n\nPrior understanding of problem structure can help us solve complex\ntasks quickly. Perhaps solving prior tasks would help acquire useful\nknowledge for solving a new task.\n\nTransfer learning is the _use of experience from one set of tasks for\nfaster learning and better performance on a new task_.\n\nFew-Shot Learning {#few-shot-learning}\n\n\"shot\" refers to the number of attempts in the target domain. For\nexample, in 0-shot learning, a policy trained in the source domain\nworks in the target domain.\n\nThis typically requires assumptions of similarity between source and\ntarget domain.\n\nTaxonomy of Transfer Learning {#taxonomy-of-transfer-learning}\n\n\"forward\" transfer\n: train on one task, transfer to a new task\n    Randomizing source domain\n    Fine-tuning\n\nmulti-task transfer\n: train on many tasks, transfer to a new task\n    generate highly randomized source domains\n    model-based RL\n    model distillation\n    contextual policies\n    modular policy networks\n\nmulti-task meta-learning\n: learn to learn from many tasks\n    RNN-based/Gradient-based meta-learning\n\nForward Transfer {#forward-transfer}\n\nFine-tuning {#fine-tuning}\n\nKey Idea: Train on the source task, then train some more on the target\ntask, for example, by retraining the weights on the last layer. Lower\nlayers are likely to learn representations from the source task that\nare useful in the target task. This works well if the source task is\nbroad and diverse.\n\nFine tuning is popular in the supervised learning setting.\n\nWhy fine-tuning doesn't work well for RL\n\n    RL tasks tend to be narrow (not broad and diverse), and features\n        are less general\n    RL methods tend to learn deterministic policies, the policies that\n        are optimal in the fully-observed MDP.\n        Low-entropy policies adapt very slowly to new settings\n        Little exploration at convergence\n\n    To increase diversity and entropy, we can do maximum-entropy learning\n    which acts as randomly as possible while collecting high rewards (Tuomas Haarnoja et al., 2017):\n\n    \\begin{equation}\n      \\pi(a|s) = \\mathrm{exp} (Q\\_\\phi(s,a)-V(s))\n    \\end{equation}\n\n    This optimizes\n\n    \\begin{equation}\n      \\sum\\t E\\{\\pi(s\\t, a\\t)}[r(s\\t, a\\t)] + E\\{\\pi(s\\t)}[\\mathcal{H}(\\pi(a\\t|s\\t))]\n    \\end{equation}\n\nManipulating the Source Domain {#manipulating-the-source-domain}\n\nThis is used where we can design the source domain (e.g. training in a\nsimulator, which can be tweaked). Injecting randomness/diversity in\nthe source tends to be helpful.\n\nResources:\n\nDomain Randomization for Sim2Real Transfer\n\nMulti-task Transfer {#multi-task-transfer}\n\nSome things remain constant across tasks, such as the laws of physics.\nModel-based RL may learn these laws, and transfer this knowledge\nacross multiple tasks.\n\nIn model distillation, multiple policies are combined into one, for\nconcurrent multi-task learning. Policy distillation\n(Rusu et al., 2015) is able to:\n\nCompress policies learnt on single games into smaller models\nBuild agents capable of playing multiple games\nImprove the stability of the DQN learning algorithm by distilling\n    online the policy of the best performing agent\n\nBibliography\nHaarnoja, T., Tang, H., Abbeel, P., & Levine, S., Reinforcement learning with deep energy-based policies, In D. Precup, & Y. W. Teh, Proceedings of the 34th International Conference on Machine Learning (pp. 1352–1361) (2017). International Convention Centre, Sydney, Australia: PMLR. ↩\n\nRusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., Mnih, V., …, Policy Distillation, CoRR, (),  (2015).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/travel",
        "title": "Travel",
        "content": "\nUseful Links {#useful-links}\n\nThe Man in Seat Sixty-One - the train travel guide...\n",
        "tags": []
    },
    {
        "uri": "/zettels/trigger_list",
        "title": "Trigger List",
        "content": "\nTrigger List {#trigger-list}\n\nUse this trigger list to recall items that are need require action:\n\nProjects started, not completed\nProjects that need to be started\nCommitments/promises to others\n    Teachers/TAs\n    Roommates\n    Classmates\n    Friends\n    \"Outside\" people\n        Parents\n        Campus organizations\n        Extracurricular organizations\n        Graduates\nCommunications to make/get\n    Internal/External\n        Initiate or respond to:\n            Phone calls\n            Text messages\n            Voice-mail\n            E-mail\n            Snail mail\n            Social Media\n    Other writing to finish/submit\n        Reports\n        Papers\n        Reviews\n        Assignments\n        Articles\n        Readings\n        Rewrites and edits\nMeetings that need to be set/requested\nWaiting for...\n    Information\n    Delegated tasks/projects\n    Completions critical to projects\nReplies to:\n    Texts\n    Calls\n    Decisions of others\n",
        "tags": []
    },
    {
        "uri": "/zettels/two_levels_of_inference",
        "title": "Two Levels Of Inference",
        "content": "\nThere are 2 levels of inference: model fitting and model comparison.\nIn model fitting, assuming a model is true (say \\\\(\\mathcal{H}\\_i\\\\)), fit\nthe model to the data by inferring what values its free parameters\nshould possibly take.\n\n\\begin{equation}\n  P\\left(\\mathbf{w} | D, \\mathcal{H}\\{i}\\right)=\\frac{P\\left(D | \\mathbf{w}, \\mathcal{H}\\{i}\\right) P\\left(\\mathbf{w} | \\mathcal{H}\\{i}\\right)}{P\\left(D | \\mathcal{H}\\{i}\\right)}\n\\end{equation}\n\nThe normalizing constant is irrelevant to the first level of\ninference. It is common to use gradient-based methods to find the\nmaximum of the posterior \\\\(\\mathbf{w}\\_{\\text{MP}}\\\\). Error bars for\nthese parameters can be obtained by evaluating the Hessian at\n\\\\(\\mathbf{w}\\_{\\text{MP}}\\\\), \\\\(\\mathbf{A}=-\\nabla \\nabla \\ln\nP\\left(\\mathbf{w} | D, \\mathcal{H}\\_{i}\\right)\\\\), and Taylor-expanding\nthe log posterior probability with \\\\(\\Delta\n\\mathbf{w}=\\mathbf{w}-\\mathbf{w}\\_{\\mathrm{MP}}\\\\):\n\n\\begin{equation}\nP\\left(\\mathbf{w} | D, \\mathcal{H}\\{i}\\right) \\simeq P\\left(\\mathbf{w}\\{\\mathrm{MP}} | D, \\mathcal{H}\\_{i}\\right) \\exp \\left(-1 / 2 \\Delta \\mathbf{w}^{\\mathrm{T}} \\mathbf{A} \\Delta \\mathbf{w}\\right)\n\\end{equation}\n\nlocally approximating the posterior as a Gaussian with covariance\nmatrix \\\\(\\mathbf{A}^{-1}\\\\).\n\nIn model comparison, we compare models in light of the data, assign\nsome sort of preference.\n\nBayesian methods can consistently and quantitatively solve both types\nof inferences, although adopting the Bayesian method for the first\ntype leads to similar results from orthodox statistical methods.\nOrthodox statistical methods will find it difficult to perform model\ncomparisons, because _it is not possible simply to choose the model\nthat fits the data itself_. For example, maximum likelihood can fail\nby choosing implausible, over-parameterized models that overfit the\ndata.\n\nHow do Bayesian methods perform model comparison? The posterior\nprobability for each model is:\n\n\\begin{equation}\n  P\\left(\\mathcal{H}\\{i} | D\\right) \\propto P\\left(D | \\mathcal{H}\\{i}\\right) P\\left(\\mathcal{H}\\_{i}\\right)\n\\end{equation}\n\nHence, if we assign equal priors to the alternative models, models\n\\\\(\\mathcal{H}\\i\\\\) are ranked by evaluating the evidence_. If the\nposterior is well approximated by a Gaussian, Bayesian model\ncomparison is a simple extension of maximum likelihood model\nselection: the evidence is obtained by multiplying the best-fit\nlikelihood by the Occam factor, obtained from the determinant of the\ncovariance matrix \\\\(\\mathbf{A}^{-1}\\\\) (the inverse Hessian).\n\nRelated {#related}\n\n§occams\\_razor\n",
        "tags": []
    },
    {
        "uri": "/zettels/uncertainty_in_robotics",
        "title": "Uncertainty in Robotics",
        "content": "\nRobotic applications increasingly deal with more unstructured\nenvironments. Robots that can perceive and deal with uncertainty are\nmuch more robust in these scenarios.\n\nUncertainty arises from:\n\nEnvironment\n: unpredictability of the physical world\n\nSensors\n: limitations in sensor perception\n\nRobots\n: robot actuations involve motors that have control noise\n\nModels\n: models of the world inherently inaccurate\n\nComputation\n: many algorithms are approximate\n\n> A robot that carries a notion of its own uncertainty that acts\n> accordingly is superior to one that does not.\n\n(Thrun {\\it et al.}, 2005)\n\nPros {#pros}\n\nRobust, and scale better to complex, unstructured environments\nWeaker requirements on the accuracy of the models compared to\n    classical planning algorithms\nSound methodology for many flavours of robot learning\nBroadly applicable to many problems, including perception and\n    action\n\nCons {#cons}\n\nRelatively computationally inefficient\nRequires approximation (exact posteriors are computationally intractable)\n\nBibliography\nThrun, S., Burgard, W., & Fox, D., Probabilistic robotics (2005), : MIT press. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/unix_awk",
        "title": "Awk",
        "content": "\ntags\n: Unix\n\nLinks {#links}\n",
        "tags": []
    },
    {
        "uri": "/zettels/unix_find",
        "title": "Find (CLI Tool)",
        "content": "\ntags\n: Unix\n\nLinks {#links}\n\nFind\n",
        "tags": []
    },
    {
        "uri": "/zettels/unix_lsof",
        "title": "Lsof",
        "content": "\ntags\n: Unix\n\nLinks {#links}\n\nlsof - Cindy Sridharan - Medium\n",
        "tags": []
    },
    {
        "uri": "/zettels/unix",
        "title": "Unix",
        "content": "\ntags\n: Linux\n",
        "tags": []
    },
    {
        "uri": "/zettels/vae",
        "title": "Variational Autoencoders",
        "content": "\nLinks {#links}\n\nmatthewvowels1/Awesome-VAEs\n",
        "tags": []
    },
    {
        "uri": "/zettels/variational_inference",
        "title": "Variational Inference",
        "content": "\ntags\n: §machine\\learning\\algorithms, §pgm\n\nTODO Variational Bounds of Mutual Information (Poole et al., 2019) {#variational-bounds-of-mutual-information}\n\nTODO Black Box Variational Inference (Ranganath et al., 2013) {#black-box-variational-inference}\n\nTODO Unbiased Implicit Variational Inference (Titsias \\& Ruiz, 2018) {#unbiased-implicit-variational-inference}\n\nTODO A Contrastive Divergence for Combining Variational Inference and MCMC (Ruiz \\& Titsias, 2019) {#a-contrastive-divergence-for-combining-variational-inference-and-mcmc}\n\n{#}\n\nBibliography\nPoole, B., Ozair, S., Oord, A. v. d., Alemi, A. A., & Tucker, G., On variational bounds of mutual information, CoRR, (),  (2019).  ↩\n\nRanganath, R., Gerrish, S., & Blei, D. M., Black Box Variational Inference, CoRR, (),  (2013).  ↩\n\nTitsias, M. K., & Ruiz, F. J. R., Unbiased Implicit Variational Inference, CoRR, (),  (2018).  ↩\n\nRuiz, F. J. R., & Titsias, M. K., A contrastive divergence for combining variational inference and mcmc, CoRR, (),  (2019).  ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/vc_dimension",
        "title": "VC-Dimension",
        "content": "\ntags\n: Bias-Complexity Tradeoff, PAC Learning\n\n(Shalev-Shwartz \\& Ben-David, 2014)\n\nWhat makes one class learnable and another unlearnable? The family of\nlearnable classes in the setup of binary valued classification with\nthe zero-one loss relies on a combinatorial notion called the\nVapnik-Chervonenkis dimension (VC-dimension).\n\nInfinite-size classes can be learnable {#infinite-size-classes-can-be-learnable}\n\nTo see that this is true, we provide a counterexample.\n\nlet \\\\(\\mathcal{H}\\\\) be the set of threshold functions over the real\nline, namely, \\\\(\\mathcal{H} = \\left\\\\{h\\_a : a \\in \\mathbb{R}\\right\\\\}\\\\),\nwhere \\\\(h\\_a : \\mathbb{R} \\rightarrow \\left\\\\{ 0,1\\rightarrow \\right\\\\}\\\\) is a\nfunction such that \\\\(h\\a(x) = \\mathbb{I}\\{[x\n\nLet \\\\(\\mathcal{H}\\\\) be a class of functions from \\\\(\\mathcal{X}\\\\) to\n\\\\(\\left\\\\{0,1\\right\\\\}\\\\), and let \\\\(C = \\\\{c\\1, \\dots, c\\m\\\\} \\subset X\\\\). The\nrestriction of \\\\(\\mathcal{H}\\\\) to \\\\(C\\\\) is the set of functions from \\\\(C\\\\)\nto \\\\(\\\\{0, 1\\\\}\\\\) that can be derived from \\\\(\\mathcal{H}\\\\). That is:\n\n\\begin{equation}\n  \\mathcal{H}\\C = \\left\\\\{ h(c\\1), \\dots, h(c\\_m) : h \\in \\mathcal{H} \\right\\\\}\n\\end{equation}\n\nwhere we represent each function from \\\\(C\\\\) to \\\\(\\\\{0, 1\\\\}\\\\) as a vector in\n\\\\(\\\\{0,1\\\\}^{|C|}\\\\).\n\nA hypothesis class \\\\(\\mathcal{H}\\\\) shatters a finite set \\\\(C \\subset\n\\mathcal{X}\\\\) if the restriction of \\\\(\\mathcal{H}\\\\) to \\\\(C\\\\) is the set of\nall functions from \\\\(C\\\\) to \\\\(\\\\{0, 1\\\\}\\\\). That is, \\\\(|\\mathcal{H}\\_C| =\n2^{|C|}\\\\).\n\nWhenever some set \\\\(C\\\\) is shattered by \\\\(\\mathcal{H}\\\\), the adversary is\nnot restricted by \\\\(\\mathcal{H}\\\\), as they can construct a distribution\nover \\\\(C\\\\) based on any target function from \\\\(C\\\\) to \\\\(\\\\{0,1\\\\}\\\\), while\nstill maintaining the realizability assumption.\n\nThis leads us to the definition of VC-dimension:\n\nThe VC-dimension of a hypothesis class \\\\(\\mathcal{H}\\\\), denoted\n\\\\(\\textrm{VCdim}(\\mathcal{H})\\\\), is the maximal size of a set \\\\(C \\subset\n\\mathcal{X}\\\\) that can be shattered by \\\\(\\mathcal{H}\\\\). If \\\\(\\mathcal{H}\\\\)\ncan shatter \\\\(C\\\\) of any arbitrary size, then \\\\(\\mathcal{H}\\\\) has infinite VC-dimension.\n\nExamples {#examples}\n\nThreshold Functions {#threshold-functions}\n\nLet \\\\(\\mathcal{H}\\\\) be the class of threshold functions over\n\\\\(\\mathbb{R}\\\\). We have shown that for an arbitrary set \\\\(C = \\\\{c\\_1\\\\}\\\\),\n\\\\(\\mathcal{H}\\\\) shatters \\\\(C\\\\). However, we have shown that for an\narbitrary set \\\\(C = \\\\{c\\1, c\\2\\\\}\\\\) where \\\\(c\\1 \\le c\\2\\\\), \\\\(\\mathcal{H}\\\\)\ndoes not shatter \\\\(C\\\\). Hence \\\\(\\textrm{VCdim}(\\mathcal{H}) = 1\\\\).\n\nIntervals {#intervals}\n\nTake \\\\(C = {1, 2}\\\\), and we see that \\\\(\\mathcal{H}\\\\) shatters \\\\(C\\\\). Hence\n\\\\(\\textrm{VCdim}(\\mathcal{H}) \\ge 2\\\\). However, take an arbitrary set \\\\(C\n= \\\\{c\\1, c\\2, c\\3\\\\}\\\\) where \\\\(c\\1 \\le c\\2 \\le c\\3\\\\). Then the labelling\n(1,0,1) cannot be obtained by an interval. Therefore,\n\\\\(\\textrm{VCdim}(\\mathcal{H}) = 2\\\\).\n\nThe Fundamental Theorem of Statistical Learning {#the-fundamental-theorem-of-statistical-learning}\n\nLet \\\\(\\mathcal{H}\\\\) be a hypothesis class of functions from a domain\n\\\\(\\mathcal{X}\\\\) to \\\\(\\\\{0, 1\\\\}\\\\) and let the loss function be the 0-1 loss.\nThen the following are equivalent:\n\n\\\\(\\mathcal{H}\\\\) has the uniform convergence property.\nAny ERM rule is a successful agnostic PAC learner for \\\\(\\mathcal{H}\\\\).\n\\\\(\\mathcal{H}\\\\) is agnostic PAC learnable.\n\\\\(\\mathcal{H}\\\\) is PAC learnable.\nAny ERM rule is a successful PAC learner for \\\\(\\mathcal{H}\\\\).\n\\\\(\\mathcal{H}\\\\) has a finite VC-dimension.\n\nSauer's Lemma and the Growth Function {#sauer-s-lemma-and-the-growth-function}\n\nWe have defined the notion of shattering, by considering the\nrestriction of \\\\(\\mathcal{H}\\\\) to a finite set of instances. The growth\nfunction measures the maximal \"effective\" size of \\\\(\\mathcal{H}\\\\) on a\nset of \\\\(m\\\\) examples. Formally:\n\nLet \\\\(\\mathcal{H}\\\\) be a hypothesis class. Then the growth function of\n\\\\(\\mathcal{H}\\\\), denoted \\\\(\\tau\\_{\\mathcal{H}}(m) : \\mathbb{N} \\rightarrow\n\\mathbb{N}\\\\), is defined as:\n\n\\begin{equation}\n  \\tau\\{\\mathcal{H}}(m) = \\textrm{max}\\{C \\subset \\mathcal{X} : |C| =\n    m} |\\mathcal{H}\\_C|\n\\end{equation}\n\n\\\\(\\tau\\_{\\mathcal{H}}(m)\\\\) is the number of different functions from a\nset \\\\(C\\\\) of size \\\\(m\\\\) to \\\\(\\\\{0,1\\\\}\\\\) that can be obtained by restricting\n\\\\(\\mathcal{H}\\\\) to \\\\(C\\\\). With this definition we can now state Sauer's\nlemma:\n\nLet \\\\(\\mathcal{H}\\\\) be a hypothesis class with\n\\\\(\\textrm{VCdim}(\\mathcal{H}) \\le d  d + 1\\\\), then \\\\(\\tau\\_{\\mathcal{H}}(m) \\le (em/d)^d\\\\).\n\nBibliography\nShalev-Shwartz, S., & Ben-David, S., Understanding machine learning: from theory to algorithms (2014), : Cambridge university press. ↩\n",
        "tags": []
    },
    {
        "uri": "/zettels/velocity_motion_model",
        "title": "Velocity Motion Model",
        "content": "\ntags\n: Odometry Motion Model, Motion Model With Maps\n\nThe velocity motion model assumes that the robot is controlled through\n2 velocities: rotational and translational velocity:\n\n\\begin{equation}\n  u\\{t}=\\left(\\begin{array}{l}{v\\{t}} \\\\ {\\omega\\_{t}}\\end{array}\\right)\n\\end{equation}\n\nA sampling algorithm generates a random pose according to the\ndistribution \\\\(p(x\\t| u\\t, x\\_{t-1})\\\\), and perturbs it with the noise,\ndrawn from the error parameters of the kinematic motion model.\n\n\\begin{equation}\n  \\left(\\begin{array}{c}{\\hat{v}} \\\\ {\\hat{\\omega}}\\end{array}\\right)=\\left(\\begin{array}{c}{v} \\\\ {\\omega}\\end{array}\\right)+\\left(\\begin{array}{c}{\\varepsilon\\{\\alpha\\{1}|v|+\\alpha\\{2}|\\omega|}} \\\\ {\\varepsilon\\{\\alpha\\{3}|v|+\\alpha\\{4}|\\omega|}}\\end{array}\\right)\n\\end{equation}\n",
        "tags": []
    },
    {
        "uri": "/zettels/web_dev_tools",
        "title": "Web Dev Tools",
        "content": "\ntags\n: §web\\_dev\n\nSVG {#svg}\n\nSVGOMG - SVGO's Missing GUI\n\nUptime Monitoring {#uptime-monitoring}\n\nUptime Robot\n",
        "tags": []
    },
    {
        "uri": "/zettels/web_dev",
        "title": "Web Development",
        "content": "",
        "tags": []
    },
    {
        "uri": "/zettels/weekly_review",
        "title": "Weekly Review",
        "content": "\nGet Clear [0/3] {#get-clear}\n\n[ ] Collect Loose Papers and Materials: Pull out all miscellaneous\n    pieces of paper, business cards, receipts, and so on that have crept\n    into the crevices of your desk, clothing, and accessories. Put it\n    all in your in-tray for processing.\n[ ] Get “IN” to Zero: Review any meeting notes and miscellaneous\n    scribbles on notepaper or in your mobile devices. Decide and list\n    any action items, projects, waiting-fors, calendar events, and\n    someday/maybes, as appropriate. File any reference notes and\n    materials. Get the “in” areas of e-mails, texts, and voice mails to\n    zero. Be ruthless with yourself, processing all notes and thoughts\n    relative to interactions, projects, new initiatives, and input that\n    have come your way since your last download, and purging those not\n    needed.\n    [ ] Papers\n    [ ] Physical Notebook\n    [ ] Physical Objects\n    [ ] Text Messages\n    [ ] Email Inbox\n\n[ ] Empty Your Head: Put into writing or text (in appropriate\n    categories) any new projects, action items, waiting-fors,\n    someday/maybes, and so forth that you haven’t yet captured and\n    clarified.\n\nGet Current [0/7] {#get-current}\n\n[ ] Review Action Lists: Mark off completed actions. Review for\n    reminders of further action steps to record. Many times I’ve been\n    moving so fast I haven’t had a chance to mark off many completed\n    items on my list, much less figure out what to do next. This is the\n    time to do that.\n[ ] Archive completed or inactive projects, briefly reviewing each\n    note and moving it to other notebooks if it could be useful there.\n ] Review [Previous Calendar Data: Review the past two to three\n    weeks of calendar entries in detail for remaining or emergent action\n    items, reference information, and so on, and transfer that data into\n    the active system. Grab every “Oh! That reminds me . . . !” with its\n    associated actions. You will likely notice meetings and events that\n    you attended, which trigger thoughts of what to do next about the\n    content. Be able to archive your past calendar with nothing left\n    uncaptured.\n[ ] Review Upcoming Calendar: Look at further calendar entries\n    (long-and short-term). Capture actions about projects and\n    preparations required for upcoming events. Your calendar is one of\n    the best checklists to review regularly, to prevent last-minute\n    stress and trigger creative front-end thinking.\\* Upcoming travel,\n    conferences, meetings, holidays, etc. should be assessed for\n    projects to add to your “Projects” and “Next Actions” lists for any\n    of those situations that are already on your radar but not yet on\n    cruise control.\n[ ] Review Waiting For List: Any needed follow-up? Need to send an\n    e-mail to get a status on it? Need to add an item to someone’s\n    Agenda list to update when you’ll talk with him or her? Record any\n    next actions. Check off any already received.\n[ ] Review Project (and Larger Outcome) Lists: Evaluate the status\n    of projects, goals, and outcomes, one by one, ensuring that at least\n    one current kick-start action for each is in your system. Browse\n    through any active and relevant project plans, support materials,\n    and any other work-in-progress material to trigger new actions,\n    completions, waiting-fors, etc.\n[ ] Review Stuck Projects: Stuck projects are project that have no\n    next actions. For the definitions of what a project is and how to\n    check if it stuck, customize the variable ‘org-stuck-projects’.\n[ ] Review Any Relevant Checklists: Is there anything else that you\n    haven’t done, that you need or want to do, given your various\n    engagements, interests, and responsibilities?a\n\nGet Creative [0/2] {#get-creative}\n\n[ ] Review Someday Maybe List: Check for any projects that may have\n    become more interesting or valuable to activate, and transfer them\n    to Projects. Delete any that have simply stayed around much longer\n    than they should, as the world and your interest have changed enough\n    to take them off even this informal radar. Add any emerging\n    possibilities that you’ve just started thinking about.\n[ ] Be Creative and Courageous: Are there any new, wonderful,\n    harebrained, creative, thought-provoking, risk-taking ideas you can\n    capture and add into your system, or “external brain”?\n\nJournal {#journal}\n\nHow are you doing right now? {#how-are-you-doing-right-now}\n\nWhat went well this week? {#what-went-well-this-week}\n\nWhat didn't go so well this week? {#what-didn-t-go-so-well-this-week}\n\nDid I learn anything important this week? {#did-i-learn-anything-important-this-week}\n\nIs there anything I can tweak, simplify, or eliminate? {#is-there-anything-i-can-tweak-simplify-or-eliminate}\n\nWhat is currently holding me back? {#what-is-currently-holding-me-back}\n\nWhat urgent questions do I have? {#what-urgent-questions-do-i-have}\n\nWhat do I feel grateful for in my life and work/school? {#what-do-i-feel-grateful-for-in-my-life-and-work-school}\n",
        "tags": []
    },
    {
        "uri": "/zettels/wisdom",
        "title": "Wisdom",
        "content": "\nJoe Rogan Experience #1309 - Naval Ravikant {#joe-rogan-experience-1309-naval-ravikant}\n\nThe greatest artists are able to start over. If you're not doing\n    that, you're just getting older. When it's a dead-end, it's the\n    right thing to do, even when you feel like you're almost there.\nWe tend to define people by narrow identities, even though people\n    are broader than that.\nBooks read is a vanity metric. Read whatever satisfies your general\n    curiousity, and for understanding. Look for ideas, things you don't\n    understand, and research it.\nWe're always signalling, looking at what other people see of us,\n    rather than looking at what we are. Social media is making\n    celebrities of all of us, and celebrities are the among the most\n    miserable people in the world.\nSocial contracts are powerful. When you tell people what you're\n    going to do, you're generally serious about it.\nThe retirement that people want is not being old and sitting in a\n    nursing home.  Retirement is when you stop sacrificing today for\n    something imaginary tomorrow.\nWhen you're unhappy, look at the underlying desire that in\n    unfulfilled. It's okay to have desires, but not too many. Pick the\n    one overwhelming desire.\nTo achieve peak performance, you must need to learn to optimize\n    both the body and the mind. Keeping calm involves letting go of\n    desires.\nIntellectual athletes need to work and rest. Cranking out work\n    everyday is not for humans, it's for machines.\nYou're not going to get rich renting out your time. You need to\n    own a piece of business to gain your financial freedom.\nIn the hunter-gatherer age, we worked for ourselves. The\n    industrial age created a model of thousands of people working on\n    one thing. If somebody can tell you when you have to go to work,\n    what you have to wear, you're not truly free.\nCoase Theorem (??)\n\n\"How To Be Successful (At Your Career, Twitter Edition)\" {#how-to-be-successful--at-your-career-twitter-edition}\n",
        "tags": []
    },
    {
        "uri": "/zettels/writing_articles",
        "title": "Writing Articles",
        "content": "\ntags\n: §writing\n\nRule of Three: have 3 main topic points with no more than 3 subtopic\n    points under them.\n\nReferences {#references}\n\nA blogging style guide | Robert Heaton\n",
        "tags": []
    },
    {
        "uri": "/zettels/writing_books",
        "title": "Writing Books",
        "content": "\ntags\n: §writing\n\nScott Meyers: Advice to Prospective Book Authors\n\nI wrote a book - DEV Community 👩‍💻👨‍💻\n\nWriting a Technical Book – zwischenzugs\n",
        "tags": []
    },
    {
        "uri": "/zettels/writing",
        "title": "Writing",
        "content": "\nBooks {#books}\n\nStyle: Toward Clarity and Grace\nOn Writing Well: The Classic Guide to Writing Nonfiction\n",
        "tags": []
    },
    {
        "uri": "/zettels/xgboost",
        "title": "XGBoost",
        "content": "\ntags\n: §machine\\learning\\algorithms\n\nXGboost is an end-to-end boosting system. It is sparsity-aware. (Chen \\& Guestrin, 2016)\n\nRegularized Learning Objective {#regularized-learning-objective}\n\nFor a given data set of \\\\(n\\\\) examples and \\\\(m\\\\) features, a tree ensemble\nmodel uses \\\\(K\\\\) additive functions to predict the output:\n\n\\begin{equation}\n  \\hat{y}\\i = \\phi(\\mathbf{x\\i}) = \\sum\\{k=1}^{K} f\\k(\\mathbf{x\\_i}),\n  f\\_k \\in F\n\\end{equation}\n\nwhere \\\\(F\\\\) is the space of regression trees, Each regression tree\ncontains a continuous score on each of the leaf.\n\n{{}}\n\nsource:\n\nWe wish to minimise the regularized objective:\n\n\\begin{equation}\n  L(\\phi) = \\sum\\i l(\\hat{y}\\i, y\\i) + \\sum\\{k} \\Omega(f\\_k)\n\\end{equation}\n\nwhere\n\n\\begin{equation}\n  \\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\lVert w \\rVert^2\n\\end{equation}\n\n\\\\(T\\\\) being the number of leaves in the tree, and \\\\(w\\_i\\\\) being the ith\nweight of the leaf.\n\nGradient Tree Boosting {#gradient-tree-boosting}\n\nSince the regularized objective contains functions, the objective\ncannot be optimized in the Euclidean space.\n\nLet \\\\(\\hat{y}\\_i^t\\\\) be the prediction of the ith instance on the $t$-th\niteration. We need to add \\\\(f\\_t\\\\) to minimize the following objective:\n\n\\begin{equation}\n  L^{(t)} = \\sum\\{i=1}^{n} l(y\\i, \\hat{y}\\_i^{(t-1)} +\n  f\\t(\\mathbf{x}\\i)) + \\Omega(f\\_t)\n\\end{equation}\n\nWe can perform a Taylor expansion, and obtain:\n\n\\begin{equation}\n  L^{(t)} = \\sum\\{i=1}^{n} l(y\\i, \\hat{y}\\i^{(t-1)}) + g\\i\n  f\\t(\\mathbf{x}\\i) + \\frac{1}{2}h\\i f\\t^2(\\mathbf{x}\\i) + \\Omega(f\\t)\n\\end{equation}\n\nwhere \\\\(g\\i = \\partial\\{\\hat{y}^{(t-1)}} l(y\\_i, \\hat{y}^{(t-1)})\\\\), and\n\\\\(h\\i = \\partial^2\\{\\hat{y}^{(t-1)}} l(y\\_i, \\hat{y}^{(t-1)})\\\\) are the\nfirst and second order gradient statistics (gradients and Hessians).\nRemoving the constant terms, at step \\\\(t\\\\), the objective is simplified to:\n\n\\begin{equation}\n  \\tilde{L}^{(t)} = \\sum\\{i=1}^{n} g\\i f\\t(\\mathbf{x}\\i) + \\frac{1}{2}h\\_i\n  f\\t^2(\\mathbf{x}\\i) + \\Omega(f\\_t)\n\\end{equation}\n\nIf we define \\\\(I\\j = \\\\{ i | q (x\\i) = j\\\\}\\\\) as the instance set of leaf\n\\\\(j\\\\), then we can rewrite:\n\n\\begin{equation}\n  \\tilde{L}^{(t)} = \\sum\\{j=1}^T [(\\sum\\{i \\in I\\j} g\\i) w\\_j +\n  \\frac{1}{2} (\\sum\\{i \\in I\\j} h\\i + \\lambda) w\\j^2] + \\gamma T\n\\end{equation}\n\nand we find that for a fixed structure \\\\(q(x)\\\\), we can compute the\noptimal weight \\\\(w\\_j^\\*\\\\) of leaf \\\\(j\\\\):\n\n\\begin{equation}\n  w\\j^{\\*} = - \\frac{\\sum\\{i \\in I\\j} g\\i}{\\sum\\{i \\in I\\j} h\\_i  + \\lambda}\n\\end{equation}\n\nThis can then be used to score a tree structure \\\\(q\\\\). However, since\nenumerating all possible trees is computationally expensive, we use\nan algorithm to iteratively add branches to the tree, by computing the\nloss reduction from splitting:\n\n\\begin{equation}\n  L\\{\\text{split}} = \\frac{1}{2} \\left[ \\frac{\\left( \\sum\\{i \\in I\\_L}\n        g\\i \\right)^2}{\\sum\\{i \\in I\\L} h\\i + \\lambda} + \\frac{\\left( \\sum\\{i \\in I\\R}\n        g\\i \\right)^2}{\\sum\\{i \\in I\\R} h\\i + \\lambda} - \\frac{\\left( \\sum\\_{i \\in I}\n        g\\i \\right)^2}{\\sum\\{i \\in I} h\\_i + \\lambda} \\right]\n\\end{equation}\n\nImplementing Distributed XGBoost {#implementing-distributed-xgboost}\n\nThe implementation of distributed XGBoost uses RABIT, and the\nAllreduce framework. XGBoost requires gradients and hessians from each\ndistributed worker. This fit the allreduce framework, which broadcasts\neach worker's reduce result across all processes. (Chen et al., )\n\nBibliography\nChen, T., & Guestrin, C., Xgboost: a scalable tree boosting system, CoRR, (),  (2016).  ↩\n\nChen, T., Cano, I., & Zhou, T., Rabit: a reliable allreduce and broadcast interface, Transfer, 3(2),  ().  ↩\n",
        "tags": [
            "machine-learning"
        ]
    },
    {
        "uri": "/zettels/zeigarnik_effect",
        "title": "Zeigarnik Effect",
        "content": "\nSimply put, the Zeigarnik effect is the observation that _interrupted\ntasks are better remembered than completed ones_. This has\nramifications for productivity.\n\nAn example of applying the Zeigarnik Effect would be to stop coding\nhalfway while having a good streak, and come back another day.\n",
        "tags": []
    },
    {
        "uri": "/zettels/zettelkasten",
        "title": "Zettelkasten",
        "content": "\ntags\n: Productivity, Note-taking\n\nThe original Zettelkasten Method {#the-original-zettelkasten-method}\n\nIn the morning, Luhmann would take notes from his readings, penning\nthem down onto a piece of paper with a source inside.\n\nIn the evening, he would go through these notes and expand on them,\nwriting about how they impact his current studies.\n\nIndexes in Zettelkasten {#indexes-in-zettelkasten}\n\nIndexes have very few terms. They don't serve as a broad overview of a\ntopic, but more of an entry point.\n\nZettelkasten Note Types {#zettelkasten-note-types}\n\nfleeting\n: notes taken as a reminder of what's in your head. These\n    are to be removed after some time\n    In Roam Research and Org-Roam, make fleeting notes in daily notes\n\nproject-related\n: notes related to a project\n\npermanent\n: notes that are standalone, and make sense in their own context.\n\nWhy Zettelkasten {#why-zettelkasten}\n\nForces you to adopt a linking system: connect ideas, tangents\nBreak ideas up into smaller pieces, increase surface area for\n    expansion\n\nThe Zettelkasten Method - LessWrong 2.0\n",
        "tags": []
    }
]