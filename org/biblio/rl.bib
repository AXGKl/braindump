@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@article{mnih16_async_method_deep_reinf_learn,
  author =       {Mnih, Volodymyr and Badia, Adri{\`a}
                  Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex
                  and Lillicrap, Timothy P. and Harley, Tim and
                  Silver, David and Kavukcuoglu, Koray},
  title =        {Asynchronous Methods for Deep Reinforcement
                  Learning},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1602.01783v2},
  abstract =     {We propose a conceptually simple and lightweight
                  framework for deep reinforcement learning that uses
                  asynchronous gradient descent for optimization of
                  deep neural network controllers. We present
                  asynchronous variants of four standard reinforcement
                  learning algorithms and show that parallel
                  actor-learners have a stabilizing effect on training
                  allowing all four methods to successfully train
                  neural network controllers. The best performing
                  method, an asynchronous variant of actor-critic,
                  surpasses the current state-of-the-art on the Atari
                  domain while training for half the time on a single
                  multi-core CPU instead of a GPU. Furthermore, we
                  show that asynchronous actor-critic succeeds on a
                  wide variety of continuous motor control problems as
                  well as on a new task of navigating random 3D mazes
                  using a visual input.},
  archivePrefix ={arXiv},
  eprint =       {1602.01783},
  primaryClass = {cs.LG},
}

@article{gu16_q_prop,
  author =       {Gu, Shixiang and Lillicrap, Timothy and Ghahramani,
                  Zoubin and Turner, Richard E. and Levine, Sergey},
  title =        {Q-Prop: Sample-Efficient Policy Gradient With an
                  Off-Policy Critic},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1611.02247v3},
  abstract =     {Model-free deep reinforcement learning (RL) methods
                  have been successful in a wide variety of simulated
                  domains. However, a major obstacle facing deep RL in
                  the real world is their high sample complexity.
                  Batch policy gradient methods offer stable learning,
                  but at the cost of high variance, which often
                  requires large batches. TD-style methods, such as
                  off-policy actor-critic and Q-learning, are more
                  sample-efficient but biased, and often require
                  costly hyperparameter sweeps to stabilize. In this
                  work, we aim to develop methods that combine the
                  stability of policy gradients with the efficiency of
                  off-policy RL. We present Q-Prop, a policy gradient
                  method that uses a Taylor expansion of the
                  off-policy critic as a control variate. Q-Prop is
                  both sample efficient and stable, and effectively
                  combines the benefits of on-policy and off-policy
                  methods. We analyze the connection between Q-Prop
                  and existing model-free algorithms, and use control
                  variate theory to derive two variants of Q-Prop with
                  conservative and aggressive adaptation. We show that
                  conservative Q-Prop provides substantial gains in
                  sample efficiency over trust region policy
                  optimization (TRPO) with generalized advantage
                  estimation (GAE), and improves stability over deep
                  deterministic policy gradient (DDPG), the
                  state-of-the-art on-policy and off-policy methods,
                  on OpenAI Gym's MuJoCo continuous control
                  environments.},
  archivePrefix ={arXiv},
  eprint =       {1611.02247},
  primaryClass = {cs.LG},
}

@Article{Mnih_2015,
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver,
                  David and Rusu, Andrei A. and Veness, Joel and
                  Bellemare, Marc G. and Graves, Alex and Riedmiller,
                  Martin and Fidjeland, Andreas K. and Ostrovski,
                  Georg and et al.},
  title        = {Human-level control through deep reinforcement
                  learning},
  year         = 2015,
  volume       = 518,
  number       = 7540,
  month        = {Feb},
  pages        = {529â€“533},
  issn         = {1476-4687},
  doi          = {10.1038/nature14236},
  url          = {http://dx.doi.org/10.1038/nature14236},
  journal      = {Nature},
  publisher    = {Springer Nature}
}

@article{schaul15_prior_exper_replay,
  author =       {Schaul, Tom and Quan, John and Antonoglou, Ioannis
                  and Silver, David},
  title =        {Prioritized Experience Replay},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1511.05952v4},
  abstract =     {Experience replay lets online reinforcement learning
                  agents remember and reuse experiences from the past.
                  In prior work, experience transitions were uniformly
                  sampled from a replay memory. However, this approach
                  simply replays transitions at the same frequency
                  that they were originally experienced, regardless of
                  their significance. In this paper we develop a
                  framework for prioritizing experience, so as to
                  replay important transitions more frequently, and
                  therefore learn more efficiently. We use prioritized
                  experience replay in Deep Q-Networks (DQN), a
                  reinforcement learning algorithm that achieved
                  human-level performance across many Atari games. DQN
                  with prioritized experience replay achieves a new
                  state-of-the-art, outperforming DQN with uniform
                  replay on 41 out of 49 games.},
  archivePrefix ={arXiv},
  eprint =       {1511.05952},
  primaryClass = {cs.LG},
}
@article{schulman15_high_dimen_contin_contr_using,
  author =       {Schulman, John and Moritz, Philipp and Levine,
                  Sergey and Jordan, Michael and Abbeel, Pieter},
  title =        {High-Dimensional Continuous Control Using
                  Generalized Advantage Estimation},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1506.02438v6},
  abstract =     {Policy gradient methods are an appealing approach in
                  reinforcement learning because they directly
                  optimize the cumulative reward and can
                  straightforwardly be used with nonlinear function
                  approximators such as neural networks. The two main
                  challenges are the large number of samples typically
                  required, and the difficulty of obtaining stable and
                  steady improvement despite the nonstationarity of
                  the incoming data. We address the first challenge by
                  using value functions to substantially reduce the
                  variance of policy gradient estimates at the cost of
                  some bias, with an exponentially-weighted estimator
                  of the advantage function that is analogous to
                  TD(lambda). We address the second challenge by using
                  trust region optimization procedure for both the
                  policy and the value function, which are represented
                  by neural networks. Our approach yields strong
                  empirical results on highly challenging 3D
                  locomotion tasks, learning running gaits for bipedal
                  and quadrupedal simulated robots, and learning a
                  policy for getting the biped to stand up from
                  starting out lying on the ground. In contrast to a
                  body of prior work that uses hand-crafted policy
                  representations, our neural network policies map
                  directly from raw kinematics to joint torques. Our
                  algorithm is fully model-free, and the amount of
                  simulated experience required for the learning tasks
                  on 3D bipeds corresponds to 1-2 weeks of real time.},
  archivePrefix ={arXiv},
  eprint =       {1506.02438},
  primaryClass = {cs.LG},
}

@inproceedings{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5713--5723},
  year={2017}
}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@incollection{jin_q_learning_provably_efficient,
title = {Is Q-Learning Provably Efficient?},
author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {4863--4873},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7735-is-q-learning-provably-efficient.pdf}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Thirtieth AAAI conference on artificial intelligence},
  year={2016}
}

@inproceedings{anschel2017averaged,
  title={Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning},
  author={Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={176--185},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5048--5058},
  year={2017}
}

@inproceedings{dabney2018distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{schultz97_neural_subst_predic_rewar,
  author =       {W. Schultz and P. Dayan and P. R. Montague},
  title =        {A Neural Substrate of Prediction and Reward},
  journal =      {Science},
  volume =       275,
  number =       5306,
  pages =        {1593-1599},
  year =         1997,
  doi =          {10.1126/science.275.5306.1593},
  url =          {https://doi.org/10.1126/science.275.5306.1593},
  DATE_ADDED =   {Sun Dec 15 13:23:11 2019},
}

@inproceedings{stolle2002learning,
  title={Learning options in reinforcement learning},
  author={Stolle, Martin and Precup, Doina},
  booktitle={International Symposium on abstraction, reformulation, and approximation},
  pages={212--223},
  year={2002},
  organization={Springer}
}

@article{haan19_causal_confus_imitat_learn,
  author =       {Haan, Pim de and Jayaraman, Dinesh and Levine,
                  Sergey},
  title =        {Causal Confusion in Imitation Learning},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1905.11979v2},
  abstract =     {Behavioral cloning reduces policy learning to
                  supervised learning by training a discriminative
                  model to predict expert actions given observations.
                  Such discriminative models are non-causal: the
                  training procedure is unaware of the causal
                  structure of the interaction between the expert and
                  the environment. We point out that ignoring
                  causality is particularly damaging because of the
                  distributional shift in imitation learning. In
                  particular, it leads to a counter-intuitive "causal
                  misidentification" phenomenon: access to more
                  information can yield worse performance. We
                  investigate how this problem arises, and propose a
                  solution to combat it through targeted
                  interventions---either environment interaction or
                  expert queries---to determine the correct causal
                  model. We show that causal misidentification occurs
                  in several benchmark control domains as well as
                  realistic driving settings, and validate our
                  solution against DAgger and other baselines and
                  ablations.},
  archivePrefix ={arXiv},
  eprint =       {1905.11979},
  primaryClass = {cs.LG},
}
