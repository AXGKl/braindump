@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@article{mnih16_async_method_deep_reinf_learn,
  author =       {Mnih, Volodymyr and Badia, Adri{\`a}
                  Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex
                  and Lillicrap, Timothy P. and Harley, Tim and
                  Silver, David and Kavukcuoglu, Koray},
  title =        {Asynchronous Methods for Deep Reinforcement
                  Learning},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1602.01783v2},
  abstract =     {We propose a conceptually simple and lightweight
                  framework for deep reinforcement learning that uses
                  asynchronous gradient descent for optimization of
                  deep neural network controllers. We present
                  asynchronous variants of four standard reinforcement
                  learning algorithms and show that parallel
                  actor-learners have a stabilizing effect on training
                  allowing all four methods to successfully train
                  neural network controllers. The best performing
                  method, an asynchronous variant of actor-critic,
                  surpasses the current state-of-the-art on the Atari
                  domain while training for half the time on a single
                  multi-core CPU instead of a GPU. Furthermore, we
                  show that asynchronous actor-critic succeeds on a
                  wide variety of continuous motor control problems as
                  well as on a new task of navigating random 3D mazes
                  using a visual input.},
  archivePrefix ={arXiv},
  eprint =       {1602.01783},
  primaryClass = {cs.LG},
}

@article{gu16_q_prop,
  author =       {Gu, Shixiang and Lillicrap, Timothy and Ghahramani,
                  Zoubin and Turner, Richard E. and Levine, Sergey},
  title =        {Q-Prop: Sample-Efficient Policy Gradient With an
                  Off-Policy Critic},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1611.02247v3},
  abstract =     {Model-free deep reinforcement learning (RL) methods
                  have been successful in a wide variety of simulated
                  domains. However, a major obstacle facing deep RL in
                  the real world is their high sample complexity.
                  Batch policy gradient methods offer stable learning,
                  but at the cost of high variance, which often
                  requires large batches. TD-style methods, such as
                  off-policy actor-critic and Q-learning, are more
                  sample-efficient but biased, and often require
                  costly hyperparameter sweeps to stabilize. In this
                  work, we aim to develop methods that combine the
                  stability of policy gradients with the efficiency of
                  off-policy RL. We present Q-Prop, a policy gradient
                  method that uses a Taylor expansion of the
                  off-policy critic as a control variate. Q-Prop is
                  both sample efficient and stable, and effectively
                  combines the benefits of on-policy and off-policy
                  methods. We analyze the connection between Q-Prop
                  and existing model-free algorithms, and use control
                  variate theory to derive two variants of Q-Prop with
                  conservative and aggressive adaptation. We show that
                  conservative Q-Prop provides substantial gains in
                  sample efficiency over trust region policy
                  optimization (TRPO) with generalized advantage
                  estimation (GAE), and improves stability over deep
                  deterministic policy gradient (DDPG), the
                  state-of-the-art on-policy and off-policy methods,
                  on OpenAI Gym's MuJoCo continuous control
                  environments.},
  archivePrefix ={arXiv},
  eprint =       {1611.02247},
  primaryClass = {cs.LG},
}

@Article{Mnih_2015,
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver,
                  David and Rusu, Andrei A. and Veness, Joel and
                  Bellemare, Marc G. and Graves, Alex and Riedmiller,
                  Martin and Fidjeland, Andreas K. and Ostrovski,
                  Georg and et al.},
  title        = {Human-level control through deep reinforcement
                  learning},
  year         = 2015,
  volume       = 518,
  number       = 7540,
  month        = {Feb},
  pages        = {529â€“533},
  issn         = {1476-4687},
  doi          = {10.1038/nature14236},
  url          = {http://dx.doi.org/10.1038/nature14236},
  journal      = {Nature},
  publisher    = {Springer Nature}
}

@article{schaul15_prior_exper_replay,
  author =       {Schaul, Tom and Quan, John and Antonoglou, Ioannis
                  and Silver, David},
  title =        {Prioritized Experience Replay},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1511.05952v4},
  abstract =     {Experience replay lets online reinforcement learning
                  agents remember and reuse experiences from the past.
                  In prior work, experience transitions were uniformly
                  sampled from a replay memory. However, this approach
                  simply replays transitions at the same frequency
                  that they were originally experienced, regardless of
                  their significance. In this paper we develop a
                  framework for prioritizing experience, so as to
                  replay important transitions more frequently, and
                  therefore learn more efficiently. We use prioritized
                  experience replay in Deep Q-Networks (DQN), a
                  reinforcement learning algorithm that achieved
                  human-level performance across many Atari games. DQN
                  with prioritized experience replay achieves a new
                  state-of-the-art, outperforming DQN with uniform
                  replay on 41 out of 49 games.},
  archivePrefix ={arXiv},
  eprint =       {1511.05952},
  primaryClass = {cs.LG},
}
@article{schulman15_high_dimen_contin_contr_using,
  author =       {Schulman, John and Moritz, Philipp and Levine,
                  Sergey and Jordan, Michael and Abbeel, Pieter},
  title =        {High-Dimensional Continuous Control Using
                  Generalized Advantage Estimation},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1506.02438v6},
  abstract =     {Policy gradient methods are an appealing approach in
                  reinforcement learning because they directly
                  optimize the cumulative reward and can
                  straightforwardly be used with nonlinear function
                  approximators such as neural networks. The two main
                  challenges are the large number of samples typically
                  required, and the difficulty of obtaining stable and
                  steady improvement despite the nonstationarity of
                  the incoming data. We address the first challenge by
                  using value functions to substantially reduce the
                  variance of policy gradient estimates at the cost of
                  some bias, with an exponentially-weighted estimator
                  of the advantage function that is analogous to
                  TD(lambda). We address the second challenge by using
                  trust region optimization procedure for both the
                  policy and the value function, which are represented
                  by neural networks. Our approach yields strong
                  empirical results on highly challenging 3D
                  locomotion tasks, learning running gaits for bipedal
                  and quadrupedal simulated robots, and learning a
                  policy for getting the biped to stand up from
                  starting out lying on the ground. In contrast to a
                  body of prior work that uses hand-crafted policy
                  representations, our neural network policies map
                  directly from raw kinematics to joint torques. Our
                  algorithm is fully model-free, and the amount of
                  simulated experience required for the learning tasks
                  on 3D bipeds corresponds to 1-2 weeks of real time.},
  archivePrefix ={arXiv},
  eprint =       {1506.02438},
  primaryClass = {cs.LG},
}

@inproceedings{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5713--5723},
  year={2017}
}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@incollection{jin_q_learning_provably_efficient,
title = {Is Q-Learning Provably Efficient?},
author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {4863--4873},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7735-is-q-learning-provably-efficient.pdf}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Thirtieth AAAI conference on artificial intelligence},
  year={2016}
}

@inproceedings{anschel2017averaged,
  title={Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning},
  author={Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={176--185},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5048--5058},
  year={2017}
}

@inproceedings{dabney2018distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{schultz97_neural_subst_predic_rewar,
  author =       {W. Schultz and P. Dayan and P. R. Montague},
  title =        {A Neural Substrate of Prediction and Reward},
  journal =      {Science},
  volume =       275,
  number =       5306,
  pages =        {1593-1599},
  year =         1997,
  doi =          {10.1126/science.275.5306.1593},
  url =          {https://doi.org/10.1126/science.275.5306.1593},
  DATE_ADDED =   {Sun Dec 15 13:23:11 2019},
}

@inproceedings{stolle2002learning,
  title={Learning options in reinforcement learning},
  author={Stolle, Martin and Precup, Doina},
  booktitle={International Symposium on abstraction, reformulation, and approximation},
  pages={212--223},
  year={2002},
  organization={Springer}
}

@article{haan19_causal_confus_imitat_learn,
  author =       {Haan, Pim de and Jayaraman, Dinesh and Levine,
                  Sergey},
  title =        {Causal Confusion in Imitation Learning},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1905.11979v2},
  abstract =     {Behavioral cloning reduces policy learning to
                  supervised learning by training a discriminative
                  model to predict expert actions given observations.
                  Such discriminative models are non-causal: the
                  training procedure is unaware of the causal
                  structure of the interaction between the expert and
                  the environment. We point out that ignoring
                  causality is particularly damaging because of the
                  distributional shift in imitation learning. In
                  particular, it leads to a counter-intuitive "causal
                  misidentification" phenomenon: access to more
                  information can yield worse performance. We
                  investigate how this problem arises, and propose a
                  solution to combat it through targeted
                  interventions---either environment interaction or
                  expert queries---to determine the correct causal
                  model. We show that causal misidentification occurs
                  in several benchmark control domains as well as
                  realistic driving settings, and validate our
                  solution against DAgger and other baselines and
                  ablations.},
  archivePrefix ={arXiv},
  eprint =       {1905.11979},
  primaryClass = {cs.LG},
}

@article{levine18_reinf_learn_contr_as_probab_infer,
  author =       {Levine, Sergey},
  title =        {Reinforcement Learning and Control As Probabilistic
                  Inference: Tutorial and Review},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1805.00909v3},
  abstract =     {The framework of reinforcement learning or optimal
                  control provides a mathematical formalization of
                  intelligent decision making that is powerful and
                  broadly applicable. While the general form of the
                  reinforcement learning problem enables effective
                  reasoning about uncertainty, the connection between
                  reinforcement learning and inference in
                  probabilistic models is not immediately obvious.
                  However, such a connection has considerable value
                  when it comes to algorithm design: formalizing a
                  problem as probabilistic inference in principle
                  allows us to bring to bear a wide array of
                  approximate inference tools, extend the model in
                  flexible and powerful ways, and reason about
                  compositionality and partial observability. In this
                  article, we will discuss how a generalization of the
                  reinforcement learning or optimal control problem,
                  which is sometimes termed maximum entropy
                  reinforcement learning, is equivalent to exact
                  probabilistic inference in the case of deterministic
                  dynamics, and variational inference in the case of
                  stochastic dynamics. We will present a detailed
                  derivation of this framework, overview prior work
                  that has drawn on this and related ideas to propose
                  new reinforcement learning and control algorithms,
                  and describe perspectives on future research.},
  archivePrefix ={arXiv},
  eprint =       {1805.00909},
  primaryClass = {cs.LG},
}

@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004},
  organization={ACM}
}

@inproceedings{ratliff2006maximum,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={729--736},
  year={2006},
  organization={ACM}
}

@inproceedings{ziebart2008_maxentrl,
author = {Ziebart, Brian and Maas, Andrew and Bagnell, J. and Dey, Anind},
year = {2008},
month = {01},
pages = {1433-1438},
title = {Maximum Entropy Inverse Reinforcement Learning.}
}




@InProceedings{pmlr-v70-haarnoja17a,
  title = 	 {Reinforcement Learning with Deep Energy-Based Policies},
  author = 	 {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1352--1361},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/haarnoja17a.html},
  abstract = 	 {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.}
}

@article{rusu15_polic_distil,
  author =       {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and
                  Gulcehre, Caglar and Desjardins, Guillaume and
                  Kirkpatrick, James and Pascanu, Razvan and Mnih,
                  Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  title =        {Policy Distillation},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1511.06295v2},
  abstract =     {Policies for complex visual tasks have been
                  successfully learned with deep reinforcement
                  learning, using an approach called deep Q-networks
                  (DQN), but relatively large (task-specific) networks
                  and extensive training are needed to achieve good
                  performance. In this work, we present a novel method
                  called policy distillation that can be used to
                  extract the policy of a reinforcement learning agent
                  and train a new network that performs at the expert
                  level while being dramatically smaller and more
                  efficient. Furthermore, the same method can be used
                  to consolidate multiple task-specific policies into
                  a single policy. We demonstrate these claims using
                  the Atari domain and show that the multi-task
                  distilled agent outperforms the single-task teachers
                  as well as a jointly-trained DQN agent.},
  archivePrefix ={arXiv},
  eprint =       {1511.06295},
  primaryClass = {cs.LG},
}
