#+SETUPFILE:./hugo_setup.org
#+TITLE: Policy Gradients

* Key Idea

The objective is:

\begin{equation}
  \theta^{\star}=\arg \max _{\theta} E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
\end{equation}

To evaluate the objective, we need to estimate this expectation, often
through sampling by generating multiple samples from the distribution:

\begin{equation}
  J(\theta)=E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \approx \frac{1}{N} \sum_{i} \sum_{t} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)
\end{equation}

Recall that:

\begin{equation}
  \nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \underbrace{\nabla_{\theta} \log \pi_{\theta}\left(\tau_{i}\right)}_{\sum_{t=1}^{T} \nabla_{\theta} \log _{\theta} \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)}r\left(\tau_{i}\right)
\end{equation}

This makes the good stuff more likely, and bad stuff less likely, but
scaled by the rewards.

** Comparison to Maximum Likelihood

- policy gradient :: $\nabla_{\theta} J(\theta) \approx \frac{1}{N}
  \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log
  \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i,
  t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t},
  \mathbf{a}_{i, t}\right)\right)$
- maximum likelihood :: $\nabla_{\theta} J_{\mathrm{ML}}(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right)$

** Partial Observability

The policy gradient method does not assume that the system follows the
[[file:markovian_assumption.org][Â§markovian_assumption]]! The algorithm only requires the ability to
generate samples, and a function approximator for
$\pi_{\theta}(a_t |o_t)$.

* REINFORCE

1. For each episode,
   1. generate $\tau = s_0, a_0, r_1, \dots, s_{t-1},
      a_{t-1}, r_t$ by following $\pi_{\theta}(a |s)$
   2. For each step $i = 0, \dots, t-1$:
      1. $R_i = \sum_{k=i}^{t} \gamma^{t-k} r_k$ (Unbiased estimate of
         remaining episode return under $\pi_{\theta}$ starting from $i$)
      2. $\hat{A_i} = R_i - b$ (Advantage function: subtract base line $b$ to lower variance)
         1. Advantage function tells you how relatively good this
            action is
      3. $\theta = $\theta + \alpha \nabla_\theta \log \pi_{\theta}
         (a| s_i) \hat{A}_i$

Objective: $J(\theta) = \sum_{\tau} P_{\theta}(\tau)R(\tau)$

\begin{align}
  \nabla_\theta J(\theta) &=  \nabla_\theta \sum_{\tau} P_\theta(\tau)
                            R(\tau) \\
                          &= \sum_{\tau} \nabla_\theta P_\theta(\tau)R(\tau)
\end{align}

Actor critics use learned estimate (e.g. $\hat{A}(s, a) = \hat{Q}(s,
a) - \hat{V}(s).)

* Actor-Critic

Need to balance between learning speed, stability.

- Conservative Policy Iteration (CPI)
  - propose surrogate objective, guarantee monotonic improvement under
    specific state distribution
- Trust Region Policy Optimization (TRPO)
  - approximates CPI with trust region constraint
- Proximal Policy Optimization (PPO)
  - replaces TRPO constraint with RL penalty + clipping
    (computationally efficient)
- Soft Actor-Critic (SAC)
  - stabilize learning by jointly maximizing expected reward and
    policy entropy (based on maximum entropy RL)
- Optimistic Actor Critic (OAC)
  - Focus on exploration in deep Actor critic approaches.
  - *Key insight*: existing approaches tend to explore conservatively
  - *Key result*: Optimistic exploration leads to efficient, stable
    learning in modern Actor Critic methods

* Resources
- [[https://nips.cc/Conferences/2016/Schedule?showEvent=6198][Deep Reinforcement Learning Through Policy Optimization - NIPS 2016 Tutorial]]

