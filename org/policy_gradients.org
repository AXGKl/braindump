#+SETUPFILE:./hugo_setup.org
#+TITLE: Policy Gradients

* REINFORCE

1. For each episode,
   1. generate $\tau = s_0, a_0, r_1, \dots, s_{t-1},
      a_{t-1}, r_t$ by following $\pi_{\theta}(a |s)$
   2. For each step $i = 0, \dots, t-1$:
      1. $R_i = \sum_{k=i}^{t} \gamma^{t-k} r_k$ (Unbiased estimate of
         remaining episode return under $\pi_{\theta}$ starting from $i$)
      2. $\hat{A_i} = R_i - b$ (Advantage function: subtract base line $b$ to lower variance)
         1. Advantage function tells you how relatively good this
            action is
      3. $\theta = $\theta + \alpha \nabla_\theta \log \pi_{\theta}
         (a| s_i) \hat{A}_i$

Objective: $J(\theta) = \sum_{\tau} P_{\theta}(\tau)R(\tau)$

\begin{align}
  \nabla_\theta J(\theta) &=  \nabla_\theta \sum_{\tau} P_\theta(\tau)
                            R(\tau) \\
                          &= \sum_{\tau} \nabla_\theta P_\theta(\tau)R(\tau)
\end{align}

Actor critics use learned estimate (e.g. $\hat{A}(s, a) = \hat{Q}(s,
a) - \hat{V}(s).)

* Actor-Critic

Need to balance between learning speed, stability.

- Conservative Policy Iteration (CPI)
  - propose surrogate objective, guarantee monotonic improvement under
    specific state distribution
- Trust Region Policy Optimization (TRPO)
  - approximates CPI with trust region constraint
- Proximal Policy Optimization (PPO)
  - replaces TRPO constraint with RL penalty + clipping
    (computationally efficient)
- Soft Actor-Critic (SAC)
  - stabilize learning by jointly maximizing expected reward and
    policy entropy (based on maximum entropy RL)
- Optimistic Actor Critic (OAC)
  - Focus on exploration in deep Actor critic approaches.
  - *Key insight*: existing approaches tend to explore conservatively
  - *Key result*: Optimistic exploration leads to efficient, stable
    learning in modern Actor Critic methods

* Resources
- [[https://nips.cc/Conferences/2016/Schedule?showEvent=6198][Deep Reinforcement Learning Through Policy Optimization - NIPS 2016 Tutorial]]

