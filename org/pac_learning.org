#+SETUPFILE: ./hugo_setup.org
#+TITLE: PAC Learning
#+HUGO_TAGS: machine-learning


* ERM for finite hypothesis classes

We note that [[file:machine_learning.org::*Empirical%20Risk%20Minimisation%20(ERM)][empirical risk minimization]] can easily overfit
to the training data. To correct for this, we introduce inductive
bias, restricting the hypothesis space $\mathcal{H}$.

The simplest type of restriction is to impose an upper bound on the
size of a class. Here, we show that if a hypothesis class is finite,
then ERM will not overfit given a sufficiently large sample size.

Let $h_S$ denote the result of applying ERM to $S$:

\begin{equation}
  h_S \in \textrm{argmin}_{h \in \mathcal{H}} L_S(h)
\end{equation}

We make 2 assumptions. First, the realizability assumption, implying
that every ERM hypothesis we have that $L_S(h_S) = 0$. However, we are
more interested in the true risk $L_{(D,f)}(h_S)$ rather than the
empirical risk.

#+begin_definition
The Realizability Assumption: There xists $h^* \in \mathcal{H}$ such
that $L_{(D,f)(h^*) = 0}. That is, with probability 1 over random
samples $S$, where the instances are sampled according to $D$, and
labelled according to $f$, we have $L_S(h^*) = 0$.
#+end_definition

Any guarantee on the error with respect to the underlying distribution
$D$, must depend on the relationship between $D$ and $S$. Here, we
make the second assumption that the training examples are drawn i.i.d.

Since $L_{(D,f)}(h_S)$ depends on the training set, which is drawn via
a random process, it is also a random variable.

We introduce 2 parameters:

1. the probability of getting a non-representative sample, denoted by
   $\delta$. We denote $(1 - \delta)$ the confidence parameter of our prediction.
2. We denote $\epsilon$ as the accuracy parameter of the prediction.
   The event that $L_{(D,f)}(h_S) > \epsilon$ is a failure of the
   learner, while $L_{(D,f)}(h_S) \le \epsilon$ is the event where the
   predictor is approximately correct.

We are interested in upper bounding the probability to sample m-tuple
of instances that will lead to failure of the learner. Formally, let
$S_x = \left(x_1, \dots, x_m \right)$ be the instances of the training
set. We would like to upper-bound:

\begin{equation}
  D^M(\left\{ S_x ; L_{(D,f)}(h_S) > \epsilon \right\})
\end{equation}

Let $H_B$ be the set of bad hypotheses, that is,

\begin{equation}
  \mathcal{H}_B = \left\{ h \in \mathcal{H} : L_{(D,f)}(h)> \epsilon \right\}
\end{equation}

In addition, let:

\begin{equation}
M = \left\{ S_x: \exists h \in \mathcal{H}_B, L_S(h) = 0 \right\}
\end{equation}

be the set of misleading samples. For every $S_x \in M$, there is a
bad hypothesis, $h \in \mathcal{H}_B$ that looks like a good
hypothesis in $S_x$. 

Since the realizability assumption implies $L_S(h_S) = 0$, then the
event $L_{(D,f)}(h_S) > \epsilon$ will only happen if our sample is
in the set of misleading examples $M$.

Then:

\begin{equation}
  D^m(\left\{ S_x : L_{(D,f)}(h_S) > \epsilon \right\}) \le D^m(M)
  =D^m(\cups_{h \in \mathcal{H}_B} {S_x: L_S(h) = 0})
\end{equation}

Because the training samples are i.i.d.:

\begin{align}
  D^m(\left\{ S_x: L_S(h) = 0\right\}) &= D^m(\left\{ S_x: \forall i,
                                         h(x_i) = f(x_i) \right\}) \\
  &=  \prod_{i=1}^{m}D(\left\{ x_i: h(x_i) = f(x_i) \right\})
\end{align}

for each individual sampling of an element of the training set, we
have:

\begin{equation}
  D(\left\{ x_i: h(x_i) = y_i \right\}) = 1 - L_{(D,f)}(h) \le 1- \epsilon
\end{equation}

Using the inequality $1 - \epsilon \le e^{-\epsilon}$, we obtain that:

\begin{equation}
  D^m(\left\{ S_x: L_S(h) = 0 \right\}) \le (1 - \epsilon)^m \le
  e^{-\epsilon m}
\end{equation}

Applying the union bound, we get:

\begin{equation}
  D^m(\left\{ S_x: L_{(D,f)}(h_S) > \epsilon \right\}) \le \left| \mathcal{H}_B \right|(1 - \epsilon)^m \le
  \left| \mathcal{H}_B \right| e^{-\epsilon m}
\end{equation}

With this result, we can show that where $m \ge
\frac{\log(|\mathcal{H}|/\delta)}{\epsilon}$, the error $L_{(D,f)(h_S)
\le \epsilon}$ for every ERM hypothesis $h_S$.
* Formulation


