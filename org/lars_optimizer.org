#+SETUPFILE:./hugo_setup.org
#+HUGO_SECTION: zettels
#+TITLE: LARS Optimizer

Layer-wise Adaptive Rate Scaling (LARS) is a [[file:nn_optimizer.org][§nn_optimizer]]. The
technique allows [[file:large_batch_training.org][§large_batch_training]] without significant
decrease in accuracy cite:you17_large_batch_train_convol_networ. One
of the secondary goals is [[file:fast_nn_training.org][§fast_nn_training]].

* Implementations
- [[https://github.com/noahgolmant/pytorch-lars][pytorch-lars]]

bibliography:./biblio/ml.bib
