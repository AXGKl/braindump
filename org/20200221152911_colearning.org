#+SETUPFILE:./hugo_setup.org
#+HUGO_SECTION: zettels
#+HUGO_SLUG: colearning
#+TITLE: Co-learning

Co-learning is the technique of aiding of modeling of a
(resource-poor) modality by exploiting knowledge from another
(resource-rich) modality. The helper modality is only used in model
training, and is not used during test-time. cite:baltrusaitis17:_multim_machin_learn

Parallel-data approaches require the data to be directly linked to
observations in other modalites. Non-parallel approaches do not
require these direct links between modalities. Hybrid-data approaches
bridge the modalities through a shared modality, or a dataset.

* Parallel data

Co-training is the process of creating more labeled training samples
when we have few labeled samples in a multi-modal problem. Weak
classifiers are built for each modality to bootstrap each other with
labels for the unlabeled data.

[[file:transfer_learning.org][Transfer learning]] exploits co-learning with parallel data, by building
[[file:20200219120814_multimodal_representation.org][multi-modal representations]] with only some modalities used during test
time. Approaches like these include multimodal [[file:20200219122016_deep_boltzmann_machines.org][Deep Boltzmann Machines]]
and [[file:20200221153531_multimodal_autoencoders.org][multi-modal autoencoders]].

* Non-parallel data

Non-parallel methods only require that different modalities share
similar categories or concepts. Methods include [[file:transfer_learning.org][transfer learning]]
using coordinated multimodal representations, or [[file:20200221154028_concept_grounding.org][concept grounding]] via
word similarity, or [[file:20200221154112_zeroshot_learning.org][zero-shot learning]].

bibliography:./biblio/biblio.bib
