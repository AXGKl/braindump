#+SETUPFILE:./hugo_setup.org
#+TITLE: Model-Based Reinforcement Learning

In model-free RL, we assumed that $p(s_{t+1} | s_t, a_t)$ is unknown,
and no attempt is made to learn it. However, we often know the
dynamics. For example, in games such as chess, and easily modeled
systems, like car navigation.

Model-based Reinforcement learning is about learning system dynamics,
and using the learnt system to make decisions.

In the stochastic /open-loop/ case,

\begin{equation}
  p_{\theta}(s_1, \dots, s_T | a_1, \dots, a_T) = p(s_1)
  \prod_{t=1}^{T} p(s_{t+1} | s_t, a_t)
\end{equation}

and we'd want to choose:

\begin{equation}
  a_1, \dots, a_T = \mathrm{argmax}_{a_1, \dots, a_T} E \left[
    \sum_{t} r(s_t, a_t) | a_1, \dots, a_T \right]
\end{equation}

In a *closed-loop* setting, the process by which the agent performs
actions receives feedback. Planning in the open-loop case can go
extremely awry.

In the stochastic /closed-loop/ case,

\begin{equation}
  p(s_1, a_1, \dots, s_T, a_T) = p(s_1) \prod_{t=1}^{T} \pi(a_t | s_t)
  p(s_{t+1} | s_t, a_t)
\end{equation}

where $\pi = \mathrm{argmax}_{\pi}E_{\tau \sim p(\tau)} \left[ \sum_t
r(s_t, a_t)\right]$
