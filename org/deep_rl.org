#+SETUPFILE: ./hugo_setup.org
#+TITLE: Deep Reinforcement Learning
#+HUGO_TAGS: machine-learning
bibliographystyle:unsrt

* Why Deep RL?

1. Reinforcement learning provides a mathematical framework for decision-making
2. Deep learning has shown to be extremely successful in unstructured
   environments (e.g. image, text)
3. Deep RL allows for end-to-end training of policies
   1. Features are tedious and difficult to hand-design, and are not
      so transferable across tasks

* Good Resources
1. [[https://www.youtube.com/playlist?list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A][CS285 Fall 2019 - YouTube]]
2. [[https://spinningup.openai.com/en/latest/][Welcome to Spinning Up in Deep RL! â€” Spinning Up documentation]]
   ([[https://github.com/openai/spinningup][Tensorflow]], [[https://github.com/kashif/firedup/][Pytorch]])

* Algorithms
** Actor Critic

Batch actor-critic algorithm:

1. sample $\left\{ s_i, a_i \right\}$ from $\pi_\theta (a|s)$ (run it
   on the robot)
2. fit $\hat{V}_\phi^\pi (s)$ to sample reward sums
3. evaluate $\hat{A}^\pi (s_i, a_i) = r(s_i, a_i) +
   \hat{V}_\phi^\pi(s_i') - \hat{V}_\phi^\pi (s_i)$
4. $\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log
   \pi_\theta(a_i|s_i) \hat{A}^\pi (s_i|a_i)$
5. $\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$

cite:sutton2000policy,mnih16_async_method_deep_reinf_learn,gu16_q_prop

** Deep RL with Q-functions

- What happens if we Just use a critic without an actor?
- Extracting a policy from a value function
- Q-learning algorithm
- Extension to Q-learning algorithms

*** Instabilities in Q-Learning

1. Correlations are present in the sequence of observations
2. Small updates to $Q$ may significantly change the policy and
   therefore change the data distribution
3. Correlations between the action-values $Q$ and the target values
   $r + \gamma \mathrm{max}_{a'}Q(s', a')$.

Full fitted Q-iteration algorithm:

1. collect dataset $\left\{ (s_i, a_i, s_i', r_i)\right\}$ using some
   policy
2. set $y_i \leftarrow r(s_i, a_i) + \gamma \mathrm{max}_{a_i'}
   Q_\phi(s_i', a_i')$
3. set $\phi \leftarrow \mathrm{argmin}_\phi \frac{1}{2} \sum_i \lVert
   Q_\phi (s_i, a_i) - y_i \rVert ^2$

Online Q-iteration algorithm:

1. take some action $a_i$ and observe $(s_i, a_i, s_i', r_i)$
2. $y_i = r(s_i, a_i) + \gamma \mathrm{max}_{a'}Q_\phi(s_i', a_i')$
3. $\phi \leftarrow \phi - \alpha \frac{dQ_\phi}{d\phi} (s_i, a_i)
   (Q_\phi (s_i, a_i) - y_i)$

Q-learning is not gradient descent, and does not converge in general,
because there are no gradients through target value.

** Deep Q-network (DQN)

DQN cite:Mnih_2015 aims to improve the stability of Q-learning by
introducing 2 mechanisms: *experience replay*, and a *periodically
updated target*.

*** Experience Replay

This idea is first proposed in cite:Mnih_2015.

All episodic steps $e_t = (S_t, A_t, R_t, S_{t+1})$ are stored in a
replay buffer $D_t = \left\{e_1, \dots, e_t\right\}$. $D_t$ has
experience tuples over many episodes. During Q-learning updates,
samples are drawn at random from the replay buffer. This allows for
multiple reuse of each episode, improving data efficiency, and smooths
changes in the data distribution.

Is uniform sampling from the replay buffer the best approach? It is
expected that some episodes may provide higher expected learning
progress, and prioritizing these episodes should lead to faster
learning. Prioritized experience replay
cite:schaul15_prior_exper_replay uses TD-error as a measure of
expected learning progress, correcting for the introduced bias by
using importance-sampling weights.

One ability humans have is to learn almost as much from achieving an
undesirable outcome as from the desired one. This property is missing
from many model-free RL algorithms. /Hindsight Experience Replay (HER)/
cite:andrychowicz17_hinds_exper_replay allows the algorithm to perform
this kind of reasoning, and can be combined with any off-policy RL
algorithm. This is applicable whenever multiple goals may be achieved.
This makes learning more sample efficient, and possible when rewards
are sparse.

*** Periodically Updated Target

The Q-function is optimized towards target values that are only
periodically updated. The Q-network is cloned, and kept frozen as the
optimization target every $K$ steps, $K$ being a tunable
hyperparameter.

The modified loss function looks like this:

\begin{equation}
  L(\theta) = \mathcal{E}_{(s,a,r,s') \sim U(D)}\left[ \left( r +
      \gamma \mathrm{max}_{a'}Q(s',a';\theta^-)-Q(s,a;\theta) \right)^2 \right]
\end{equation}

where $U(D)$ is a uniform distribution over the replay buffer, and
$\theta^-$ is the parameters of the frozen target Q-network.



bibliography:rl.bib
