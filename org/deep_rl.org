#+SETUPFILE: ./hugo_setup.org
#+TITLE: Deep Reinforcement Learning
#+HUGO_TAGS: machine-learning
bibliographystyle:unsrt

* Why Deep Reinforcement Learning?

1. Reinforcement learning provides a mathematical framework for decision-making
2. Deep learning has shown to be extremely successful in unstructured
   environments (e.g. image, text)
3. Deep RL allows for end-to-end training of policies
   1. Features are tedious and difficult to hand-design, and are not
      so transferable across tasks
   2. Features are informed by the task

* Anatomy of Deep RL algorithms

\begin{equation}
  \theta^{\star}=\arg \max _{\theta} E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
\end{equation}

- policy gradients :: directly differentiate above objective
- value-based :: estimate value/q-function of the optimal policy (no
  explicit policy)
- actor-critic :: estimate value/q-function of the current policy, use
  it to improve policy
- model-based RL :: estimate the transition model, and then...
  - use it for planning (no explicit policy)
    - Trajectory optimization/optimal control (continuous spaces)
    - Discrete planning in discrete action spaces ([[file:mcts.org][§mcts]])
  - use it to improve a policy (e.g. via backpropagation, with some tricks)
  - use the model to learn a value function (e.g. through dynamic programming)

* Why so many RL algorithms?
- Different tradeoffs
  - sample efficiency
    - is it off policy: can improve policy without generating new
      samples from that policy?
    - however, are samples cheap to obtain?

#+CAPTION: Sample efficiency comparison
[[file:images/deep_rl/screenshot2019-12-16_01-35-50_.png]]

- stability and ease of use (does it converge, and if so to what?)
  - Q-learning: fixed point iteration
  - Model-based RL: model is not optimized for expected reward
- Different assumptions
  - fully observable?
    - generally assumed by value function fitting methods (mitigated
      by adding recurrence)
    - episodic learning
      - generally assumed by pure policy gradient methods
      - assumed by some model-based RL methods
    - continuity or smoothness?
      - assumed by some continuous value function learning methods
      - often assumed by some model-based RL methods
  - stochastic or deterministic?
- Different things are easy or hard in different settings
  - easier to represent the policy?
  - easier to represent the model?

* Resources
1. [[https://www.youtube.com/playlist?list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A][CS285 Fall 2019 - YouTube]]
2. [[https://spinningup.openai.com/en/latest/][Welcome to Spinning Up in Deep RL! — Spinning Up documentation]]
   ([[https://github.com/openai/spinningup][Tensorflow]], [[https://github.com/kashif/firedup/][Pytorch]])
3. [[https://www.icml.cc/2016/tutorials/deep_rl_tutorial.pdf][David Silver's Deep RL ICML Tutorial]]

* Subtopics
- [[file:policy_gradients.org][§policy_gradients]]
- [[file:q_learning.org][§q_learning]]
- [[file:model_based_rl.org][§model_based_rl]]
- [[file:inverse_rl.org][§inverse_rl]]
- [[file:control_as_inference.org][§control_as_inference]]
- [[file:transfer_learning.org][§transfer_learning]]
