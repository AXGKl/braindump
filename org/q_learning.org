#+SETUPFILE:./hugo_setup.org
#+TITLE: Q-Learning 

* Q-learning
Q-learning learns an action-utility representation instead of learning
utilities. We will use the notation $Q(s,a)$ to denote the value of
doing action $a$ in state $s$.

\begin{equation}
  U = max_a Q(s, a)
\end{equation}

*A TD agent that learns a Q-function does not need a model of the form
$P(s' | s, a)$, either for learning or for action selection.*
Q-learning is hence called a model-free method. We can write a
constraint equation as follows:

\begin{equation}
  Q(s,a) = R(s) + \gamma \sum_{s'} P(s' | s, a) max_{a'} Q(s', a')
\end{equation}

However, this equation requires a model to be learnt, since it depends
on $P(s' | s, a)$. The TD approach requires no model of state
transitions.

The updated equation for TD Q-learning is:

\begin{equation}
  Q(s, a) \leftarrow Q(s, a) + \alpha (R(s) + \gamma max_{a'} Q(s',
  a') - Q(s,a))
\end{equation}

which is calculated whenever action $a$ is executed in state $s$
leading to state $s'$.

Q-learning has a close relative called SARSA
(State-Action-Reward-State-Action). The update rule for SARSA is as
follows:

\begin{equation}
  Q(s, a) \leftarrow Q(s, a) + \alpha (R(s) + \gamma Q(s', a') - Q(s, a))
\end{equation}

where $a'$ is the action actually taken in state $s'$. The rule is
applied at the end of each $s, a, r, s', a'$ quintuplet, hence the
name.

Whereas Q-learning backs up the best Q-value from the state reached in
the observed transition, SARSA waits until an action is actually taken
and backs up the Q-value for that action. For a greedy agent that
always takes the action with best Q-value, the two algorithms are
identical. When exploration is happening, they differ significanty.

Because Q-learning uses the best Q-value, it pays no attention to the
actual policy being followed - it is an off-policy learning algorithm.
However, SARSA is an on-policy algorithm.

Q-learning is more flexible in the sense that a Q-learning agent can
learn how to behave well even when guided by a random or adversarial
exploration policy. On the other hand, SARSA is more realistic: for
example if the overall policy is even partly controlled by other
agents, it is better to learn a Q-function for what will actually
happen rather than what the agent would like to happen.

Q-learning has been shown to be sample efficient in the tabular
setting cite:jin_q_learning_provably_efficient.

* Key Ideas

1. If we have a policy $\pi$, and we know $Q^{\pi}(s_t, a_t)$, then we
   can improve $\pi$ by setting $\pi'(a|s) = 1$ where $a =
   \mathrm{argmax}_aQ^{\pi}(s , a)$.
2. We can compute the gradient to increase the probability of good
   actions $a$: if $Q^{\pi}(s, a) > V^{\pi}(s)$, then a is better than
   average ($V$ is expectation of $Q$ over $\pi(a|s)$).

* Q-learning with function approximation

To generalize over states and actions, parameterize Q with a function
approximator, e.g. a neural net:

\begin{equation}
  \delta = r_t + \gamma \mathrm{max}_a Q(s_{t+1}, a; \theta) - Q(s_t,
a ; \theta)
\end{equation}

and turn this into an optimization problem minimizing the loss on the
TD error:

\begin{equation}
J(\theta) = \left| \delta \right|^2
\end{equation}

The key problem with Q-learning is stability, coined the "deadly
triad".

1. Off-policy learning
2. flexible function approximation
3. Bootstrapping

In the presence of all three, learning is unstable. DQN is the first
algorithm that stabilized deep Q-learning ([[file:mnih2013_atari_deeprl.org][§mnih2013_atari_deeprl]]). 

* Related

- [[file:td_learning.org][§td_learning]]
  
bibliography:biblio/rl.bib
