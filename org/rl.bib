@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@article{mnih16_async_method_deep_reinf_learn,
  author =       {Mnih, Volodymyr and Badia, Adri{\`a}
                  Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex
                  and Lillicrap, Timothy P. and Harley, Tim and
                  Silver, David and Kavukcuoglu, Koray},
  title =        {Asynchronous Methods for Deep Reinforcement
                  Learning},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1602.01783v2},
  abstract =     {We propose a conceptually simple and lightweight
                  framework for deep reinforcement learning that uses
                  asynchronous gradient descent for optimization of
                  deep neural network controllers. We present
                  asynchronous variants of four standard reinforcement
                  learning algorithms and show that parallel
                  actor-learners have a stabilizing effect on training
                  allowing all four methods to successfully train
                  neural network controllers. The best performing
                  method, an asynchronous variant of actor-critic,
                  surpasses the current state-of-the-art on the Atari
                  domain while training for half the time on a single
                  multi-core CPU instead of a GPU. Furthermore, we
                  show that asynchronous actor-critic succeeds on a
                  wide variety of continuous motor control problems as
                  well as on a new task of navigating random 3D mazes
                  using a visual input.},
  archivePrefix ={arXiv},
  eprint =       {1602.01783},
  primaryClass = {cs.LG},
}

@article{gu16_q_prop,
  author =       {Gu, Shixiang and Lillicrap, Timothy and Ghahramani,
                  Zoubin and Turner, Richard E. and Levine, Sergey},
  title =        {Q-Prop: Sample-Efficient Policy Gradient With an
                  Off-Policy Critic},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1611.02247v3},
  abstract =     {Model-free deep reinforcement learning (RL) methods
                  have been successful in a wide variety of simulated
                  domains. However, a major obstacle facing deep RL in
                  the real world is their high sample complexity.
                  Batch policy gradient methods offer stable learning,
                  but at the cost of high variance, which often
                  requires large batches. TD-style methods, such as
                  off-policy actor-critic and Q-learning, are more
                  sample-efficient but biased, and often require
                  costly hyperparameter sweeps to stabilize. In this
                  work, we aim to develop methods that combine the
                  stability of policy gradients with the efficiency of
                  off-policy RL. We present Q-Prop, a policy gradient
                  method that uses a Taylor expansion of the
                  off-policy critic as a control variate. Q-Prop is
                  both sample efficient and stable, and effectively
                  combines the benefits of on-policy and off-policy
                  methods. We analyze the connection between Q-Prop
                  and existing model-free algorithms, and use control
                  variate theory to derive two variants of Q-Prop with
                  conservative and aggressive adaptation. We show that
                  conservative Q-Prop provides substantial gains in
                  sample efficiency over trust region policy
                  optimization (TRPO) with generalized advantage
                  estimation (GAE), and improves stability over deep
                  deterministic policy gradient (DDPG), the
                  state-of-the-art on-policy and off-policy methods,
                  on OpenAI Gym's MuJoCo continuous control
                  environments.},
  archivePrefix ={arXiv},
  eprint =       {1611.02247},
  primaryClass = {cs.LG},
}

@Article{Mnih_2015,
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver,
                  David and Rusu, Andrei A. and Veness, Joel and
                  Bellemare, Marc G. and Graves, Alex and Riedmiller,
                  Martin and Fidjeland, Andreas K. and Ostrovski,
                  Georg and et al.},
  title        = {Human-level control through deep reinforcement
                  learning},
  year         = 2015,
  volume       = 518,
  number       = 7540,
  month        = {Feb},
  pages        = {529â€“533},
  issn         = {1476-4687},
  doi          = {10.1038/nature14236},
  url          = {http://dx.doi.org/10.1038/nature14236},
  journal      = {Nature},
  publisher    = {Springer Nature}
}

@article{andrychowicz17_hinds_exper_replay,
  author =       {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex
                  and Schneider, Jonas and Fong, Rachel and Welinder,
                  Peter and McGrew, Bob and Tobin, Josh and Abbeel,
                  Pieter and Zaremba, Wojciech},
  title =        {Hindsight Experience Replay},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1707.01495v3},
  abstract =     {Dealing with sparse rewards is one of the biggest
                  challenges in Reinforcement Learning (RL). We
                  present a novel technique called Hindsight
                  Experience Replay which allows sample-efficient
                  learning from rewards which are sparse and binary
                  and therefore avoid the need for complicated reward
                  engineering. It can be combined with an arbitrary
                  off-policy RL algorithm and may be seen as a form of
                  implicit curriculum. We demonstrate our approach on
                  the task of manipulating objects with a robotic arm.
                  In particular, we run experiments on three different
                  tasks: pushing, sliding, and pick-and-place, in each
                  case using only binary rewards indicating whether or
                  not the task is completed. Our ablation studies show
                  that Hindsight Experience Replay is a crucial
                  ingredient which makes training possible in these
                  challenging environments. We show that our policies
                  trained on a physics simulation can be deployed on a
                  physical robot and successfully complete the task.},
  archivePrefix ={arXiv},
  eprint =       {1707.01495},
  primaryClass = {cs.LG},
}

@article{schaul15_prior_exper_replay,
  author =       {Schaul, Tom and Quan, John and Antonoglou, Ioannis
                  and Silver, David},
  title =        {Prioritized Experience Replay},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1511.05952v4},
  abstract =     {Experience replay lets online reinforcement learning
                  agents remember and reuse experiences from the past.
                  In prior work, experience transitions were uniformly
                  sampled from a replay memory. However, this approach
                  simply replays transitions at the same frequency
                  that they were originally experienced, regardless of
                  their significance. In this paper we develop a
                  framework for prioritizing experience, so as to
                  replay important transitions more frequently, and
                  therefore learn more efficiently. We use prioritized
                  experience replay in Deep Q-Networks (DQN), a
                  reinforcement learning algorithm that achieved
                  human-level performance across many Atari games. DQN
                  with prioritized experience replay achieves a new
                  state-of-the-art, outperforming DQN with uniform
                  replay on 41 out of 49 games.},
  archivePrefix ={arXiv},
  eprint =       {1511.05952},
  primaryClass = {cs.LG},
}