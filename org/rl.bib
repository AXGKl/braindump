@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@article{mnih16_async_method_deep_reinf_learn,
  author =       {Mnih, Volodymyr and Badia, Adri{\`a}
                  Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex
                  and Lillicrap, Timothy P. and Harley, Tim and
                  Silver, David and Kavukcuoglu, Koray},
  title =        {Asynchronous Methods for Deep Reinforcement
                  Learning},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1602.01783v2},
  abstract =     {We propose a conceptually simple and lightweight
                  framework for deep reinforcement learning that uses
                  asynchronous gradient descent for optimization of
                  deep neural network controllers. We present
                  asynchronous variants of four standard reinforcement
                  learning algorithms and show that parallel
                  actor-learners have a stabilizing effect on training
                  allowing all four methods to successfully train
                  neural network controllers. The best performing
                  method, an asynchronous variant of actor-critic,
                  surpasses the current state-of-the-art on the Atari
                  domain while training for half the time on a single
                  multi-core CPU instead of a GPU. Furthermore, we
                  show that asynchronous actor-critic succeeds on a
                  wide variety of continuous motor control problems as
                  well as on a new task of navigating random 3D mazes
                  using a visual input.},
  archivePrefix ={arXiv},
  eprint =       {1602.01783},
  primaryClass = {cs.LG},
}

@article{gu16_q_prop,
  author =       {Gu, Shixiang and Lillicrap, Timothy and Ghahramani,
                  Zoubin and Turner, Richard E. and Levine, Sergey},
  title =        {Q-Prop: Sample-Efficient Policy Gradient With an
                  Off-Policy Critic},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1611.02247v3},
  abstract =     {Model-free deep reinforcement learning (RL) methods
                  have been successful in a wide variety of simulated
                  domains. However, a major obstacle facing deep RL in
                  the real world is their high sample complexity.
                  Batch policy gradient methods offer stable learning,
                  but at the cost of high variance, which often
                  requires large batches. TD-style methods, such as
                  off-policy actor-critic and Q-learning, are more
                  sample-efficient but biased, and often require
                  costly hyperparameter sweeps to stabilize. In this
                  work, we aim to develop methods that combine the
                  stability of policy gradients with the efficiency of
                  off-policy RL. We present Q-Prop, a policy gradient
                  method that uses a Taylor expansion of the
                  off-policy critic as a control variate. Q-Prop is
                  both sample efficient and stable, and effectively
                  combines the benefits of on-policy and off-policy
                  methods. We analyze the connection between Q-Prop
                  and existing model-free algorithms, and use control
                  variate theory to derive two variants of Q-Prop with
                  conservative and aggressive adaptation. We show that
                  conservative Q-Prop provides substantial gains in
                  sample efficiency over trust region policy
                  optimization (TRPO) with generalized advantage
                  estimation (GAE), and improves stability over deep
                  deterministic policy gradient (DDPG), the
                  state-of-the-art on-policy and off-policy methods,
                  on OpenAI Gym's MuJoCo continuous control
                  environments.},
  archivePrefix ={arXiv},
  eprint =       {1611.02247},
  primaryClass = {cs.LG},
}

@Article{Mnih_2015,
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver,
                  David and Rusu, Andrei A. and Veness, Joel and
                  Bellemare, Marc G. and Graves, Alex and Riedmiller,
                  Martin and Fidjeland, Andreas K. and Ostrovski,
                  Georg and et al.},
  title        = {Human-level control through deep reinforcement
                  learning},
  year         = 2015,
  volume       = 518,
  number       = 7540,
  month        = {Feb},
  pages        = {529â€“533},
  issn         = {1476-4687},
  doi          = {10.1038/nature14236},
  url          = {http://dx.doi.org/10.1038/nature14236},
  journal      = {Nature},
  publisher    = {Springer Nature}
}

@article{andrychowicz17_hinds_exper_replay,
  author =       {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex
                  and Schneider, Jonas and Fong, Rachel and Welinder,
                  Peter and McGrew, Bob and Tobin, Josh and Abbeel,
                  Pieter and Zaremba, Wojciech},
  title =        {Hindsight Experience Replay},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1707.01495v3},
  abstract =     {Dealing with sparse rewards is one of the biggest
                  challenges in Reinforcement Learning (RL). We
                  present a novel technique called Hindsight
                  Experience Replay which allows sample-efficient
                  learning from rewards which are sparse and binary
                  and therefore avoid the need for complicated reward
                  engineering. It can be combined with an arbitrary
                  off-policy RL algorithm and may be seen as a form of
                  implicit curriculum. We demonstrate our approach on
                  the task of manipulating objects with a robotic arm.
                  In particular, we run experiments on three different
                  tasks: pushing, sliding, and pick-and-place, in each
                  case using only binary rewards indicating whether or
                  not the task is completed. Our ablation studies show
                  that Hindsight Experience Replay is a crucial
                  ingredient which makes training possible in these
                  challenging environments. We show that our policies
                  trained on a physics simulation can be deployed on a
                  physical robot and successfully complete the task.},
  archivePrefix ={arXiv},
  eprint =       {1707.01495},
  primaryClass = {cs.LG},
}

@article{schaul15_prior_exper_replay,
  author =       {Schaul, Tom and Quan, John and Antonoglou, Ioannis
                  and Silver, David},
  title =        {Prioritized Experience Replay},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1511.05952v4},
  abstract =     {Experience replay lets online reinforcement learning
                  agents remember and reuse experiences from the past.
                  In prior work, experience transitions were uniformly
                  sampled from a replay memory. However, this approach
                  simply replays transitions at the same frequency
                  that they were originally experienced, regardless of
                  their significance. In this paper we develop a
                  framework for prioritizing experience, so as to
                  replay important transitions more frequently, and
                  therefore learn more efficiently. We use prioritized
                  experience replay in Deep Q-Networks (DQN), a
                  reinforcement learning algorithm that achieved
                  human-level performance across many Atari games. DQN
                  with prioritized experience replay achieves a new
                  state-of-the-art, outperforming DQN with uniform
                  replay on 41 out of 49 games.},
  archivePrefix ={arXiv},
  eprint =       {1511.05952},
  primaryClass = {cs.LG},
}
@article{schulman15_high_dimen_contin_contr_using,
  author =       {Schulman, John and Moritz, Philipp and Levine,
                  Sergey and Jordan, Michael and Abbeel, Pieter},
  title =        {High-Dimensional Continuous Control Using
                  Generalized Advantage Estimation},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1506.02438v6},
  abstract =     {Policy gradient methods are an appealing approach in
                  reinforcement learning because they directly
                  optimize the cumulative reward and can
                  straightforwardly be used with nonlinear function
                  approximators such as neural networks. The two main
                  challenges are the large number of samples typically
                  required, and the difficulty of obtaining stable and
                  steady improvement despite the nonstationarity of
                  the incoming data. We address the first challenge by
                  using value functions to substantially reduce the
                  variance of policy gradient estimates at the cost of
                  some bias, with an exponentially-weighted estimator
                  of the advantage function that is analogous to
                  TD(lambda). We address the second challenge by using
                  trust region optimization procedure for both the
                  policy and the value function, which are represented
                  by neural networks. Our approach yields strong
                  empirical results on highly challenging 3D
                  locomotion tasks, learning running gaits for bipedal
                  and quadrupedal simulated robots, and learning a
                  policy for getting the biped to stand up from
                  starting out lying on the ground. In contrast to a
                  body of prior work that uses hand-crafted policy
                  representations, our neural network policies map
                  directly from raw kinematics to joint torques. Our
                  algorithm is fully model-free, and the amount of
                  simulated experience required for the learning tasks
                  on 3D bipeds corresponds to 1-2 weeks of real time.},
  archivePrefix ={arXiv},
  eprint =       {1506.02438},
  primaryClass = {cs.LG},
}
