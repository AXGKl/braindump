@misc{gibson18_neural_ordin_differ_equat,
  author =       {Kevin Gibson},
  howpublished =
                  {https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/},
  note =         {Online; accessed 28 February 2019},
  title =        {Neural networks as Ordinary Differential Equations},
  year =         2018,
}


@misc{colyer_tmp_neural_ode,
  author =       {Adrian Colyer},
  howpublished =
                  {https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/},
  note =         {Online; accessed 28 February 2019},
  title =        {Neural Ordinary Differential Equations},
  year =         {2019},
}

@article{chen18_neural_ordin_differ_equat,
  author =       {Chen, Ricky T. Q. and Rubanova, Yulia and
                  Bettencourt, Jesse and Duvenaud, David},
  title =        {Neural Ordinary Differential Equations},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1806.07366v4},
  abstract =     {We introduce a new family of deep neural network
                  models. Instead of specifying a discrete sequence of
                  hidden layers, we parameterize the derivative of the
                  hidden state using a neural network. The output of
                  the network is computed using a black-box
                  differential equation solver. These continuous-depth
                  models have constant memory cost, adapt their
                  evaluation strategy to each input, and can
                  explicitly trade numerical precision for speed. We
                  demonstrate these properties in continuous-depth
                  residual networks and continuous-time latent
                  variable models. We also construct continuous
                  normalizing flows, a generative model that can train
                  by maximum likelihood, without partitioning or
                  ordering the data dimensions. For training, we show
                  how to scalably backpropagate through any ODE
                  solver, without access to its internal operations.
                  This allows end-to-end training of ODEs within
                  larger models.},
  archivePrefix ={arXiv},
  eprint =       {1806.07366},
  primaryClass = {cs.LG},
}

@article{he15_deep_resid_learn_image_recog,
  author =       {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and
                  Sun, Jian},
  title =        {Deep Residual Learning for Image Recognition},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1512.03385v1},
  abstract =     {Deeper neural networks are more difficult to train.
                  We present a residual learning framework to ease the
                  training of networks that are substantially deeper
                  than those used previously. We explicitly
                  reformulate the layers as learning residual
                  functions with reference to the layer inputs,
                  instead of learning unreferenced functions. We
                  provide comprehensive empirical evidence showing
                  that these residual networks are easier to optimize,
                  and can gain accuracy from considerably increased
                  depth. On the ImageNet dataset we evaluate residual
                  nets with a depth of up to 152 layers---8x deeper
                  than VGG nets but still having lower complexity. An
                  ensemble of these residual nets achieves 3.57 \%
                  error on the ImageNet test set. This result won the
                  1st place on the ILSVRC 2015 classification task. We
                  also present analysis on CIFAR-10 with 100 and 1000
                  layers. The depth of representations is of central
                  importance for many visual recognition tasks. Solely
                  due to our extremely deep representations, we obtain
                  a 28 \% relative improvement on the COCO object
                  detection dataset. Deep residual nets are
                  foundations of our submissions to ILSVRC \& COCO
                  2015 competitions, where we also won the 1st places
                  on the tasks of ImageNet detection, ImageNet
                  localization, COCO detection, and COCO
                  segmentation.},
  archivePrefix ={arXiv},
  eprint =       {1512.03385},
  primaryClass = {cs.CV},
}

@article{chen18_neural_ordin_differ_equat,
  author =       {Chen, Ricky T. Q. and Rubanova, Yulia and
                  Bettencourt, Jesse and Duvenaud, David},
  title =        {Neural Ordinary Differential Equations},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1806.07366v4},
  abstract =     {We introduce a new family of deep neural network
                  models. Instead of specifying a discrete sequence of
                  hidden layers, we parameterize the derivative of the
                  hidden state using a neural network. The output of
                  the network is computed using a black-box
                  differential equation solver. These continuous-depth
                  models have constant memory cost, adapt their
                  evaluation strategy to each input, and can
                  explicitly trade numerical precision for speed. We
                  demonstrate these properties in continuous-depth
                  residual networks and continuous-time latent
                  variable models. We also construct continuous
                  normalizing flows, a generative model that can train
                  by maximum likelihood, without partitioning or
                  ordering the data dimensions. For training, we show
                  how to scalably backpropagate through any ODE
                  solver, without access to its internal operations.
                  This allows end-to-end training of ODEs within
                  larger models.},
  archivePrefix ={arXiv},
  eprint =       {1806.07366},
  primaryClass = {cs.LG},
}