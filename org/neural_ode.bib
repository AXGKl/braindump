@misc{gibson18_neural_ordin_differ_equat,
  author =       {Kevin Gibson},
  howpublished =
                  {https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/},
  note =         {Online; accessed 28 February 2019},
  title =        {Neural networks as Ordinary Differential Equations},
  year =         2018,
}


@misc{colyer_tmp_neural_ode,
  author =       {Adrian Colyer},
  howpublished =
                  {https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/},
  note =         {Online; accessed 28 February 2019},
  title =        {Neural Ordinary Differential Equations},
  year =         {2019},
}

@article{chen18_neural_ordin_differ_equat,
  author =       {Chen, Ricky T. Q. and Rubanova, Yulia and
                  Bettencourt, Jesse and Duvenaud, David},
  title =        {Neural Ordinary Differential Equations},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1806.07366v4},
  abstract =     {We introduce a new family of deep neural network
                  models. Instead of specifying a discrete sequence of
                  hidden layers, we parameterize the derivative of the
                  hidden state using a neural network. The output of
                  the network is computed using a black-box
                  differential equation solver. These continuous-depth
                  models have constant memory cost, adapt their
                  evaluation strategy to each input, and can
                  explicitly trade numerical precision for speed. We
                  demonstrate these properties in continuous-depth
                  residual networks and continuous-time latent
                  variable models. We also construct continuous
                  normalizing flows, a generative model that can train
                  by maximum likelihood, without partitioning or
                  ordering the data dimensions. For training, we show
                  how to scalably backpropagate through any ODE
                  solver, without access to its internal operations.
                  This allows end-to-end training of ODEs within
                  larger models.},
  archivePrefix ={arXiv},
  eprint =       {1806.07366},
  primaryClass = {cs.LG},
}

@article{he15_deep_resid_learn_image_recog,
  author =       {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and
                  Sun, Jian},
  title =        {Deep Residual Learning for Image Recognition},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1512.03385v1},
  abstract =     {Deeper neural networks are more difficult to train.
                  We present a residual learning framework to ease the
                  training of networks that are substantially deeper
                  than those used previously. We explicitly
                  reformulate the layers as learning residual
                  functions with reference to the layer inputs,
                  instead of learning unreferenced functions. We
                  provide comprehensive empirical evidence showing
                  that these residual networks are easier to optimize,
                  and can gain accuracy from considerably increased
                  depth. On the ImageNet dataset we evaluate residual
                  nets with a depth of up to 152 layers---8x deeper
                  than VGG nets but still having lower complexity. An
                  ensemble of these residual nets achieves 3.57 \%
                  error on the ImageNet test set. This result won the
                  1st place on the ILSVRC 2015 classification task. We
                  also present analysis on CIFAR-10 with 100 and 1000
                  layers. The depth of representations is of central
                  importance for many visual recognition tasks. Solely
                  due to our extremely deep representations, we obtain
                  a 28 \% relative improvement on the COCO object
                  detection dataset. Deep residual nets are
                  foundations of our submissions to ILSVRC \& COCO
                  2015 competitions, where we also won the 1st places
                  on the tasks of ImageNet detection, ImageNet
                  localization, COCO detection, and COCO
                  segmentation.},
  archivePrefix ={arXiv},
  eprint =       {1512.03385},
  primaryClass = {cs.CV},
}

@article{chen18_neural_ordin_differ_equat,
  author =       {Chen, Ricky T. Q. and Rubanova, Yulia and
                  Bettencourt, Jesse and Duvenaud, David},
  title =        {Neural Ordinary Differential Equations},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1806.07366v4},
  abstract =     {We introduce a new family of deep neural network
                  models. Instead of specifying a discrete sequence of
                  hidden layers, we parameterize the derivative of the
                  hidden state using a neural network. The output of
                  the network is computed using a black-box
                  differential equation solver. These continuous-depth
                  models have constant memory cost, adapt their
                  evaluation strategy to each input, and can
                  explicitly trade numerical precision for speed. We
                  demonstrate these properties in continuous-depth
                  residual networks and continuous-time latent
                  variable models. We also construct continuous
                  normalizing flows, a generative model that can train
                  by maximum likelihood, without partitioning or
                  ordering the data dimensions. For training, we show
                  how to scalably backpropagate through any ODE
                  solver, without access to its internal operations.
                  This allows end-to-end training of ODEs within
                  larger models.},
  archivePrefix ={arXiv},
  eprint =       {1806.07366},
  primaryClass = {cs.LG},
}

@article{rackauckas19_diffeq,
  author =       {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and
                  Bettencourt, Jesse and White, Lyndon and Dixit,
                  Vaibhav},
  title =        {Diffeqflux.jl - a Julia Library for Neural
                  Differential Equations},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1902.02376v1},
  abstract =     {DiffEqFlux.jl is a library for fusing neural
                  networks and differential equations. In this work we
                  describe differential equations from the viewpoint
                  of data science and discuss the complementary nature
                  between machine learning models and differential
                  equations. We demonstrate the ability to incorporate
                  DifferentialEquations.jl-defined differential
                  equation problems into a Flux-defined neural
                  network, and vice versa. The advantages of being
                  able to use the entire DifferentialEquations.jl
                  suite for this purpose is demonstrated by counter
                  examples where simple integration strategies fail,
                  but the sophisticated integration strategies
                  provided by the DifferentialEquations.jl library
                  succeed. This is followed by a demonstration of
                  delay differential equations and stochastic
                  differential equations inside of neural networks. We
                  show high-level functionality for defining neural
                  ordinary differential equations (neural networks
                  embedded into the differential equation) and
                  describe the extra models in the Flux model zoo
                  which includes neural stochastic differential
                  equations. We conclude by discussing the various
                  adjoint methods used for backpropogation of the
                  differential equation solvers. DiffEqFlux.jl is an
                  important contribution to the area, as it allows the
                  full weight of the differential equation solvers
                  developed from decades of research in the scientific
                  computing field to be readily applied to the
                  challenges posed by machine learning and data
                  science.},
  archivePrefix ={arXiv},
  eprint =       {1902.02376},
  primaryClass = {cs.LG},
}

@article{lu17_beyon_finit_layer_neural_networ,
  author =       {Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and
                  Dong, Bin},
  title =        {Beyond Finite Layer Neural Networks: Bridging Deep
                  Architectures and Numerical Differential Equations},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1710.10121v2},
  abstract =     {In our work, we bridge deep neural network design
                  with numerical differential equations. We show that
                  many effective networks, such as ResNet, PolyNet,
                  FractalNet and RevNet, can be interpreted as
                  different numerical discretizations of differential
                  equations. This finding brings us a brand new
                  perspective on the design of effective deep
                  architectures. We can take advantage of the rich
                  knowledge in numerical analysis to guide us in
                  designing new and potentially more effective deep
                  networks. As an example, we propose a linear
                  multi-step architecture (LM-architecture) which is
                  inspired by the linear multi-step method solving
                  ordinary differential equations. The LM-architecture
                  is an effective structure that can be used on any
                  ResNet-like networks. In particular, we demonstrate
                  that LM-ResNet and LM-ResNeXt (i.e. the networks
                  obtained by applying the LM-architecture on ResNet
                  and ResNeXt respectively) can achieve noticeably
                  higher accuracy than ResNet and ResNeXt on both
                  CIFAR and ImageNet with comparable numbers of
                  trainable parameters. In particular, on both CIFAR
                  and ImageNet, LM-ResNet/LM-ResNeXt can significantly
                  compress ($>50$\ \%) the original networks while
                  maintaining a similar performance. This can be
                  explained mathematically using the concept of
                  modified equation from numerical analysis. Last but
                  not least, we also establish a connection between
                  stochastic control and noise injection in the
                  training process which helps to improve
                  generalization of the networks. Furthermore, by
                  relating stochastic training strategy with
                  stochastic dynamic system, we can easily apply
                  stochastic training to the networks with the
                  LM-architecture. As an example, we introduced
                  stochastic depth to LM-ResNet and achieve
                  significant improvement over the original LM-ResNet
                  on CIFAR10.},
  archivePrefix ={arXiv},
  eprint =       {1710.10121},
  primaryClass = {cs.CV},
}

@article{haber17_stabl_archit_deep_neural_networ,
  author =       {Haber, Eldad and Ruthotto, Lars},
  title =        {Stable Architectures for Deep Neural Networks},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1705.03341v3},
  abstract =     {Deep neural networks have become invaluable tools
                  for supervised machine learning, e.g.,
                  classification of text or images. While often
                  offering superior results over traditional
                  techniques and successfully expressing complicated
                  patterns in data, deep architectures are known to be
                  challenging to design and train such that they
                  generalize well to new data. Important issues with
                  deep architectures are numerical instabilities in
                  derivative-based learning algorithms commonly called
                  exploding or vanishing gradients. In this paper we
                  propose new forward propagation techniques inspired
                  by systems of Ordinary Differential Equations (ODE)
                  that overcome this challenge and lead to well-posed
                  learning problems for arbitrarily deep networks. The
                  backbone of our approach is our interpretation of
                  deep learning as a parameter estimation problem of
                  nonlinear dynamical systems. Given this formulation,
                  we analyze stability and well-posedness of deep
                  learning and use this new understanding to develop
                  new network architectures. We relate the exploding
                  and vanishing gradient phenomenon to the stability
                  of the discrete ODE and present several strategies
                  for stabilizing deep learning for very deep
                  networks. While our new architectures restrict the
                  solution space, several numerical experiments show
                  their competitiveness with state-of-the-art
                  networks.},
  archivePrefix ={arXiv},
  eprint =       {1705.03341},
  primaryClass = {cs.LG},
}