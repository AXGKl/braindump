@misc{gibson18_neural_ordin_differ_equat,
  author =       {Kevin Gibson},
  howpublished =
                  {https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/},
  note =         {Online; accessed 28 February 2019},
  title =        {Neural networks as Ordinary Differential Equations},
  year =         2018,
}


@misc{colyer_tmp_neural_ode,
  author =       {Adrian Colyer},
  howpublished =
                  {https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/},
  note =         {Online; accessed 28 February 2019},
  title =        {Neural Ordinary Differential Equations},
  year =         {2019},
}

@article{chen18_neural_ordin_differ_equat,
  author =       {Chen, Ricky T. Q. and Rubanova, Yulia and
                  Bettencourt, Jesse and Duvenaud, David},
  title =        {Neural Ordinary Differential Equations},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1806.07366v4},
  abstract =     {We introduce a new family of deep neural network
                  models. Instead of specifying a discrete sequence of
                  hidden layers, we parameterize the derivative of the
                  hidden state using a neural network. The output of
                  the network is computed using a black-box
                  differential equation solver. These continuous-depth
                  models have constant memory cost, adapt their
                  evaluation strategy to each input, and can
                  explicitly trade numerical precision for speed. We
                  demonstrate these properties in continuous-depth
                  residual networks and continuous-time latent
                  variable models. We also construct continuous
                  normalizing flows, a generative model that can train
                  by maximum likelihood, without partitioning or
                  ordering the data dimensions. For training, we show
                  how to scalably backpropagate through any ODE
                  solver, without access to its internal operations.
                  This allows end-to-end training of ODEs within
                  larger models.},
  archivePrefix ={arXiv},
  eprint =       {1806.07366},
  primaryClass = {cs.LG},
}

@article{he15_deep_resid_learn_image_recog,
  author =       {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and
                  Sun, Jian},
  title =        {Deep Residual Learning for Image Recognition},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1512.03385v1},
  abstract =     {Deeper neural networks are more difficult to train.
                  We present a residual learning framework to ease the
                  training of networks that are substantially deeper
                  than those used previously. We explicitly
                  reformulate the layers as learning residual
                  functions with reference to the layer inputs,
                  instead of learning unreferenced functions. We
                  provide comprehensive empirical evidence showing
                  that these residual networks are easier to optimize,
                  and can gain accuracy from considerably increased
                  depth. On the ImageNet dataset we evaluate residual
                  nets with a depth of up to 152 layers---8x deeper
                  than VGG nets but still having lower complexity. An
                  ensemble of these residual nets achieves 3.57 \%
                  error on the ImageNet test set. This result won the
                  1st place on the ILSVRC 2015 classification task. We
                  also present analysis on CIFAR-10 with 100 and 1000
                  layers. The depth of representations is of central
                  importance for many visual recognition tasks. Solely
                  due to our extremely deep representations, we obtain
                  a 28 \% relative improvement on the COCO object
                  detection dataset. Deep residual nets are
                  foundations of our submissions to ILSVRC \& COCO
                  2015 competitions, where we also won the 1st places
                  on the tasks of ImageNet detection, ImageNet
                  localization, COCO detection, and COCO
                  segmentation.},
  archivePrefix ={arXiv},
  eprint =       {1512.03385},
  primaryClass = {cs.CV},
}

@article{chen18_neural_ordin_differ_equat,
  author =       {Chen, Ricky T. Q. and Rubanova, Yulia and
                  Bettencourt, Jesse and Duvenaud, David},
  title =        {Neural Ordinary Differential Equations},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1806.07366v4},
  abstract =     {We introduce a new family of deep neural network
                  models. Instead of specifying a discrete sequence of
                  hidden layers, we parameterize the derivative of the
                  hidden state using a neural network. The output of
                  the network is computed using a black-box
                  differential equation solver. These continuous-depth
                  models have constant memory cost, adapt their
                  evaluation strategy to each input, and can
                  explicitly trade numerical precision for speed. We
                  demonstrate these properties in continuous-depth
                  residual networks and continuous-time latent
                  variable models. We also construct continuous
                  normalizing flows, a generative model that can train
                  by maximum likelihood, without partitioning or
                  ordering the data dimensions. For training, we show
                  how to scalably backpropagate through any ODE
                  solver, without access to its internal operations.
                  This allows end-to-end training of ODEs within
                  larger models.},
  archivePrefix ={arXiv},
  eprint =       {1806.07366},
  primaryClass = {cs.LG},
}

@article{rackauckas19_diffeq,
  author =       {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and
                  Bettencourt, Jesse and White, Lyndon and Dixit,
                  Vaibhav},
  title =        {Diffeqflux.jl - a Julia Library for Neural
                  Differential Equations},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1902.02376v1},
  abstract =     {DiffEqFlux.jl is a library for fusing neural
                  networks and differential equations. In this work we
                  describe differential equations from the viewpoint
                  of data science and discuss the complementary nature
                  between machine learning models and differential
                  equations. We demonstrate the ability to incorporate
                  DifferentialEquations.jl-defined differential
                  equation problems into a Flux-defined neural
                  network, and vice versa. The advantages of being
                  able to use the entire DifferentialEquations.jl
                  suite for this purpose is demonstrated by counter
                  examples where simple integration strategies fail,
                  but the sophisticated integration strategies
                  provided by the DifferentialEquations.jl library
                  succeed. This is followed by a demonstration of
                  delay differential equations and stochastic
                  differential equations inside of neural networks. We
                  show high-level functionality for defining neural
                  ordinary differential equations (neural networks
                  embedded into the differential equation) and
                  describe the extra models in the Flux model zoo
                  which includes neural stochastic differential
                  equations. We conclude by discussing the various
                  adjoint methods used for backpropogation of the
                  differential equation solvers. DiffEqFlux.jl is an
                  important contribution to the area, as it allows the
                  full weight of the differential equation solvers
                  developed from decades of research in the scientific
                  computing field to be readily applied to the
                  challenges posed by machine learning and data
                  science.},
  archivePrefix ={arXiv},
  eprint =       {1902.02376},
  primaryClass = {cs.LG},
}